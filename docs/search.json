[
  {
    "objectID": "HBI analysis.html",
    "href": "HBI analysis.html",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "",
    "text": "Last update: 12 April 2023\nannual report: 2021\nshare price data from: 4/20/2022\n\\(\\Large {\\color {red} {\\text {update dates and stock history before publishing}}}\\)\nUse this file. Some edits were made in the HPL folder.\nWrite this up as a case study.\nIn Feb 2023 HBI anounced end of dividends and stock fell from \\$8.78 to \\$6.20, now trades at \\$4.91.\nhttps://www.fool.com/investing/2023/02/03/hanesbrands-drops-its-dividend-as-times-get-tough/\nhttps://www.yahoo.com/now/hanesbrands-plunges-gloomy-outlook-dividend-152149039.html\nhttps://www.dividendpower.org/2023/03/10/hanesbrands-hbi-dividend-cut-to-zero/\nIs this now a value stock?\n451 shares\nweighted average cost\n\\((116*8.595+110*9.71+153*13.1199+72*14.6763)/(116+110+153+72) = 11.372856541019958\\)\nTotal cost basis\n\\((116*8.595+110*9.71+153*13.1199+72*14.6763) = 5129.158300000001\\)\nnumber of shares\n\\((116+110+153+72) = 451\\)\ncurrent value 2,214\nHold stock for now since intrinic value is above current price.\nData from May 3, 2023\n\\(\\Large {\\color {red} {\\text {Note}}}\\)\nexporting to MD for offline editing, files to bring for ASUS\n- MD file - HTML and convert to pdf - annual report - compustat report"
  },
  {
    "objectID": "HBI analysis.html#abstract",
    "href": "HBI analysis.html#abstract",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "Abstract",
    "text": "Abstract\nThis notebook presents analysis and commentary for HanesBrands (NYSE: HBI). The analysis presented is based on examination of the business fundamentals. A discount cash flow analysis is used to estimate the intrinsic value of the company. A second evaluation method based on earnings history and historical price to earnings ratio is calculated. Using some judgment calls, as explained in the analysis, an intrinsic stock value is calculated. Some shares of HBI were purchased based on dividend yield and the intrinsic stock value. As described in the analysis, HBI suspended the dividend in order to direct funds to pay down the debt. Since the company is not paying a dividend, does it make sense to hold the company as a value stock? The analysis concludes that there is some merit to think the company might be a value stock, but as a non-dividend paying stock, having HBI does not fit my investment goals of holding quality dividend paying stocks.\nAt the time of writing this report, some stock analysis were suggesting that HBI might be a value play, that is, buying this stock on the cheap and holding until the price recovers or the dividend is re-instated. See the articles here and here. My analysis shown below indicates that the even if NOP can be increased over time by 10%, the ratio of NOP to total liabilities remains above 7. (need to recalculate projected total liabilities and clean up the analysis). Historically the ratio was near 5 when HBI initiated their dividend."
  },
  {
    "objectID": "HBI analysis.html#introduction",
    "href": "HBI analysis.html#introduction",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "Introduction",
    "text": "Introduction\nShares of HBI were purchased based on dividend yield and intrinsic value of the company. The dividend yield at the time of purchase was running about 4%. I currently hold 451 shares and my purchase history is:\n\n\n\nDate\nQuantity\nPrice\n\n\n\n\n04/20/2022\n72\n\\$14.67\n\n\n05/02/2022\n153\n\\$13.11\n\n\n06/16/2022\n110\n\\$9.71\n\n\n09/01/2022\n116\n\\$8.595\n\n\n\nMy weighted average cost is \\$11.372 per share and my cost basis is \\$5,129.16. The current value of my shares are \\$2,214, a loss of about 57%. If I hold the shares until September of 2023, all the shares will be long term capital losses.\nSince HBI has canceled the dividend payments, should this stock be retained for its value or for possible restoration of dividends? The most recent financial results for the quarter ending are shown in the current news section below."
  },
  {
    "objectID": "HBI analysis.html#company-description",
    "href": "HBI analysis.html#company-description",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "Company description",
    "text": "Company description\nHanesbrands Inc. is an American multinational clothing company based in Winston-Salem, North Carolina. It employs 65,300 people internationally. On September 6, 2006, the company and several brands were spun off by the Sara Lee Corporation. Hanesbrands owns several clothing brands, including Hanes, Champion, Playtex, Bali, L’eggs, Just My Size, Barely There, Wonderbra, Maidenform, Berlei, and Bonds.\nHanesbrands Inc., a Maryland corporation, is a consumer goods company with a portfolio of leading apparel brands, including Hanes, Champion, Bonds, Maidenform, Bali, Playtex, Bras N Things, JMS/Just My Size, Gear for Sports, Wonderbra, Berlei, Comfortwash, and Alternative. The Company designs, manufactures, sources and sells a broad range of basic apparel such as T-shirts, bras, panties, shapewear, underwear, socks and activewear. The Company’s fiscal year ends on the Saturday closest to December 31. All references to “2022”, “2021” and “2020” relate to the 52-week fiscal year ended on December 31, 2022 and January 1, 2022, and the 53-week fiscal year ended on January 2, 2021, respectively. Two subsidiaries of the Company close one day after the Company’s consolidated year end. The difference in reporting of financial information for these subsidiaries did not have a material impact on the Company’s financial condition, results of operations or cash flows.\nIn late 2020, the Company undertook a comprehensive global business review focused on building consumer-centric growth. The review resulted in the Company’s Full Potential plan, which is its multi-year growth strategy that focuses on four pillars to drive growth and enhance long-term profitability and identifies the initiatives to unlock growth. The Company’s four pillars of growth are to grow the Champion brand globally, drive growth in Innerwear with brands and products that appeal to younger consumers, drive consumer-centricity by delivering innovative products and improving awareness through investments in brand marketing and digital capabilities, and streamline its global portfolio.\nIn the fourth quarter of 2020, the Company began the implementation of its Full Potential plan and as part of its strategy to streamline its portfolio, the Company determined that its personal protective equipment (“PPE”) business was no longer a growth opportunity and recorded a charge of \\$362,913 to write down its entire PPE inventory balance to its estimated net realizable value and a charge of \\$26,400 to accrue for vendor commitments for PPE materials that were paid in 2021. Additionally, the Company commenced an initiative to reduce 20% of its SKUs in inventory in order to streamline product offerings while also implementing a formal lifecycle management process. As a result, the Company recorded a charge of \\$192,704 to write down inventory to its estimated net realizable value taking into account these initiatives. These initiatives will position the Company for long-term growth by driving higher margin sales, lowering costs and improving service to customers.\nIn the first quarter of 2021, the Company announced that it reached the decision to exit its European Innerwear business as part of its strategy to streamline its portfolio under its Full Potential plan and determined that this business met held-for-sale and discontinued operations accounting criteria. Accordingly, the Company began to separately report the results of its European Innerwear business as discontinued operations in its Consolidated Statements of Income, and to present the related assets and liabilities as held for sale in the Consolidated Balance Sheets. On November 4, 2021, the Company announced that it reached an agreement to sell its European Innerwear business to an affiliate of Regent, L.P. and completed the sale on March 5, 2022. Unless otherwise noted, discussion within these notes to the consolidated financial statements relates to continuing operations. See Note “Assets and Liabilities Held for Sale” for additional information.\nIn addition, in the fourth quarter of 2021, the Company reached the decision to divest its U.S. Sheer Hosiery business, including the L’eggs brand, as part of its strategy to streamline its portfolio under its Full Potential plan and determined that this business met held-for-sale accounting criteria, The related assets and liabilities are presented as held for sale in the Consolidated Balance Sheets at December 31, 2022 and January 1, 2022. The operations of the U.S. Sheer Hosiery business are reported in “Other” for all periods presented in Note “Business Segment Information”. The Company is currently exploring potential purchasers for this business and expects to complete the sale within the next 12 months. See Note “Assets and Liabilities Held for Sale” for additional information.\nHBI operations are managed and reported in three operating segments, each of which is a reportable segment for financial reporting purposes: Innerwear, Activewear and International. These segments are organized principally by product category and geographic location. Each segment has its own management team that is responsible for the operations of the segment’s businesses, but the segments share a common supply chain and media and marketing platforms.\nThe following table summarizes HBI operating segments by product category:\n\n\n\n\n\n\n\n\nSegment\nPrimary Products\nPrimary Brands\n\n\n\n\nInnerwear\nBasics, including men’s underwear, women’s panties, children’s underwear and socks and intimate apparel, such as bras and shapewear\nHanes, Maidenform, Bali, Champion, Playtex, JMS/Just My Size, Bras N Things, Polo Ralph Lauren\n\n\nActivewear\nT-shirts, fleece, sport shirts, performance T-shirts and shorts, sports bras, thermals and teamwear\nChampion, Hanes, Gear for Sports, Comfortwash, Alternative, JMS/Just My Size, Hanes Beefy-T\n\n\nInternational\nActivewear, men’s underwear, women’s panties, children’s underwear, intimate apparel, socks and home goods\nChampion, Bonds, Sheridan, Bras N Things, Hanes, Wonderbra, Berlei, Playtex, Zorba, Sol y Oro, Rinbros, Polo Ralph Lauren\n\n\n\nInnerwear net sales decreased 11% compared to prior year primarily due to softer point-of-sale trends, impacts to replenishment orders from retailers’ decisions to reduce broader inventory positions, business disruption as a result of the ransomware attack in the second quarter of 2022 and the overlap of last year’s sales benefits from retailer restocking and government-stimulus spending partially offset by pricing actions taken and retail space gains in the first quarter of 2022.\nActivewear net sales decreased 7% compared to prior year primarily due to softer point-of-sale trends primarily related to the Champion brand, retailer inventory levels and business disruption as a result of the ransomware attack in the second quarter of 2022. The net sales decrease was partially offset by growth in the collegiate and printwear channels and pricing actions primarily taken in the third quarter of 2022.\nNet sales in the International segment decreased 7% compared to prior year due to unfavorable foreign currency exchange rates. The unfavorable impact of foreign currency exchange rates decreased net sales approximately \\$182 million in 2022. International net sales on a constant currency basis, defined as net sales excluding the impact of foreign currency, increased 1%. The impact of foreign currency exchange rates is calculated by applying prior period exchange rates to the current year financial results.\nOther net sales decreased primarily as a result of lower sales at our retail outlets during 2022 compared to prior year partially offset by increased sales from our supply chain to the European Innerwear business. HBI has continued certain sales from our supply chain to this business on a transitional basis after the sale in the first quarter of 2022. These sales and the related profit are included in Other in all periods presented and have not been eliminated as intercompany transactions in consolidation for the period when this business was owned by us.\nHBI’s multi-year growth strategy (“Full Potential plan”) focuses on four pillars to drive growth and enhance long-term profitability and identifies the initiatives to unlock growth. HBI four pillars of growth are to grow the Champion brand globally, drive growth in Innerwear with brands and products that appeal to younger consumers, build e-commerce excellence across channels and streamline our global portfolio. In order to deliver this growth and create a more efficient and productive business model, HBI has launched a multi-year cost savings program intended to self-fund the investments necessary to achieve the Full Potential plan’s objectives. We remain confident that our strong brand portfolio, world-class supply chain and diverse category and geographic footprint will help us unlock our full potential, deliver long-term growth and create stockholder value.\nIncluded in restructuring and other action-related charges within operating profit in 2022 and 2021 were \\$60 million and\\$132 million, respectively, of charges related to the implementation of our Full Potential plan. Full Potential plan charges in 2022 included charges related to supply chain segmentation of \\$18 million to position our manufacturing network to align with revenue growth opportunities of our Full Potential plan demand trends, \\$10 million related to corporate headcount reductions and a non-cash gain of\\$4 million to adjust the valuation allowance related to the U.S. Sheer Hosiery business resulting from a decrease in carrying value due to changes in working capital. Full Potential plan charges in 2021 included a charge of \\$16 million for an action to resize our U.S. corporate office workforce through a voluntary retirement program and impairment charges of \\$7 million related to the full impairment of an indefinite-lived trademark related to a specific brand within the European Innerwear business that was excluded from the disposal group as it was not marketed for sale.\nThe Board of Directors has recently eliminated its prior dividend policy pursuant to which HBI has historically paid a cash dividend on our common stock on a quarterly basis in order to direct free cash flow toward reducing our debt. The declaration and payment of any dividend in the future will be subject to the approval of the Board of Directors and our dividend may thereafter be discontinued or reduced at any time. The Board of Directors regularly evaluates our capital allocation strategy and dividend policy, and any future determination to continue to pay dividends, and the amount of such dividends, will be at the discretion of the Board of Directors. The ability to pay cash dividends is also limited by restrictions or limitations on our ability to obtain sufficient funds through dividends from subsidiaries, as well as by contractual restrictions, including the requirements of the agreements governing our indebtedness. There can be no assurance that HBI will declare cash dividends in the future in any particular amounts, or at all.\nOLD }——————- HanesBrands (NYSE: HBI) makes everyday apparel that is known and loved by consumers around the world for comfort, quality and value. Among the company’s iconic brands are Hanes, the leading basic apparel brand in the United States; Champion, an innovator at the intersection of lifestyle and athletic apparel; and Bonds, which is setting new standards for design and sustainability.\nHBI employs 51,000 associates in 32 countries and has built a strong reputation for workplace quality and ethical business practices. In May 2021, HBI launched its Full Potential plan – the company’s roadmap to drive improved revenue and profits during the next three years and beyond.\nUnlike most apparel companies, more than 70% of the apparel we sell is manufactured in our own facilities or those of dedicated contractors. Owning the majority of our supply chain not only impacts cost, scale and flexibility, but also the ability to adhere to best-in-class workplace and sustainability practices.\nIn 2021, HBI was one of two apparel manufacturers named one of the World’s Most Ethical Companies by Ethisphere and garnered a spot on Barron’s 100 Most Sustainable Companies for the third consecutive year. This follows the December 2020 announcement that HBI earned “A List” recognition for leadership in corporate sustainability in the CDP 2020 Climate Change Report.\nWe are committed to making the world a more comfortable, livable and inclusive place. We have established new, wide-ranging 2030 global sustainability goals and launched a new sustainability website, www.HBISustains.com, designed to increase our transparency on key metrics. We approach sustainability from a broad, holistic perspective and focus our efforts in areas addressed by the United Nations’ Sustainable Development Goals under three pillars: People, Planet and Product.\nhttps://ir.hanesbrands.com/\nhttps://finance.yahoo.com/quote/HBI\nPrevious Close  4.8800\nOpen    4.8000\nBid 4.4200 x 2900\nAsk 4.4000 x 29200\nDay's Range 4.3200 - 4.8500\n52 Week Range   4.3200 - 13.2800\nVolume  15,869,504\nAvg. Volume 12,093,670\nMarket Cap  1.533B\nBeta (5Y Monthly)   N/A\nPE Ratio (TTM)  N/A\nEPS (TTM)   -0.3500\nEarnings Date   May 03, 2023\nForward Dividend & Yield    N/A (N/A)\nEx-Dividend Date    Nov 21, 2022\n1y Target Est   4.64\n\nSector(s): Consumer Cyclical\nIndustry: Apparel Manufacturing\nFull Time Employees: 50,000\nHanesbrands Inc., a consumer goods company, designs, manufactures, sources, and sells a range of basic apparel for men, women, and children. The company operates through three segments: Innerwear, Activewear, and International.\nwww.hanes.com/corporate"
  },
  {
    "objectID": "HBI analysis.html#bottom-line-up-front",
    "href": "HBI analysis.html#bottom-line-up-front",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "Bottom line up front",
    "text": "Bottom line up front\nHBI fails some of the decision model checks. Revisit to see if buy or sell in June. Follow the link to the Conclusion."
  },
  {
    "objectID": "HBI analysis.html#revision-history",
    "href": "HBI analysis.html#revision-history",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "Revision history",
    "text": "Revision history\n\n1/10/2022: Copied from VZ notebook and reorganized\nFeb 2022: updated quick look, reorganized flow of calculations, corrected usage of financial rates, organized end sections\n23 Mar 2022: Cleaning up financial data spreadsheet. Removed NAIC tab. Removed duplicate reveneu data.\n27 Mar 2022: MFG template copied from BMY\n27 Jun 2023:\n\nUpdates to narrative\nupdated Market Cap and total value of common equity plot to look at last full year and current price history\nremoved dividend payout analysis\nIn the Financial ratios section, added return on capital to plot\nIn the NAIC section, added profit margin to plot\nAdded new section called Earnings yield\nRemoved Percent earned on equity since it was another way of saying RoE\nAvg closing price calculated"
  },
  {
    "objectID": "HBI analysis.html#analysis",
    "href": "HBI analysis.html#analysis",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "Analysis",
    "text": "Analysis\nThe following sections of this notebook contain the financial analysis for the company.\nContents \n\nStock screener results\n\nLoad financial spreadsheet\n\nDiscounted cash flow analysis, baseline\n\nDCF Scenarios\n\nNACI stock selection guide analysis\n\nFuture stock price\n\nDividend payout\n\nManagement performance\n\nDecision model\n\nConclusion\n\nNotes\n\nReferences"
  },
  {
    "objectID": "HBI analysis.html#stock-screener-results",
    "href": "HBI analysis.html#stock-screener-results",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "1) Stock screener results ",
    "text": "1) Stock screener results \nThis company was selected from the Fidelity stock screener results. The search results are based on Dividend yield (high and very high, 2.83% and higher), Volume 90 average (high and very high. 535k and higher) and Revenue Growth 5 years (0 or higher).\nCurrent news\nA review of receint SEC filings and financial news sites yahoo and google showed the following:\nOn May 24, 2022, the Company identified that it had become subject to a ransomware attack and activated its incident response and business continuity plans designed to contain the incident. As part of the Company’s forensic investigation and assessment of the impact, the Company determined that certain of its information technology systems were affected by the ransomware attack.\nUpon discovering the incident, the Company took a series of measures to further safeguard the integrity of its information technology systems, including working with cybersecurity experts to contain the incident and implementing business continuity plans to restore and support continued operations. These measures also included resecuring data, remediation of the malware across infected machines, rebuilding critical systems, global password reset and enhanced security monitoring. The Company notified appropriate law enforcement authorities as well as certain data protection regulators, and in addition to the Company’s public announcements of the incident, the Company provided breach notifications and regulatory filings as required by applicable law starting in August 2022. At this time, the Company believes the incident has been contained, the Company has restored its critical information technology systems, and manufacturing, retail and other internal operations continue. There is no ongoing operational impact on the Company’s ability to provide its products and services. The Company maintains insurance, including coverage for cyber-attacks, subject to certain deductibles and policy limitations, in an amount that the Company believes appropriate.\nThe Company is named in two pending lawsuits in connection with its previously disclosed ransomware incident. On October 7, 2022, a putative class action was filed against “Hanes Brands Inc.” alleging, among other things, negligence, negligence per se, breach of implied contract, unjust enrichment, breach of implied covenant of good faith and fair dealing, unfair business practices under the California Business and Professions Code, and violations of the California Confidentiality of Medical Information Act in connection with the ransomware incident. The litigation is entitled, Roman v. Hanes Brands Inc., and is pending in the United States District Court for the Central District of California. Plaintiff Roman also subsequently filed a second putative class action with regard to the ransomware incident in the United States District Court for the Middle District of North Carolina on January 16, 2023, entitled Roman v. Hanesbrands Inc., which was voluntarily dismissed without prejudice on January 20, 2023. On October 13, 2022, another putative class action was filed against HanesBrands Inc. alleging, among other things, negligence, negligence per se, breach of implied contract, invasion of privacy, and unjust enrichment in connection with the ransomware incident. The litigation is entitled, Toussaint v. HanesBrands Inc. and is pending in the United States District Court for the Middle District of North Carolina. The pending lawsuits seek, among other things, monetary and injunctive relief. The Company is vigorously defending these matters and believes the cases are without merit. The Company does not expect any of these claims, individually or in the aggregate, to have a material adverse effect on its consolidated financial position or results of operations. However, at this early stage in the proceedings, the Company is not able to determine the probability of the outcome of these matters or a range of reasonably expected losses, if any. The Company maintains insurance, including coverage for cyber-attacks, subject to certain deductibles and policy limitations, in an amount that the Company believes appropriate.\nDuring the year ended December 31, 2022, the Company incurred costs of \\$15,427, net of expected insurance recoveries, related to the ransomware attack. The costs for the year ended December 31, 2022 included \\$14,168 related primarily to supply chain disruptions, which are reflected in the “Cost of sales” line of the Consolidated Statements of Income and \\$1,259, net of expected insurance recoveries, related primarily to legal, information technology and consulting fees, which are reflected in the “Selling, general and administrative expenses” line of the Consolidated Statements of Income. The Company continues to assess the security event and cannot determine, at this time, the full extent of the impact from such event on its business, results of operations or financial condition or whether such impact will ultimately have a material adverse effect.\nReview quarterly results\nSince this analysis mainly looks at the annual reports, a review of the quarterly reports and the most recent 12 months is needed to see if the recent quarterly trends match the yearly trends. - yahoo finance - The Compustat Company Research from Fidelity - SEC filings from Hanesbrands Inc. Investor Relations\n\n\n\n\n\n\n\n\nCondensed Consolidated Statements of Income\n04/01/23\n04/02/22\n\n\n\n\nNet sales\n\\$1,389,410\n\\$1,576,156\n\n\nCost of sales\n\\$939,717\n\\$991,978\n\n\nGross profit\n\\$449,693\n\\$584,178\n\n\nSelling, general and administrative expenses\n\\$392,374\n\\$413,666\n\n\nOperating profit\n\\$57,319\n\\$170,512\n\n\nOther expenses\n\\$14,771\n\\$987\n\n\nInterest expense, net\n\\$58,452\n\\$31,963\n\n\nIncome (loss) from continuing operations before income tax expense\n-\\$15,904\n\\$137,562\n\n\nIncome tax expense\n\\$18,500\n\\$23,385\n\n\nIncome (loss) from continuing operations\n-\\$34,404\n\\$114,177\n\n\n(amounts in thousands)\n\n\n\n\n\nAs is shown in the table above, HBI’s sales are down and they experienced a loss from continuing operations. Since I’m going to wait until later in the year to sell my shares, I can evaluate the second and third quarter results before making a final decision.\nAverage daily volume\nAverage daily volume: 5,510,768\nDividend yield\nForward dividend yield: 5.16% - no longer valid\n\\(\\Large {\\color {red} {\\text {add review of compustat report}}}\\)\nCompustat Company Research Hanesbrands Inc NYSE: HBI Mar. 14, 2023"
  },
  {
    "objectID": "HBI analysis.html#load-financial-spreadsheet",
    "href": "HBI analysis.html#load-financial-spreadsheet",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "2) Load financial spreadsheet ",
    "text": "2) Load financial spreadsheet \nData from consolidated financial statements and annual reports was collected and entered into a spreadsheet. All numerical data is converted from thousands or millions of dollars to dollars. The stock share price history was obtained from yahoo and is included as a tab in the spreadsheet. Other tabs in the spreadsheet are various worksheets.\n\\(\\Large {\\color {red} {\\text {need to organize folders on drive}}}\\)\n/home/jeff32/HPL_legacy_files/HPL_legacy_files/Documents/Dividend Investing/DCF data/HBI_Financials.xlsx\n/home/jeff32/Documents/Dividend Investing/DCF data/HBI_Financials.xlsx\n\nticker = 'HBI' # company ticker symbol\n#os.chdir('/home/jim/Documents/Dividend Investing/DCF data/')\nos.chdir('/home/jeff32/Documents/Dividend Investing/DCF data/')\n\nfile_name = ticker+'_Financials.xlsx'\ndf_dcf_sheet = pd.read_excel(file_name,sheet_name='DCF data')\n#df_NAIC_financials = pd.read_excel(file_name,sheet_name='NAIC data')\ndf_metrics_sheet = pd.read_excel(file_name,sheet_name='metrics')\ndf_price_history = pd.read_excel(file_name,sheet_name='Historical Prices')\n\n# change the working director back to the Jupyter folder\n#os.chdir('/home/jim/Documents/JupyterLab/Discount Cash Flow Analysis/')\nos.chdir('/home/jeff32/Documents/JupyterLab/Discount Cash Flow Analysis/')\n\n\n# convert dates from string to datetime format in stock price history\nprice_date_list = []\nfor i in range(len(df_price_history)):\n    price_date_list.append(datetime.strptime(str(df_price_history['Date'][i]), '%Y-%m-%d'))\n\ndf_price_history.insert(0, 'datetime', price_date_list)  # insert a new column with datetime data\ndf_price_history.sort_values(by=['datetime'], inplace=True) # sort data frame by datetime\n\ndf_price_history.set_index('datetime',inplace=True)\n\n#df_price_history.head()\n\n\n2.1) Format data frame \nGenerate a new data frame that holds the financial data needed for the DCF model. Data from financial statements is copied into a spreadsheet which contains the data used in the analysis. The data in the DCF_data tab is in a consistent format for ease of use by this notebook. Standard names are used for the rows and columns.\n\n#column names: fiscal years \nfy_data = df_dcf_sheet.columns[1:].values.astype('datetime64[Y]')-1970\n#line 0: Total revenue  \nrevenue_data = df_dcf_sheet.iloc[0].to_numpy()[1:].astype('float')\n#line 1: Cost of goods sold\nCost_of_goods_sold_data = df_dcf_sheet.iloc[1].to_numpy()[1:].astype('float')\n#line 2: General and administrative\nGeneral_and_administrative_data = df_dcf_sheet.iloc[2].to_numpy()[1:].astype('float')\n#line 3: Research and development\nResearch_and_development_data = df_dcf_sheet.iloc[3].to_numpy()[1:].astype('float')\n#line 4: Depreciation and amortization\nDepreciation_and_amortization_data = df_dcf_sheet.iloc[4].to_numpy()[1:].astype('float')\n#line 5: Investment\nInvestment_data = df_dcf_sheet.iloc[5].to_numpy()[1:].astype('float')\n# line 6: Income before income taxes\nIncome_before_income_taxes_data = df_dcf_sheet.iloc[6].to_numpy()[1:].astype('float')\n# line 7: Income tax\nIncome_tax_data = df_dcf_sheet.iloc[7].to_numpy()[1:].astype('float')\n# line 8: Accounts receivable\nAccounts_receivable_data = df_dcf_sheet.iloc[8].to_numpy()[1:].astype('float')\n# line 9: Inventories\nInventories_data = df_dcf_sheet.iloc[9].to_numpy()[1:].astype('float')\n# line 10: Accounts payable\nAccounts_payable_data = df_dcf_sheet.iloc[10].to_numpy()[1:].astype('float')\n# line 11: Current assets\nCurrent_assets_data = df_dcf_sheet.iloc[11].to_numpy()[1:].astype('float')\n# line 12: Current liabilities\nCurrent_liabilities_data = df_dcf_sheet.iloc[12].to_numpy()[1:].astype('float')\n# line 13: Long term debt\nLong_term_debt_data = df_dcf_sheet.iloc[13].to_numpy()[1:].astype('float')\n# line 14: Shares outstanding\nShares_outstanding_data = df_dcf_sheet.iloc[14].to_numpy()[1:].astype('float')\n\n\n# make a new data frame to store selected financial data\ndf_dcf_data = pd.DataFrame(data={\n    'FY':fy_data[::-1],\n    'revenue':revenue_data[::-1],\n    'cost_of_goods_sold':Cost_of_goods_sold_data[::-1],\n    'general_and_administrative':General_and_administrative_data[::-1],\n    'research_and_development':Research_and_development_data[::-1],\n    'depreciation':Depreciation_and_amortization_data[::-1],\n    'investment':Investment_data[::-1],\n    'income_before_income_taxes':Income_before_income_taxes_data[::-1],\n    'income_tax':Income_tax_data[::-1],\n    'accounts_receivable':Accounts_receivable_data[::-1],\n    'inventories':Inventories_data[::-1],\n    'accounts_payable':Accounts_payable_data[::-1], \n    'current_assets':Current_assets_data[::-1],\n    'current_liabilities':Current_liabilities_data[::-1],\n    'long_term_debt':Long_term_debt_data[::-1],\n    'shares_outstanding':Shares_outstanding_data[::-1]\n    })\n\n#df_dcf_data"
  },
  {
    "objectID": "HBI analysis.html#discounted-cash-flow-analysis-baseline",
    "href": "HBI analysis.html#discounted-cash-flow-analysis-baseline",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "3) Discounted cash flow analysis, baseline ",
    "text": "3) Discounted cash flow analysis, baseline \nDiscounted cash flow (DCF) is a valuation method used to estimate the value of an investment based on its expected future cash flows. DCF analysis attempts to figure out the value of an investment today, based on projections of how much money it will generate in the future. In finance, discounted cash flow (DCF) analysis is a method of valuing a security, project, company, or asset using the concepts of the time value of money. The DCF method used in this notebook follows [1].\nThe value of any financial investment equals the present value of the expected future cash flows, discounted for risk and timing of these cash flows. The DCF method to value stocks is a four step process.\n1. Develop a set of future free cash flows for the corporation based on revenue growth, net operating profit margin, income tax rate and fix and working capital requirements. 2. Estimate the discount rate for the cash flows based on expected timing and risk. 3. Discount the cash flows and total them to calculate the value for the corporation as a whole. 4. Subtract the debt, preferred stock value and other claims and divide by the number of shares outstanding to get the intrinsic value.\nSections - Revenue growth rate\n- Net operating profit margin\n- Tax rate\n- Depreciation Rate\n- Investment Rate\n- Working Capital Rate\n- Current Assets\n- Current Liabilities\n- Value of Debt Outstanding\n- Current stock price\n- Shares outstanding\n- 10 year treasury bond yield\n- Bond yield spread to treasury\n- Preferred stock yield\n- Equity risk premium\n- Company specific beta\n- DCF model inputs\n- Future cash flows\n\nFuture forecast based on historical data\nThe DCF model uses historical financial data to estimate future cash flows. However, future changes are largely unpredictable, so we assume that the past record can be used as a rough guide to the future. The more questionable this assumption is, the less valuable is the analysis. So the DCF model is more useful when applied to stable well established companies, since companies with stable earnings are easier to forecast.\n\n\nRevenue growth rate \nThe revenue growth rate (also sometimes called net sales) of the corporation plus any other revenues associated with the main operations of the business. It does not include dividends, interest income or non-operating income. Historic revenue data is obtained from consolidated income statements. The year over year change in revenue is calculated and converted to a percent, then an average revenue growth rate is calculated.\nAdjustments for Hanesbrands Inc.\nNo adjustments for this company.\n\n# calculate the percent change in revenue\npcr = np.zeros(len(df_dcf_data['revenue'].to_numpy())) # percent change in revenue\nfor i in range(len(df_dcf_data['revenue'].to_numpy()[0:-1])):\n    pcr[i+1] = ((df_dcf_data['revenue'].to_numpy()[i+1] - df_dcf_data['revenue'].to_numpy()[i])/\n                df_dcf_data['revenue'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Revenue, $B')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['revenue']/1e9, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcr,'+-g')\n    \nax2.set_ylabel('% Change in revenue',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((-10,15))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Revenue')\nplt.show()\n\n\n\n\nObservation:\nNet sales decreased 8% during 2022 primarily due to the following: - Softer point-of-sale trends and higher retailer inventory levels as a result of the macroeconomic pressures; - The impact of the ransomware attack to the business; - Global supply chain disruptions resulting in product delays; - Ongoing COVID-related pressures on consumer traffic in certain markets in Asia; and - The unfavorable impact from foreign currency exchange rates in our International business of approximately \\$182 million. - Partially offset by: Pricing actions taken throughout 2022.\n\n\n\nNet Sales\nfor the year ending 12/31/2022\n\n\n\n\nInnerwear\n\\$2,429,966\n\n\nActivewear\n\\$1,555,062\n\n\nInternational\n\\$1,914,268\n\n\nOther\n\\$334,354\n\n\nTotal\n\\$6,233,650\n\n\n(amounts in thousands)\n\n\n\n\nHBI products are primarily distributed through HBI wholesale customers’ stores and websites, as well as through our own stores and websites. In 2022, approximately 69% of HBI total net sales were in the United States and approximately 31% were outside the United States. HBI largest customer is Walmart Inc. (“Walmart”), accounting for 16% of total net sales in 2022.\nSales to mass merchants in the United States accounted for approximately 19% of HBI total net sales in 2022 and included all of our product categories under our Hanes, Playtex, Maidenform and JMS/Just My Size brands, as well as licensed logo apparel. Mass merchants feature high-volume, low-cost sales of basic apparel items along with a diverse variety of consumer goods products, such as grocery and drug products and other hard lines. HBI largest mass merchant customer is Walmart.\nA significant percentage of HBI total revenues (approximately 31% in 2022) is derived from markets outside the United States. HBI sells a majority of products in transactions denominated in U.S. dollars; however, HBI purchases many of their raw materials, pay a portion of wages and make other payments to participants in the supply chain in foreign currencies. As a result, when the U.S. dollar weakens against any of these currencies, HBI cost of sales could increase substantially.\nOutside the United States, HBI may pay for materials or finished products in U.S. dollars, and in some cases a strengthening of the U.S. dollar could effectively increase HBI costs where they use foreign currency to purchase the U.S. dollars they need to make such payments.\nOutlook for 2023 HBI 2023 guidance as follows: - Net sales of approximately \\$6.05 billion to \\$6.20 billion, net of approximately \\$42 million of unfavorable foreign exchange impact;\n- Operating profit of approximately \\$446 million to \\$496 million, net of approximately \\$6 million of unfavorable foreign exchange impact;\n- Restructuring and other action-related charges totaling \\$60 million including Full Potential plan-related charges of approximately \\$54 million included in operating profit and refinancing charges of \\$6 million included in other expenses;\n- Interest expense and other expenses of approximately \\$306 million combined;\n- Tax expense from continuing operations of approximately \\$90 million to \\$100 million;\n- Diluted earnings per share from continuing operations of approximately \\$0.14 to \\$0.25;\n- Cash flow from operating activities of approximately \\$500 million; and\n- Capital investments of approximately \\$150 million, including capital expenditures of \\$70 million within investing cash flow activities and cloud computing assets of \\$80 million within operating cash flow activities.\nThird-party brick-and-mortar wholesale revenue is primarily generated by sales of the Company’s products to retailers to support their brick-and-mortar operations. Also included within third-party brick-and-mortar wholesale revenue is royalty revenue from licensing agreements. The Company earns royalties through license agreements with manufacturers of other consumer products that incorporate certain of the Company’s brands. The Company accrues revenue earned under these contracts based upon reported sales from the licensees. Additionally, third-party brick-and-mortar wholesale revenue for the year ended January 2, 2021 includes \\$518,309 of revenue from contracts with governments generated from the sale of both cloth face coverings and gowns for use to help mitigate the spread of the virus during the COVID-19 pandemic.\n\nrgr_avg = pcr[-5:].mean()/100 # last five years\nprint('average revenue growth rate: {:.2f}%'.format(rgr_avg*100))\n\naverage revenue growth rate: -0.88%\n\n\n\n\nNet operating profit margin \nNet Operating Profit should reflect the future revenue generating ability and expense requirements of the operating business that comprise the ongoing operations of the company.\n\\(\\text{NOPM} = \\frac{\\text{Revenue} - \\text{Expenses}}{\\text{Revenue}}\\)\n\\(\\text{Expenses} = \\text{Cost of Goods Sold (CGS)} + \\text{General and Administrative (G\\&A)} + \\text{Research and Development (R\\&D)}\\)\nGeneral and Administrative (G&A) is also called Sales, General and Administrative (SG&A)\nAdjustments for Hanesbrands Inc.\nNo adjustments for this company.\n\n# NOP = (Revenue - Expenses)\nnop = (df_dcf_data['revenue'].to_numpy() - \\\n    (df_dcf_data['cost_of_goods_sold'].to_numpy() + \\\n    df_dcf_data['general_and_administrative'].to_numpy() + \\\n    df_dcf_data['research_and_development'].to_numpy()) )\n\n# net operating profit margin as percent of revenue\nnopm = nop/df_dcf_data['revenue'].to_numpy()\n\n# plot as four grouped bar chart with labels on right and working capital rate on left\n# calculate position of bars\nx1_bar_position = []\nx2_bar_position = []\nx3_bar_position = []\nx4_bar_position = []\nfor i in df_dcf_data['FY']:\n    x1_bar_position.append(i-relativedelta(months=3))\n    x2_bar_position.append(i-relativedelta(months=1))\n    x3_bar_position.append(i+relativedelta(months=1))\n    x4_bar_position.append(i+relativedelta(months=3))\n    \nwidth = 40  # the width of the bars\n    \n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Net operating profit, \\\\$B')\n\nax1.bar(x1_bar_position,df_dcf_data['cost_of_goods_sold'].to_numpy()/1e9, width,label='CGS')\nax1.bar(x2_bar_position,df_dcf_data['general_and_administrative'].to_numpy()/1e9, width,label='G&A')\nax1.bar(x3_bar_position,df_dcf_data['research_and_development'].to_numpy()/1e9, width,label='R&D')\nax1.bar(x4_bar_position,nop/1e9, width,label='NOP')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,4))\nax1.legend()\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:cyan'\n\nax2.plot(df_dcf_data['FY'],nopm*100,'+-c')\n    \nax2.set_ylabel('% NOPM',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,40))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Net Operating Profit')\nplt.show()\n\n\n\n\nObservation\nThe COVID-19 pandemic has impacted HBI business operations and financial results for 2020, as described in more detail under “Consolidated Results of Operations - Year Ended January 2, 2021 (“2020”) Compared with Year Ended December 28, 2019 (“2019”)” below, due to decreased customer traffic and temporary retail store closures worldwide. While most of our retail stores were temporarily closed for varying periods of time throughout 2020, most reopened by the end of the second quarter but have experienced, and are expected to continue to experience, reductions in customer traffic, and as a result, net sales. Sales of PPE, used to help mitigate the spread of the COVID-19 virus, partially offset the negative impact of the decline in net sales and earnings due to the COVID-19 pandemic on our financial results. Our e-commerce sites have remained open in all regions and online sales have grown as consumer spending continued to shift towards online shopping experiences due to the changing retail landscape as a result of the COVID-19 pandemic.\nDuring the second quarter of 2020, HBI recorded \\$11 million of bad debt charges for customer bankruptcies and \\$20 million of charges to reserve for increased excess and obsolete inventory related primarily to canceled orders of seasonal inventory. Also during the second quarter of 2020, BHI completed a quantitative impairment analysis for certain indefinite-lived intangible assets as a result of the significant impact of the COVID-19 pandemic on their performance. Based on this analysis, HBI recorded impairment charges of \\$20 million on certain indefinite-lived trademarks and other intangible assets within the European Innerwear business. In the third quarter of 2020, HBI recorded \\$49 million of supply chain re-start up charges primarily related to incremental costs incurred, such as freight and sourcing premiums, to expedite product to meet customer demand following the extended shut-down of parts of our manufacturing network as a result of the COVID-19 pandemic. Additionally, in the fourth quarter of 2020, HBI recorded a \\$25 million charge for the impairment of goodwill related to the U.S. Hosiery reporting unit primarily as a result of the significant impact that the COVID-19 pandemic has had on this business.\nOperating profit as a percentage of net sales was 8.3% in 2022, representing a decrease from 11.7% in the prior year. Operating margin decreased as a result of lower sales volume, input cost inflation, impact from the ransomware attack, costs associated with our manufacturing time-out inventory reduction actions, deleverage from a higher proportion of transportation and distribution costs, unfavorable impact from foreign currency exchange rates and increased Full Potential plan-related investments in brand marketing and technology partially offset by pricing actions and cost reduction actions. Included in operating profit in 2022 and 2021 were charges of \\$60 million and \\$132 million, respectively, related to the implementation of the Full Potential plan.\nOperating Activities Our overall liquidity has historically been driven by our cash flow provided by operating activities, which is dependent on net income and changes in our working capital. As compared to the prior year, higher net cash used by operating activities was due to changes in working capital primarily accounts payable, accruals, inventory due to inflationary increases, softer point-of-sale trends and supply chain disruptions, and increased capital investments in our cloud computing assets partially offset by improvement in accounts receivable and lower pension plan contributions in 2022. Net cash from operating activities includes a \\$40 million contribution to our U.S. pension plan made in the first quarter of 2021.\n\nAdvertising represents one of several brand building methods used by the Company. Advertising costs, which include the development and production of advertising materials and the communication of these materials through various forms of media, are expensed in the period the advertising first takes place. The Company recognized advertising expense in the “Selling, general and administrative expenses” line in the Consolidated Statements of Income of \\$208,881, \\$208,998 and \\$113,586 in 2022, 2021 and 2020, respectively.\n\nRevenue received for shipping and handling costs is included in net sales and was \\$13,578, \\$19,461 and \\$18,943 in 2022, 2021 and 2020, respectively. Shipping costs, which comprise payments to third-party shippers, and handling costs, which consist of warehousing costs in the Company’s various distribution facilities, were \\$415,989, \\$447,131 and \\$389,252 in 2022, 2021 and 2020, respectively. The Company recognizes shipping, handling and distribution costs in the “Selling, general and administrative expenses” line in the Consolidated Statements of Income.\n\nResearch and development costs are expensed as incurred and are included in the “Selling, general and administrative expenses” line in the Consolidated Statements of Income. Research and development includes expenditures for new product, technological improvements for existing products and process innovation, which primarily consist of salaries, consulting and supplies attributable to time spent on research and development activities. Additional costs include depreciation and maintenance for research and development equipment and facilities. Research and development expense was \\$38,911, \\$39,320 and \\$37,367 in 2022, 2021 and 2020, respectively.\n\n\n#Average net operating profit margin\nnopm_avg = nopm[-5:].mean()\nprint('average net operating profit margin: {:.2f}%'.format(nopm_avg*100))\n\naverage net operating profit margin: 9.13%\n\n\n\n\nTax rate \nTax payments are taken from the consolidated income statement, provision for income taxes. The effect of taxes on profits is accounted for.\n\\(\\text{Tax rate} = \\frac{\\text{Income taxes}}{\\text{Income before income taxes}}\\)\nAdjustments for Hanesbrands Inc.\nNo adjustments for this company.\n\n# plot as Grouped bar chart with labels on right and tax rate on left\n# calculate position of bars\nx1_bar_position = []\nx2_bar_position = []\nfor i in df_dcf_data['FY']:\n    x1_bar_position.append(i-relativedelta(months=1))\n    x2_bar_position.append(i+relativedelta(months=1))\n\n# calculate tax rate\ntax_rate = df_dcf_data['income_tax']/df_dcf_data['income_before_income_taxes']\n\nwidth = 50  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, \\\\$M')\n\nrects1 = ax1.bar(x1_bar_position,df_dcf_data['income_before_income_taxes']/1e6, width,\n    label='Income before income taxes')\nrects2 = ax1.bar(x2_bar_position,df_dcf_data['income_tax']/1e6, width,\n    label='Income taxes')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((-2e3,2e3))\nax1.legend()\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],tax_rate * 100,'+-g')\n    \nax2.set_ylabel('% Tax rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,100))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Tax Rate')\nplt.show()\n\n\n\n\nObservation:\nHBI has a complex multinational tax structure with multiple types of intercompany transactions, and allocation of profits and losses among HBI and subsidiaries through intercompany transfer pricing agreements is subject to review by the Internal Revenue Service and other tax authorities.\nThe company’s tax payments and tax rate from 2016 to 2022 departed from the historically contestant average of about 12% as shown in the plot above. The 10K’s for the years 2017, 2020 and 2022 describe the changes to the tax law which caused the changes in these years. Additionally, the company has deferred taxes, tax credits and unrecognized tax benefits which complicate the analysis of the tax burden. The average tax rate calculated over the last five years is 47%, which as shown below results in a base line ISV of \\$7. Adjusting the average tax rate to 20% would change the baseline ISV to \\$10, which shows that the tax rate has a significant effect on the value of the company.\nTaxes for 2017\n\nThe 2017 enacted Tax Act significantly revised U.S. corporate income tax law by, among other things, reducing the corporate income tax rate to 21%, imposing a new minimum tax on global intangible low-taxed income (“GILTI”) and implementing a modified territorial tax system that includes a one-time transition tax on deemed repatriated earnings of foreign subsidiaries. Some of the tax provisions that become effective in the fiscal year 2018 are expected to increase HBI’s effective tax rates, such as the GILTI tax. Due to the complexities involved in accounting for the enactment of the Tax Act, SEC Staff Accounting Bulletin 118 (“SAB 118”) allows companies to record provisional estimates of the impacts of the Tax Act during a measurement period which is similar to the measurement period of up to one year from the enactment which is similar to the measurement period used when accounting for business combinations. The Company will continue to assess the impact of the recently enacted tax law on its consolidated financial statements.\n\nIncome tax expense for 2017 includes a one-time provisional charge related to U.S. tax reform of \\$457 million, primarily for the transition tax on deemed repatriated earnings of foreign subsidiaries and revaluation of our deferred tax assets and liabilities, to the lower corporate income tax rate of 21%.\n\nIncome Tax Expense – income tax expense for 2017 includes a provisional charge related to the Tax Act of \\$435 million, which includes a \\$360 million transition tax charge on deemed repatriated earnings of foreign subsidiaries, a charge of \\$72 million for the revaluation of our deferred tax assets and liabilities to the lower corporate income tax rate of 21% and a \\$3 million charge related to the deductibility of employee compensation. In addition, HBI incurred incremental tax costs of approximately \\$22 million for other impacts of tax reform and other actions taken in 2017. HBI’s effective income tax rate was 6.0% and 9.5% in 2016 and 2015, respectively. The lower effective income tax rate was primarily attributable to a lower proportion of earnings attributed to domestic subsidiaries, which are taxed at rates higher than foreign subsidiaries. Income tax expense also benefited from the adoption of accounting rules related to accounting for stock compensation, which required excess tax benefits and deficiencies to be recognized in income as they occur.\n\nTaxes for 2020 (See 10K-2020 pdf pages 103 to 107)\n\nThe Company generated income (loss) before income tax expense of \\$(183,122), \\$679,727, and \\$643,581 for the years 2020, 2019, and 2018, respectively.\n\nIn 2020, the Company continued to analyze the impacts of the Tax Act and recently issued regulations that have been published to help taxpayers interpret and apply the legislation. As a result of its analysis, Management changed its estimate of the tax liability due in connection with the one-time mandatory transition tax and recognized a \\$38,315 income tax benefit in the current period.\n\nTaxes for 2022\n\nAs of December 31, 2022, the valuation allowance for deferred tax assets was \\$626,540, made up of \\$306,743 for foreign loss carry forwards, \\$21,232 for other foreign deferred tax assets, \\$63,619 for federal and state operating loss carry forwards, and \\$234,946 for other federal and state deferred tax assets. The net change in the total valuation allowance for 2022 was \\$320,319, which relates to an increase of \\$24,172 for foreign loss carry forwards, an increase of \\$9,166 for other foreign deferred tax assets, an increase of \\$52,035 for federal and state operating loss carryforwards and an increase of \\$234,946 for other federal and state deferred tax assets.\n\nDuring 2022, the Company recorded \\$696,028 of additional foreign net operating losses due to tax-deductible impairments in Switzerland and Luxembourg. These losses are subject to recapture in Switzerland and Luxembourg such that they will be taxable in a future year, therefore deferred tax liabilities were recorded. The Company believes it is reasonably possible that the deferred tax liability in Switzerland will reverse within the next twelve months due to expected actions by the Company in 2023.\n\nIncome Tax Expense – The effective income tax rate was 137.2% and 10.3% for 2022 and 2021, respectively. The higher effective tax rate for 2022 was primarily due to non-cash discrete tax charges of \\$423 million for valuation allowances established against U.S. deferred tax assets and tax impairments in Switzerland which generated deferred tax liabilities during 2022.\nCurrent and deferred tax provisions (benefits) were:\n\n\n\nYear ended December 31, 2022\nCurrent\nDeferred\nTotal\n\n\n\n\nDomestic\n\\$15,188\n\\$201,112\n\\$216,300\n\n\nForeign\n\\$83,607\n\\$95,558\n\\$179,165\n\n\nState\n-\\$2,712\n\\$91,154\n\\$88,442\n\n\nTotal\n\\$96,083\n\\$387,824\n\\$483,907\n\n\n(amounts in thousands)\n\n\n\n\n\n\nDeferred tax assets relate to temporary differences (differences between the assets and liabilities in the consolidated financial statements and the assets and liabilities in the calculation of taxable income) including net operating losses.\nHBI continues to use a portfolio approach to release the income tax effects in accumulated other comprehensive loss related to pension and post retirement benefits. Under this approach, the income tax effects are released from accumulated other comprehensive loss based on the pre-tax adjustments to pension liabilities or assets recognized within other comprehensive income. Any tax effects remaining in accumulated other comprehensive loss are released only when the entire portfolio of the pension and post retirement benefits is liquidated, sold or extinguished.\nIn December 31, 2022, the Company had domestic tax credit carry forwards totaling \\$10,859, which expire beginning after 2022.\nIn 2022, 2021, and 2020, the Company recognized reductions of unrecognized tax benefits for tax positions of prior years of \\$311, \\$12,599, and \\$18,385, respectively. In 2022, 2021, and 2020, income tax benefits recognized in connection with the expiration of statutes of limitations were \\$7,191, \\$147, and \\$16,655, respectively. The Company believes it is reasonably possible that the amount of unrecognized tax benefits may decrease by \\$3,267 within the next 12 months due to expirations in statutes of limitations. (See pdf page 100 for table)\nAt December 31, 2022, the balance of the Company’s unrecognized tax benefits, which would, if recognized, affect the Company’s annual effective tax rate was \\$28,444. The Company’s policy is to recognize interest and/or penalties related to income tax matters in income tax expense. The Company recognized \\$81, \\\\(933 and \\\\\\)(5,206) in 2022, 2021 and 2020, respectively, for interest and penalties classified as income tax expense (benefit) in the Consolidated Statements of Income. At December 31, 2022 and January 1, 2022, the Company had a total of \\$6,303 and \\$5,865, respectively, of interest and penalties accrued related to unrecognized tax benefits.\n\n# Average tax rate\ntax_rate_avg = tax_rate[-5:].mean()\nprint('average tax rate: {:.2f}%'.format(tax_rate_avg*100))\n\naverage tax rate: 46.81%\n\n\n\n\nDepreciation Rate \nThe depreciation rate is used to project the future net investment cash flows. The effect is to reduce the amount of FCFF. Depreciation amounts are from the Consolidated Statement of Cash Flows, Depreciation and Amortization.\n\\(\\text{Depreciation Rate}=\\frac{\\text{Depreciation and Amortization}}{\\text{Revenues}}\\)\nDepreciation is the write off or expensing of a percentage of the historical cost of an asset over the asset’s useful life. Property, plant and equipment (PP&E) are long term or non current assets owned or controlled by the company and used to manufacture and or sell the company’s products. The balance sheet typically shows all categories of PP&E grouped together, net of accumulated depreciation. Depreciation represents wear and tear on an asset or the fact that an asset gets used up over time. Companies record depreciation expense in the income statement every year for all depreciable assets in service or used by the company during the year. The difference between GAAP and Tax Accounting methods is handled through deferred taxes.\nAmortization is the write off or expensing of the cost of a financial instrument or an intangible asset over the shorter of its useful life or legal life. Amortization is similar to depreciation and reflects the declining useful life and value of the intangible asset over time. Companies in research and development intensive fields typically have many patents. Such industries include high technology, pharmaceuticals and chemicals.\n\n# depreciation rate\ndepreciation_rate = df_dcf_data['depreciation'] / df_dcf_data['revenue'].to_numpy()\n\n# plot depreciation on left and rate on right\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $B')\n\nax1.bar(df_dcf_data['FY'],df_dcf_data['depreciation']/1e9, width=100,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:Blue'\n\nax2.plot(df_dcf_data['FY'],depreciation_rate*100,'+-')\n    \nax2.set_ylabel('% Depreciation rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((0,30))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Depreciation')\nplt.show()\n\n\n\n\nObservation:\nAverage depreciation rate as as percentage of revenue over the last 5 years is 1.84%. The depreciation and amortization expense for 2022 is comprised of the following:\n\n\n\nDepreciation and amortization expense:\n12/31/22\n\n\n\n\nInnerwear\n\\$26,518\n\n\nActivewear\n\\$22,420\n\n\nInternational\n\\$19,670\n\n\nOther\n\\$3,341\n\n\nCorporate\n\\$32,538\n\n\nTotal depreciation and amortization expense\n\\$106,267\n\n\n(amounts in thousands)\n\n\n\n\nProperty is stated at historical cost and depreciation expense is computed using the straight-line method over the estimated useful lives of the assets. Machinery and equipment is depreciated over periods ranging from one to 15 years and buildings and building improvements over periods of up to 40 years. A change in the depreciable life is treated as a change in accounting estimate and the accelerated depreciation is accounted for in the period of change and future periods. Additions and improvements that substantially extend the useful life of a particular asset and interest costs incurred during the construction period of major properties are capitalized. Repairs and maintenance costs are expensed as incurred. Upon sale or disposition of an asset, the cost and related accumulated depreciation are removed from the accounts.\n\n# average depreciation rate\ndepreciation_rate_avg = depreciation_rate[-5:].mean()\nprint('average depreciation rate: {:.2f}%'.format(depreciation_rate_avg*100))\n\naverage depreciation rate: 1.84%\n\n\n\n\nInvestment Rate \nTaken from Consolidated Statement of Cash Flows, Cash used for investing activities. Net investment in the dollar amount needed to support the growth of the firm. Included investments in properties, plant equipment in excess of the depreciation expenses associated with past investments. Net investment decreases the amount of money available to the stockholders. Investment in property, plant and equipment is necessary to both maintain service and sales and also to grow revenues and profits. Investment amounts should include capital expenditures and research and development.\n\\(Ir=\\frac {\\text {Capital Expenditures}}{\\text{Revenues}}\\)\nFor this company, the yearly investment amounts are taken from the Consolidated Statements of Cash Flows, Net Cash Used in Investing Activities.\nAdjustments for Hanesbrands Inc.\nNo adjustments for this comapany.\n\n# investment rate\ninvestment_rate = df_dcf_data['investment'] / df_dcf_data['revenue'].to_numpy()\n\n# plot investment on left and rate on right\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, \\\\$B')\n\nax1.bar(df_dcf_data['FY'],df_dcf_data['investment']/1e9, width=100,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:Blue'\n\nax2.plot(df_dcf_data['FY'],investment_rate*100,'+-')\n    \nax2.set_ylabel('% New Investment Rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-10,40))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('New Investment')\nplt.show()\n\n\n\n\nObservation:\nAverage investment rate as as percentage of revenue over the last 5 years is 2.52%.\nThe investing activities shown in the 2022 K-10 are shown in the table below:\n\n\n\nInvesting activities\n12/31/22\n\n\n\n\nCapital expenditures\n-\\$112,122\n\n\nPurchase of trademarks\n-\\$103,000\n\n\nProceeds from sales of assets\n\\$157\n\n\nOther\n-\\$1,463\n\n\nNet cash from investing activities\n-\\$216,428\n\n\n(amounts in thousands)\n\n\n\n\nThe following investments for the years 2013, 2016 and 2018 are described below.\n\nIn October 2013, HBI expanded their portfolio of brands through the acquisition of Maidenform, a global intimate apparel company. Maidenform is a leading seller of bras, shapewear and panties under brands such as Maidenform , Flexees, Lilyette, Self Expressions and Sweet Nothings , as well as Donna Karan and DKNY intimate apparel under license. The acquisition was an all cash transaction valued at approximately \\$581 million.\n\nHBI acquired Champion Europe on June 30, 2016. The acquisition, combined with Champion brand rights previously owned, unites the Champion brand globally and gives HBI a powerful platform for growth on every continent.\n\nHBI acquired Hanes Australasia on July 14, 2016.\n\nOn July 14, 2016, the Company acquired 100% of the outstanding shares of Pacific Brands Limited (“Hanes Australasia”) for a total purchase price of AUD \\$1,049,360 (US \\$800,871).\n\nOn February 12, 2018, HBI acquired 100% of the outstanding equity of BNT Holdco Pty Limited (“Bras N Things”) for a total purchase price of AUD \\$498,236 (US \\$391,572). The purchase price was subsequently revised to AUD \\$495,224 (US \\$389,205) due to a final working capital adjustment.\n\nIn June of 2022, HBI purchased the Champion trademark for footwear in the United States, Puerto Rico and Canada from Keds, LLC (“KEDS”) for \\$103 million.\n\n\n# average investment rate\ninvestment_rate_avg = investment_rate[-5:].mean()\nprint('average investment rate: {:.2f}%'.format(investment_rate_avg*100))\n\naverage investment rate: 2.52%\n\n\n\n\nWorking Capital Rate \nWorking capital is needed to support the corporate sales effort of any company. Often a company’s incremental change in net working capital either positive or negative is approximately proportional to its change in revenue.\n\\(\\text{Working capital} = \\text{Accounts Receivable} + \\text{Inventories} - \\text{Accounts Payable}\\)\nWorking capital is a company’s net investment in its accounts receivable and its inventories (cash outflows), minus its accounts payable (a cash inflow). Working capital and taxes are cash outflows from the corporation that are not available to pay debts and stockholders.\nAdjustments for Hanesbrands Inc.\nNo adjustments for this company.\n\n# plot as four grouped bar chart with labels on right and working capital rate on left\n# calculate position of bars\nx1_bar_position = []\nx2_bar_position = []\nx3_bar_position = []\nx4_bar_position = []\nfor i in df_dcf_data['FY']:\n    x1_bar_position.append(i-relativedelta(months=3))\n    x2_bar_position.append(i-relativedelta(months=1))\n    x3_bar_position.append(i+relativedelta(months=1))\n    x4_bar_position.append(i+relativedelta(months=3))\n\n# calculate working capital rate\nworking_capital = (df_dcf_data['accounts_receivable'] + df_dcf_data['inventories']) - \\\n    df_dcf_data['accounts_payable']\nworking_capital_rate = working_capital / df_dcf_data['revenue']\n\nwidth = 40  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, \\\\$B')\n\nrects1 = ax1.bar(x1_bar_position,df_dcf_data['accounts_receivable']/1e9, width,\\\n    label='Accounts Receivable')\nrects2 = ax1.bar(x2_bar_position,df_dcf_data['inventories']/1e9, width, label='Inventory')\n\nrects2 = ax1.bar(x3_bar_position,df_dcf_data['accounts_payable']/1e9, width, label='Accounts Payable')\nrects2 = ax1.bar(x4_bar_position,working_capital/1e9, width, label='Working Capital')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((-50,200))\nax1.legend()\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:Blue'\n\nax2.plot(df_dcf_data['FY'],working_capital_rate * 100,'+-')\n    \nax2.set_ylabel('% Working Capital Rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,50))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Working Capital')\nplt.show()\n\n\n\n\nObservation:\nAverage working capital rate as as percentage of revenue over the last 5 years is 24%.\nCustomers increasingly require some of products on an exclusive basis, which could cause an increase in the number of stock keeping units, or “SKUs,” that must carried and, consequently, increase inventory levels and working capital requirements\nHBI’s overall liquidity has historically been driven by cash flow provided by operating activities, which is dependent on net income and changes in working capital. As compared to the prior year, higher net cash used by operating activities was due to changes in working capital primarily accounts payable, accruals, inventory due to inflationary increases, softer point-of-sale trends and supply chain disruptions, and increased capital investments in cloud computing assets partially offset by improvement in accounts receivable and lower pension plan contributions in 2022.\nAccounts receivable consist primarily of amounts due from customers. HBI carries accounts receivable at their net realizable value. In determining the appropriate allowance for doubtful accounts, HBI evaluates receivables on a collection (pool) basis which are aggregated based on similar risk characteristics and consider a combination of factors, such as historical losses, the aging of trade receivables, industry trends, and our customers’ financial strength, credit standing and payment and default history. Changes in the characteristics of accounts receivables and the aforementioned factors, among others, are reviewed quarterly and may lead to adjustments in allowance for doubtful accounts. The calculation of the required allowance involves judgment by our management as to the impact of these and other factors on the ultimate realization of trade receivables. Charges to the allowance for doubtful accounts are reflected in the “Selling, general and administrative expenses” line and charges to the allowance for customer chargebacks and other customer deductions are primarily reflected as a reduction in the “Net sales” line in Consolidated Statements of Income.\nAccounts receivable are stated at their net realizable value. The allowance for doubtful accounts reflects the Company’s best estimate of probable losses inherent in the accounts receivable portfolio. Trade receivables are evaluated on a collection (pool) basis and aggregated on the basis of similar risk characteristics which are determined on the basis of historical losses, the aging of trade receivables, industry trends, and its customers’ financial strength, credit standing and payment and default history.\nThe Company has entered into agreements to sell selected trade accounts receivable to financial institutions based on programs offered by certain of the Company’s largest customers as well as programs sponsored by the Company. As a result of the strong credit worthiness of these customers, the discount taken on most of these programs is less than the marginal borrowing rate on the Company’s variable rate credit facilities. In all agreements, after the sale, the Company does not retain any beneficial interests in the receivables. The applicable financial institution services and collects the accounts receivable directly from the customer for programs offered by the Company’s customers. For programs sponsored by the Company, the Company maintains continued involvement as the servicer to collect the accounts receivable from the customer and remit payment to the financial institution. Net proceeds of these accounts receivable sale programs are recognized in the Consolidated Statements of Cash Flows as part of operating cash flows. The Company recognized total funding fees of \\$8,823, \\$3,312 and \\$4,932 in 2022, 2021 and 2020, respectively, for sales of accounts receivable to financial institutions in the “Other expenses” line in the Consolidated Statements of Income.\nSee ARS Facility on pdf page 91\nHBI carries inventory on the balance sheet at the estimated lower of cost or market. Cost is determined by the first-in, first-out, or “FIFO,” method for our inventories. HBI carries obsolete, damaged and excess inventory at the net realizable value, which they determine by assessing historical recovery rates, current market conditions and our future marketing and sales plans.\nInventories are stated at the estimated lower of cost or net realizable value. Cost is determined by the first-in, first-out, or “FIFO”, method for inventories. Obsolete, damaged, and excess inventory is carried at the net realizable value, which is determined by assessing historical recovery rates, current market conditions and future marketing and sales plans. Rebates, discounts and other cash consideration received from a vendor related to inventory purchases are reflected as reductions in the cost of the related inventory item and are therefore reflected in cost of sales when the related inventory item is sold.\n\n\n\nInventories\n12/31/22\n\n\n\n\nRaw materials\n\\$69,279\n\n\nWork in process\n\\$107,904\n\n\nFinished goods\n\\$1,802,489\n\n\nTotal\n\\$1,979,672\n\n\n(amounts in thousands)\n\n\n\n\n\n# average working capital rate\nworking_capital_rate_avg = working_capital_rate[-5:].mean()\nprint('average working capital rate: {:.2f}%'.format(working_capital_rate_avg*100))\n\naverage working capital rate: 24.19%\n\n\n\n\nCurrent assets \nTotal Current Assets from the most recent balance sheet statement of the company. Current assets include inventory, cash and accounts receivables.\nAdjustments for Hanesbrands Inc.\nNone for this company.\n\n# plot Short Term Assets\nwidth = 100  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nplt.bar(df_dcf_data['FY'],df_dcf_data['current_assets']/1e9, width)\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Current assets')\nplt.ylabel('dollars, \\\\$B')\n\nplt.grid()\nplt.show()\n\n\n\n\nObservation:\nThe current assets for HBI are \\$3.13B. Current assets are used to calculate the total corporate value.\n\nsta = df_dcf_data['current_assets'].iloc[-1]\nprint('Current assets: ${:.2f}B'.format(sta/1e9))\n\nCurrent assets: $3.13B\n\n\n\n\nCurrent liabilities \nTotal Current Liabilities from the most recent balance sheet consolidated statement.\nAdjustments for Hanesbrands Inc.\nNone for this company.\n\n# plot Short Term Liabilities\n\nwidth = 100  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nplt.bar(df_dcf_data['FY'],df_dcf_data['current_liabilities']/1e9, width)\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Current liabilities')\nplt.ylabel('dollars, \\\\$B')\n\nplt.grid()\nplt.show()\n\n\n\n\n\nprint('Average of current liabilities: ${:.2f}B'.format(df_dcf_data['current_liabilities'].mean()/1e9))\n\nAverage of current liabilities: $1.54B\n\n\nObservation:\nThe current liabilities for HBI are \\$1.79B. Current liabilities are used to calculate the total value of common equity.\n\nstl = df_dcf_data['current_liabilities'].iloc[-1]\nprint('Current liabilities: ${:.2f}B'.format(stl/1e9))\n\nCurrent liabilities: $1.79B\n\n\n\n\nValue of Debt Outstanding \nAmount of debt outstanding from the most recent balance sheet of the company.\nAdjustments for Hanesbrands Inc.\nNone for this company.\n\n# calculate the percent change in debt, pcd\npcd = np.zeros(len(df_dcf_data['long_term_debt'].to_numpy())) # percent change in debt\nfor i in range(len(df_dcf_data['long_term_debt'].to_numpy()[0:-1])):\n    pcd[i+1] = ((df_dcf_data['long_term_debt'].to_numpy()[i+1] - df_dcf_data['long_term_debt'].to_numpy()[i])/\n                df_dcf_data['long_term_debt'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars, $B')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['long_term_debt']/1e9, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcd,'+-g')\n    \nax2.set_ylabel('% Change in debt',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-40,100))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('debt')\nplt.show()\n\n\n\n\n\ndgr_avg = pcd[1:].mean()/100\nprint('average debt growth rate: {:.2f}%'.format(dgr_avg*100))\n\naverage debt growth rate: 3.04%\n\n\nObservation:\nThe total long term debt and other for HBI is \\$3.61B. Currently the Debt to NOP ratio is 7. HBI Directors eliminated the quarterly cash dividend as they recently shifted their capital allocation strategy to pay down debt to bring leverage back to a range that is no greater than two to three times on a net debt-to-adjusted EBITDA basis. Future principal payments for all of the facilities described above are as follows: \\$247,000 due in 2023, \\$1,485,275 due in 2024, \\$62,500 due in 2025, and \\$2,077,500 due in 2026.\nA summary of the Company’s debt is presented below:\n\n\n\n\n\n\n\n\n\nSenior Secured Credit Facility\nInterest Rate\nPrincipal Amount\nMaturity Date\n\n\n\n\nRevolving Loan Facility\n5.83%\n\\$352,500\n11/01/26\n\n\nTerm Loan A\n5.92%\n\\$975,000\n11/01/26\n\n\n4.875% Senior Notes\n4.88%\n\\$900,000\n05/01/26\n\n\n4.625% Senior Notes\n4.63%\n\\$900,000\n05/01/24\n\n\n3.5% Senior Notes\n3.50%\n\\$535,275\n06/01/24\n\n\nAccounts Receivable Securitization Facility\n5.09%\n\\$209,500\n06/01/23\n\n\n\nas of December 31, 2022\n(amounts in thousands)\n\n\n\n\nTotal cash paid for interest related to debt in 2022, 2021 and 2020 was \\$150,452, \\$161,202 and \\$157,094, respectively.\nDuring 2022, 2021 and 2020, the Company paid \\$3,159, \\$8,346 and \\$15,010, respectively, in capitalized debt issuance costs related to the Company’s financing arrangements within continuing operations. Debt issuance costs are amortized to interest expense over the respective lives of the debt instruments, which range from one to 10 years. As of December 31, 2022, the net carrying value of unamortized debt issuance costs for the revolving loan facilities, which is included in “Other noncurrent assets” in the Consolidated Balance Sheets, was \\$6,831 and the net carrying value of unamortized debt issuance costs for the remainder of the Company’s debt, which is included in “Long-term debt” in the Consolidated Balance Sheets was \\$13,198. The Company’s debt issuance cost amortization in continuing operations was \\$7,300, \\$12,305 and \\$11,349 in 2022, 2021 and 2020, respectively.\n\nvod = df_dcf_data['long_term_debt'].iloc[-1]\nprint('Total long term debt and other: ${:.2f}B'.format(vod/1e9))\n\nTotal long term debt and other: $3.61B\n\n\n\n\nCurrent stock price \nMost recent stock price for the company. The current stock price is used to calculate the market value of the firm. Use the market value when looking at market capitalization for common stock.\n\ncsp = 4.4 # current stock price\nprint('current stock price: ${:,.2f}'.format(csp))\n\ncurrent stock price: $4.40\n\n\nThe current stock price: \\$4.40. The 52 week range is \\$3.85 to \\$12.13.\n\n\nShares outstanding \nThe number of shares outstanding is used to calculate the intrinsic stock value.\n\nso = df_dcf_data['shares_outstanding'].iloc[-1] # shares outstanding\nprint('shares outstanding, basic: {:,.0f}'.format(so))\n\nshares outstanding, basic: 349,361,517\n\n\n\n# calculate the percent change in shares outstanding, pcso\npcso = np.zeros(len(df_dcf_data['shares_outstanding'].to_numpy())) # percent change in debt\nfor i in range(len(df_dcf_data['shares_outstanding'].to_numpy()[0:-1])):\n    pcso[i+1] = ((df_dcf_data['shares_outstanding'].to_numpy()[i+1] - df_dcf_data['shares_outstanding'].to_numpy()[i])/\n                df_dcf_data['shares_outstanding'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('shares outstanding, M')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['shares_outstanding']/1e6, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcso,'+-g')\n    \nax2.set_ylabel('% Change in shares outstanding',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-5,25))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Shares outstanding')\nplt.show()\n\n\n\n\n\nprint('average shares outstanding growth rate: {:.2f}%'.format(pcso[1:].mean()))\n\naverage shares outstanding growth rate: -0.83%\n\n\nObservation:\nUnder the February 6, 2020 share repurchase program, the Company entered into transactions to repurchase 14,464 shares at a weighted average repurchase price of \\$13.83 per share for the year ended January 2, 2021. These shares were repurchased at a total cost of \\$200,269. The Company did not purchase any shares of the Company’s common stock under the February 6, 2020 share repurchase program during 2021.\nUnder the new program, the Company entered into transactions to repurchase 1,577 shares at a weighted average repurchase price of \\$15.84 per share for the year ended December 31, 2022. The shares were repurchased at a total cost of \\$25,018 including broker’s commissions of \\$31. The Company did not repurchase any shares under the previous share repurchase program during 2022 through the expiration of the program on February 2, 2022. At December 31, 2022, the remaining repurchase authorization under the current share repurchase program totaled \\$575,013.\n\n\nDilution\nDilution occurs when a company issues new shares that result in a decrease in existing stockholders’ ownership percentage of that company. Stock dilution can also occur when holders of stock options, such as company employees, or holders of other optionable securities exercise their options. When the number of shares outstanding increases, each existing stockholder owns a smaller, or diluted, percentage of the company, making each share less valuable.\nInvestigate why there is a historic growth trend in number of shares outstanding. Search annual report for dilutive actions:\n\nshare sales\n\nconvertable debt\n\nemployee options\n\nSearch results:\nThe Company established the Omnibus Incentive Plan to award stock options, stock appreciation rights, restricted stock, restricted stock units, deferred stock units, performance shares and cash to its employees, non-employee directors and employees of its subsidiaries to promote the interests of the Company, incent performance and retain employees. In April 2020, the stockholders of the Company approved the Hanesbrands Inc. 2020 Omnibus Incentive Plan (the “2020 Omnibus Plan”). The Company satisfies the requirement for common shares for share-based payments to employees pursuant to the 2020 Omnibus Plan by issuing newly authorized shares. The 2020 Omnibus Plan authorized a total of 11,000 shares of common stock of the Company for awards granted under the 2020 Omnibus Plan, plus the number of shares of common stock of the Company available for grant under the predecessor HanesbrandsInc. Omnibus Incentive Plan (the “Prior Plan”) that had not yet been made subject to awards under the Prior Plan as of the effective date of the 2020 Omnibus Plan. The 2020 Omnibus Plan authorized 74,220 shares for awards of stock options and restricted stock units, of which 14,033 shares were available for future grants as of December 31, 2022.\nIn addition, during 2020, the Company granted stock awards to two newly hired executive officers outside of the 2020 Omnibus Plan in reliance on the employment inducement exemption under the New York Stock Exchange’s Listed Company Manual Rule 303A.08.\nThere were no stock option exercises during 2022 or 2021. The total intrinsic value of options that were exercised during 2020 was \\$3,299.\nThe total fair value of shares vested during 2022, 2021 and 2020 was \\$13,199, \\$25,201 and \\$15,325, respectively.\nAt December 31, 2022, there was \\$23,329 of total unrecognized compensation cost related to non-vested stock-based compensation arrangements, of which \\$16,349, \\$6,097, and \\$883 is expected to be recognized in continuing operations in 2023, 2024, and 2025, respectively.\nCertain of the international plans, specifically those acquired in connection with the purchase of Champion Europe, are in substance nonretirement postemployment benefit plans, which are future liabilities funded through future operational results of the Company. However, for purposes of consolidation, the Company is including these plans within the defined benefit reporting. At December 31, 2022 and January 1, 2022, the total amounts accrued for these plans were \\$871 and \\$1,171, respectively and the total expense was \\$9, \\$8 and \\$16 for 2022, 2021 and 2020, respectively.\nThe potential dilution seems to be 85,224 shares (11,000 + 74,220) authorized under the Omnibus Incentive Plan. This represents 0.024% of the shares outstanding. At \\$5 per share, this has a value of \\$4.26M.\n\n\n10 year treasury bond yield \nThe 10 year treasury yield is used as a measure of the risk free rate.\nYield: 3.45%\niShares 7-10 Year Treasury Bond ETF (IEF)\nAverage Yield to Maturity: 3.5%\n\ntby = (3.45+3.5)/2/100  # 10 year treasury bond yield, average of data from sources listed above\nprint('10 year treasury bond yield: {:,.2f}%'.format(tby*100))\n\n10 year treasury bond yield: 3.48%\n\n\n\n\nBond yield spread to treasury \nThe spread to treasury implies that all corporate debt will have a higher yield than yields associated with comparable maturity US Treasury Bonds. The best way to determine default risk is to see how a particular company’s debt is trading in the market and compare it on a spread basis with comparable maturity yields.\nLook at the following or use a default rating systems that are published by the three major rating agencies, Standards and Poors Corp, Moody’s Investor Services and Fitch & Company.\nPIMCO Active Bond Exchange-Traded Fund (BOND)\nYield: 3.44%\niShares 5-10 Year Investment Grade Corporate Bond ETF (IGIB)\nAverage Yield to Maturity: 5.15%\niShares 10+ Year Investment Grade Corporate Bond ETF (IGLB)\nAverage Yield to Maturity: 5.35%\nWeb resources: - http://www.standardpoor.com/\n- http://bond.yahoo.com/rates.html\n- http://www.moodys.com/cust/default.asp\n- http://www.fitchibca.com/corporate/index.cfm\n\nbystt = ((3.44+5.15+5.35)/3-tby)/100           # bond yield spread (average) to treasury spread\nprint('Bond yield spread to treasury: {:,.2f}%'.format(bystt*100))\n\nBond yield spread to treasury: 4.61%\n\n\n\n\nPreferred stock yield \nAmount of preferred stock outstanding from the most recent balance sheet of the company.\nAdjustments for Hanesbrands Inc.\nPreferred stock (50,000,000 authorized shares; \\$.01 par value) Issued and outstanding — None\n\npsy = 0/100  # preferred stock yield\nprint('preferred stock yield: {:,.2f}%'.format(psy*100))\n\nvps = 0 # value of preferred stock\nprint('value of preferred stock: {:,.2f}'.format(vps))\n\npreferred stock yield: 0.00%\nvalue of preferred stock: 0.00\n\n\n\n\nEquity risk premium \nThe expected excess return a hypothetical average investor would require of a diversified portfolio of stock (assumed beta = 1.0) over the yield on the 10-year Treasury Bond. The equity risk premium has been going down over the years.\n- 1926 to 1990: 5.5%\n- 1962 to 1990: 3.25%\n- 1981 to 1990: 0.19%\nIn times of sustained economic growth the risk premium demanded by investors generally declines.\nI’m going to use 3% as the equity risk premium.\n\neq_rp = 3.0/100             # equity risk premium\nprint('Equity risk premium: {:,.2f}%'.format(eq_rp*100))\n\nEquity risk premium: 3.00%\n\n\n\n\nCompany specific beta \nThe Beta used is Beta of Equity. Beta is the monthly price change of a particular company relative to the monthly price change of the S&P 500. The time period for Beta is 5 years when available. This value can be obtained at yahoo finance.\nA measure of risk of an individual stock. It measures volatility of return - a higher beta means a higher risk. A financial model that uses Beta as its sole measure of risk (signal factor model) is called a Capital Asset Pricing Model (CAPM).\n\nbeta = 1.59 # company specific beta\nprint('Company specific beta: {:,.2f}'.format(beta))\n\nCompany specific beta: 1.59\n\n\n\n\nDCF model inputs \nBelow are the DCF model inputs. These values were calculated above.\n\n# various rates\nrgr = rgr_avg              # revenue growth rate\nprint('revenue growth rate: {:,.2f}%'.format(rgr*100))\nnopm = nopm_avg             # net operating profit margin\nprint('net operating profit margin: {:,.2f}%'.format(nopm*100))\ntr = tax_rate_avg               # tax rate\nprint('tax rate: {:,.2f}%'.format(tr*100))\ndr = depreciation_rate_avg              # depreciation rate (% of revenue)\nprint('depreciation rate: {:,.2f}%'.format(dr*100))\nir = investment_rate_avg              # investment rate (% of revenue)\nprint('investment rate: {:,.2f}%'.format(ir*100))\nwcr = working_capital_rate_avg            # working capital rate (% of revenue)\nprint('working capital rate: {:,.2f}%'.format(wcr*100))\n\nrevenue growth rate: -0.88%\nnet operating profit margin: 9.13%\ntax rate: 46.81%\ndepreciation rate: 1.84%\ninvestment rate: 2.52%\nworking capital rate: 24.19%\n\n\nExcess return period\nThe excess return period is based on a judgment call. The authors of [1] use the 1-5-7-10 rule. They group companies into one of four general categories and excess return periods. They use a 10 year excess return period to calculate what they would consider the maximum value. They use a more conservative 1 year, 5 year or 7 year return period to calculate a more reasonable or minimum value.\n\n1 year: Boring companies that operate in a highly competitive, low margin industry in which they have nothing particular going for them.\n\n5 year: Decent companies that have a recognizable name and decent reputation and perhaps a regulatory benefit (utility company), but can’t control pricing or growth.\n\n7 year: Good companies with good brand names, large companies of scale, good marketing channels and consumer identification (e.g. McDonald’s)\n\n10 year: Great companies with great growth potential, tremendous marketing power, band names and in-place benefits (e.g. Intel, Microsoft, Coca Cola, Disney)\n\nThe excess return period used for the base case is ten years, which should lead to a higher calculated intrinsic value.\n\n# General Inputs\nfy_start = df_dcf_data['FY'].iloc[-1].year # fiscal year to start excess return period\nerp = 10 # excess return period, years\nrev_start = df_dcf_data['revenue'].to_numpy()[-1] # starting revenues for excess return period\nprint('starting year: {:.0f}'.format(fy_start))\nprint('excess return period: {:.0f} years'.format(erp))\nprint('starting revenues: ${:,.2f}B'.format(rev_start/1e9))\nprint('shares outstanding: {:,.0f}'.format(so))\n\nstarting year: 2022\nexcess return period: 10 years\nstarting revenues: $6.23B\nshares outstanding: 349,361,517\n\n\n\nps_mv = vps               # preferred stock, market value \nprint('preferred stock, market value : ${:,.2f}B'.format(ps_mv/1e9))\ncs_mv = csp*so            # common stock, market value \nprint('common stock, market value: ${:,.2f}B'.format(cs_mv/1e9))\n\npreferred stock, market value : $0.00B\ncommon stock, market value: $1.54B\n\n\nLong Term Debt, Market Value, ltd_mv\nUse the book value for long term debt. Various online resources can be used to research this item. These include, Bondsonline and Bloomberg. The book value of debt and preferred stock is an accounting measure that relates to how much money was raised by the company when each security was issued. The market value of debt and the preferred and common stock is the price that specific obligations would trade at in today’s market.\nLong term debt for firms can take one of two forms. It can be a long-term loan from a bank or other financial institution or it can be a long-term bond issued to financial markets, in which case the creditors are the investors in the bond. Firms often have long term obligations that are not captured in the long term debt item. These include obligations to lessors on assets that firms have leased, to employees in the form of pension fund and health care benefits yet to be paid, and to the government in the form of taxes deferred. In the last two decades, accountants have increasingly moved towards quantifying these liabilities and showing them as long term liabilities.\n\nltd_mv = vod              # market value of long term debt\ntmv = ltd_mv+ps_mv+cs_mv  # total market value \nprint('total market value: ${:,.2f}B'.format(tmv/1e9))\n\ntotal market value: $5.15B\n\n\nCost of Common Equity, cce\nThe expected excess return a hypothetical average investor would require of a diversified portfolio of stock (assumed beta = 1.0) over the yield on the 10-year Treasury Bond. The annual rate of return that an investor expects to earn when investing in shares of a company is known as the cost of common equity. It includes dividends and increases in the market value.\n\ncce = tby+beta*eq_rp      # cost of common equity or the expected return for the stock\nprint('cost of common equity: {:,.2f}%'.format(cce*100))\n\ncost of common equity: 8.24%\n\n\nLong Term Debt, Average Yield, ltd_ay\nThe total cost of long term debt.\n\nltd_ay = tby+bystt        # long term debt average yield\nprint('long term debt average yield: {:,.2f}%'.format(ltd_ay*100))\n\nlong term debt average yield: 8.09%\n\n\nLong Term Debt, After Tax Yield, ltd_aty\nThe tax benefits of long term debt. Interest payments are tax deductible for the company.\n\nltd_aty = ltd_ay*(1-tr)   # long term debt after tax yield\nprint('long term debt after tax yield: {:,.2f}%'.format(ltd_aty*100))\n\nltd_pc = vod/tmv          # weight for long term debt \nltd_ate = ltd_aty*ltd_pc  # after tax effect of long term debt \nps_ay = psy               # preferred stock, average yield \nps_aty = ps_ay            # preferred stock, average yield \nprint('preferred stock, average yield: {:,.2f}%'.format(ps_aty*100))\n\nps_pc = ps_mv/tmv         # preferred stock, % capital \nps_ate = ps_aty*ps_pc     # preferred stock, after tax effect \ncs_ay = cce               # common stock, average yield \ncs_aty = cce              # common stock, after tax yield \nprint('common stock, after tax yield: {:,.2f}%'.format(cs_aty*100))\n\ncs_pc = cs_mv/tmv         # common stock, % capital \ncs_ate = cs_aty*cs_pc     # common stock, after tax effect \nprint('common stock, after tax effet: {:,.2f}%'.format(cs_ate*100))\n\ntate = ltd_ate+ps_ate+cs_ate # total after tax effect \nprint('total after tax effect: {:,.2f}%'.format(tate*100))\ntpc = ltd_pc+ps_pc+cs_pc     # total % Capital\nprint('total % Capital: {:,.2f}%'.format(tpc*100))\n\nlong term debt after tax yield: 4.30%\npreferred stock, average yield: 0.00%\ncommon stock, after tax yield: 8.24%\ncommon stock, after tax effet: 2.46%\ntotal after tax effect: 5.48%\ntotal % Capital: 100.00%\n\n\nWeighted average cost of capital\nA company’s weighted average cost of capital (WACC) is the weighted average of the company’s current cost of debt and equity calculated by using current debt, preferred stock and common stock market values. The WACC of the company, calculated after tax, is the discount rate used in the DCF valuation procedures. The WACC, which is the cost of the different components of financing used by the firm, weighted by their market value proportions. These include debt, preferred stock, and common stock.\nWACC: Weighted Average Cost of Capital, the rate used to discount cash flows, based on the following three factors. 1. Base rate of return. 2. Expected return based on debt and preferred stock. 3. Expected return on common stock and Beta.\nAll adjusted for the tax advantage of interest payments and the percentage of debt, preferred stock and common stock.\n\nwacc = tate\nprint('weighted average cost of capital: {:.1f}%'.format(wacc*100))\n\nweighted average cost of capital: 5.5%\n\n\n\n\nFuture cash flows \nThe future cash flows to the firm are projected based on revenue growth. The cash flows are then discounted using the WACC and the ISV is calculated.\n\n# make a list of the fiscal years in excess return period \nfy = np.zeros(erp+1)\nfy[0] = fy_start\nfor i in range(1,erp+1): \n    fy[i]=fy_start+i\n\nrev = np.zeros(len(fy))\nciwc = np.zeros(len(fy))\nrev[0] = rev_start  #*rgr+rev_start   # find the future revenue using constant revenue growth rate \n\nfor i in range(1,len(fy)): \n    rev[i] = rev[i-1]*rgr+rev[i-1]  # find the future revenue \n    ciwc[i] = (rev[i]-rev[i-1])*wcr  # find the change in working capital \n\nnet_op = np.zeros(len(fy)) # net operating profit\nadj_taxes = np.zeros(len(fy))\nnopat = np.zeros(len(fy))\ninvest = np.zeros(len(fy))\ndepre = np.zeros(len(fy))\nnet_invest = np.zeros(len(fy))\nfcff = np.zeros(len(fy))\ndisc_fact = np.zeros(len(fy))\ndisc_fcff = np.zeros(len(fy))                \n\n# calculate values in table \nfor i in range(1,len(fy)):\n    net_op[i] = rev[i]*nopm # net operating profit margin\n    adj_taxes[i] = net_op[i]*tr # net operating profit adjusted for taxes\n    nopat[i] = net_op[i]-adj_taxes[i] # after tax net operating profit\n    invest[i] = rev[i]*ir # future investments\n    depre[i] = rev[i]*dr # future depreciations\n    net_invest[i] = invest[i]-depre[i] # net investments\n    fcff[i] = nopat[i]-net_invest[i]-ciwc[i] # free cash flow to the firm\n    disc_fact[i] = 1/((1+wacc)**i) # discount factor\n    disc_fcff[i] = disc_fact[i]*fcff[i] # discounted free cash flow to the firm\n    \ndcrv = nopat[-1]/wacc*disc_fact[-1] # discounted corporate residual value\nderp_fcff = disc_fcff.sum() # discounted excess return period FCFF\n\ntcv = derp_fcff+dcrv+sta # total corporate value\ntvce = tcv-vod-vps-stl # total value of common equity\nisv = tvce/so # intrinsic stock value\n\n# print cash flows in a table\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format('Year','Rev','NOP','AdjTaxes',\n    'NOPAT','Invest.','Deprec.','dInvest.','dWC','FCFF','DF','DF*FCFF'))\nfor i in range(len(fy)):\n    print('{:4.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.4f}{:10,.0f}'.format(fy[i],\n        rev[i]/1e6,net_op[i]/1e6,adj_taxes[i]/1e6,nopat[i]/1e6,invest[i]/1e6,depre[i]/1e6,(invest[i]-depre[i])/1e6,ciwc[i]/1e6,\n        fcff[i]/1e6,disc_fact[i],disc_fcff[i]/1e6))\n\nYear       Rev       NOP  AdjTaxes     NOPAT   Invest.   Deprec.  dInvest.       dWC      FCFF        DF   DF*FCFF\n2022     6,234         0         0         0         0         0         0         0         0    0.0000         0\n2023     6,179       564       264       300       156       114        42       -13       271    0.9481       257\n2024     6,124       559       262       297       154       113        42       -13       269    0.8988       242\n2025     6,070       554       259       295       153       112        41       -13       267    0.8521       227\n2026     6,017       549       257       292       151       111        41       -13       264    0.8079       213\n2027     5,964       544       255       290       150       110        41       -13       262    0.7659       201\n2028     5,911       540       253       287       149       109        40       -13       260    0.7261       188\n2029     5,859       535       250       284       147       108        40       -13       257    0.6884       177\n2030     5,808       530       248       282       146       107        39       -12       255    0.6526       166\n2031     5,756       525       246       280       145       106        39       -12       253    0.6187       156\n2032     5,706       521       244       277       144       105        39       -12       251    0.5866       147\n\n\n\n# Intrinsic Value\nprint('discounted excess return period FCFF: ${:,.2f}B'.format(derp_fcff/1e9))\nprint('discounted corporate residual value: ${:,.2f}B'.format(dcrv/1e9))\nprint('total corporate value: ${:,.2f}B'.format(tcv/1e9))\nprint('total value of common equity: ${:,.2f}B'.format(tvce/1e9))\ntvce_baseline = tvce # save value as baseline case\nisv_baseline = isv # save the isv for the baseline case\nprint('intrinsic stock value, baseline case: ${:,.2f}'.format(isv_baseline))\nprint('current stock price: ${:,.2f}'.format(csp))\n\ndiscounted excess return period FCFF: $1.98B\ndiscounted corporate residual value: $2.97B\ntotal corporate value: $8.07B\ntotal value of common equity: $2.67B\nintrinsic stock value, baseline case: $7.64\ncurrent stock price: $4.40\n\n\nObservation:\nThe base line DCF analysis produces an intrinsic stock value of \\$7.64. An intrinsic value greater than the current price indicates that the stock is a potential value stock. Some adjustments will be made in the scenario 1 case.\n\n\nList of all inputs to the DCF model\nThe following print statements format the inputs to the model similar to how they are presented on the Valuepro page.\n\nprint('{:&gt;35s} {:&lt;10.0f} {:&gt;35s} {:,.3f}'.format('Excess return period, years:',erp,'Depreciation rate, %:',dr*100))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.3f}'.format('Starting revenues, $B:',\n    rev_start/1e9,'Investment rate, %:',ir*100))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:,.3f}'.format('Revenue growth rate, %:',\n    rgr*100,'Working capital rate, %:',wcr*100))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:,.3f}'.format('Net operating profit margin, %:',\n    nopm*100,'Current assets, $B:',sta/1e9))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:.3f}'.format('Tax rate, %:',\n    tr*100,'Current liabilities, $B:',stl/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.2f}'.format('Current stock price, $:',\n    csp,'Equity risk premium, %:',eq_rp*100))\nprint('{:&gt;35s} {:&lt;10,.0f} {:&gt;35s} {:,.2f}'.format('Shares outstanding, basic, M:',\n    so/1e6,'Company specific beta:',beta))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:.3f}'.format('10 year treasury bond yield, %:',\n    tby*100,'Total long term debt and other, $B:',vod/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.3f}'.format('Bond yield spread to treasury, %:',\n    bystt*100,'Value of preferred stock, $B:',vps/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f}'.format('Preferred stock yield, %:',psy*100))\n\n       Excess return period, years: 10                       Depreciation rate, %: 1.838\n             Starting revenues, $B: 6.23                       Investment rate, %: 2.517\n            Revenue growth rate, %: -0.881                Working capital rate, %: 24.192\n    Net operating profit margin, %: 9.128                      Current assets, $B: 3.132\n                       Tax rate, %: 46.805                Current liabilities, $B: 1.791\n            Current stock price, $: 4.40                   Equity risk premium, %: 3.00\n      Shares outstanding, basic, M: 349                     Company specific beta: 1.59\n    10 year treasury bond yield, %: 3.48       Total long term debt and other, $B: 3.612\n  Bond yield spread to treasury, %: 4.61             Value of preferred stock, $B: 0.000\n          Preferred stock yield, %: 0.00      \n\n\n\n# weighted average cost of capital inputs\nprint('Weighted Average Cost of Capital')\nprint('Cost of common equity')\nprint('{:s}'.format('-'*37))\nprint('{:&gt;32s} {:,.2f}'.format('10 year treasury bond yield, %:',tby*100))\nprint('{:&gt;32s} {:,.2f}'.format('Company specific beta:',beta))\nprint('{:&gt;32s} {:,.2f}'.format('Equity risk premium, %:',eq_rp*100))\nprint('{:s}'.format('-'*37))\nprint('{:&gt;32s} {:,.2f}'.format('Cost of common equity, %:',cce*100))\nprint()\n\nprint('Market Capitalization and After-Tax Weighted Average Cost of Capital')\nprint()\nprint('{:s}{:^10s}{:^10s}{:^10s}{:^15s}{:^15s}'.format(' '*20,'Current','After-Tax','Market','%','Weighted After-'))\nprint('{:s}{:^10s}{:^10s}{:^10s}{:^15s}{:^15s}'.format(' '*20,'Yield','Yield','Value','Capitalization','Tax Yield'))\n\nprint('{:s}'.format('-'*80))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Long term debt',\n    ltd_ay*100,(tby+eq_rp)*(1-tr)*100,vod/1e9,ltd_pc*100,ltd_ate*100))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Preferred stock',\n     psy*100,ps_ate*100,vps/1e9,ps_pc*100,ps_ate*100))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Common stock',\n     cs_ay*100,cs_aty*100,cs_mv/1e9,cs_pc*100,cs_aty*100))\nprint('{:s}'.format('-'*80))\nprint('{:&lt;37s}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('',tmv/1e9,tpc*100,wacc*100))\n\nWeighted Average Cost of Capital\nCost of common equity\n-------------------------------------\n 10 year treasury bond yield, %: 3.48\n          Company specific beta: 1.59\n         Equity risk premium, %: 3.00\n-------------------------------------\n       Cost of common equity, %: 8.24\n\nMarket Capitalization and After-Tax Weighted Average Cost of Capital\n\n                     Current  After-Tax   Market         %       Weighted After-\n                      Yield     Yield     Value   Capitalization    Tax Yield   \n--------------------------------------------------------------------------------\nLong term debt         8.09      3.44         4       70.15           3.02\nPreferred stock        0.00      0.00         0        0.00           0.00\nCommon stock           8.24      8.24         2       29.85           8.24\n--------------------------------------------------------------------------------\n                                              5      100.00           5.48"
  },
  {
    "objectID": "HBI analysis.html#dcf-scenarios",
    "href": "HBI analysis.html#dcf-scenarios",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "4) DCF Scenarios ",
    "text": "4) DCF Scenarios \nThe following adjustments were made to various model parameters.\n\nexcess return period was adjusted to a more conservative 5 years\n\nrevenue growth rate was adjusted to 0% (base case = -0.881 %)\n\nnet operating profit margin was adjusted to 9% (base case = 9.128%)\n\ntax rate was adjusted to 30% (base case = 46.805%)\n\ndepreciation rate was adjusted to 2% (base case = 1.838%)\n\ninvestment rate was adjust to 2% (base case = 2.517%)\n\nworking capital rate was set to an even 24% (base case = 24.192%)\n\nweighted average cost of capital was adjusted up by 2% to reflect higher interest rates and provide a margin of safety (base case = 5.59%)\n\n\nprint('adjusted DCF input values and rates')\nerp = 5\nprint('excess return period: {:,.0f} years'.format(erp))\nrgr = 0/100\nprint('revenue growth rate: {:,.1f}%'.format(rgr*100))\nnopm = isv_s1_nopm = 9/100  # save nopm rate for NAIC preferred method\nprint('net operating profit margin: {:.2f}%'.format(nopm*100))\ntr = isv_s1_tr = 30/100  # save tax rate for NAIC preferred method\nprint('tax rate: {:.2f}%'.format(tr*100))\ndr = 2/100\nprint('depreciation rate: {:,.2f}%'.format(dr*100))\nir = 2/100              # investment rate (% of revenue)\nprint('investment rate: {:,.2f}%'.format(ir*100))\nwcr = 24/100\nprint('working capital rate: {:,.1f}%'.format(wcr*100))\nwacc = (wacc+0.02) # weighted average cost of capital, increased by 2%\nprint('weighted average cost of capital: {:.1f}%'.format(wacc*100))\n\nadjusted DCF input values and rates\nexcess return period: 5 years\nrevenue growth rate: 0.0%\nnet operating profit margin: 9.00%\ntax rate: 30.00%\ndepreciation rate: 2.00%\ninvestment rate: 2.00%\nworking capital rate: 24.0%\nweighted average cost of capital: 7.5%\n\n\n\n# make a list of the fiscal years in excess return period \nfy = np.zeros(erp+1)\nfy[0] = fy_start\nfor i in range(1,erp+1): \n    fy[i]=fy_start+i\n\nrev = np.zeros(len(fy))\nciwc = np.zeros(len(fy))\nrev[0] = rev_start  #*rgr+rev_start   # find the future revenue using constant revenue growth rate \n\nfor i in range(1,len(fy)): \n    rev[i] = rev[i-1]*rgr+rev[i-1]  # find the future revenue \n    ciwc[i] = (rev[i]-rev[i-1])*wcr  # find the change in working capital \n\nnet_op = np.zeros(len(fy))\nadj_taxes = np.zeros(len(fy))\nnopat = np.zeros(len(fy))\ninvest = np.zeros(len(fy))\ndepre = np.zeros(len(fy))\nnet_invest = np.zeros(len(fy))\nfcff = np.zeros(len(fy))\ndisc_fact = np.zeros(len(fy))\ndisc_fcff = np.zeros(len(fy))                \n\n# calculate values in table \nfor i in range(1,len(fy)): \n    net_op[i] = rev[i]*nopm # net operating profit\n    adj_taxes[i] = net_op[i]*tr # net operating profit adjusted for taxes\n    nopat[i] = net_op[i]-adj_taxes[i] # after tax net operating profit\n    invest[i] = rev[i]*ir # future investments\n    depre[i] = rev[i]*dr # future depreciations\n    net_invest[i] = invest[i]-depre[i] # net investments\n    fcff[i] = nopat[i]-net_invest[i]-ciwc[i] # free cash flow to the firm\n    disc_fact[i] = 1/((1+wacc)**i) # discount factor\n    disc_fcff[i] = disc_fact[i]*fcff[i] # discounted free cash flow to the firm\n    \ndcrv = nopat[-1]/wacc*disc_fact[-1] # discounted corporate residual value\nderp_fcff = disc_fcff.sum() # discounted excess return period FCFF\n\ntcv = derp_fcff+dcrv+sta # total corporate value\ntvce = tcv-vod-vps-stl # total value of common equity\nisv = tvce/so # intrinsic stock value\n\n# print cash flows in a table\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format(\n    'Year','Rev','NOP','AdjTaxes',\n    'NOPAT','Invest.','Deprec.','dInvest.','dWC','FCFF','DF','DF*FCFF'))\nfor i in range(len(fy)):\n    print('{:4.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.4f}{:10,.0f}'.\n        format(fy[i],rev[i]/1e6,net_op[i]/1e6,adj_taxes[i]/1e6,nopat[i]/1e6,invest[i]/1e6,depre[i]/1e6,\n        (invest[i]-depre[i])/1e6,ciwc[i]/1e6,fcff[i]/1e6,disc_fact[i],disc_fcff[i]/1e6))\n\nYear       Rev       NOP  AdjTaxes     NOPAT   Invest.   Deprec.  dInvest.       dWC      FCFF        DF   DF*FCFF\n2022     6,234         0         0         0         0         0         0         0         0    0.0000         0\n2023     6,234       561       168       393       125       125         0         0       393    0.9304       365\n2024     6,234       561       168       393       125       125         0         0       393    0.8657       340\n2025     6,234       561       168       393       125       125         0         0       393    0.8054       316\n2026     6,234       561       168       393       125       125         0         0       393    0.7494       294\n2027     6,234       561       168       393       125       125         0         0       393    0.6972       274\n\n\n\n# Intrinsic Value\nprint('discounted excess return period FCFF: ${:,.2f}B'.format(derp_fcff/1e9))\nprint('discounted corporate residual value: ${:,.2f}B'.format(dcrv/1e9))\nprint('total corporate value: ${:,.2f}B'.format(tcv/1e9))\nprint('total value of common equity: ${:,.2f}B'.format(tvce/1e9))\ntvce_S1 = tvce # save value as scenario 1\nisv_S1 = isv # save the isv for scenario 1 case\nprint('intrinsic stock value, scenario 1 case: ${:,.2f}'.format(isv_S1))\nprint('current stock price: ${:,.2f}'.format(csp))\n\ndiscounted excess return period FCFF: $1.59B\ndiscounted corporate residual value: $3.66B\ntotal corporate value: $8.38B\ntotal value of common equity: $2.98B\nintrinsic stock value, scenario 1 case: $8.53\ncurrent stock price: $4.40\n\n\nThe DCF model calculates with adjustments an intrinsic stock value of \\$5.36, which is greater than the current stock price."
  },
  {
    "objectID": "HBI analysis.html#naci-stock-selection-guide-analysis",
    "href": "HBI analysis.html#naci-stock-selection-guide-analysis",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "5) NACI stock selection guide analysis ",
    "text": "5) NACI stock selection guide analysis \nThis analysis follows the NAIC stock selection guide (SSG) [2]. The SSG relates revenue growth, EPS and share price history and makes a prediction about the future share price.\nThe National Association of Investors Clubs (NAIC) is a nonprofit organization dedicated to educating individual investors and investment clubs to become successful lifelong investors. NAIC’s Stock Selection Guide (SSG) is used in the following cells to analyze the company’s growth and whether the stock is selling at a reasonable price.\nThe SSG was originally developed in the 1950s as a paper worksheet by the not-for-profit National Association of Investors Corporation (NAIC). The SSG aims to aid individual investors in the fundamental analysis and selection of common stocks by reviewing components of a company’s growth, quality, and value.\n\nLoad data from metrics sheet\n\n# column names: fiscal years \nfy_data = df_metrics_sheet.columns[1:].values.astype('datetime64[Y]')-1970\n# line 0: Net income\nnet_income_data = df_metrics_sheet.iloc[0].to_numpy()[1:].astype('float')\n# line 1: Shareholder equity\nshareholder_equity_data =  df_metrics_sheet.iloc[1].to_numpy()[1:].astype('float')\n# line 2: Total liabilities\ntotal_liabilities_data = df_metrics_sheet.iloc[2].to_numpy()[1:].astype('float')\n# line 3: Free cash flow, Net cash provided by operating activities \nfree_cash_flow_data =  df_metrics_sheet.iloc[3].to_numpy()[1:].astype('float')\n# line 4: Dividends\ndividends_data =  df_metrics_sheet.iloc[4].to_numpy()[1:].astype('float')\n# line 5: Total assets\ntotal_assets_data = df_metrics_sheet.iloc[5].to_numpy()[1:].astype('float')\n# line 6: Earnings per share\neps_data = df_metrics_sheet.iloc[6].to_numpy()[1:].astype('float')\n# line 7: Dividends per share  \ndps_data = df_metrics_sheet.iloc[7].to_numpy()[1:].astype('float')\n# line 8: Total tangible assets\ntotal_tangible_assets_data = df_metrics_sheet.iloc[8].to_numpy()[1:].astype('float')\n# line 9: Liabilities w/o deposits\nliabilities_wo_deposits_data = df_metrics_sheet.iloc[9].to_numpy()[1:].astype('float')\n# line 10: Provision for credit losses\nprovision_for_credit_losses_data = df_metrics_sheet.iloc[10].to_numpy()[1:].astype('float')\n# line 11: Short-term borrowings\nshort_term_borrowings_data = df_metrics_sheet.iloc[11].to_numpy()[1:].astype('float')\n# line 12: Preferred stock\npreferred_stock_data = df_metrics_sheet.iloc[12].to_numpy()[1:].astype('float')\n# line 13: Net cash used in investing activities \nnet_cash_used_in_investing_activities_data = df_metrics_sheet.iloc[13].to_numpy()[1:].astype('float')\n\n\n# make a new data frame to store data from metrics sheet\ndf_metrics_data = pd.DataFrame(data={\n    'FY':fy_data[::-1],\n    'net_income':net_income_data[::-1],\n    'shareholder_equity':shareholder_equity_data[::-1],\n    'total_liabilities':total_liabilities_data[::-1],\n    'free_cash_flow':free_cash_flow_data[::-1],\n    'dividends':dividends_data[::-1],\n    'total_assets':total_assets_data[::-1],\n    'eps':eps_data[::-1],    \n    'dps':dps_data[::-1],\n    'total_tangible_assets':total_tangible_assets_data[::-1],\n    'liabilities_wo_deposits':liabilities_wo_deposits_data[::-1],    \n    'provision_for_credit_losses':provision_for_credit_losses_data[::-1],\n    'short_term_borrowings':short_term_borrowings_data[::-1], \n    'preferred_stock':preferred_stock_data[::-1],\n    'net_cash_used_in_investing_activities':net_cash_used_in_investing_activities_data[::-1]\n    })\n\n#df_metrics_data\n\ncheck for matching years in both data frames\n\nif all(df_dcf_data['FY'] == df_metrics_data['FY']) != True:\n    print('error, years in data frame don\\'t match')\n    stop # this is not python code, so jupyterlab will throw an error\nelse:\n    print('OK, years in data frame match')\n\nOK, years in data frame match\n\n\n\n\nNAIC section 1: Visual analysis\nHigh and low price history for each year\nFrom the daily price history obtained from yahoo finance, the high and low closing price for each is obtained and the data saved to the financial data frame as new columns.\n\\(\\Large {\\color {red} {\\text {Avg closing price calculated, add to template}}}\\)\n\n#column names: fiscal years \nyears_list = df_metrics_sheet.columns[1:].values.astype('str')[::-1]\n\n# convert years to datetime format\nyear_ended_list = []\nfor i in years_list:\n    year_ended_list.append(datetime.strptime(i, '%Y'))\n\n# make empty lists to store open, close, average close, high and low price data for each fiscal year\nfy_open = []\nfy_close = []\nfy_avg_close = []\nfy_high = []\nfy_low = []\n\nfor i in year_ended_list:\n    start = i\n    end = i + relativedelta(years=1)\n    p1 = df_price_history.truncate(before=start, after=end)\n    if len(p1) == 0:\n        fy_open.append(np.nan)\n        fy_close.append(np.nan)\n        fy_avg_close.append(np.nan)\n        fy_high.append(np.nan)\n        fy_low.append(np.nan)\n    else:\n        fy_open.append(p1['Open'].iloc[0])\n        fy_close.append(p1['Close'].iloc[-1])\n        fy_avg_close.append(p1['Close'].mean()) # could also use median\n        fy_high.append(p1['Close'].max())\n        fy_low.append(p1['Close'].min())\n\n# convert from list to numpy array\nfy_open = np.asarray(fy_open)\nfy_close = np.asarray(fy_close)\nfy_avg_close = np.asarray(fy_avg_close)\nfy_high = np.asarray(fy_high)\nfy_low = np.asarray(fy_low)\n\n\nfy_close\n\narray([ 6.35    ,  5.465   ,  8.955   , 17.567499, 27.905001, 29.43    ,\n       21.57    , 20.91    , 12.53    , 14.85    , 14.58    , 16.719999,\n        6.36    ])\n\n\n\nfy_avg_close\n\narray([ 6.53318452,  6.7058631 ,  7.49818   , 13.47691467, 22.92302585,\n       30.99999004, 26.50714288, 22.03254979, 18.60091629, 16.0516666 ,\n       13.12505929, 18.31206347, 11.18338648])\n\n\nPlotting the data\nThe annual sales, EPS and the high and low share price is plotted on a semilog plot. A consistent percentage change in the data will plot on the semi-log chart as a straight line.\nThe stock price is plotted separately from the sales and earnings for clarity.\nfig, axs = plt.subplots(ncols=2, nrows=2, figsize=(5.5, 3.5),layout=“constrained”)\nax1 = plt.subplot(212) ax1.margins(0.05) # Default margin is 0.05, value 0 means fit ax1.plot(t1, f(t1))\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\n# figsize() function to adjust the size\nplt.subplots(1,2,figsize=(15, 5)) # }--- put fix here\n\n# using subplot function and creating\n# plot one\nplt.subplot(1, 2, 1)  # reseach this }------ generates warnings\nwidth = 3  # the width of the bars\n#plt.bar(year_ended_list,fy_high-fy_low, width,bottom=fy_low,label='price')\nj = 0\nfor i in year_ended_list:\n    color = 'green'\n    if fy_open[j] &gt; fy_close[j]: color= 'red'\n    # high/low lines\n    plt.plot([i,i],[fy_low[j],fy_high[j]],color=color, linewidth=width)\n    # open marker\n    plt.plot([i,i-relativedelta(months=1)], [fy_open[j],fy_open[j]], color=color, linewidth=width)\n    # close marker\n    plt.plot([i,i+relativedelta(months=1)], [fy_close[j],fy_close[j]], color=color, linewidth=width)\n    j += 1\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\n#plt.ylim((20,80))\nplt.title('Yearly stock high and low price range')\nplt.ylabel('stock price, $')\n#plt.legend()\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n\nplt.plot(df_metrics_data['FY'],df_dcf_data['revenue']/1e9,'+-',label='revenue, $B')\nplt.plot(df_metrics_data['FY'],df_metrics_data['eps'],'+-',label='EPS, $')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\n#plt.yscale('log')\n#plt.yticks([0.1,1,10,100,1000,10000],['0.1','1','10','100','1000','10000'])\n#plt.ylim((0.1,1000))\nplt.title('Revenue and EPS')\nplt.ylabel('Revenue and EPS')\nplt.legend()\n\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4) this line generates a warning\n'''\n/tmp/ipykernel_2327/525771534.py:10: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  plt.subplot(1, 2, 1)\n\n/tmp/ipykernel_2327/3890704171.py:10: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  plt.subplot(1, 2, 1)\n\n'''\n\n# show plot\nplt.show()\n\n\n\n\nObservation:\nThe share price dramatically fell when the dividend was discontinued and has continued to decline as the EPS has fallen. The 52 week range is between \\$3.85 and \\$11.91. The pandemic and the current recession have impacted the business. It is interesting to note that the share price has trended down since 2015, while the revenue was increasing over the period from 2013 to 2019. In 2016 HBI took on additional debt to fund acquisitions.\n\\(\\Large {\\color {red} {\\text {fix warning about axes}}}\\)\n\n\nNAIC section 3, Price earnings history\nSection 3 of the SSG is the Price-Earnings history. The following table is built from the high and low prices each year and the earnings per share. The high and low Price/Earnings ratios are calculated for each year and are listed in the columns labeled h-per and l-per.\n\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format('year','high','low','eps',\n    'h-per','l-per'))\nfor i in range(len(year_ended_list)):\n    print('{:s}{:10,.2f}{:10,.2f}{:10,.2f}{:10,.2f}{:10,.2f}'.format(year_ended_list[i].strftime(\"%Y\"),\n        fy_high[i], fy_low[i],df_metrics_data['eps'][i],\n        fy_high[i]/df_metrics_data['eps'][i],\n        fy_low[i]/df_metrics_data['eps'][i]))\n\nyear      high       low       eps     h-per     l-per\n2010      7.70      5.42      0.55     14.07      9.89\n2011      8.31      5.46      0.68     12.18      8.00\n2012      9.12      5.55      0.42     21.86     13.29\n2013     17.74      8.90      0.83     21.44     10.75\n2014     28.93     16.04      1.00     28.79     15.96\n2015     34.58     26.54      1.07     32.32     24.80\n2016     31.18     21.53      1.41     22.11     15.27\n2017     25.67     18.98      0.17    151.00    111.65\n2018     23.24     11.62      1.48     15.70      7.85\n2019     19.14     12.52      1.65     11.60      7.59\n2020     17.62      7.17     -0.21    -83.90    -34.14\n2021     22.37     14.40      0.22    101.68     65.45\n2022     17.36      5.84     -0.36    -48.22    -16.22\n\n\nAverage high and P/E for select years\nThe average price to earning ratio based on high and low stock prices is calculated.\n\n#Average high P/E for years \npe_avg_high = (fy_high/df_metrics_data['eps']).mean()\nprint('average high P/E {:.2f}'.format(pe_avg_high))\n#Average low P/E for years \npe_avg_low = (fy_low/df_metrics_data['eps']).mean()\nprint('average low P/E {:.2f}'.format(pe_avg_low))\n\naverage high P/E 23.12\naverage low P/E 18.47\n\n\n\nEstimate future EPS\nA least squares fit is used to get the slope of the EPS data points.\n\ny = df_metrics_data['eps']\nx = np.arange(len(y))\nm, c = np.polyfit(x, y, 1)\nprint('EPS slope: {:.2f}'.format(m))\nprint('EPS intercept: {:.2f}'.format(c))\nlstsq_fit = m*x + c  # data points for each year\n\nEPS slope: -0.04\nEPS intercept: 0.94\n\n\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('EPS')\n\nax1.plot(df_metrics_data['FY'],df_metrics_data['eps'], 'o',label='EPS')\nax1.plot(df_metrics_data['FY'],lstsq_fit, '-',label='least squares fit')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,4))\nax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('EPS and least squares fit')\nplt.show()\n\n\n\n\nUsing the equation for the best fit line, find the y value for the eps point at five years in the future.\n\n# estimated eps in 5 years\neps_5yr_est = m*(x[-1]+5) + c\nprint('estimated eps in 5 years: {:.1f}'.format(eps_5yr_est))\n\nestimated eps in 5 years: 0.2\n\n\nUsing the high and low price to earning ratio from above and the projected eps, calculate the range of stock price in five years.\n\nnaic_price_eps_low = eps_5yr_est*pe_avg_low\nnaic_price_eps_high = eps_5yr_est*pe_avg_high\nprint('estimated price range in 5 years: ${:.2f} to ${:.2f}'.format(naic_price_eps_low,naic_price_eps_high))\n\nestimated price range in 5 years: $4.01 to $5.02\n\n\nThis is the estimated price range of the stock based on projected EPS and is a guide for what the stock price might be if conditions remain the same. Since the slope of the EPS history is negative, the projected stock price is negative.\n\n\nNAIC section 3: 5 year estimated EPS, preferred method\nSee page 87 and figure 10-1, Need the following data:\n- estimate sales in 5 years based on sales growth\n- NOPM\n- Tax rate\n- shares outstanding\nNet Operating Profit should reflect the future revenue generating ability and expense requirements of the operating business that comprise the ongoing operations of the company.\n\\(\\text{NOPM} = \\frac{\\text{Revenue} - \\text{Expenses}}{\\text{Revenue}}\\)\nTax payments are taken from the consolidated income statement, provision for income taxes. The effect of taxes on profits is accounted for.\n\\(\\text{Tax rate} = \\frac{\\text{Income taxes}}{\\text{Income before income taxes}}\\)\nTo get future EPS\n\\(\\text{future EPS} = \\frac {\\text{future revenue} \\times \\text{NOPM} \\times \\text{(1-tax rate)}}{\\text{number of shares}}\\)\nRevenue and least square fit\n\ny = df_dcf_data['revenue']/1e6\nx = np.arange(len(y))\nm, c = np.polyfit(x, y, 1)\nprint('revenue slope: {:.2f}'.format(m))\nprint('revenue intercept: {:.2f}'.format(c))\nlstsq_fit = m*x + c  # data points for each year\n\nrevenue slope: 228.20\nrevenue intercept: 4411.07\n\n\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $M')\n\nax1.plot(df_metrics_data['FY'],df_dcf_data['revenue']/1e6, 'o',label='revenue')\nax1.plot(df_metrics_data['FY'],lstsq_fit, '-',label='least squares fit')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,4))\nax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Revenue and least squares fit')\nplt.show()\n\n\n\n\nUsing the equation for the best fit line, find the y value for the EPS point at five years in the future.\n\n# estimated revenue in 5 years\nrev_5yr_est = m*(x[-1]+5) + c\nprint('estimated rev in 5 years: ${:,.1f}M'.format(rev_5yr_est))\n\nestimated rev in 5 years: $8,290.5M\n\n\nneed to include estimate of number of shares outstanding in 5 years\n\nprint('starting revenues: ${:,.2f}'.format(rev_start/1e9))\n\nstarting revenues: $6.23\n\n\nUsing the adjusted NOPM and tax rate from scenario 1.\nadjusted DCF input values and rates\n\npm_nopm = isv_s1_nopm # use nopm from scenario 1\npm_tax_rate = isv_s1_tr # use tr from scenario 1\n\npm_eps_5yr_est = rev_5yr_est*pm_nopm*(1-pm_tax_rate)*1e6/df_dcf_data['shares_outstanding'].iloc[-1] \n#pm_eps_5yr_est = rev_5yr_est*nopm_avg*1e6/df_dcf_data['shares_outstanding'].iloc[-1] \nprint('using preferred method: estimated eps in 5 years: ${:.2f}'.format(pm_eps_5yr_est))\n\nusing preferred method: estimated eps in 5 years: $1.50\n\n\nUsing the high and low price to earning ratio from above and the projected EPS, calculate the range of stock price in five years.\n\nnaic_price_pm_low = pm_eps_5yr_est*pe_avg_low\nnaic_price_pm_high = pm_eps_5yr_est*pe_avg_high\nprint('estimated price range in 5 years from preferred method: {:.2f} to {:.2f}'.format(\n    naic_price_pm_low,naic_price_pm_high))\n\nestimated price range in 5 years from preferred method: 27.62 to 34.57\n\n\nObservation: Based on revenue growth, the projected stock price is a bit higher than the current price. However, based on price history, the stock is not expected to appreciate."
  },
  {
    "objectID": "HBI analysis.html#future-stock-price",
    "href": "HBI analysis.html#future-stock-price",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "6) Future stock price ",
    "text": "6) Future stock price \nThe projected future stock price is estimated from the results shown in this notebook based on DCF intrinsic stock value, the NAIC method or a combination of both. The DCF method does not consider market sentiment or popularity of the stock, whereas the NAIC method looks at the PE and EPS to develop the historical consensus that the market has put on the price of the stock. Both the NAIC and the DCF valuation should be considered. The DCF valuation is of the current ISV which is used as an indication of the future value, since it is assumed that the market price will converge eventually to the intrinsic value.\nThe estimated future stock price considers the following:\n- base case ISV\n- Senario ISV\n- NAIC EPS growth\n- NAIC preferred method\nUsing 5 year NAIC as a conservative estimate for the 10 year value and the analysis results, a judgment call is made concerning the price to put on the future value of the stock.\n\nprint('estimated price range in 5 years from EPS: ${:.2f} to ${:.2f}'.format(naic_price_eps_low,naic_price_eps_high))\nprint('estimated price range in 5 years from preferred method: ${:.2f} to ${:.2f}'.format(\n    naic_price_pm_low,naic_price_pm_high))\n\nprint('intrinsic stock value, baseline case: ${:,.2f}'.format(isv_baseline))\nprint('intrinsic stock value, scenario 1 case: ${:,.2f}'.format(isv_S1))\n\nprint('current stock price: ${:,.2f}'.format(csp))\n\nestimated price range in 5 years from EPS: $4.01 to $5.02\nestimated price range in 5 years from preferred method: $27.62 to $34.57\nintrinsic stock value, baseline case: $7.64\nintrinsic stock value, scenario 1 case: $8.53\ncurrent stock price: $4.40\n\n\nThe estimated price range in 5 years from the preferred method is \\$86.14 to \\$116.85. Taking the average and using that value on the IRR calculations.\n\nfsp = (naic_price_pm_low+isv_S1)/2 # estimated future stock price\nprint('estimated future stock price: ${:,.2f}'.format(fsp))\n\nestimated future stock price: $18.07"
  },
  {
    "objectID": "HBI analysis.html#dividend-payout",
    "href": "HBI analysis.html#dividend-payout",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "7) Dividend payout ",
    "text": "7) Dividend payout \nThe dividend payout examines the amount shareholders are getting from the company relative to earnings or revenue. It is an important metric to determine how the business is operating and whether it has enough growth potential.\n\nDividend history\nNo dividends paid in 2010, 2011 and 2012. HBI Board of Directors eliminated the quarterly cash dividend as they recently shifted their capital allocation strategy to pay down debt to bring leverage back to a range that is no greater than two to three times on a net debt-to-adjusted EBITDA basis.\nThe code cells below have been set to raw since there is no dividend data and the yearly future cash flows are also set to zero."
  },
  {
    "objectID": "HBI analysis.html#management-performance",
    "href": "HBI analysis.html#management-performance",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "8) Management performance ",
    "text": "8) Management performance \nThe following analysis somewhat follows the Warren Buffett strategy as outlined in [3]. This strategy is essentially value investing where companies are chosen that meet a set of criteria and who’s stock price is below the intrinsic value plus a margin of safety. These investments are usually held for the long term.\n\nFinancial metrics\nThe following analysis looks at financial ratios over the evaluation period. Financial ratios can be used to judge management performance. Consistent favorable trends are an indication that management is taking care of the company.\n\nTotal liabilities to total assets ratio\n\nDebt to equity and debt to NOP ratios\n\nFinancial ratios: RoE, RoA and PM\n\nNAIC section 2: Evaluating management\n\nNormalized data from consolidated statements\n\n\nMarket metrics\n\nOne dollar premise\n\nShare price vs EPS\n\nMarket capitalization\n\n\nQualitative metrics\n\nSimple and understandable business model\n\nFavorable long term prospects\n\nCommodity reliance\n\nConsistent operating history\n\nrationality:\n\nfocus on core aspects\n\nonly invest in high ROE businesses\n\nfocus on shareholder equity\n\n\n\n\nFinancial metrics \nThe following financial metrics are examined over the evaluation period. We are looking for favorable trends and evidence of consistent operations. Some red flags will also be evident in the plots.\nRed flags:\n- Shrinking gross profit margin\n- Receivables growing faster than sales\n- Rising debt-to-equity ratio\n- Several years of revenue trending down\n- Unsteady cash flow\n- Rising accounts receivable or inventory in relation to sales\n- Rising outstanding share count\n- Consistently higher liabilities than assets\n- Decreasing gross profit margin\n- Increasing revenue while cash flow remains the same\n- Unusual changes in key financial ratios\n\nTotal liabilities to total assets ratio\nThe ratio of liabilities to assets is plotted over the evaluation period. For most companies examined the liabilities are the total liabilities and the ratio is calculated using total assets and total tangible assets. Total tangible assets have goodwill and intangibles removed from the total. The ratio gives an indication of how much the company is worth versus how much the company owes. Ideally the ratio of liabilities to assets should be less than one.\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('ratio')\n\n# plot revenue as single bar\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['total_assets'], '-+',\n    label='total liabilities to total assets')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['total_tangible_assets'], '-*',\n    label='total liabilities to total tangible assets')\n\nax1.tick_params(axis='y')\nax1.legend(bbox_to_anchor=(1.8, 1))\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\n#ax2.plot(year_ended_list,pcd,'+-g')\nax2.plot(df_metrics_data['FY'],\n    (df_metrics_data['total_assets']-df_metrics_data['total_tangible_assets'])/df_metrics_data['total_assets']*100,\n    ':',color=color,label='intangible assets to total assets')\n    \nax2.set_ylabel('% intangible assets',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,100))\nax2.legend(bbox_to_anchor=(1.7, 0))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Total liabilities to total assets ratio')\nplt.show()\n\n\n\n\nHBI has significant goodwill and intangible assets. As of January 2, 2021, HBI had approximately \\$1.3 billion of goodwill and \\$1.6 billion of trademarks and other identifiable intangibles on the balance sheet, which together represent 37% of the total assets. HBI does not amortize goodwill, but they assess for impairment at least annually and more often as triggering events occur. The timing of annual goodwill impairment testing is the first day of the third fiscal quarter.\nEquity computed using tangible assets is negative. This would made debt to equity ratios shown below negative.\n\n\nDebt to equity and debt to NOP ratios\nThe debt-to-equity ratio (D/E) is another key characteristic Buffett considers carefully. Buffett prefers to see a small amount of debt so that earnings growth is being generated from shareholders’ equity as opposed to borrowed money. The D/E ratio is calculated as follows:\n\\(\\text{Debt-to-Equity Ratio} = \\frac {\\text{Total Liabilities}} {\\text{Shareholders' Equity}} \\text{  OR  } \\frac {\\text{Long term debt}} {\\text{Shareholders' Equity}}\\)\nThis ratio shows the proportion of equity and debt the company uses to finance its assets, and the higher the ratio, the more debt—rather than equity—is financing the company. A high debt level compared to equity can result in volatile earnings and large interest expenses. For a more stringent test, investors sometimes use only long-term debt instead of total liabilities in the calculation above.\nD/E is the traditional way to look at a company’s debt. Some rules of thumb say that the D/E should not be above 2 or 3. However the D/E company’s typically vary by industry. The ratio of LT debt to NOP gives the number of years it would take the company to pay back debt from NOP, the lower the number the shorter amount of time.\n\\(\\text{Debt-to-NOP Ratio} = \\frac {\\text{Total Liabilities}} {\\text{NOP}}\\)\n\ntangible_equity = df_metrics_data['total_tangible_assets'] - df_metrics_data['total_liabilities']\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('ratio')\n\nax1.plot(df_metrics_data['FY'],df_dcf_data['long_term_debt']/df_metrics_data['shareholder_equity'],\n    '-^',label='(LT debt)/Equity')\n#ax1.plot(year_ended_list,df_dcf_data['long_term_debt']/tangible_equity, '-',label='(LT debt)/(Tangible Equity)')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['shareholder_equity'],\n    '-*',label='(total liabilities)/Equity')\n#ax1.plot(year_ended_list,total_liabilities/BV, '-^',label='(total liabilities)/BV')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/nop, '-+',label='(total liabilities)/NOP')\n#ax1.plot(year_ended_list,total_liabilities/net_income, '-+',label='(total liabilities)/(net income)')\n#ax1.plot(year_ended_list,df_dcf_data['current_liabilities']/nop, '-*',label='(current liabilities)/NOP')\n#ax1.plot(year_ended_list,Liabilities_wo_deposits/nop, '-+',label='(Liabilities w/o deposits)/NOP')\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,20))\n#ax1.legend()\nax1.legend(bbox_to_anchor=(1.6, 1))\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Various debt ratios')\nplt.show()\n\n\n\n\n\\(\\Large {\\color {red} {\\text {fix plot, why the spike in 2020?}}}\\)\nThree debt ratios are plotted above. Prior to 2020 the debt ratios were somewhat consistent from year to year. In 2020 the NOP fell to almost zero and the total liabilities to NOP ratio when off the chart. From 2019 to the present the debt ratios have been rising and are now at concerning levels. It will be challenging for the HBI to reduce debt and grow revenues in the coming years.\n—- OLD (LT debt)/Equity is plotted and is below 2 for each year in the evaluation period. A threshold of 2 is traditionally the upper limit for a reasonable amount of debt that a company should carry.\n(total liabilities)/Equity is plotted and except for 2020 has been below the threshold of 2.\n(total liabilities)/NOP to is plotted for each year in the evaluation period and except for 2019 is below 10. A value of 10 has been chosen as the threshold for this ratio and indicates how many years it would take the company to pay off total liabilities from the NOP generated each year. A threshold of ten seems like a reasonable level of debt measured against NOP.\n\n\nFinancial ratios (change)\n\n\nFinancial returns\nVarious ratios can be used to judge management performance. Consistent favorable trends are an indication that management is taking care of the company.\nReturn on equity\nSometimes return on equity (RoE) is referred to as stockholder’s return on investment. It reveals the rate at which shareholders earn income on their shares. Buffett always looks at RoE to see whether a company has consistently performed well compared to other companies in the same industry. RoE is calculated as follows:\n\\(\\text{Return on Equity} = \\frac {\\text{Net Income}} {\\text{Shareholder's Equity}}\\)\nLooking at the RoE in just the last year isn’t enough. The investor should view the RoE from the past five to 10 years to analyze historical performance.\n\\(\\text{Shareholders’ Equity} = \\text{Total Assets} − \\text{Total Liabilities}\\)\nFor this company, this method of getting Shareholders’ Equity gives negative values. On the Consolidated Balance Sheets, there is a line for Total stockholders’ equity, which is used.\nReturn on Assets\nReturn on assets is a profitability ratio that provides how much profit a company is able to generate from its assets. In other words, return on assets (RoA) measures how efficient a company’s management is in generating earnings from their economic resources or assets on their balance sheet.\n\\(\\text{Return on assets} = \\frac {\\text{Net Income}} {\\text{Tangible Assets}}\\)\nCalculating the RoA of a company can be helpful in comparing a company’s profitability over multiple quarters and years as well as comparing to similar companies. However, it’s important to compare companies of similar size and industry.\nFor example, banks tend to have a large number of total assets on their books in the form of loans, cash, and investments. A large bank could easily have over \\$2 trillion in assets while putting up a net income that’s similar to companies in other industries. Although the bank’s net income or profit might be similar to an unrelated company and the bank might have high-quality assets, the bank’s RoA will be lower. The larger number of total assets must be divided into the net income, creating a lower RoA for the bank.\nSimilarly, auto manufacturing requires huge facilities and specialized equipment. A lucrative software company that sells downloadable programs online may generate the same net profits, but it could have a significantly higher RoA than its more asset-heavy counterparts. When utilizing this metric to compare productivity across businesses, it’s important to take into account what types of assets are required to function in a given industry, rather than simply comparing the figures.\nGoodwill is a historical cost that does not have to be constantly replaced. Therefore, in most cases, return on tangible capital alone (excluding goodwill) will be a more accurate reflection of a business’s return on capital going forward. The ROE and ROA calculations used by many investment analysts are therefore often distorted by ignoring the difference between reported equity and assets and tangible equity and assets.\nProfit Margin\nA company’s profitability depends not only on having a good profit margin, but also on consistently increasing it. This margin is calculated by dividing net income by net sales. For a good indication of historical profit margins, investors should look back at least five years. A high-profit margin indicates the company is executing its business well, but increasing margins mean management has been extremely efficient and successful at controlling expenses.\n\\(\\text{Profit Margin} = \\frac {\\text{Net Income}} {\\text{Revenue}}\\)\n\n\n\nNew Section\nReturn on Capital\nThe Return on Capital (RoC) is plotted to compare to RoA and RoE. RoC shows how much capital is needed to conduct the company’s business. RoC is the ratio of NOP to Working Capital, Tangible Assets and Depreciation. In addition to working capital requirements, a company must also fund the purchase of fixed assets necessary to conduct its business, such as real estate, plant, and equipment. The depreciated net cost of these fixed assets was then added to the working capital requirements to arrive at an estimate for tangible capital employed. Return on Capital (RoC) is calculated as follows:\n\\(\\text{Return on Capital} = \\frac {\\text{NOP}} {\\text{Working Capital + Tangible Assets + Depreciation}}\\)\nThe RoC calculation uses NOP instead of net income to filter distortions due to taxes and debt.\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('percent')\n\nax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_metrics_data['shareholder_equity']*100,\n    '-+',label='Return on Equity')\n\n#ax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_metrics_data['total_assets']*100,\n#    '-*',label='Return on Assets')\n\nax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_metrics_data['total_tangible_assets']*100,\n    '-*',label='Return on Tangable Assets')\n\nax1.plot(df_metrics_data['FY'],nop/(working_capital+df_metrics_data['total_tangible_assets']+df_dcf_data['depreciation'])*100,\n    '-*',label='Return on Capital')\n\n#ax1.plot(df_metrics_data['FY'],total_liabilities/shareholder_equity, '-^',label='D/E')\n\n#ax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_dcf_data['revenue']*100,\n#    '-^',label='Profit margin')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,14))\n#ax1.legend()\nax1.legend(bbox_to_anchor=(1.05, 1))\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Financial Returns')\nplt.show()"
  },
  {
    "objectID": "HBI analysis.html#decision-model",
    "href": "HBI analysis.html#decision-model",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "9) Decision model ",
    "text": "9) Decision model \nThe decision model establishes thresholds that are to used in the purchase decision. There are three hard decision thresholds in this model which are:\n1. Intrinic value\n2. Debt\n3. Dividend payout ratio\n4. Dividend IIR\n5. NACI managemet evaluation\n6. One dollar premise\n\\(\\Large {\\color {red} {\\text {update template}}}\\)\nThe first threshold is based on the intrinisic value of the company as calculated by the DCF model semario 1. Reconizing that absolute intrinsic value is an elusive concept, judgement, justified by facts (assets, earnings, dividends, debt and cash flow), establishes the value by adjusting various rates, based on judgement and using a five year forward projection period. This should give a intrinsic value that is based on the historical data, modified by judgement.\nI’m using a threshold of the intrinsic value calculated in senario 1 (isv_S1) that is greater than 70% of the current stock price, provided that the NAIC valuation is above the current stock price. This accounts for the inadequacy or incorrectness of the data, the uncertainties of the future, and considers the behavior of the market.\nThe second threshold is the level of debt. The ratios of (LT debt)/Equity, (total liabilities)/Equity and (total liabilities)/NOP are ploted for the evaluation period. Over the evaluation period the (LT debt)/Equity and (total liabilities)/Equity should be less than 2 and stable. A threshold of 2 has been discussed in the litureture as a level of debt that a company can reasonably take on.\nThe thereshold for (total liabilities)/NOP is set at 10. This means that the company can pay off all the liabilities with tens years worth of NOP, which seems like a reasonable timeframe for an established and stable company.\nThe third threshold is the dividend payout ratio and is a relative measure of how much the company is paying to shareholders in dividends compared to the metrics of NOP and free cash flow (Net cash provided by operating activities). The payout ratio is useful for assessing a dividend’s sustainability. Payout ratio for a REIT is established by tax law and not used as an evaluation criteria. For other industries a threshold of 50% has been set as the limit.\nThe dividend IRR threshold is the internal rate of return for investor dividend cash flow (divident_irr) should be greater than 10 year treasury bond yield (tby) plus the equity risk premium (eq_rp). Otherwise, other investment operatunities should be looked at.\nIn the decision model there are soft thresholds based on judgement. Soft thresholds are a collection of ratios and analysis that taken together tell a story of the performance of the conmpany and manatgments ability to run the company and support dividends over the long term. Use judgement and make an evalaution.\nThe third critiera is a collection of ratios and analysis that taken together tell a story of the performance of the conmpany and manatgments ability to run the company and support dividends over the long term. Use judgement and make an evalaution. These are the following:\n1. Financial metrics\n2. Market metrics\n3. Qualitative metrics\nThe soft thresholds are discused in section 10.\n\nCheck DCF and NAIC value thresholds\n\n# check DCF senario 1\ndcf_score = isv_S1/csp #ratio of isv to csp\ndcf_threshold = 0.7\nif dcf_score &lt; 0.7:\n    print('FAIL, DCF score is less than {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\nelse:\n    print('PASS, DCF score is above {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\n\n# check NAIC\nnaic_score = np.array([naic_price_eps_low,naic_price_pm_low]).min()/csp\nnaic_threshold = 1\nif naic_score &lt; 1:\n    print('FAIL, NAIC score is less than {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\nelse:\n    print('PASS, NAIC score is above {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\n\n# check both scores\nif naic_score &lt; 1 or dcf_score &lt; 0.7:\n    print('One or both DCF and NAIC scores failed')\nelse:\n    print('Both DCF and NAIC scores pass')\n\nPASS, DCF score is above 0.7 at 1.9\nFAIL, NAIC score is less than 1.0 at 0.9\nOne or both DCF and NAIC scores failed\n\n\n\n\nCheck debt thresholds\n\ndebt_lookback = 4\navg_LT_debt2EQ = df_dcf_data['long_term_debt'][-debt_lookback:].mean()/df_metrics_data['shareholder_equity'][-debt_lookback:].mean()\navg_TLiability2EQ = df_metrics_data['total_liabilities'][-debt_lookback:].mean()/df_metrics_data['shareholder_equity'][-debt_lookback:].mean()\navg_TLiability2NOP = df_metrics_data['total_liabilities'][-debt_lookback:].mean()/nop[-debt_lookback:].mean()\n\nprint('long term debt to shareholder equity ratio = {:.2f}'.format(avg_LT_debt2EQ))\nprint('total liabilities to shareholder equity ratio = {:.2f}'.format(avg_TLiability2EQ))\nprint('total liabilities to NOP ratio = {:.2f}'.format(avg_TLiability2NOP))\n\nif (avg_LT_debt2EQ &gt; 2) or (avg_TLiability2EQ &gt; 2) or (avg_TLiability2NOP &gt; 10):\n    print('FAILED one of the debt threshold limits')\n\nlong term debt to shareholder equity ratio = 4.42\ntotal liabilities to shareholder equity ratio = 8.08\ntotal liabilities to NOP ratio = 11.51\nFAILED one of the debt threshold limits\n\n\n\n\nCheck dividend payout and IIR thresholds\n\\(\\Large {\\color {red} {\\text {update comments about IIR}}}\\)\n\n# check dividend payout ratio average the last three years\nprint('Dividends are paid at {:.1f}% of cash flow'.format(\n    (df_metrics_data['dividends']/df_metrics_data['free_cash_flow'])[-3:].mean()*100))\nprint('Dividends are paid at {:.1f}% of NOP'.format((df_metrics_data['dividends']/nop)[-3:].mean()*100))\n\nif ((df_metrics_data['dividends']/nop)[-3:].mean() or (df_metrics_data['dividends']/df_metrics_data['free_cash_flow'])[-3:].mean()) &gt; 0.5:\n    print('FAIL, dividend payout ration too high')\n\nDividends are paid at 7.4% of cash flow\nDividends are paid at 1100.9% of NOP\nFAIL, dividend payout ration too high\n\n\n\n# Check dividend IRR limit\nif dividend_irr &lt; (tby+eq_rp):\n    print('FAIL, dividend IRR is less than {:.2f} at {:.2f}'.format((tby+eq_rp)*100,dividend_irr*100))\nelse:\n    print('PASS, dividend IRR is above {:.2f} at {:.2f}'.format((tby+eq_rp)*100,dividend_irr*100))\n\nFAIL, dividend IRR is less than 6.48 at 4.41\n\n\n\n# check DCF senario 1\ndcf_score = isv_S1/csp #ratio of isv to csp\ndcf_threshold = 0.7\nif dcf_score &lt; 0.7:\n    print('FAIL, DCF score is less than {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\nelse:\n    print('PASS, DCF score is above {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\n\n# check NAIC\nnaic_score = np.array([naic_price_eps_low,naic_price_pm_low]).min()/csp\nnaic_threshold = 1\nif naic_score &lt; 1:\n    print('FAIL, NAIC score is less than {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\nelse:\n    print('PASS, NAIC score is above {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\n\n# check both scores\nif naic_score &lt; 1 or dcf_score &lt; 0.7:\n    print('One or both DCF and NAIC scores failed')\nelse:\n    print('Both DCF and NAIC scores pass')\n\nPASS, DCF score is above 0.7 at 1.9\nFAIL, NAIC score is less than 1.0 at 0.9\nOne or both DCF and NAIC scores failed\n\n\ncomments:\n\nNACI managemet evaluation\n\n\n# check profit margin trends\nif pm_trend &lt; 0:\n    print('FAIL, profit margin trend over the evaluation period is negative at {:.1f}%'.format(pm_trend*100))\nelse:\n    print('PASS, profit margin trend over the evaluation period is positive at {:.1f}%'.format(pm_trend*100))\n\nif pm1_trend &lt; 0:\n    print('FAIL, profit margin trend over the last {:.0f} years is negative at {:.1f}%'.format(-left_yr,pm1_trend*100))\nelse:\n    print('PASS, profit margin trend over the last {:.0f} years is positive at {:.1f}%'.format(-left_yr,pm1_trend*100))    \n\nFAIL, profit margin trend over the evaluation period is negative at -0.5%\nFAIL, profit margin trend over the last 6 years is negative at -1.3%\n\n\nChecking trends based on EPS and NOP.\n\n# check yield trend based on EPS\nif pm_trend &lt; 0:\n    print('FAIL, EPS yield trend over the evaluation period is negative at {:.1f}%'.format(y1_trend*100))\nelse:\n    print('PASS, EPS yield trend over the evaluation period is positive at {:.1f}%'.format(y1_trend*100))\n\nif pm1_trend &lt; 0:\n    print('FAIL, EPS yield trend over the last {:.0f} years is negative at {:.1f}%'.format(-left_yr,y1a_trend*100))\nelse:\n    print('PASS, EPS yield trend over the last {:.0f} years is positive at {:.1f}%'.format(-left_yr,y1a_trend*100))    \n\nFAIL, EPS yield trend over the evaluation period is negative at -0.7%\nFAIL, EPS yield trend over the last 6 years is negative at -1.5%\n\n\n\n# check yield trend based on NOP\nif pm_trend &lt; 0:\n    print('FAIL, NOP yield trend over the evaluation period is negative at {:.1f}%'.format(y2_trend*100))\nelse:\n    print('PASS, NOP yield trend over the evaluation period is positive at {:.1f}%'.format(y2_trend*100))\n\nif pm1_trend &lt; 0:\n    print('FAIL, NOP yield trend over the last {:.0f} years is negative at {:.1f}%'.format(-left_yr,y2a_trend*100))\nelse:\n    print('PASS, NOP yield trend over the last {:.0f} years is positive at {:.1f}%'.format(-left_yr,y2a_trend*100))    \n\nFAIL, NOP yield trend over the evaluation period is negative at -0.3%\nFAIL, NOP yield trend over the last 6 years is negative at -0.2%\n\n\n\nOne dollar premise\n\nretained_earnings = df_metrics_data[‘net_income’].sum() - df_metrics_data[‘dividends’].sum()\nprint(‘retained earnings: ${:,.2f}B’.format(retained_earnings/1e9))\nretained earnings: $1.69B\nRetained earnings are $1.7B"
  },
  {
    "objectID": "HBI analysis.html#conclusion",
    "href": "HBI analysis.html#conclusion",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "10) Conclusion ",
    "text": "10) Conclusion \nThe following is a summary of the results described above:\n\nStock screener results: This company was selected from the Fidelity stock screener results. The company had increasing revenues and a dividend yield above 2.38%.\n\nCurrent news: A few new articles were noted, nothing of significance found.\n\nReview quarterly results: Not applicable since 10K was just released.\n\nAverage daily volume: Above 1M per day.\n\nDividend yield: Above 3%.\n\nDiscounted cash flow analysis, baseline: The baseline DCF analysis yielded a very high ISV, driven primarily by revenue growth rate and low WACC. The baseline ISV doesn’t seem realistic.\n\nDCF Scenarios: Adjustments were made and the ISV is now below the current price.\n\nNACI stock selection guide analysis: Share price has been trading in the 50 to 70 dollar range the last 7 years, indicating that the market is not impressed with the company. Average EPS (ignoring 2020) has been flat. Based on EPS growth, the estimated price range in 5 years is \\$8.99 to \\$12.19 per share. Based on NAIC’s preferred method, the estimated price range in 5 years is \\$86.14 to \\$116.85 per share.\n\nDividend payout: Dividend payout has been increasing over the years. The dividend yield for the past five years has been in the 2.5 to 3.5 percent range.\nDividend pay ratios for the last 3 years are near or below 50%. Dividend cash flow IRR is 8.01%.\n\nManagement performance:\n\nFinancial metrics: The value assigned to goodwill and intangibles is about \\$1 billion. The ratio indicates the company has taken on a lot of debt relative to assets and is something of concern. Various debt ratios seem OK. Except for 2020 performance ratios and trends are acceptable. The divergence of total liabilities from revenue is of concern.\n\nMarket metrics: The range in share price is roughly the same across the range of EPS. This means that investors are not valuing the company’s EPS. The company’s market capitalization is higher than the total value of common equity as calculated using scenario 1 DCF data. This implies that the company is overvalued.\n\nQualitative metrics: see above\n\nConcerns: increasing total liabilities, low ISV, negative retained earnings, 60% of the total assets are intangible.\nSummary: The analysis presented in this report is based on the company’s fundamentals and tries to establish the value of the company. This strategy is essentially value investing where companies are chosen that meet a set of criteria and who’s stock price is below the intrinsic value plus a margin of safety. These investments are usually held for the long term. Company revenue, earnings, debt and dividends were examined. Adjustments were made to the DCF and the ISV is below the current price. The share price has been trading in the 50 to 70 dollar range the last 7 years, indicating that the market is not impressed with the company. The dividend payout has been increasing over the years and the dividend cash flow IRR is 8.01%. The value assigned to goodwill and intangibles is about $1 billion. The ratio indicates the company has taken on a lot of debt relative to assets. The divergence of total liabilities from revenue is of concern. Negative retained earnings is a concern.\nRecommendation: Don’t buy above $50 per share."
  },
  {
    "objectID": "HBI analysis.html#notes",
    "href": "HBI analysis.html#notes",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "11) Notes ",
    "text": "11) Notes \nThe following notes outline the changes to the DCF model for financial and REIT companies.\nValuing a REIT\nNotes from Valuepro Book, page 237\n\nNOPM: To calculate operating income take rental revenue and subtracted total real estate expenses and G&A expenses. To arrive at the NOPM divide the adjusted income from real estate by real estate rental revenue. For the REIT, take income from real estate, which includes depreciation and amortization, and subtract GSA. Exclude other income, gains on sale of real estate and interest expenses.\n\nREIT has no traditional R&D costs\n\nREIT is not taxed at the corporate level, tax rate: should be near zero.\nDepreciation and capital expenditures are significantly higher for REITs than in other companies.\nNew property acquisitions are not directly accounted for in the DCF model for a REIT.\n\nWorking capitol: accounts payable, rents and security deposits\n\nShort term assets: cash, rents and other receivables and prepaid expenses\n\nShort term liabilities: accounts payable, advance rents security deposits\n\nWorking capital is almost zero, which is similar to other financial companies.\nThe consolidated balance sheet lists the assets as:\n- Real estate held for investment, at cost:\n- Land\n- Buildings and improvements\n- Total real estate held for investment, at cost\n- Less accumulated depreciation and amortization\n- Real estate held for investment, net\n- Real estate and lease intangibles held for sale, net\n- Cash and cash equivalents &lt;- current asset\n- Accounts receivable, net &lt;- current asset\n- Lease intangible assets, net\n- Other assets, net\nThe line items indicated above have been taken to be the current assets. Intangibles and long term items have been excluded.\nThe consolidated balance sheet lists the liabilities as:\n- Distributions payable &lt;- current liabilities\n- Accounts payable and accrued expenses &lt;- current liabilities\n- Lease intangible liabilities, net\n- Other liabilities\n- Line of credit payable and commercial paper &lt;- current liabilities\n- Term loans, net\n- Mortgages payable, net &lt;- current liabilities\n- Notes payable, net\nThe line items indicated above have been taken to be the current liabilities.\nValuing a financial company\nNotes from Valuepro Book, page 206\n\nTotal revenue comes from the total interest and dividend income line on the income statement. The calculation of operating income is more inclusive for a financial company than for an industrial or high tech company. For financial companies, operating revenue includes all normal revenue items plus interest income, dividends received and other investment income.\nCost of Goods Sold (CGS) comes from the Total interest expense line on the statement of income.\n\nGeneral and Administrative (G&A) are set to zero since they are included in the interest expense line\n\nA financial company has no traditional R&D costs\n\\(\\text{Cost of Goods Sold (CGS)} = \\text{Total interest expense} + \\text{Total non-interest expense}\\)\n\nGeneral and Administrative (G&A) are set to zero since they are included in the interest expense line\n\nA financial company has no traditional R&D costs\nDepreciation and amortization of premises and equipment from Consolidated Statements of Cash Flows.\n\nAmortization of other acquisition-related intangible assets is not included.\nNew investment and Depreciation: Property, plant and equipment expenditures and depreciation charges are significantly lower for a financial company. A typical manufacturing company, in order to grow its business, invests a significant portion of its revenues in plant, property and equipment (PPE). Financial companies invest very little in the way of PPE. However, software, risk management systems and acquisitions of other businesses, need to be included.\n\nFrom the Consolidated Statements of Cash Flows, under Cash Flows from Investing Activities\n- Purchases of premises and equipment\n- Purchases of leased equipment, net\n\nWorking capital supports manufacturing and service activities of nonfinancial companies. For financial companies, their principal liabilities and assets are financial claims that take the place of working capital. Because there is no differentiation between current and long term assets and liabilities for a financial company, we adjust working capital charges to zero. A financial company generally invests all of its funds in other financial assets, which have characteristics of current assets rather than PP&E.\n\\(\\text{Accounts Receivable} = 0\\)\n\\(\\text{Inventories} = 0\\)\n\\(\\text{Accounts Payable} = 0\\)\n\\(\\text{working capital} = 0\\)\nShort term assets: The balance sheets of most financial companies do not separate assets and liabilities into current and long term categories. When calculating the short term assets take the total assets and subtract goodwill and intangible assets also subtract other assets of questionable value. Subtract long term assets such as PP&E from total assets.\n\n\\(\\text{Short term assets} = \\text{Total assets} - \\text{good will and others of questionable value} - \\text{Premises and equipment}\\)\n\nA financial company’s principal liabilities are deposits, Federal funds purchased, trading account liabilities, insurance policy and claims reserves, contract holder funds and short term borrowing. To be consistent with the treatment of interest and an operating expense for financial companies, include long term debt in the short term liability category.\n\nShort term liabilities: Include long term debt.\n\n\\(\\text{Long term debt} = 0\\)\nExcess return period\nThe excess return period is based on a judgment call. The authors of [2] use the 1-5-7-10 rule. They group companies into one of four general categories and excess return periods. They use a 10 year excess return period to calculate what they would consider the maximum value. They use a more conservative 1 year, 5 year or 7 year return period to calculate a more reasonable or minimum value.\n- 1 year: Boring companies that operate in a highly competitive, low margin industry in which they have nothing particular going for them.\n- 5 year: Decent companies that have a recognizable name and decent reputation and perhaps a regulatory benefit (utility company), but can’t control pricing or growth.\n- 7 year: Good companies with good brand names, large companies of scale, good marketing channels and consumer identification (e.g. McDonald’s)\n- 10 year: Great companies with great growth potential, tremendous marketing power, band names and in-place benefits (e.g. Intel, Microsoft, Coca Cola, Disney)\nNotes about negative working capital\nThe company has a negative working capital rate. Negative working capital describes a situation where a company’s current liabilities exceed its current assets as stated on the firm’s balance sheet. In other words, there is more short-term debt than there are short-term assets.\nNegative working capital most often arises when a business generates cash very quickly because it can sell products to its customers before it has to pay the bills to its vendors for the original goods or raw materials. In this way, the company is effectively using the vendor’s money to grow.\nDividend Aristocrat, Achiever & Champion\nThis company was selected for analysis because it is on the Dividend Aristocrat list and passes the quick look tests. This notebook will be used as a template when analyzing other companies.\n\nAristocrat: S&P 500 Dividend Aristocrats is designed to measure the performance of S&P 500 index constituents that have followed a policy of consistently increasing dividends every year for at least 25 consecutive years.\nAchiever: The Broad Dividend Achievers Index. Eligible companies must be incorporated in the U.S. or its territories, trade on the NYSE, NASDAQ or AMEX, and have increased its annual regular dividend payments for the last 10 or more consecutive years.\nhttps://dividendvaluebuilder.com/dividend-achievers-list/\nhttps://www.marketbeat.com/dividends/achievers/\nChampion: This list includes companies that had increased their dividend for at least 25 consecutive years, and includes additional companies that had paid higher dividends without having increased the payout in every calendar year.\nhttps://dividendvaluebuilder.com/dividend-champions-list/\nhttps://www.dividendgrowthinvestor.com/p/dividend-champions-list.html"
  },
  {
    "objectID": "HBI analysis.html#references",
    "href": "HBI analysis.html#references",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "12) References ",
    "text": "12) References \n\nGray, Gary, et al. Streetsmart Guide to Valuing a Stock: the Savvy Investors Key to Beating the Market. McGraw-Hill, 2004.\n\nO’Hara, Thomas E., and Ken Janke. Starting and Running a Profitable Investment Club: the Official Guide from the National Association of Investors Corporation. Times Business, 1998.\n\nRobert G. Hagstrom, The Warren Buffett Way, Wiley, 2013"
  },
  {
    "objectID": "password_card.html",
    "href": "password_card.html",
    "title": "Password card",
    "section": "",
    "text": "last update: 2 Feb 2023"
  },
  {
    "objectID": "password_card.html#abstract",
    "href": "password_card.html#abstract",
    "title": "Password card",
    "section": "Abstract",
    "text": "Abstract\nThis notebook describes the use of a password card and the Python code that generates the password card. The password card is a grid of random letters, numbers and special characters that provides a convenient aid to generate and remember long and complex passwords. A basic description of the password card and instructions are provided along with the Python code used to generate the password card. A security analysis and an analysis of alternatives is provided."
  },
  {
    "objectID": "password_card.html#motivation",
    "href": "password_card.html#motivation",
    "title": "Password card",
    "section": "Motivation",
    "text": "Motivation\nComing up with secure passwords and remembering a unique password for each account is a challenge. The safest way to store your passwords is to memorize them, but for long and complex passwords this is not practical. Passwords that humans can remember and use are guessable. The password card described in this notebook is a paper card that once generated is off-line, an independent record of your passwords and is protected by a secret word that you memorize. In one sense, the password card is a Password manager, but it is not computerized."
  },
  {
    "objectID": "password_card.html#introduction",
    "href": "password_card.html#introduction",
    "title": "Password card",
    "section": "Introduction",
    "text": "Introduction\nThis notebook describes the generation and use of a password card similar to the card called “C@RD”, a laminated card sold by Russtopia Labs for $4.25. The password card provides an easy way to generate and remember long and complex passwords. The version of the card described here has the upper case letters of the alphabet printed on line number one of the card. The card consists of an array of letters, numbers and special characters that can be used as passwords for web pages and can be printed on a credit-card sized piece of paper which can be laminated and carried in your wallet or pocket. A sample card is shown below:\n\nThe first three rows are the upper case letters of the alphabet, the lower case letters, the special characters and numbers used on the card. All the characters used in the passwords are printed here as a reference since for some fonts, some characters are hard to distinguish, for example, l and 1. Rows four through nine are random combinations of the letters of the alphabet, numbers and special characters. The columns of rows four through nine contain the characters used in the passwords and each column contains at least one uppercase letter, one lowercase letter, one digit and one special character in addition to two randomly selected characters, for a total of six characters. The last row on the card is a random sequence of numbers that can be used for numerical PINs.\nThe passwords for your accounts are generated from two parts, the first part of your password comes from the card, the second part is a secret word, committed to memory, consisting of a combination of letters, numbers or special characters that only you know. When combined, the combination yields passwords which are complex and of adequate length for most applications.\nGood passwords have the following characteristics: - They do not contain personal information or common words - They do not contain words or phrases from songs lyrics, poetry, literature etc. - They are long enough to resist brute force guessing attacks from current and future password cracking systems - They are not from a list of previously compromised passwords e.g. names, names with numbers or symbols, words from dictionaries etc. - They do not contain simple substitutions such as @ for a, 5 or $ for s, 1 for i, etc.\nWhich implies that the password should be random, not easily guessed and as a result can be hard to memorize.\n\nUnique passwords for each account\nEach account should have a unique password. If the same password is used for multiple accounts, then all accounts using that same password are vulnerable if your reused password is compromised. This advice is almost universally given by security experts and journalists who write on the subject. Credential stuffing is a type of cyber attack in which the attacker collects stolen account credentials, typically consisting of lists of usernames and/or email addresses and the corresponding passwords (often from a data breach), and then uses the credentials to gain unauthorized access to user accounts on other systems through large-scale automated login requests directed against a web application. Unlike credential cracking, credential stuffing attacks do not attempt to use brute force or guess any passwords – the attacker simply automates the logins for a large number (thousands to millions) of previously discovered credential pairs using standard web automation tools. The best way to protect against credential stuffing is to use unique passwords on accounts.\n\n\nOther password cards\nThe following are links to other types of password cards. - PasswordCard: The user picks a row and column and reads the password in the direction chosen. Could be left, right or diagonal. - Password card generator: You can pick on a row, a column, click on random characters and/or draw images on your Password Card that are easy to remember. - PassCard: similar to PasswordCard above - PasswordWrench: similar to Password card generator above - Qwertycards: the password is a character substitution cipher of the web site’s name.\n\n\nCanadian patent\nThe “C@RD” from Russtopia Labs was designed and patented in Canada and was protected under Canadian patent number 2,895,597. In 2016 and 2019 the patent expired due to failure to respond and failure to pay application maintenance fee."
  },
  {
    "objectID": "password_card.html#password-card-instructions",
    "href": "password_card.html#password-card-instructions",
    "title": "Password card",
    "section": "Password card instructions",
    "text": "Password card instructions\nInstructions for the use of the card are provided by Russtopia Labs and are summarized here.\n\nChoose a secret word or phrase known only to you. This ensures that even if your password card is lost or stolen, no one will know the full password you generate from the card.\nUsing the first two letters of the website or name of the company holding your account, choose the appropriate columns and read downwards in rows four through six. Every column has at least one letter (both upper- and lower-case), a digit and a special character.\n\nThe image below, shows the card with sections highlighted in colored boxes.\n\nPasswords are generated by using the first two letters of the website or company holding your account and then reading down the columns of rows four through six. If the web site is amazon.com, choose columns “A” and “M”, and read the characters from rows four through six. These are highlighted in the black and blue boxes. In column “A” (in blue), the characters are “8@%yP!”, and in column “M” (in black), the characters are “kfoC?2”. If your personal secret word is “apple”, then the full password for your amazon account would be “8@%yP!kfoC?2apple”, which is a complex 17 character password containing upper and lower case letters, digits and special characters. Since you know the procedure, you don’t have to remember the 17 characters, just use the procedure along with your card to reconstruct your password.\n\nRotating passwords\nFor rotating passwords, add two characters from rows four and five of numbered columns in row 3, (01=Jan), (02=Feb), for each month. (eg., in row four, two characters in column for 1st month digit, then two characters in column for 2nd month digit, February=02, under 0 in row four, read “5e”. Then under 2 in row five, read “dA” and prefix to password. The resulting password would be “5edA” plus the remaining base password, “5edA8#&yP!kfoC?2apple”. In three months or whenever the password expires, use the month to generate a new prefix. This is the procedure outlined by Russtopia Labs.\nSome sites will check to see if the new password is sufficiently different than, say for example, the last ten passwords used. If the passwords are stored as a hashed value, then any difference, even one character difference would be sufficient. But if the prior passwords are stored as plain text or in an encrypted file, the system could check the degree of difference between old passwords and the new one. So if the site is enforcing a new password that is different from your old password by some amount of characters, then the following procedure can be used.\nAt the beginning of the year, in January for example, if a new password is required, construct the new password as follows: - The months are numbered from 1 to C as shown in the table below and the characters are from the column under that month number or letter. So in January, month number 1, use the characters from column 1 which are: 3bXz%f. - The next characters are from column A and M (for Amazon), but only three characters from each column are used: A = 8@% and M = kfo. - Then append your secret word.\n\n\n\nmonth\ncol #\ncol chars\nA col\nM col\nsecret word\n\n\n\n\nJan\n1\n3bXz%f\n8@%\nkfo\napple\n\n\nFeb\n2\ndAk)$1\n8@%\nkfo\napple\n\n\nMar\n3\n?y(1?J\n8@%\nkfo\napple\n\n\nApr\n4\n*-1@Ui\n8@%\nkfo\napple\n\n\nMay\n5\n66!A$b\n8@%\nkfo\napple\n\n\nJun\n6\n&jcPQ6\n8@%\nkfo\napple\n\n\nJul\n7\nmrc3B#\n8@%\nkfo\napple\n\n\nAug\n8\n&gt;&lt;^nJ7\n8@%\nkfo\napple\n\n\nSep\n9\n5DktW&lt;\n8@%\nkfo\napple\n\n\nOct\nA\n8@%yP!\n8@%\nkfo\napple\n\n\nNov\nB\nX4Mz+b\n8@%\nkfo\napple\n\n\nDec\nC\nax!p2M\n8@%\nkfo\napple\n\n\n\nA new password for January would be: 3bXz%f8@%kfoapple. In April, if the password has to be changed, it would be: *-1@Ui8@%kfoapple. You only need to remember in which month you created the new password.\n\n\nNumeric PIN\nFor a numeric PIN, pick a letter and start with that number in the random number row. For example, if your bank is Commerce Bank, under the letter “C” in the first row, the PIN numbers are “141845” (red highlight). You can use four numbers if you want.\nFor sites that restrict the number of characters in the password, choose a 4-letter secret word and only read the first two or three symbols from each column in Step 2) above. This will give you an 8 or 10 character password to meet password restrictions of old systems.\n\n\nRestricted characters\nFor sites with restrict the use of special characters, read the columns as usual but each time you encounter a prohibited special character, use a letter instead (for example, “&lt;” becomes “n”, “&gt;” becomes “o” etc.) which are the letters in row two directly above the special characters in row three.\nIf the website or account has repeated letters, for example “aa.com”, this would result in using the same column “a” twice, which according to the original procedure is acceptable. However if you feel uncomfortable using a password with any repeated sequences, shift to the left or right adjacent column for the second letter – in essence treating “aa” as “ab” (second “a” becomes “b”).\nTypically, a user would keep a list of web page names and the corresponding user name, web site letter code and notes. The user would refer to this list for websites that are not often used or have restricted characters. The user would refer to the list until it became committed to memory.\n\n\nSecret word selection\nThe purpose of the secret word is to keep at least part of your password secret in the event that someone finds your password card. The secret word part is memorized and not written down anywhere. In the examples above, the secret word used was “apple”. You should use something different.\nThere are about 255 thousand defined words in the Merriam-Webster’s Collegiate Dictionary English dictionary, choosing one random word out of a dictionary provides about 17 bits of entropy. Your secret word should have at least 30 bits of entropy, in order to prove a modest level of security. One random word would provide a bit of security that might keep a nosy family member out of your email account if they “borrowed” your password card. Two random words concatenated would provide about 35 bits of entropy, enough to keep even the most determined family member out of your email account. For example the two words, “program” and “blind” could be concatenated to generate a secret passphrase, “programblind”.\nMultiple random words concatenated is called a passphrase. The more words that are in the passphrase, the more secure it is, but harder to memorize. To prevent personal bias from affecting your choice of words you could use dice to aid in choosing words from the dictionary. Roll some dice and choose a dictionary page number based on the dice rolls. Roll the dice some more and choose a word on the page. Repeat until you have the number of words you want. One way to harden short passphrases is to mangle the passphase, for example “programblind” could be mangled to be “pRogram=bli2nd”. Not impossible to memorize, but nearly impossible to guess. With the password card, “pRogram=bli2nd” is the only part you need to commit to memory."
  },
  {
    "objectID": "password_card.html#python-code",
    "href": "password_card.html#python-code",
    "title": "Password card",
    "section": "Python Code",
    "text": "Python Code\nThis section of the notebook contains the Python code used to generate the password card. The Python library Secrets is used to randomly choose elements from the character set. The secrets module is used for generating cryptographically strong random numbers suitable for managing data such as passwords, account authentication, security tokens, and related secrets.\nThe Python code described below can be used to generate a unique card for each person.\n\nimport math\nimport sys\nimport re\nimport hashlib\nimport secrets\ngen = secrets.SystemRandom()\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n\nfrom sympy.solvers import solve\nfrom sympy import Symbol\n\n\nCharacter set\nThe following characters are used in the password card. Upper and lowercase letters of the alphabet, the digits 0 to 9, as well as the so-called special characters are used. A special character is one that is not considered a number or letter. Special characters are often required when creating a strong password. This is because they add complexity to the password. There are 33 characters classified as ASCII punctuation and symbols are also sometimes referred to as ASCII special characters.\nThe special characters used in this notebook are: !@#$%^&*()-+=&lt;&gt;?\nHowever, other characters from the keys on a typical US standard keyboard could also be used: ~!@#$%^&*()_+-=,./&lt;&gt;?;’:“[]{}|\nAny of these could be easily substituted to give the password card additional variation. Up to 16 special characters will fit into the scheme.\n\nlower = 'abcdefghijklmnopqrstuvwxyz'\nupper = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\ndigits = '0123456789'\nspecial = '!@#$%^&*()-+=&lt;&gt;?'\nchar_set_size = len(upper)+len(lower)+len(digits)+len(special)\nprint('character set size: {:d}'.format(char_set_size))\n\ncharacter set size: 78\n\n\n\n\nSet the number of rows\nThe number of rows in the random part of the password card is set to 6, as in the original card. Variations of the password card could be made with a different number of rows.\n\nnum_rows = 6\n\n\n\nPrint character sets as first three lines of the card\nThis provides a visual reference since in some fonts characters l and 1, and 0 and O, are hard to tell apart.\n\n# print character sets as first three lines of the card\nprint(upper)\nprint(lower)\nprint(special+digits)\n\nABCDEFGHIJKLMNOPQRSTUVWXYZ\nabcdefghijklmnopqrstuvwxyz\n!@#$%^&*()-+=&lt;&gt;?0123456789\n\n\n\n\nGenerate the password random characters\nEvery column has one upper, one lower, one digit and one special character, followed by two random characters from the complete set. And when two columns are used, the password has at least two lower case, two upper case, two digits and two special characters.\n\nA=[]    #make empty list for random characters\nfor j in range(26):\n    temp = '' # make an empty character string\n    temp += secrets.choice(lower)\n    temp += secrets.choice(upper)\n    temp += secrets.choice(digits)    \n    temp += secrets.choice(special)      \n    # add more random characters\n    for i in range(num_rows-4):\n        temp += secrets.choice((lower + digits + upper + special))  \n    # take the list and randomize the order\n    temp = secrets.SystemRandom.sample(gen, temp, num_rows)\n    A.append(temp)\n\n#printing the rows of the password card\nfor i in range(num_rows):\n    for j in range(len(lower)):\n        print(A[j][i],end='') # end='' does not print new line for each call to print\n    print()\n\nR*^ahl4G1abF@3cfG&V2i$3y65\n8q3R&gt;D&-0R?#2L@Ve*X&lt;qo&gt;22J\n4%V^W24uv1)4@uKP1Pp16YKKZu\n&7Pv1Yl6%JY2V%3(&gt;24i$?hH9t\no7czU(E0ZDUx&lt;G@8own8T0q1w0\nBE94v5Rw6%0DiAMF?K#BZ5W?@?\n\n\nThe block of text can be copied into a document to save the results. Every time the code above is run, a new block of random characters is generated.\n\n\nRandom row of digits\nThe last row in the password card is a row of random digits which can be used for PIN’s.\n\n#print a row of random numbers\ntemp = ''\nfor i in range(26):\n    temp += secrets.choice(digits)\nprint(temp)\n\n71073634961270785886390837\n\n\nCopy the rows of the password card to a word document for formatting and printing. The card works best if every other column is highlighted to make reading easier.\nABCDEFGHIJKLMNOPQRSTUVWXYZ\nabcdefghijklmnopqrstuvwxyz\n!@#$%^&*()-+=&lt;&gt;?0123456789\n)0A4+^ZYwAk17%21?33F(NOo94\n72!5dFZzgCN^t9@oqiISqlq1Da\njDbVqD3N-4&lt;yiInG!c-0ddW$rR\nU!8!U5c0X&gt;soys*)*Zx?M-?I?#\nbE7l7m$b9o0E&%OoFVWh$fwI%@\nEvyR-&lt;f&gt;?)qQD*hW2)V9457$@N\n85640243163554984484266573\n\n\nSecret word\nThe secret word is the part of your password that you commit to memory. It doesn’t change, you only have to memorize this part. In the example the secret word is “apple”, but as discussed above is not sufficient. The following code uses simulated dice rolls to choose two random words from a list such as list of English words (479k words) to construct a pass phrase and then mangles the passphrase to defend against dictionary attacks.\nThe word list file is read in and filtered by length. Words with spaces, all caps and punctuation are also removed from the list.\n\nwords = []\nmin_length = 5\nmax_length = 9\nvalid_chars = '.'\nregexp = re.compile(\"^{0}{{{1},{2}}}$\".format(valid_chars,min_length,max_length))\n\n# read words from file into wordlist\nwith open('wordlist.txt') as wlf:\n    for line in wlf:\n        thisword = line.strip()\n        if regexp.match(thisword) is not None:\n            words.append(thisword)\n\n# remove words with spaces, all caps, and words with punctuation\ntemp_list = []\nfor i in words:\n    if not i.isupper() and i.isalpha():\n        temp_list.append(i)\n\nThe words in the list are shuffled since the selection later is somewhat Gaussian and would tend to pull words from the middle of the list.\n\nwordlist = []\nwhile len(temp_list) &gt; 0:\n    picked_item = secrets.choice(temp_list)\n    wordlist.append(picked_item)\n    temp_list.remove(picked_item)\n\n\n# number of bits to use in the index\nnum_bits = 15\n# check to see if word list is long enough\nlen(wordlist) &gt; 2**num_bits\n\nTrue\n\n\nThe fill list is trimmed so that the length is equal to \\(2^{\\text{number of bits}}\\)\n\nabridged_wordlist = wordlist[0:2**num_bits]\nprint('number of words in the abridged word list: {:,.0f}'.format(len(abridged_wordlist)))\n\nnumber of words in the abridged word list: 32,768\n\n\nThe first 150 words in the abridge_wordlist are:\n\ni=0\nfor j in range(10):\n    for k in range(15):\n        print(abridged_wordlist[i]+' ',end='')\n        i += 1\n    print()\n\nhashish finch backboard Penates respected coryza forge tillable spoke ripost bocci cycling cicatrice grasping sackful \ncoveralls incense nerve Dempsey magnetise notchback niece doubly nobly promenade perfervid debit Burnett Paleozoic syndrome \nprecursor occiput fishhook Hitlerian superman seconds mayfly lightener reveille conga celled geyser diverse midsize lunkhead \nventricle devoted screed detest starless cosign tactility sturdy caliph Dwaine spearmint smudgy thrift philter adore \ninebriety addictive tensely Croatian printout waxwork frostily sweat Nikolayev paleness tinker creosote offish Dotty ferryboat \nwarped snoopily roughhewn Graham bodacious rancour gazpacho Wankel bamboozle staggers souvenir sobering fiver incarnate pickling \nsinger unweave cellaret Sheridan wiggler exact beverage august insomuch gorgeous sweating looniness inciter affirm reflexly \nschwa picot boozing whitebait fistfight edacity unfilled exemplify ambler Lavern teaching guide battery greenness valiantly \nscamperer adoptee Everest backcourt oxygenous Colombia purposely Myrtle lighted benzoate volcanism alveolus surrey centigram Okinawa \ningenuous gradient striving regicide solidus setback anarchist yardman dollish decillion Fritz Wabash uncombed Croce unloader \n\n\nAs shown above the words in the abridge list are not in alphabetical order.\n\nprint('the abridged list includes words from \"{:s}\" to \"{:s}\"'.format(min(abridged_wordlist),max(abridged_wordlist)))\n\nthe abridged list includes words from \"Aalborg\" to \"zymurgy\"\n\n\nThe following code simulates dice rolls. Actual dice could be used and the results entered into the code below.\n\nnum_rolls = 50\nprint('{:d} dice rolls have an entropy of: {:.0f} bits'.format(num_rolls, math.log2(6**num_rolls)))\n\n50 dice rolls have an entropy of: 129 bits\n\n\n\n# generate n dice rolls\nrd_str = ''\nfor i in range(num_rolls):\n    rd_str += secrets.choice('123456')\n\nprint('the string of {:d} dice rolls: {:s}'.format(num_rolls,rd_str))\n\nthe string of 50 dice rolls: 25666355551553246453252641525155213415543612634242\n\n\nPhysical dice can be rolled and the digits copied into the string variable.\n\nDice value frequency\nThe following graph checks the dice value frequency, it should be somewhat uniform and there should be no missing digits. This is a useful graph if you are rolling physical dice and entering the results into the program.\n\n# using collections.Counter() to get a count of the occurrence of each digit in string  \nmgram = Counter(rd_str) \n\nsorted_dict = {key: value for key, value in sorted(mgram.items())}\n\nplt.bar(range(len(sorted_dict)), sorted_dict.values(), align='center')\nplt.xticks(range(len(sorted_dict)), list(sorted_dict.keys()))\nplt.show()\n\n\n\n\nUsing the above graph to check the distribution of digits, which should appear somewhat uniform and at least there should be no missing digits. The list of dice rolls is converted into an integer by hashing the string, that way all the dice rolls are used.\n\nm = hashlib.sha256()\nm.update(rd_str.encode('utf-8'))\nprint('the hashed value of the dice roll string: {:s}'.format(m.hexdigest()))\n\nthe hashed value of the dice roll string: 77733b54a98eca78d68e5a68ddd309e7b5aa716b76f8202410f27d460a76e171\n\n\n\nrn_int = int(m.hexdigest(),base=16)\nprint('convert the hashed value into an integer: {:d}'.format(rn_int))\nprint('length of digit string: {:d} characters'.format(len(str(rn_int))))\n\nconvert the hashed value into an integer: 54028825879384713162013971062835122052298057344052197559299222554916219707761\nlength of digit string: 77 characters\n\n\n\n\nDigit frequency\nThe following graph displays the digit frequency of the converted integer.\n\n# using collections.Counter() to get a count of the occurrence of each digit in string  \nmgram = Counter('{:d}'.format(rn_int)) \n\nsorted_dict = {key: value for key, value in sorted(mgram.items())}\n\nplt.bar(range(len(sorted_dict)), sorted_dict.values(), align='center')\nplt.xticks(range(len(sorted_dict)), list(sorted_dict.keys()))\nplt.show()\n\n\n\n\n\nprint('number of bits in the number: {:d}'.format(len('{:b}'.format(rn_int))))\n\nnumber of bits in the number: 255\n\n\n\n# number of words to use in the pass phrase\nnum_words = 2\n\nThe words in the word list are indexed by taking 15 bits at a time from the number generated from the hashed dice string.\n\nprint('number of words that can be indexed with hashed value: {:.0f}'.format(len('{:b}'.format(rn_int))/num_bits))\n\nnumber of words that can be indexed with hashed value: 17\n\n\n\npass_phrase = ''\nfor i in range(num_words):\n    pass_phrase += abridged_wordlist[(rn_int &gt;&gt; num_bits*i) & 2**num_bits-1]\n    print('{:2d}: {:5d} = {:s}'.format(i+1, (rn_int &gt;&gt; num_bits*i) & 2**num_bits-1, abridged_wordlist[(rn_int &gt;&gt; num_bits*i) & 2**num_bits-1]) )\nprint(pass_phrase)\n\n 1: 24945 = apologize\n 2:  5357 = modishly\napologizemodishly\n\n\nMangle the passphrase to defend against dictionary attacks.\nCheck for any caps, if not add one in a random place\n\nif pass_phrase.islower():\n    i = secrets.randbelow(len(pass_phrase))\n    pass_phrase = pass_phrase[:i] + pass_phrase[i].upper() + pass_phrase[i+1:]\npass_phrase\n\n'aPologizemodishly'\n\n\nInsert random special and random digit into the passphrase.\n\ni = secrets.randbelow(len(pass_phrase))+1\nmangled_pass_phrase = pass_phrase[:i] + secrets.choice(special+digits) + pass_phrase[i:]\ni = secrets.randbelow(len(mangled_pass_phrase))+1\nmangled_pass_phrase = mangled_pass_phrase[:i] + secrets.choice(special+digits) + mangled_pass_phrase[i:]\nmangled_pass_phrase\n\n'aPo&gt;lo%gizemodishly'\n\n\n\nprint('length of mangled passphrase {:d}'.format(len(mangled_pass_phrase)))\n\nlength of mangled passphrase 19\n\n\nEvery time the code is run, a new selection of words is made. On a previous run, the randomly chosen words were:\n1: 16611 = avowedly\n2: 5383 = hesitance\navowedlyhesitance\nOne generated example: ‘a@vowedlyhesitAn0ce’\nlength of mangled passphrase 19\nThis passphrase of 19 characters, is two random words and has a random capital letter, symbol and number inserted into the string of letters at random positions. The particular pass phrase “a@vowedlyhesitAn0ce” is but one instance in a vast pool of possible passphrases built with the algorithm. The hacker would need to try a vast number of three word combinations with a random capital letter, symbol and number inserted into the string of letters at random positions."
  },
  {
    "objectID": "password_card.html#implementation-and-use-of-the-password-card",
    "href": "password_card.html#implementation-and-use-of-the-password-card",
    "title": "Password card",
    "section": "Implementation and use of the password card",
    "text": "Implementation and use of the password card\nMy recommendation is not to use the password card for financial, email accounts or highly used social media accounts. These accounts, which are high value, and which I’ll call Tier 1 accounts, should have 16 to 20 character random character strings used as passwords. Tier 1 passwords should be written in a password book and kept secure. Furthermore, a dedicated computer should be used for financial Tier 1 accounts, not one that has been used for general web surfing and might have become infected from daily contact with the internet.\nGenerating the password card requires a computer and the ability to run the Python code shown in this notebook. The computer has most likely been connected to the internet at some time or is currently connected to the internet. This may not be acceptable if you have a high level of paranoia. Most people would be OK with generating, printing the card and then deleting or encrypting the backing up the files. A password card can be printed by copying the grid of characters to a word document for formatting and printing. Highlight alternate columns and font to Courier New size 12 for printing and lamination. Making an encrypted copy of the password card word document would probably be OK for most people. The printer and home computer are behind a locked door and the generation and use of the password card can be kept relatively private. After the card is printed, it could be laminated to make it more durable. You can find instructions for laminating cards online by using a household iron.\nThe secret phrase used with the password card should be written down somewhere until it becomes committed to memory. Additionally, 2FA should be enabled wherever it is available."
  },
  {
    "objectID": "password_card.html#security-analysis",
    "href": "password_card.html#security-analysis",
    "title": "Password card",
    "section": "Security analysis",
    "text": "Security analysis\nThe purpose of this section is to perform an analysis of the password card and look at the security properties. A hacker will try to exploit exploit weaknesses found in the design or usage of the password card. Two common analysis techniques are black-box and white-box analysis.\nA black-box analysis is an attack that works independently of the password algorithm. The most common black-box analysis is the brute-force search, where all known combinations are tried. A modified version is to try weak or short passwords, rather than an exhaustive search.\nThe opposite of a black-box analysis is a system where the inner components or logic are available for inspection. This is commonly referred to as a white-box or glass-box. This looks at the analysis from the point of view that the attacker knows how the algorithm works.\nThe security analysis will look at both points of view, with zero knowledge and complete knowledge.\n\nAnalysis of password card random characters\nAs described above, the secrets library can be used to generate the password’s random characters. The selection of letters, numbers and special characters is cryptographically random. Variations in the character set usually involve choosing the special characters. In this notebook, the following character sets are used.\n\n\n\ncharacter set\n\n\n\n\n\nlower\nabcdefghijklmnopqrstuvwxyz\n\n\nupper\nABCDEFGHIJKLMNOPQRSTUVWXYZ\n\n\ndigits\n0123456789\n\n\nspecial\n!@#$%^&*()-+=&lt;&gt;?\n\n\n\nThe random part of the password card contains a random selection of: 2 upper + 2 lower + 2 digits + 2 special + 4 random, the number of combinations is:\n\n# password card combinations\npwc_combinations = len(lower)**2 * len(upper)**2 * len(digits)**2 * len(special)**2 * char_set_size**4\nnum_of_chars = 12\nprint('possible combinations of {:d} characters in the password card is {:,.3e}'.format(num_of_chars, pwc_combinations))\n\npossible combinations of 12 characters in the password card is 4.330e+17\n\n\nChoosing the 12 random characters in the way that is used to generate the password card limits the number of possible combinations.\nAn adversary could attack the first six characters of the password since the first six characters only contain one upper case, one lower case, one special and two random characters. This was done to satisfy restrictive password policies. The second six characters are constructed similarly. This means that the password card will have at most three uppercase characters in the first group. Similarly, the same goes for lower case, digits and specials. This limitation can be exploited and reduce the search space. The code generating the password card will include at least one character from each group, but as a consequence, strings of five digits are excluded. This means that not all possible combinations are included in the password.\nThe calculation below shows that the number of combinations of 12 characters, if all combinations are allowed, is about five orders of magnitude larger than available on the password card.\n\nnum_of_chars = 12\nprint('combinations of {:d} characters from a character set of {:d} is {:,.3e}'.format(num_of_chars,\n    char_set_size,char_set_size**num_of_chars))\n\ncombinations of 12 characters from a character set of 78 is 5.071e+22\n\n\n\n\nAnalysis of secret word space\nThe secret word, since it consists of two words concatenated, is called a passphrase. There are 32,768 five to nine character long words in the abridged word list. Most people will tend to pick shorter words or familiar words. Using dice will prevent this tendency from happening and being exploited.\n\nwordlist_size = len(abridged_wordlist)\nprint('The number of combinations of two random words from the word list is {:,.3e}'.format(wordlist_size**num_words))\n\nThe number of combinations of two random words from the word list is 1.074e+09\n\n\nAdditionally the passphrase is mangled. The passphrase is two random words of minimum length of 5 characters, with word mangling, therefore not directly in any dictionary. The word mangling is obtained by randomly converting one character to uppercase, if there is not one already and inserting a random digit and special character into the passphrase. This will force the password cracker to exhaustively search the entire space of possibilities, since no substitution rules were followed.\nTo calculate the size of the passphrase space, we need to consider how many items there are. - There are two random words taken from a list of \\(2^{15}\\) words - There are two random characters inserted from a list of 26 digits and special characters - One character is converted to an uppercase if there is not already a upper case\n\nprint('number of items: {:,d}'.format((wordlist_size)**num_words + len(special) + len(digits)))\n\nnumber of items: 1,073,741,850\n\n\nThere are two words and two special or digits selected from the number of items.\n\nsecretword_space = (len(abridged_wordlist)**num_words + len(special) + len(digits))**(num_words+1+1)\nprint('secret word space: {:.3e}'.format(secretword_space))\n\nsecret word space: 1.329e+36\n\n\nCan also look at secret word space from the point of view as if it were 12 to 20 random characters.\n\nprint('Min secret word space, if considered as random chars: {:.3e}'.format(char_set_size**12))\nprint('Max secret word space, if considered as random chars: {:.3e}'.format(char_set_size**20))\n\nMin secret word space, if considered as random chars: 5.071e+22\nMax secret word space, if considered as random chars: 6.949e+37\n\n\nSeems like calculating secret word space based on the way the secret word is built should generate a space between the limits.\n\n\nAnalysis of password card PIN\nThe PIN digits are not protected by any additional secrets. Someone who has access to your password card can read your PIN’s directly. For example, if you lose your wallet containing your ATM card and your password card, someone can use the card to access your funds. To defend against this, you could offset the PIN numbers used either by physical position or by adding modulo 10 some secret offset. In other words, actual PIN’s are shifted left or right by some position or the numbers offset in value by some amount; or maybe both."
  },
  {
    "objectID": "password_card.html#threat-model",
    "href": "password_card.html#threat-model",
    "title": "Password card",
    "section": "Threat model",
    "text": "Threat model\nIn this section attacks against passwords will be examined. A password or passphrase is typically a string of characters or a list or words, that can be random and is a secret shared between the individual and the computer system that confirms their identity.\nThreat modeling is a risk analysis exercise where potential threats and mitigations are identified. In this notebook I’ll be looking at threats directly made on the password itself.\n\nAttacks against passwords\nPhishing, Man-in-the-Middle and Key-Logging attacks attempt to steal the password as does the so-called “$5 wrench attack”. Because these attacks do not exploit a weakness in the password itself, they are not covered in this analysis. Mitigations against password cracking threats will be discussed.\nCredential Stuffing: Credential based attacks occur when attackers use previously cracked passwords at many different sites, looking for passwords that have been re-used.\nDictionary attacks: Password lists, Dictionary, word mangling: These attacks occur when attackers use dictionary lists of known cracked passwords, dictionary words and word mangling to attempt to guess passwords.\nBrute force: This attack involves trying all combinations of characters up to some limit, looking for short passwords.\n\n\nExamples of bad and compromised passwords\nExamples of bad and compromised passwords are: Youknow123, drowssap, My2password, Qwerty12345@, StephenASmith1, Andrew24, ZaqXsw12, Johnny#12345, P@55w0rd, Jp1234567890, abdcefg, password, monkey, 123456, password, qwerty, football, baseball, welcome, abc123, 1qaz2wsx, dragon, master, monkey, letmein, login, princess, qwertyuiop, solo, passw0rd, starwars\nMost systems enforce some level of password complexity, for example: Passwords need characters from all three of the following categories: - Uppercase characters - Lowercase characters - Non-alphanumeric characters\nMost people use similar patterns (i.e. capital letter in the first position, a symbol in the last, and a number in the last 2). Password crackers know this, so they run their dictionary attacks using the common substitutions, such as “$” for “s”, “@” for “a,” “1” for “l” and so on. See the following article for password audit test.\n\n\nCounter measures\nThe counter measures that can be used to strengthen passwords are those that limit attacks to brute force attacks. Using unique passwords for each login prevents compromised passwords from being used against other sites. Using passwords that are long and complex should be used. Dictionary words, combinations of words or names should not be used. Typical word mangling, letter substitutions pre and post pending of special characters or numbers are well known techniques for obfuscating dictionary words or names and should not be used.\n\n\nOther mitigation\nMulti-factor authentication (MFA) is an additional verification step used to gain access to an online account. Typically, secret questions, codes sent to your phone or hardware tokens are used as the second factor. MFA increases security because if one credential becomes compromised, unauthorized users will be unable to meet the second authentication requirement and will not be able to access the targeted physical space, computing device, network, or database. MFA should be enabled whenever it is supported. Web pages typically ask for MFA when an attempted login is from an unrecognized device or browser."
  },
  {
    "objectID": "password_card.html#analysis-of-alternatives",
    "href": "password_card.html#analysis-of-alternatives",
    "title": "Password card",
    "section": "Analysis of alternatives",
    "text": "Analysis of alternatives\nThere are several password management schemes that are typically used. Assuming that unique passwords are being used for each login, there might be about 100 passwords that a typical user would need to keep track of. Three password management schemes are considered. - Password book: This is a handwritten password list in notebook, perhaps organized alphabetically in a binder. Use some sort of offline method and create good passwords. Keep the notebook in a secure place. Pros and cons of a computer based password manager verus a password book are discussed here. A password book can be purchased here. Backups of a password notebook can be made by photocopying (or scanning) the pages and keeping the backup in a safe deposit box. Other than banks and other financial institutions, most web sites will put a cookie on your computer and allow you to re-visit the site without re-logins. - Password card: A password card is also a paper solution, and one suitable to carry on a daily basis. A Password Card is a credit card-sized card which lets you pick very secure passwords, without having to remember them. Backups of the password card can be a photocopy which is kept in a secure location. - Computer based password manager: There are various options available and this is a big topic of discussion online. Some type of password manager is appropriate for most people and might require a subscription. Computer based password manager requires a very strong master password because it protects all your other passwords. - Combination of alternatives: A combination of a Password book, Password card and Computer based password manager can also be used.\nBasically the options are paper based and computer based. Paper based solutions are only as secure as the storage location of the password book. A password book locked in your house at home and its existence is not known is pretty secure because no one is actively looking for such a book. Needless to say, you should not be carrying around your password book in your backpack with the words “passwords” on the front of it. A password book could be used to record all your passwords of which only a few are needed on a daily basis and the password card could be used for those needed more frequently or when away from home.\nWeb based passwords managers can be hacked. For example, Lastpass reported on December 22, 2022, that a threat actor was able to copy a backup of customer vault data from the encrypted storage container. The encrypted fields remain secured with 256-bit AES encryption and can only be decrypted with a unique encryption key derived from each user’s master password. Web based password managers are not recommended for financial or email accounts, mainly for two reasons. 1) They represent an attractive target with large payoff potential. 2) It breaks the don’t put all your eggs in one basket rule.\nWhen you use a password manager like LastPass or 1Password, it stores a list containing all of the user names and passwords for the sites and apps you use, including banking, health care, email and social networking accounts. It keeps track of that list, called the vault, in its online cloud so you have easy access to your passwords from any device. LastPass said hackers had stolen copies of the list of usernames and passwords of every customer from the company’s servers.\nMost web browsers offer at least a rudimentary password manager. This is better than reusing the same password everywhere, but browser-based password managers are limited. In recent years Google has improved the password manager built into Chrome.\nComputer based password managers like Bitwarden offer a self hosted soultion in addition to the normal cloud based storage.\nCloud based password managers are targets for attack. See Bitwarden password vaults targeted in Google ads phishing attack which describes how Bitwarden and other password managers are being targeted in Google ads phishing campaigns to steal users’ password vault credentials. Most password managers are cloud-based, allowing users to access their passwords through websites and mobile apps. These passwords are stored in the cloud in “password vaults” that keep the data in an encrypted format, usually encrypted using users’ master passwords. Recent security breaches at LastPass and credential stuffing attacks at Norton have illustrated that a master password is a weak point for a password vault.\nFor this reason, threat actors have been spotted creating phishing pages that target your password vault’s login credentials, potentially authentication cookies, as once they gain access to these, they have full access to your vault. Typosquatting also called URL hijacking, a sting site, or a fake URL, is a form of cybersquatting, and possibly brandjacking which relies on mistakes such as typos made by Internet users when inputting a website address into a web browser. Should a user accidentally enter an incorrect website address, they may be led to any URL (including an alternative website owned by a cybersquatter).\nThe typosquatter’s URL will usually be one of five kinds, all similar to the victim site address: - A common misspelling, or foreign language spelling, of the intended site - A misspelling based on a typographical error - A plural of a singular domain name - A different top-level domain: (i.e. .com instead of .org) - An abuse of the Country Code Top-Level Domain (ccTLD) (.cm, .co, or .om instead of .com)\nOnce in the typosquatter’s site, the user may also be tricked into thinking that they are in fact in the real site, through the use of copied or similar logos, website layouts, or content.\nSpam emails sometimes make use of typosquatting URLs to trick users into visiting malicious sites that look like a given bank’s site, for instance.\nThe domain used in the ad was ‘appbitwarden.com’ and, when clicked, redirected users to the site ‘bitwardenlogin.com.’ The page at ‘bitwardenlogin.com’ was an exact replica of the legitimate Bitwarden Web Vault login page.\nUsers should always configure Multi-factor authentication in the password manager. In case credentials are inadvertently entered into a phishing site, your multi-factor authentication should prevent a total breach. Unfortunately, even with MFA protection, your accounts can still be vulnerable to advanced adversary-in-the-middle (AiTM) phishing attacks. AiTM phishing attacks are when threat actors utilize specialized toolkits like Evilginx2, Modlishka, and Muraena to create phishing landing pages that proxy to legitimate login forms at a targeted service. Using this method, visitors to the phishing page will see a legitimate service’s login form, such as Microsoft 365. When they enter their credentials and MFA verification codes, this information is also relayed to the actual site. Once a user logs in and the legitimate site sends the MFA-backed session cookie, the phishing toolkit can steal these tokens for later use.\nKeepassX is a password manager that is offline.\nA master password is required for a password manager. The master password for a password manager is similar to having just one password, since with the master password all other passwords in the vault can be obtained. The Password Manager will encrypt the user’s password vault with something like AES-256 using a key derived from the master password. But if the master password is weak, then the derived 256 bit key does not provide a AES-256 level of protection.\nA cloud based password manager requires a very strong master password, especially if all your sensitive account credentials are protected by the password manager. A 256 bit AES key derived from a process that starts with 90 equivalent bits of entropy is not 256 bits strong, but is only 90 bits strong. For example a pass phrase of four random words taken from a word list of 255,000 words, provides an equivalent password entropy of 72 bits.\n\npass_phrase_len = 4\nword_list_len = 255e3\nprint('number of passphrases: {:.3e} with {:d} words'.format(word_list_len**pass_phrase_len,pass_phrase_len))\nprint('equivalent entropy of {:.0f} bits from using {:d} words'.format(math.log2(word_list_len**pass_phrase_len),pass_phrase_len))\n\nnumber of passphrases: 4.228e+21 with 4 words\nequivalent entropy of 72 bits from using 4 words\n\n\nSuch a password could be cracked in short order.\n\nE = round(math.log2(word_list_len**pass_phrase_len))\nprint('{:,.0f} min to evaluate 1% of combinations from {:d} bits of entropy'.format(0.01*((2**E)/pw_eval_per_hour*60),E ))\nprint('which is equivalent to a password of length {:,.1f} characters'.format(solve((char_set_size**L)-2**E, L)[0].evalf()))\n\n39 min to evaluate 1% of combinations from 72 bits of entropy\nwhich is equivalent to a password of length 11.5 characters\n\n\nIf the 256 bit encryption key is derived from a short pass phrase, the effective key length is 72 bits and not 256 bits.\nIf the master password word is 16 random characters of gibberish, the equivalent key length is 101 bits.\n\n# how many characters in a jibberish password?\nnum_of_chars = 16\nchar_set_size = len('abcdefghijklmnopqrstuvwxyz'+'ABCDEFGHIJKLMNOPQRSTUVWXYZ'+'0123456789'+'!@#\\$%^&*()-+=&lt;&gt;?')\nE = round(math.log2(char_set_size**num_of_chars))\nprint('equivalent entropy of {:.0f} bits from {:d} random characters'.format(E,num_of_chars))\nprint('{:,.0f} years to evaluate 1% of combinations from {:d} bits of entropy'.format(0.01*((2**E)/pw_eval_per_hour)/24/365,E ))\n\nequivalent entropy of 101 bits from 16 random characters\n40,197 years to evaluate 1% of combinations from 101 bits of entropy\n\n\nYou need 21 random characters or 8 random words to exceed the entropy of a 128 bit key."
  },
  {
    "objectID": "password_card.html#conclusion",
    "href": "password_card.html#conclusion",
    "title": "Password card",
    "section": "Conclusion",
    "text": "Conclusion\nThe password card is secure when coupled with a 15 character long mangled passphrase. The 15 character long mangled passphrase is the only part that needs to be memorized. Using the procedure outlined, a unique and strong password can be generated for each account. The password card provides a convenient and portable way to generate and use unique passwords."
  },
  {
    "objectID": "password_card.html#revision-history",
    "href": "password_card.html#revision-history",
    "title": "Password card",
    "section": "Revision History",
    "text": "Revision History\n\n10/10/2015: Ver 1 - coding started\n20 Nov 2022: Python program converted to this IPython notebook"
  },
  {
    "objectID": "password_card.html#copyright",
    "href": "password_card.html#copyright",
    "title": "Password card",
    "section": "Copyright",
    "text": "Copyright\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "Two amplifier RIAA Phono Preamp.html",
    "href": "Two amplifier RIAA Phono Preamp.html",
    "title": "Two amplifier RIAA phono preamp",
    "section": "",
    "text": "Photo of a phono stylus playing a record (photo taken from Holland Koningsdam, hallway, deck 7, midship)"
  },
  {
    "objectID": "Two amplifier RIAA Phono Preamp.html#abstract",
    "href": "Two amplifier RIAA Phono Preamp.html#abstract",
    "title": "Two amplifier RIAA phono preamp",
    "section": "Abstract",
    "text": "Abstract\nThis paper analyizes the circuit for the RIAA preamp given in the Texas Instruments application note AN346, High-Performance Audio Applications of the LM833, shown in the app note as Figure 3. The schematic for the phono preamplifier was entered into LTSpice and the circuit net list was generated. A circuit analysis method called the Modified Nodal Analysis was used to derive the symbolic circuit equations and Python libraries were used to solve the equations. The preamplifier transfer function was used to calculate the Bode, impuse and step response plots. The Python results were compared to those from LTSpice. Deviation from the RIAA response curve was also examined. The sensitivity, Monte Carlo and worst case analysis for the preamplifier circuit was performed. The JupyterLab notebook show cases the use of Python in electrical engineering and circuit analysis.\nContents\n1. Introduction\n2. RIAA pre-emphasis curve\n3. AN346 RIAA Phono Preamplifier Design Procedure\n4. Analysis of the phono preamplifier circuit\n5. Summary"
  },
  {
    "objectID": "Two amplifier RIAA Phono Preamp.html#introduction",
    "href": "Two amplifier RIAA Phono Preamp.html#introduction",
    "title": "Two amplifier RIAA phono preamp",
    "section": "Introduction",
    "text": "Introduction\nThis JupyterLab notebook uses the SymPy, NumPy,SciPy and the Python programming language libraries to analyze a phono preamplifier circuit from the Texas Instruments application note, AN346, High-Performance Audio Applications of the LM833. The purpose of this analysis is to demonstrate the capability of using the Python libraries in electrical engineering circuit analysis. The circuit chosen for this analysis is a two stage RIAA Phono Preamplifier described in the application note. The preamplifier is designed to accurately reproduce the RIAA equalization curve required for play back of Vinyl LP records. The preamplifier provides about 35 dB of gain at 1kHz along with the proper gain profile and phase response. The schematic of the circuit is shown below with each node explicity annotated.\n\nThere has been a resurgence on the popularity of Vinyl LP records over the last few years. So the use of a RIAA Phono Preamplifier is still relevant in this age where most music is delivered by streaming. Two internet news stories describe the current market for Vinyl LP records:\nThey Said the Album Was Dying. They Were Wrong\n\nVinyl sales have grown steadily for 17 years, but jumped by a stunning 46% in 2020 and 51% in 2021 …\n\nWhy Vinyl Records Are Making a Comeback in 2022\n\nThis year, 2020, marks the first year in more than a generation since record sales — that is to say physical vinyl records — have surpassed CD sales. The reasons for this are twofold: CD sales have dropped dramatically in recent years, while sales of vinyl records are actually up this year. And while you might think it’s nostalgic Boomers or Gen Xers behind the renaissance of records, in fact surveys show it’s millennial consumers driving the rising trend in vinyl sales.\n\n\nSo vinyl is here to stay, it seems, despite all technological advances that would have seemed to threaten it. The same RIAA study that found records surpassing CDs also revealed that streaming music now account for more than 85% of all music enjoyed. Only 6% of music is now downloaded, even less than is physically purchased in the form of records, CDs, or the last tapes.\n\nToday there are hundreds of phono products sold on Amazon. Phono preamps range in price from $10 to $1,000 dollars.\nThe Phono Preamplifier also known as a phono stage, is an audio component that amplifies the signal from your turntable to a level that allows you to connect it to your sound system the same way you would with any other audio source. In addition to boosting the signal from the phono carterage, the preamp applies the RIAA equalization curve to the signal, reverting it back to the shape it was on the original recording. Phono cartridge output varies depending on the type of phono cartridge. Moving Magnet (MM) or Moving Iron (MI) cartridges typically produce a maximum output of 5mV. Moving Coil (MC) cartridges produce a much lower output, typically around 0.5mV maximum. Most phono preamps have switch that allow users to select the type of coil they have installed on their turn table arm.\nTexas Instruments provided the schematic of the preamplifier in their application note to highlight the types of applications their LM833 operational amplifier can support. Application notes are sometimes part of the marketing literature provided along with component data sheets by semiconductor manufactures.\nRIAA equalization is a specification for the recording and playback of phonograph records, established by the Recording Industry Association of America (RIAA). RIAA was formed in 1952. Its original mission was to administer recording copyright fees and problems, work with trade unions, and do research relating to the record industry and government regulations. Early RIAA standards included the RIAA equalization curve, the format of the stereophonic record groove and the dimensions of 33 1/3, 45, and 78 rpm records.\nThe purposes of the equalization are to permit greater recording times (by decreasing the mean width of each groove), to improve sound quality and to reduce the groove damage that would otherwise arise during playback. RIAA equalization is a form of pre-emphasis on recording and de-emphasis on playback. A recording is made with the low frequencies reduced and the high frequencies boosted, and on playback, the opposite occurs. The net result is a flat frequency response, but with attenuation of high-frequency noise such as hiss and clicks that arise from the recording medium. Reducing the low frequencies also limits the excursions the cutter needs to make when cutting a groove. Groove width is thus reduced, allowing more grooves to fit into a given surface area, permitting longer recording times. This also reduces physical stresses on the stylus, which might otherwise cause distortion or groove damage during playback.\n\nScope\nThe analysis presented in this notebook is intended to illustrate the use of Python for circuit analysis. This is not a tutorial on how to design a better phono preamp. The circuit taken from the Texas Instruments application note is examined for what it is, which is a suggested application for the use of their audio grade op amp. However, in this analysis I don’t address the performance of the op amp relative to the implementation of the RIAA equalization curve. I’m more concerned with examining the circuit’s ability to reproduce the proper gain and phase over the audio band. The performance LM833 op amp is assumed to be sufficient for this application and in my analysis of the circuit, I’ve replaced the LM833 with an ideal op amp model. Also, it is assumed that the reader is familiar with electronic components such as resistors, capacitors and operational amplifiers also known as op amps or opamps.\n\n\nMethodology\nThe analysis presented in this notebook will cover a topics that are often presented during a design review. Ususaly during a design review conformance to requirements is presented. For the phono preamp circuit, the main performance requirement is minimum deviation from the RIAA curve. The application note from TI stated that the deviation is less than 0.1 dB over the audio band when using 1% resistors.\nIn this notebook the analysis is divided into sections.\n\nThe analysis will start with an description of the circuit operation and some basic calculations.\nThere are many symbols used in the equations and these are listed in a table for reference. I also tried to be constant with variable names.\nThe RIAA pre-emphasis curve is discussed and the transfer function, pole/zero plot and amplitude and phase response is plotted.\nCalculations for the phono preamplifier design procedure as covered in the application note are presented. The element values obtain with this procedure are the ones used in the analysis.\nThe equations for the transfer function of the preamp are derived by using a tecnhique known as modified nodal analysis.\nThe preamp poles and zeros are plotted and some comments about stability are provided.\nThe amplitude and phase responce of the preamp transfer function is plotted.\nThe impulse, step and group dealy are plotted\nThe amplitude and phase response of the preamp transfer function is plotted against results taken from LTSpice as a check and comparision.\nThe deviation of the amplitude and phase responce from the RIAA curve is plotted.\nSensitivity analsysis, component selection, monte carlo and worst case analysis are presented.\n\n\nimport os\nimport sys\nimport random\nfrom sympy import *\nimport numpy as np\nfrom scipy import signal\nimport matplotlib.pyplot as plt\ninit_printing()\n\n\n\nPython library versions\n\npython: 3.10.9\nnumpy: 1.23.5\nsympy: 1.11.1\nscipy: 1.10.0\nmatplotlib: 3.7.0\n\nDefine a function to return the system gain at a frequency: get_gain()\n\ndef get_gain(freq_Hz, sys):\n    '''\n    freq_Hz: the frequency in Hz for which the system gain is desired\n    sys: a SciPy instance of the LTI class or a tuple describing the system\n    '''\n    f1 = freq_Hz - freq_Hz*0.1 # lower limit of the frequency range\n    f1a = freq_Hz - freq_Hz*0.01 # lower interpolation point\n    f2 = freq_Hz + freq_Hz*0.1 # upper limit of the frequency range\n    f2a = freq_Hz + freq_Hz*0.01  # upper interpolation point\n\n    x_axis_range = np.linspace(f1*2*np.pi, f2*2*np.pi, 1000, endpoint=True) # define the range frequency range\n    w, mag, phase = sys.bode(w=x_axis_range)\n\n    index_for_f1a = np.where(w &gt; f1a*2*np.pi)[0][0]\n    index_for_f2a = np.where(w &gt; f2a*2*np.pi)[0][0]\n\n    return np.interp(freq_Hz, [w[index_for_f1a]/(2*np.pi),w[index_for_f2a]/(2*np.pi)], [mag[index_for_f1a],mag[index_for_f2a]])\n\n\n\nSchematic and circuit description\nThe circuit from Figure 3 of AN346 was entered into LTSpice and the circuit nodes were numbered as shown above. Any schematic capture program could be used to for this as long as a Spice like netlist can be generated. In the schematic, the voltage source V1, is set to 5 mV to represent the output of a Moving Magnet (MM) or Moving Iron (MI) cartridge. The input to the preamp is shunted by a capacitance, which is equal to the sum of the input cable capacitance and the cartridge. This capacitance resonates with the inductance of the moving magnet cartridge to determine the frequency response of the transducer, so when a moving magnet pickup is used, Cp should be carefully chosen so that the total capacitance is equal to the recommended load capitance for that particular cartridge. 100 pF is used in this analysis. Rp is the recommended resistive load for the phono cartridge. In some comercial preamp designs, the value of Rp is user selectable with switches. As shown in the calculations, Cp and Rp have a resonant frequency of 33.86kHz.\n\nCp = 100e-12\nRp = 47e3\nprint('resonant frequency: {:,.1f} Hz'.format(1/(2*np.pi*Rp*Cp)))\n\nresonant frequency: 33,862.8 Hz\n\n\n33,862 Hz is well above the audio range.\nThe first operational amplifier, U1, takes care of the 50 Hz and 500 Hz breakpoints. For the analysis with Python, the op amp is modeled as and ideal opamp. There is expected to be some differences between the LTSpice results and the Python analysis. Using two amplifiers results in accurate conformance to the RIAA curve without reverting to the noisy inverting topology, as well as lower distortion due to the fact that each amplifier is operating at a lower gain than would be the case in a single-amplifier design.\nThe resistor, R1, which has a value of 80.6k\\(\\Omega\\) and the capacitor C1, which has a value of 0.039 \\(\\mu\\)Farads, form a resonant pair with frequency of 50.6 Hz.\n\nR1 = 80.6e3\nC1 = 0.039e-6\nprint('resonant frequency: {:,.1f} Hz'.format(1/(2*np.pi*R1*C1)))\n\nresonant frequency: 50.6 Hz\n\n\n50 Hz is one of the RIAA time constants required by the RIAA specification.\nFrom here on, I’ll refer to resistors that have values in 1000’s of Ohms by using k for thousands of Ohms or just the numerical value if it’s less than 1000. Capacitors will have values indicated in \\(\\mu\\) for micro Farads and p for pico Farads, designated as \\(\\mu\\) or p. \n\nR2 = 8.45e3\nprint('resonant frequency: {:,.1f} Hz'.format(1/(2*np.pi*R2*C1)))\n\nresonant frequency: 482.9 Hz\n\n\nC1 and R2, which has a value of 8.45k, have a resonant frequency of 482.94 Hz. As describe later, these frequencies correspond to the time constants required by the RIAA specification.\nCo provides an AC ground for the non-inverting configuration of U1. Ro along with R1 and R2 set the low frequency gain of U1.\n\nRo = 499\nCo = 200e-6\nprint('resonant frequency: {:,.1f} Hz'.format(1/(2*np.pi*Ro*Co)))\n\nresonant frequency: 1.6 Hz\n\n\nRo=499 and Co=200\\(\\mu\\) have a resonant frequency of 1.6 Hz.\n\nRf=R1+R2\nprint('low frequency voltage gain of U1: {:,.2f} or {:,.1f}dB'.format(1+Rf/Ro, 20*np.log10(1+Rf/Ro)))\n\nlow frequency voltage gain of U1: 179.46 or 45.1dB\n\n\nRo along with R1 and R2 set the low frequency gain of U1 at 45 dB.\n\nR3 = 2.37e3\nC3 = 0.033e-6\nprint('resonant frequency: {:,.1f} Hz'.format(1/(2*np.pi*R3*C3)))\n\nresonant frequency: 2,035.0 Hz\n\n\nR3=2.37k and C3=0.033\\(\\mu\\) have a resonant frequency of 2034.96 Hz and corresponds to the the third time constant specified by RIAA.\nC4=2\\(\\mu\\) and R6=54.9k form a high pass filter with a corner frequency of 1.45Hz.\n\nR6 = 54.9e3\nC4 = 2e-6\nprint('resonant frequency: {:,.1f} Hz'.format(1/(2*np.pi*R6*C4)))\n\nresonant frequency: 1.4 Hz\n\n\nU2, R4 and R5 form a non inverting configuration with a voltage gain of 3.15 or 9.98 dB.\n\nR4=2e3\nR5=4.3e3\nprint('voltage gain of U2: {:,.2f} or {:,.1f}dB'.format(1+R5/R4, 20*np.log10(1+R5/R4)))\n\nvoltage gain of U2: 3.15 or 10.0dB\n\n\n\n\nSymbols\nIn this notebook the following symbols are used:\n\n\n\n\n\n\n\nSymbol\ndefininition\n\n\n\n\ns\nwhen used in a polynominal: the Laplace variable equal to \\(\\alpha + j\\omega\\)\n\n\n\\(\\mu\\)\n\\(1 \\times 10^{-6}\\) multiplier, either: \\(1 \\times 10^{-6}\\) seconds or \\(1 \\times 10^{-6}\\) Farads\n\n\nT\ntime constant: T1, T2, T3\n\n\n\\(\\omega\\)\nangular frequency, radians per second, \\(\\omega = 1/T\\)\n\n\nf\nfrequency in cycles per second, \\(f = \\frac{\\omega}{2\\pi}\\)\n\n\nR\nresistor: R1, R2 etc.\n\n\nC\ncapacitor: C1, C2 etc.\n\n\nv\nnode voltage: v1, v2, v3 etc.\n\n\nV\nvoltage source, e.g. V1\n\n\nA\nmatrix describing the connectivity of the resistors, capacitors and G type (VCCS) circuit elements\n\n\nX\nvector of unknown node voltages and unknown currents\n\n\nZ\nvector of known voltages and currents\n\n\nRIAA_num\nnumerator of the RIAA pre-emphsis transfer function\n\n\nRIAA_den\ndenominator of the RIAA pre-emphsis transfer function\n\n\nw_RIAA\nradian frequncy of the RIAA pre-emphsis transfer function\n\n\nmag_RIAA\nmagnitude of the RIAA pre-emphsis transfer function\n\n\nphase_RIAA\nphase of the RIAA pre-emphsis transfer function\n\n\nRIAA_gain_1kHz\ngain of the RIAA pre-emphsis transfer function at 1kHz\n\n\npreamp_equ_sym\npreamp circuit equations with symbolic values\n\n\nU_sym\nsymbolic solution to network equations, node voltages and unknown currents\n\n\nH_sym\ntransfer function with symbolic coefficients\n\n\npreamp_equ\ncircuit equations with numeric element values\n\n\nH_preamp_num\nnumerator of the transfer function\n\n\nH_preamp_denom\ndenominator of the transfer function\n\n\npreamp_sys\nSciPy representation of the preamp system\n\n\npreamp_gain_1kHz\ngain of the preamp transfer function at 1kHz\n\n\nw_preamp\nradian frequncy of the preamp transfer function\n\n\nmag_preamp\nmagnitude of the preamp transfer function\n\n\nphase_preamp\nphase of the preamp transfer function"
  },
  {
    "objectID": "Two amplifier RIAA Phono Preamp.html#riaa-pre-emphasis-curve",
    "href": "Two amplifier RIAA Phono Preamp.html#riaa-pre-emphasis-curve",
    "title": "Two amplifier RIAA phono preamp",
    "section": "RIAA pre-emphasis curve",
    "text": "RIAA pre-emphasis curve\nThe RIAA equalization curve was established in 1954. The equalization is defined by time constants, T1, T2 and T3. During the Phonograph record manufacturing process, a pre-emphsis is applied to the signal, which allows for longer playback times on phonograph records by decreasing the average width of the groove cut into vinyl phonograph disks. The curve attenuates low frequencies and amplifies high frequencies, relative to 1 kHz. Since low frequencies cause wide undulations in the record groove, they must be attenuated to keep the grove within its bounds. Above 1 kHz, the frequencies are amplified which helps overcome the inherent noise produced by the phonograph needle during play-back.\nThe RIAA disc recording/reproduction standard specifies the time constants of, \\(T1 = 75 \\mu s\\), \\(T2 = 318 \\mu s\\) and \\(T3 = 3180 \\mu s\\) and the pre-emphasis transfer function:\n\\(RIAA(s)=\\frac {(sT_{1}+1)(sT_{3}+1)}{(sT_{2}+1)}\\)\nThe three time constants correspond to the frequencies calculated below.\n\nT1 = 75e-6\nT2 = 318e-6\nT3 = 3180e-6\nprint('{:.0f} \\u03BCs corresponds to {:,.1f} Hz'.format(T1*1e6,1/(T1*2*np.pi)))\nprint('{:.0f} \\u03BCs corresponds to {:,.1f} Hz'.format(T2*1e6,1/(T2*2*np.pi)))\nprint('{:.0f} \\u03BCs corresponds to {:,.1f} Hz'.format(T3*1e6,1/(T3*2*np.pi)))\n\n75 μs corresponds to 2,122.1 Hz\n318 μs corresponds to 500.5 Hz\n3180 μs corresponds to 50.0 Hz\n\n\nThe time constants are put in polynominal form using s as the Laplace variable. The numerator and denominator of the pre-emphasis transfer function is defined below.\n\ns = symbols('s')\nRIAA_num = Eq(((s*T3+1)*(s*T1+1)),0)\nRIAA_denom = Eq(s*T2+1,0)\n\nSolve for the poles and zeros of the pre-emphasis transfer function and plot the locations on the complex s-plane. The zeros of the transfer function are the roots of the numerator polinominal. The poles of the transfer function are the roots of the denominal polinominal.\n\nRIAA_zeros = solve(RIAA_num,s)\nRIAA_poles = solve(RIAA_denom,s)\n\n\nplt.plot(np.real(RIAA_zeros), np.imag(RIAA_zeros), 'ob', markerfacecolor='none')\nplt.plot(np.real(RIAA_poles), np.imag(RIAA_poles), 'xr')\nplt.legend(['Zeros', 'Poles'], loc=2)\nplt.title('Pole / Zero Plot')\nplt.xlabel('real part, \\u03B1')\nplt.ylabel('imaginary part, j\\u03C9')\nplt.grid()\nplt.show()\n\n\n\n\n\nprint('number of zeros: {:d}'.format(len(RIAA_zeros)))\nfor i in RIAA_zeros:\n    print('{:,.2f} Hz'.format(i/(2*np.pi)))\n\nnumber of zeros: 2\n-2,122.07 Hz\n-50.05 Hz\n\n\n\nprint('number of poles: {:d}'.format(len(RIAA_poles)))\nfor i in RIAA_poles:\n    print('{:,.2f} Hz'.format(i/(2*np.pi)))\n\nnumber of poles: 1\n-500.49 Hz\n\n\nAs shown in the plot above, the poles and zeros lay on the negative real axis. The de-emphisis transfer function of the phono pre-amplifier should have poles at the zero locations and a zero in the pole location in the plot above.\nThe code below is used to convert SymPy symbolic equations to a numpy polynomial representation. The SciPy function, TransferFunction, represents the system as the continuous-time transfer function. The Numpy function, logspace, is used to generate data points on a log scale for plotting. The SciPy function, bode, is used to generate the magnitude and phase data of a continuous-time system.\n\na = np.array(Poly(RIAA_num, s).all_coeffs(), dtype=float)\nb = np.array(Poly(RIAA_denom, s).all_coeffs(), dtype=float)\nRIAA_sys = signal.TransferFunction(a,b)\n\nx_axis_range = np.logspace(1, 5.5, 100, endpoint=True)*2*np.pi\n\nw_RIAA, mag_RIAA, phase_RIAA = RIAA_sys.bode(w=x_axis_range) # returns: rad/s, mag in dB, phase in deg\n\nFind the gain at 1kHz so the plots can be normalized for 0 dB at 1 kHz.\n\nRIAA_gain_1kHz = get_gain(1000,RIAA_sys)\n\n\nprint('The RIAA gain at 1kHz: {:.3f} dB'.format(RIAA_gain_1kHz))\n\nThe RIAA gain at 1kHz: 19.911 dB\n\n\nPlot the magnitude and phase of the RIAA curve.\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nplt.semilogx(w_RIAA/(2*np.pi), mag_RIAA-RIAA_gain_1kHz,'-k')    # Bode magnitude plot, normalized to 0 at 1kHz\n\n# mark individual points\np3 = np.where(w_RIAA &gt; 49.9*(2*np.pi))[0][0]\np2 = np.where(w_RIAA &gt; 499.9*(2*np.pi))[0][0]\np1 = np.where(w_RIAA &gt; 2122*(2*np.pi))[0][0]\n\nplt.semilogx(w_RIAA[p1]/(2*np.pi), mag_RIAA[p1]-RIAA_gain_1kHz,'^k')\nplt.semilogx(w_RIAA[p2]/(2*np.pi), mag_RIAA[p2]-RIAA_gain_1kHz,'^k')\nplt.semilogx(w_RIAA[p3]/(2*np.pi), mag_RIAA[p3]-RIAA_gain_1kHz,'^k')\n\nplt.text(w_RIAA[p1]/(2*np.pi), mag_RIAA[p1]-25,'T1')\nplt.text(w_RIAA[p2]/(2*np.pi), mag_RIAA[p2]-25,'T2')\nplt.text(w_RIAA[p3]/(2*np.pi), mag_RIAA[p3]-25,'T3')\n\n# hightlight the audio band, 20 to 20kHz\nplt.axvspan(20, 20e3, color='y', alpha=0.3)\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\nplt.semilogx(w_RIAA/(2*np.pi), phase_RIAA,':',color='b',label='phase')  # Bode phase plot\n\n# mark individual points\nplt.semilogx(w_RIAA[p1]/(2*np.pi), phase_RIAA[p1],'xb')\nplt.semilogx(w_RIAA[p2]/(2*np.pi), phase_RIAA[p2],'xb')\nplt.semilogx(w_RIAA[p3]/(2*np.pi), phase_RIAA[p3],'xb')\n\nax2.set_ylabel('phase, deg',color='b')\nax2.tick_params(axis='y', labelcolor='b')\nax2.set_ylim((0,100))\n\nax2.plot(np.NaN, np.NaN, color='k', label='magnitude')\n\nplt.legend()\nplt.title('RIAA pre-emphasis Bode plot')\nplt.show()\n\n\n\n\nThe plot above shows the frequence response of the RIAA curve. The frequencies coresponsing to the time constants T1, T2 and T3 are plotted on the magnitude and phase curves. The audio band of 20Hz to 20kHz is highlighted. One thing to notice about this curve is that the amplitdue is increasing as the frequency increases. This is not a realistic function, real circuits do not have and inifinite gain at as the frequency goes tio infinity. Also, there is no zero at \\(j\\omega=0\\), so the pre-emphisis transfer function does not block DC."
  },
  {
    "objectID": "Two amplifier RIAA Phono Preamp.html#phono-preamplifier-design-procedure",
    "href": "Two amplifier RIAA Phono Preamp.html#phono-preamplifier-design-procedure",
    "title": "Two amplifier RIAA phono preamp",
    "section": "Phono Preamplifier Design Procedure",
    "text": "Phono Preamplifier Design Procedure\nThe following notebook cells walk through the design procedure given in the application note, starting on page 5.\nA design procedure is shown below with an illustrative example using 1% tolerance E96 components for close conformance to the ideal RIAA curve. Since 1% tolerance capacitors are often difficult to find except in 5% or 10% standard values, the design procedure calls for re-calculation of a few component values so that standard capacitor values can be used.\n\nChoose \\(R_o\\). \\(R_o\\) should be small for minimum noise contribution, but not so small that the feedback network excessively loads the amplifier. Example: Choose \\(R_o = 500\\)\nChoose 1 kHz gain, A1 of first amplifier. This will typically be around 20 dB to 30 dB. Example: Choose A1 = 26 dB = 20\nCalculate \\(R_11 = 8.058 \\times R_o \\times A_1\\)\n\n\nA1 = 20\nRo = 500\nR1 = 8.058 * Ro * A1\nprint('R1={:,.0f}'.format(R1))\n\n\nCalculate C1\n\n\\(C_1 = \\frac {3.18 \\times 10^{-3}}{R_1}\\)\n\nC1 = 3.18e-3/R1\nprint('C1={:.4e}'.format(C1))\n\nC1=3.9464e-08\n\n\nThe calculated value for capacitor C1 is not a standard value, so step 5 takes care of this.\n\nIf C1 is not a convenient value, choose the nearest convenient value and calculate a new R1 from:\n\n\\(R_1 = \\frac {3.18 \\times 10^{-3}}{3.9 \\times 10^{-8}}\\)\nChoose C1 to be 0.039\\(\\mu\\), which is a standard capacitor value.\n\nC1 = 0.039e-6\nR1 = 3.18e-3/C1\nprint('R1={:,.0f}'.format(R1))\n\nR1=81,538\n\n\nNow choose a standard resistor value close to the the calculated value, which is 80.6k.\nE96 resistor values are a set of perfered values for 1% resistors. When doing the calculations to determin the resistor values, the closest standandard value is chosen from the E96 series. The E series of preferred numbers derived for use in electronic components. It consists of the E3, E6, E12, E24, E48, E96 and E192 series, where the number after the ‘E’ designates the quantity of logarithmic value “steps” per decade.\n\nR1 = 80.6e3\n\nCalculate a new value for Ro.\n\\(R_o=\\frac {R_1}{8.058A_1}\\)\n\nRo = R1/(8.058*A1)\nprint('R1={:,.0f}'.format(Ro))\n\nR1=500\n\n\nChoose a standard value close to this value, which is 499.\n\nRo = 499\n\n\nCaluclate R2\n\\(R_2=\\frac {R_1}{9} - R_o\\)\n\n\nR2 = R1/9-Ro\nprint('R1={:,.0f}'.format(R2))\n\nR1=8,457\n\n\nChoose a standard value close to this value, which is 8.45k.\n\nR2 = 8.45e3\n\n\nChoose a convenient value for C3 in the range from 0.01 \\(\\mu\\)F to 0.05 \\(\\mu\\)F.\nExample: C3 = 0.033 \\(\\mu\\)F\n\n\nC3 = 0.033e-6\nprint('C3={:,.3f}\\u03BC'.format(C3*1e6))\n\nC3=0.033μ\n\n\n\nCalculate Rp\n\\(R_p=\\frac {75 \\mu s }{C_3}\\)\n\n\\(75 \\mu s\\) is one of the RIAA time constants.\n\nRp = 75e-6/C3\nprint('Rp={:,.0f}'.format(Rp))\n\nRp=2,273\n\n\n\nChoose a standard value for R3 that is slightly larger than Rp.\nExample: R3 = 2.37k, which is a standard resistor value.\n\n\nR3 = 2.37e3\n\n\nCalculate R6 from \\(1/R_6 = 1/R_P − 1/R_3\\)\n\n\nR6 = 1/(1/Rp-1/R3)\nprint('R6={:,.0f}'.format(R6))\n\nR6=55,374\n\n\n54.9k is the closest standard value.\n\nR6 = 54.9e3\n\n\nCalculate \\(C_4\\) for low-frequency rolloff below 1 Hz from design Equation 5.\n\n\\(C_4=\\frac{1}{2\\pi f_L(R_3+R_6)}\\)\nWhere \\(f_L\\) is the low frequency -3dB corner of the second stage.\nIn the application note, there is a comment on page 4:\n&gt; If the preamplifier is to follow the IEC recommendation (IEC Publication 98, Amendment #4), fL should equal 20.2 Hz.\nThe calculations in the app note use 1 Hz.\n\nf_L = 1.0 # Hz\nC4 = 1/(2*np.pi*f_L*(R3+R6))\nprint('C4={:,.3f}\\u03BC'.format(C4*1e6))\n\nC4=2.779μ\n\n\n2\\(\\mu\\)F is a standard value close to the calculated value.\nExample: C4 = 2 \\(\\mu\\)F.\n\nChoose gain of second amplifier.\nExample: The 1 kHz gain up to the input of the second amplifier is about 26 dB for this example. For an overall 1 kHz gain equal to about 36 dB we choose:\n\\(A_2 = 10 dB = 3.16\\)\nChoose value for R4.\nExample: R4 = 2k\n\n\nR4 = 2e3\n\n\nCalculate \\(R_5 = (A_2 − 1) R_4\\)\n\n\nA2 = 3.16\nR5 = (A2-1)*R4\nprint('R5={:,.0f}'.format(R5))\n\nR5=4,320\n\n\n4.3k is a standard value close to the calculated value.\n\nR5 = 4.3e3\n\n\nCalculate Co for low-frequency rolloff below 1 Hz from design Equation 7.\n\n\\(C_o=\\frac {1}{2\\pi f_o R_o}\\)\nwhere fo is the low-frequency −3 dB corner of the first amplifier. fo is chosen to be 1Hz for the calculations since this frequency is well below the audible frequency range.\n\nfo = 1 # 1 Hz\nCo = 1/(2*np.pi*fo*Ro)\nprint('Co={:,.3f}\\u03BC'.format(Co*1e6))\n\nCo=318.948μ\n\n\nThe value chosen in the app note for this component is 200\\(\\mu\\)F.\n\nCo = 200e-6\n\n\nprint('resonant frequency of Ro and Co: {:.2f}Hz'.format(1/(Co*Ro*2*np.pi)))\n\nresonant frequency of Ro and Co: 1.59Hz"
  },
  {
    "objectID": "Two amplifier RIAA Phono Preamp.html#analysis-of-an-346-phono-preamplifier-circuit",
    "href": "Two amplifier RIAA Phono Preamp.html#analysis-of-an-346-phono-preamplifier-circuit",
    "title": "Two amplifier RIAA phono preamp",
    "section": "Analysis of AN-346 phono preamplifier circuit",
    "text": "Analysis of AN-346 phono preamplifier circuit\nThe schematic of the preamp was entered into LTSpice and the netlist was generated. Starting with a schematic and then using LTSpice to generate the net list eliminates errors that would occure if these circuits were analyised by hand. For small circuits with a handful of components, symbolic solutions of the node equations can be of interest, but for larger circuits, not so much. This is because the number of symbols and equations is too large to offer in insight with out some simplicication.\nIn this section the modified nodal analysis method will be used to generate the circuit equations. The modified nodal analysis provides an algorithmic method for generating systems of independent equations for linear circuit analysis. Most problems that an electrical engineer encounters on the job are complex enough that they use computers to analyze the circuits. The Python code that generates the circuits equations is located here.\n\nModified nodal analysis\nThe preamp circuit has 15 branches, 9 nodes, 3 unknown currents, 14 passive components and 2 op amps. The net list generated by LTSpice and some edits were made to put the component values into scientific notation with units of Ohms, Farads and Henerys and the opamp statements were fixed. The edited netlist is:\nV1 1 0 5e-3m\nO1 3 1 6 \nO2 9 8 2 \nC1 3 5 0.039e-6\nCo 4 0 200e-6\nRo 3 4 499\nR3 6 7 2.37e3\nR1 3 5 80.6e3\nR2 5 6 8.45e3\nCp 1 0 100e-12\nRp 1 0 47e3\nC3 7 0 0.033e-6\nC4 8 7 2e-6\nR6 8 0 54.9e3\nR4 9 0 2e3\nR5 2 9 4.3e3\nThis netlist is read into the Symbolic Modified Nodal Analysis Jupyter notebook and the following circuit equations were generated.\n\\(I_{V1} + v_{1} \\left(Cp s + \\frac{1}{R_{20}}\\right) = 0\\)\n\\(v_{2} \\left(C_{1} s + \\frac{1}{Ro} + \\frac{1}{R_{1}}\\right) + v_{4} \\left(- C_{1} s - \\frac{1}{R_{1}}\\right) - \\frac{v_{3}}{Ro} = 0\\)\n\\(v_{3} \\left(Co s + \\frac{1}{Ro}\\right) - \\frac{v_{2}}{Ro} = 0\\)\n\\(v_{2} \\left(- C_{1} s - \\frac{1}{R_{1}}\\right) + v_{4} \\left(C_{1} s + \\frac{1}{R_{2}} + \\frac{1}{R_{1}}\\right) - \\frac{v_{5}}{R_{2}} = 0\\)\n\\(I_{O1} + v_{5} \\cdot \\left(\\frac{1}{R_{3}} + \\frac{1}{R_{2}}\\right) - \\frac{v_{6}}{R_{3}} - \\frac{v_{4}}{R_{2}} = 0\\)\n\\(- C_{4} s v_{7} + v_{6} \\left(C_{3} s + C_{4} s + \\frac{1}{R_{3}}\\right) - \\frac{v_{5}}{R_{3}} = 0\\)\n\\(- C_{4} s v_{6} + v_{7} \\left(C_{4} s + \\frac{1}{R_{6}}\\right) = 0\\)\n\\(v_{8} \\cdot \\left(\\frac{1}{R_{5}} + \\frac{1}{R_{4}}\\right) - \\frac{v_{9}}{R_{5}} = 0\\)\n\\(I_{O2} + v_{9} \\cdot \\left(\\frac{1}{R_{5}} + \\frac{1}{R_{22}}\\right) - \\frac{v_{8}}{R_{5}} = 0\\)\n\\(v_{1} = V_{1}\\)\n\\(- v_{1} + v_{2} = 0\\)\n\\(- v_{7} + v_{8} = 0\\)\nThe symbols and matrices generated by the modified nodal analysis code are copied here so that the circuit equations can be solved symbilically and later numerically. All the symboles that SymPy needs defined are delared. The A matrix describs the connectivity of the resistors, capacitors and G type (VCCS) circuit elements. The X matrix contains the unknown node voltages and unknown currents. The Z matrix contains the known voltages and currents sources, e.g. V1.\n\nRp, v6, Co, C4, v2, C3, s, I_V1, R6, Ro, R4, C1, R3, I_O2, R5, v3, I_O1, v4, v8, v7, V1, Cp, v1, R1, R2, v5, v9 = symbols(' Rp  v6  Co  C4  v2  C3  s  I_V1  R6  Ro  R4  C1  R3  I_O2  R5  v3  I_O1  v4  v8  v7  V1  Cp  v1  R1  R2  v5  v9 ')\nA = Matrix([[Cp*s + 1/Rp, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 1/R5, 0, 0, 0, 0, 0, 0, -1/R5, 0, 0, 1], [0, 0, C1*s + 1/Ro + 1/R1, -1/Ro, -C1*s - 1/R1, 0, 0, 0, 0, 0, 0, 0], [0, 0, -1/Ro, Co*s + 1/Ro, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, -C1*s - 1/R1, 0, C1*s + 1/R2 + 1/R1, -1/R2, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, -1/R2, 1/R3 + 1/R2, -1/R3, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, -1/R3, C3*s + C4*s + 1/R3, -C4*s, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, -C4*s, C4*s + 1/R6, 0, 0, 0, 0], [0, -1/R5, 0, 0, 0, 0, 0, 0, 1/R5 + 1/R4, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [-1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0]])\nX = Matrix( [v1, v2, v3, v4, v5, v6, v7, v8, v9, I_V1, I_O1, I_O2] )\nZ = Matrix( [0, 0, 0, 0, 0, 0, 0, 0, 0, V1, 0, 0] )\n\nThe equations are displayed in maxtrix form below.\n\npreamp_equ_sym = Eq(A*X,Z)\npreamp_equ_sym\n\n\\(\\displaystyle \\left[\\begin{matrix}I_{V1} + v_{1} \\left(Cp s + \\frac{1}{Rp}\\right)\\\\I_{O2} + \\frac{v_{2}}{R_{5}} - \\frac{v_{9}}{R_{5}}\\\\v_{3} \\left(C_{1} s + \\frac{1}{Ro} + \\frac{1}{R_{1}}\\right) + v_{5} \\left(- C_{1} s - \\frac{1}{R_{1}}\\right) - \\frac{v_{4}}{Ro}\\\\v_{4} \\left(Co s + \\frac{1}{Ro}\\right) - \\frac{v_{3}}{Ro}\\\\v_{3} \\left(- C_{1} s - \\frac{1}{R_{1}}\\right) + v_{5} \\left(C_{1} s + \\frac{1}{R_{2}} + \\frac{1}{R_{1}}\\right) - \\frac{v_{6}}{R_{2}}\\\\I_{O1} + v_{6} \\cdot \\left(\\frac{1}{R_{3}} + \\frac{1}{R_{2}}\\right) - \\frac{v_{7}}{R_{3}} - \\frac{v_{5}}{R_{2}}\\\\- C_{4} s v_{8} + v_{7} \\left(C_{3} s + C_{4} s + \\frac{1}{R_{3}}\\right) - \\frac{v_{6}}{R_{3}}\\\\- C_{4} s v_{7} + v_{8} \\left(C_{4} s + \\frac{1}{R_{6}}\\right)\\\\v_{9} \\cdot \\left(\\frac{1}{R_{5}} + \\frac{1}{R_{4}}\\right) - \\frac{v_{2}}{R_{5}}\\\\v_{1}\\\\- v_{1} + v_{3}\\\\- v_{8} + v_{9}\\end{matrix}\\right] = \\left[\\begin{matrix}0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\V_{1}\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\nThese equations can be solved in thier symbolic form using the solve function from SymPy. The solution time takes about 3 seconds on my i3 laptop.\n\nU_preamp_sym = solve(preamp_equ_sym,X)\n\nThe transfer function for the preamp is the equation for the output node, 2, divided by the equation for the input node 1.\n\nH_preamp_sym = U_preamp_sym[v2]/U_preamp_sym[v1]\n\nThe symbolic solution obtained by SymPy, while not being very interesting since they are unweildly, they illustrate the power of SymPy to easily obtain symbolic solutions that would be very difficult to obtain by hand.\n\nH_preamp_sym\n\n\\(\\displaystyle \\frac{C_{1} C_{4} Co R_{1} R_{2} R_{4} R_{6} V_{1} s^{3} + C_{1} C_{4} Co R_{1} R_{2} R_{5} R_{6} V_{1} s^{3} + C_{1} C_{4} Co R_{1} R_{4} R_{6} Ro V_{1} s^{3} + C_{1} C_{4} Co R_{1} R_{5} R_{6} Ro V_{1} s^{3} + C_{1} C_{4} R_{1} R_{4} R_{6} V_{1} s^{2} + C_{1} C_{4} R_{1} R_{5} R_{6} V_{1} s^{2} + C_{4} Co R_{1} R_{4} R_{6} V_{1} s^{2} + C_{4} Co R_{1} R_{5} R_{6} V_{1} s^{2} + C_{4} Co R_{2} R_{4} R_{6} V_{1} s^{2} + C_{4} Co R_{2} R_{5} R_{6} V_{1} s^{2} + C_{4} Co R_{4} R_{6} Ro V_{1} s^{2} + C_{4} Co R_{5} R_{6} Ro V_{1} s^{2} + C_{4} R_{4} R_{6} V_{1} s + C_{4} R_{5} R_{6} V_{1} s}{V_{1} \\left(C_{1} C_{3} C_{4} Co R_{1} R_{3} R_{4} R_{6} Ro s^{4} + C_{1} C_{3} C_{4} R_{1} R_{3} R_{4} R_{6} s^{3} + C_{1} C_{3} Co R_{1} R_{3} R_{4} Ro s^{3} + C_{1} C_{3} R_{1} R_{3} R_{4} s^{2} + C_{1} C_{4} Co R_{1} R_{3} R_{4} Ro s^{3} + C_{1} C_{4} Co R_{1} R_{4} R_{6} Ro s^{3} + C_{1} C_{4} R_{1} R_{3} R_{4} s^{2} + C_{1} C_{4} R_{1} R_{4} R_{6} s^{2} + C_{1} Co R_{1} R_{4} Ro s^{2} + C_{1} R_{1} R_{4} s + C_{3} C_{4} Co R_{3} R_{4} R_{6} Ro s^{3} + C_{3} C_{4} R_{3} R_{4} R_{6} s^{2} + C_{3} Co R_{3} R_{4} Ro s^{2} + C_{3} R_{3} R_{4} s + C_{4} Co R_{3} R_{4} Ro s^{2} + C_{4} Co R_{4} R_{6} Ro s^{2} + C_{4} R_{3} R_{4} s + C_{4} R_{4} R_{6} s + Co R_{4} Ro s + R_{4}\\right)}\\)\n\n\nThe SymPy function, cancel(), can be used to put the preamp transfunction in to standard canonical form, where the polynomials are expanded with no common factors and the leading coefficients do not have denominators (i.e., are integers).\n\ncancel(H_preamp_sym,s)\n\n\\(\\displaystyle \\frac{s^{3} \\left(C_{1} C_{4} Co R_{1} R_{2} R_{4} R_{6} + C_{1} C_{4} Co R_{1} R_{2} R_{5} R_{6} + C_{1} C_{4} Co R_{1} R_{4} R_{6} Ro + C_{1} C_{4} Co R_{1} R_{5} R_{6} Ro\\right) + s^{2} \\left(C_{1} C_{4} R_{1} R_{4} R_{6} + C_{1} C_{4} R_{1} R_{5} R_{6} + C_{4} Co R_{1} R_{4} R_{6} + C_{4} Co R_{1} R_{5} R_{6} + C_{4} Co R_{2} R_{4} R_{6} + C_{4} Co R_{2} R_{5} R_{6} + C_{4} Co R_{4} R_{6} Ro + C_{4} Co R_{5} R_{6} Ro\\right) + s \\left(C_{4} R_{4} R_{6} + C_{4} R_{5} R_{6}\\right)}{C_{1} C_{3} C_{4} Co R_{1} R_{3} R_{4} R_{6} Ro s^{4} + R_{4} + s^{3} \\left(C_{1} C_{3} C_{4} R_{1} R_{3} R_{4} R_{6} + C_{1} C_{3} Co R_{1} R_{3} R_{4} Ro + C_{1} C_{4} Co R_{1} R_{3} R_{4} Ro + C_{1} C_{4} Co R_{1} R_{4} R_{6} Ro + C_{3} C_{4} Co R_{3} R_{4} R_{6} Ro\\right) + s^{2} \\left(C_{1} C_{3} R_{1} R_{3} R_{4} + C_{1} C_{4} R_{1} R_{3} R_{4} + C_{1} C_{4} R_{1} R_{4} R_{6} + C_{1} Co R_{1} R_{4} Ro + C_{3} C_{4} R_{3} R_{4} R_{6} + C_{3} Co R_{3} R_{4} Ro + C_{4} Co R_{3} R_{4} Ro + C_{4} Co R_{4} R_{6} Ro\\right) + s \\left(C_{1} R_{1} R_{4} + C_{3} R_{3} R_{4} + C_{4} R_{3} R_{4} + C_{4} R_{4} R_{6} + Co R_{4} Ro\\right)}\\)\n\n\nThe Sympy function, factor(), can be used to factor the polynominals it into irreducible factors over the rational numbers.\n\nH_preamp_sym.factor()\n\n\\(\\displaystyle \\frac{C_{4} R_{6} s \\left(R_{4} + R_{5}\\right) \\left(C_{1} Co R_{1} R_{2} s^{2} + C_{1} Co R_{1} Ro s^{2} + C_{1} R_{1} s + Co R_{1} s + Co R_{2} s + Co Ro s + 1\\right)}{R_{4} \\left(C_{1} R_{1} s + 1\\right) \\left(Co Ro s + 1\\right) \\left(C_{3} C_{4} R_{3} R_{6} s^{2} + C_{3} R_{3} s + C_{4} R_{3} s + C_{4} R_{6} s + 1\\right)}\\)\n\n\nThe symbolic solutions obtained above will be used later when the sinsitivity analysis of the preamp is performed. Otherwise the roots in symbolic form don’t seem to be particulary insightful, but are easily obtained by SymPy.\n\nNumerical solution\nThe element values are put into the Python dictionary format so that numerical values can be substituted into the equations.\n\nnominal_component_value = {V1:5.0000e-03, C1:3.9000e-08, Co:2.0000e-04, Ro:4.9900e+02, R3:2.3700e+03, R1:8.0600e+04, \n    R2:8.4500e+03, Cp:1.0000e-10, Rp:4.7000e+04, C3:3.3000e-08, C4:2.0000e-06, R6:5.4900e+04, R4:2.0000e+03, R5:4.3000e+03}\n\n# put the element values into the equations\npreamp_equ = preamp_equ_sym.subs(nominal_component_value)\n\nNow we can diplay the network equations with values for the components instear of symbols.\n\npreamp_equ\n\n\\(\\displaystyle \\left[\\begin{matrix}I_{V1} + v_{1} \\cdot \\left(1.0 \\cdot 10^{-10} s + 2.12765957446809 \\cdot 10^{-5}\\right)\\\\I_{O2} + 0.000232558139534884 v_{2} - 0.000232558139534884 v_{9}\\\\v_{3} \\cdot \\left(3.9 \\cdot 10^{-8} s + 0.00201641496392288\\right) - 0.00200400801603206 v_{4} + v_{5} \\left(- 3.9 \\cdot 10^{-8} s - 1.24069478908189 \\cdot 10^{-5}\\right)\\\\- 0.00200400801603206 v_{3} + v_{4} \\cdot \\left(0.0002 s + 0.00200400801603206\\right)\\\\v_{3} \\left(- 3.9 \\cdot 10^{-8} s - 1.24069478908189 \\cdot 10^{-5}\\right) + v_{5} \\cdot \\left(3.9 \\cdot 10^{-8} s + 0.000130750143157091\\right) - 0.000118343195266272 v_{6}\\\\I_{O1} - 0.000118343195266272 v_{5} + 0.000540284123536314 v_{6} - 0.000421940928270042 v_{7}\\\\- 2.0 \\cdot 10^{-6} s v_{8} - 0.000421940928270042 v_{6} + v_{7} \\cdot \\left(2.033 \\cdot 10^{-6} s + 0.000421940928270042\\right)\\\\- 2.0 \\cdot 10^{-6} s v_{7} + v_{8} \\cdot \\left(2.0 \\cdot 10^{-6} s + 1.82149362477231 \\cdot 10^{-5}\\right)\\\\- 0.000232558139534884 v_{2} + 0.000732558139534884 v_{9}\\\\v_{1}\\\\- v_{1} + v_{3}\\\\- v_{8} + v_{9}\\end{matrix}\\right] = \\left[\\begin{matrix}0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0.005\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\nUsing the SymPy solve function we can solve the system of equations.\n\nU_preamp = solve(preamp_equ,X)\n\nThe values of the exponents are very large in the solution. The numerator and denominator for v2 could be normalized. Another option for avoiding large exponents is to 1st normalize the component values by frequency scaling. I suppose that large exponents don’t become a problem as long as they remain under two digits.\nAlmost all platforms map Python floats to the IEEE754 double precision - 64 total bits. The float information using the sys package can be as shown as follows:\n\nsys.float_info\n\nsys.float_info(max=1.7976931348623157e+308, max_exp=1024, max_10_exp=308, min=2.2250738585072014e-308, min_exp=-1021, min_10_exp=-307, dig=15, mant_dig=53, epsilon=2.220446049250313e-16, radix=2, rounds=1)\n\n\n\nprint(sys.float_info)\n\nsys.float_info(max=1.7976931348623157e+308, max_exp=1024, max_10_exp=308, min=2.2250738585072014e-308, min_exp=-1021, min_10_exp=-307, dig=15, mant_dig=53, epsilon=2.220446049250313e-16, radix=2, rounds=1)\n\n\nThe maximum exponent that Python can use is 308.\nLetting SciPy do the math and not worrying about the size of the exponents. The transfer function can be obtained by divideing the equation for node 2 by the equation for node 1. The system transfer function, is \\(H(s) = \\frac {v2}{V1}\\)\n\nH_preamp = U_preamp[v2]/U_preamp[v1]\nH_preamp\n\n\\(\\displaystyle \\frac{200.0 \\cdot \\left(2.76499422921242 \\cdot 10^{69} s^{3} + 8.80353368479522 \\cdot 10^{72} s^{2} + 4.91462150480132 \\cdot 10^{71} s\\right)}{7.65600122987477 \\cdot 10^{65} s^{4} + 1.0469819689888 \\cdot 10^{70} s^{3} + 3.44479259637884 \\cdot 10^{72} s^{2} + 6.18286042227129 \\cdot 10^{73} s + 2.84188944100537 \\cdot 10^{74}}\\)\n\n\nfactor() takes a polynomial and factors it into irreducible factors over the rational numbers. For example:\n\nH_preamp.factor()\n\n\\(\\displaystyle \\frac{6.19554973375798 s \\left(0.000314077770155853 s^{2} + 1.0 s + 0.0558255546098434\\right)}{2.69398278462456 \\cdot 10^{-9} s^{4} + 3.68410520790143 \\cdot 10^{-5} s^{3} + 0.012121487017314 s^{2} + 0.21756161 s + 1.0}\\)\n\n\n\n\nConvert transfer function to SciPy system\nIn this section we convert the SymPy equations into Numpy format.\nExtract the numerator and denominator polynomials so that the system can be defined in SciPy.\n\nH_preamp_num, H_preamp_denom = fraction(H_preamp) #returns numerator and denominator\n\nThe SciPy function, TransferFunction, represents the system as the continuous-time transfer function and takes as inputs the coeeficients of the numerator and denominator polynominals.\n\n# convert symbolic to numpy polynomial\na2 = np.array(Poly(H_preamp_num, s).all_coeffs(), dtype=float)\nb2 = np.array(Poly(H_preamp_denom, s).all_coeffs(), dtype=float)\npreamp_sys = signal.TransferFunction(a2,b2)\n\nThe poles and zeros of the transfer function can easly be obtained with the following code:\n\npreamp_sys_zeros = np.roots(preamp_sys.num)\npreamp_sys_poles = np.roots(preamp_sys.den)\n\n\n\n\nPole zero plot\nThe poles and zeros of the preamp transfer function are plotted.\n\nplt.plot(np.real(preamp_sys_zeros), np.imag(preamp_sys_zeros), 'ob', markerfacecolor='none')\nplt.plot(np.real(preamp_sys_poles), np.imag(preamp_sys_poles), 'xr')\nplt.legend(['Zeros', 'Poles'], loc=2)\nplt.title('Pole / Zero Plot')\nplt.xlabel('real part, \\u03B1')\nplt.ylabel('imaginary part, j\\u03C9')\nplt.grid()\nplt.show()\n\n\n\n\nPoles and zeros of the transfer function plotted on the complex plane. The units are in radian frequency.\nPrinting these values in Hz.\n\nprint('number of zeros: {:d}'.format(len(preamp_sys_zeros)))\nfor i in preamp_sys_zeros:\n    print('{:,.2f} Hz'.format(i/(2*np.pi)))\n\nnumber of zeros: 3\n-506.73 Hz\n-0.01 Hz\n0.00 Hz\n\n\n\nprint('number of poles: {:d}'.format(len(preamp_sys_poles)))\nfor i in preamp_sys_poles:\n    print('{:,.2f} Hz'.format(i/(2*np.pi)))\n\nnumber of poles: 4\n-2,122.88 Hz\n-50.63 Hz\n-1.59 Hz\n-1.39 Hz\n\n\nWe can see that the RIAA time constants, displayed in terms of frequency are present, althought the values diffeer by a few Hz. There are two zeros and two poles at nearly zero hz and these cancel each other.\n\n\nStability\nBy inspecting the plot above, we can tell the preamplifier is stable since the phase shift at 0 dB of gain is less than 180 degrees. Additionally, all the poles of the transfer function are in the left hand plane.\nNow we can find the preamp gain at 1 kHz, so that the bode plots can be normailized.\n\npreamp_gain_1kHz = get_gain(1000, preamp_sys)\nprint('preamp gain at 1kHz: {:f} dB'.format(preamp_gain_1kHz))\n\npreamp gain at 1kHz: 34.783614 dB\n\n\n\n\nBode plot\nUse the SciPy function bode to plot the magnitude and phase of the filter. In electrical engineering, a Bode plot is a graph of the frequency response of a system. It is usually a combination of the magnitude (usually in decibels) of the frequency response and the phase shift. As originally conceived by Hendrik Wade Bode in the 1930s, the plot is an asymptotic approximation of the frequency response, using straight line segments. Bode plots are used to assess the stability of systems by finding the gain and phase margins.\n\nextended_x_axis_range = np.logspace(-2, 8, 5000, endpoint=True)*2*np.pi\n\nw_preamp, mag_preamp, phase_preamp = preamp_sys.bode(w=extended_x_axis_range)\n\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nplt.semilogx(w_preamp/(2*np.pi), mag_preamp,'-k')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\nplt.grid()\nplt.axvspan(20, 20e3, color='y', alpha=0.3)\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'b' #'tab:blue'\n\nplt.semilogx(w_preamp/(2*np.pi), phase_preamp,':',color=color,label='phase')  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.plot(np.NaN, np.NaN, '-', color='k', label='magnitude')\n\nplt.legend(loc=0)\nplt.title('preamplifier Bode plot')\nplt.show()\n\n\n\n\nThe Bode plot for the preamplifier is plotted from 0.01 Hz to 100 MHz. The preamplifier circuit blocks DC because C4 in in series with the audio path.\n\n\nImpulse and step response\nUse the SciPy functions impulse2 and step2 to plot the impulse and step response of the system.\nThe impulse and step response of the filter are plotted below. Any linear, time-invariant is completely characterized by its impulse response. The transfer function is the Laplace transform of the impulse response. The impulse response defines the response of a linear time-invariant system for all frequencies.\nIn electronic engineering and control theory, step response is the time behavior of the outputs of a general system when its inputs change from zero to one in a very short time.\n\nplt.subplots(1,2,figsize=(15, 5))\n\n# using subplot function and creating\n# plot one\nplt.subplot(1, 2, 1)\n\n# impulse response\nt, y = signal.impulse2(preamp_sys,N=500)\nplt.plot(t/1e-3, y)\nplt.title('AN-346 phono preamplifier Impulse response')\nplt.ylabel('volts')\nplt.xlabel('time, msec')\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n\nt, y = signal.step2(preamp_sys,N=500)\nplt.plot(t/1e-3, y)\nplt.title('AN-346 phono preamplifier Step response')\nplt.ylabel('volts')\nplt.xlabel('time, msec')\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\n\nGroup delay\nThe following python code calculates and plots group delay. Frequency components of a signal are delayed when passed through a circuit and the signal delay will be different for the various frequencies unless the circuit has the property of being linear phase. The delay variation means that signals consisting of multiple frequency components will suffer distortion because these components are not delayed by the same amount of time at the output of the device.\nGroup delay: \\(\\tau _{g}(\\omega )=-\\frac {d\\phi (\\omega )}{d\\omega }\\)\n\nw_preamp, mag_preamp, phase_preamp = preamp_sys.bode(w=x_axis_range)\n\nplt.title('AN-346 phono preamplifier group delay')\nplt.semilogx(w_preamp/(2*np.pi), -np.gradient(phase_preamp*np.pi/180)/np.gradient(w_preamp),'-',label='group delay')\n\n#plt.semilogx(w_c1/(2*np.pi), -np.gradient(phase_c1)/w_c1/1e-3,'-',label='phase delay')\nplt.axvspan(20, 20e3, color='y', alpha=0.3)\nplt.ylabel('Group delay, sec')\nplt.xlabel('Frequency, Hz')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nThe plot above shows that for frequencies below 100 Hz, the group delay is as much as 4 ms in the audio band. According to paper, Audibility of Group-Delay Equalization, the threshold is 2 ms. The abstract for the paper states:\n\nThe audibility thresholds for group-delay variation from several previous related studies are shown in Fig. 1. If not otherwise stated, these studies have been conducted using headphones. Green applied Huffman sequences, or truncated impulse responses of second-order allpass filters, to study the audibility of phase distortion. He found a threshold value for the peak group delay of about 2 ms for center frequencies of 625 Hz, 1875 Hz, and 4062 Hz.\n\nThe preamp group delay in the low end of the audio band is of concern and some re-design should be implemented if this preamp was to be implemented. The group delay plotted above agrees with the group delay results obtained from LTSpice simulation of the preamp circuit.\n\n\nComparing results to LTSpice\nThe LM833 TINA-TI Spice Model was entered into LTSpice, but it has some errors when run with LTSpice. So rather than debug the errors, an Analog Devices’s LT1115, was substiuded to obtain simulation results with an opamp model in order to comapre with the Python results, which uses an ideal opamp.\nThe table blow shows that the LM833 and LT1115 have simular performance characteristics.\n\n\n\n\n\n\n\n\n\n\n\nOp Amp\nDistortion %, THD + N at 1 kHz\nNoise, at 1 kHz (nV√Hz)\nSlew Rate, (V/µs)\nGBW (MHz)\nPower Bandwidth\n\n\n\n\nLM833N\n0.002\n4.5\n7\n15\n120 kHz @ 27 Vpp, RL = 2 kΩ, THD ≤ 1%\n\n\nLT1115\n0.002\n0.9\n10\n40\n180 kHz @ 30 VP-P, RL=2k\n\n\n\n\nos.chdir('/home/jeff32/Documents/Solving Electrical Engineering Problems with Python Blog/MNA Problem Circuits/Two amplifier RIAA Phone Preamp/') # change directory to csv file location\n\nfn = 'Two amplifier RIAA Phone Preamp.csv' # data from LTSpice\nLTSpice_data = np.genfromtxt(fn, delimiter=',')\n\n# change the working director back to the Jupyter folder\nos.chdir('/home/jeff32/Documents/JupyterLab/Node Analysis/')  \n\n\nfrequency = np.zeros(len(LTSpice_data))\nvoltage = np.zeros(len(LTSpice_data)).astype(complex)\n\nfor i in range(len(LTSpice_data)):\n    frequency[i] = LTSpice_data[i][0]\n    voltage[i] = LTSpice_data[i][1] + LTSpice_data[i][2]*1j\n\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nplt.semilogx(frequency, 20*np.log10(np.abs(voltage))+11.257,'-k')    # Bode magnitude plot, adding 11.257 dBV offset to normalized LTSpice data at 1KHz\nplt.semilogx(w_preamp/(2*np.pi), mag_preamp-preamp_gain_1kHz,'-b')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\nax1.set_ylim((-40,20))\nplt.grid()\nplt.axvspan(20, 20e3, color='y', alpha=0.3)\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'blue'\n\nplt.semilogx(frequency, np.angle(voltage)*180/np.pi,':',color=color,label='LT1115 phase')  # Bode phase plot\nplt.semilogx(w_preamp/(2*np.pi), phase_preamp,':',color='black',label='MNA phase')  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-5,25))\n\nax2.plot(np.NaN, np.NaN, '-', color='b', label='LT1115 magnitude')\nax2.plot(np.NaN, np.NaN, '-.', color='k', label='MNA magnitude')\n\nplt.legend(loc=0)\n\nplt.title('Bode plot')\nplt.show()\n\n\n\n\nAs is evident in the plot above, the results from LTSpice and Python agree.\n\n\nPreamplifier deviation from RIAA response\nThe plot below shows the deviation of the preamplifier from the RIAA response. The TI app note says conformance to the RIAA curve is within 0.1 dB from 20 Hz to 20 kHz.\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nplt.semilogx(w_preamp/(2*np.pi), (mag_RIAA-RIAA_gain_1kHz) + mag_preamp-preamp_gain_1kHz,'-k')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\nax1.set_ylim((-0.2,0.1))\nplt.grid()\nplt.axvspan(20, 20e3, color='y', alpha=0.3)\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:blue'\n\nplt.semilogx(w_preamp/(2*np.pi), phase_RIAA+phase_preamp,':',color=color,label='phase of S2')  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n\nax2.plot(np.NaN, np.NaN, '-', color='k', label='magnitude of S1')\nplt.legend(loc=0)\n\nplt.title('Bode plot')\nplt.show()\n\n\n\n\nThe calculations below will find the minimum and maximunation deviation of the amplitude response from the RIAA curve.\n\nidx_low = np.where(w_preamp/(2*np.pi) &gt; 20)[0][0]\nidx_high = np.where(w_preamp/(2*np.pi) &gt; 20e3)[0][0]\nprint('preamp deviation from RIAA curve: {:.3f} to {:.3f} dB'.format(((mag_RIAA-RIAA_gain_1kHz) + mag_preamp-preamp_gain_1kHz)[idx_low:idx_high].min(),((mag_RIAA-RIAA_gain_1kHz) + mag_preamp-preamp_gain_1kHz)[idx_low:idx_high].max()))\n\npreamp deviation from RIAA curve: -0.045 to 0.066 dB\n\n\nThe calculations above show that within the audio range the deviation of the preamp amplitude response from the RIAA curve varies from -0.044 to 0.066 dB when the nominal component values are used.\n\n\nSensitivity analysis\nAll circuits have characteristics that dependent on the values of the component. The sensitivity of a circuit’s performance is a measure of how much a particular circuit characteristic changes as a particular component value varies. In this analysis i’ll look at the changes of each pole or zero relative to the compenents value.\nThe root sensitivity function \\(S_x^y\\) gives the change occuring in filter characteristic per \\(\\delta y/ \\delta x\\).\n\\(S_x^y\\) is read as the sensitivity of the characteristic (i.e. y = \\(\\omega_n,\\) or Q or some other characteristic) with respect to the element x.\n\\(S_x^y = \\frac {x}{y} \\frac{\\delta y}{\\delta x}\\)\nWhere x is the filter component that is varied and y is the filter characteristic (\\(\\omega_n,\\) or Q etc.) that we wish to evaluate as x is varied.\nThe preamp transfer function is symbolic form is, H_preamp_sym, and we can get the numerator and denominator with the SymPy fraction function.\n\nH_sym_num, H_sym_denom = fraction(H_preamp_sym)\n\nThe Sympy solve function is used to find the root of the numerator and denimator polynominals.\n\nH_sym_zeros = solve(H_sym_num,s)\nH_sym_poles = solve(H_sym_denom,s)\n\n\nZeros\nHow many roots are there for the numerator polynominial?\n\nprint('there are {:d} zeros'.format(len(H_sym_zeros)))\n\nthere are 3 zeros\n\n\n\n\nZ0\nThe first zero is at DC.\n\nH_sym_Z0 = H_sym_zeros[0]\nH_sym_Z0\n\n\\(\\displaystyle 0\\)\n\n\n\n\nZ1\nThe second zero is given symbolically by the expression:\n\nH_sym_Z1 = H_sym_zeros[1]\nH_sym_Z1\n\n\\(\\displaystyle - \\frac{C_{1} R_{1} + Co R_{1} + Co R_{2} + Co Ro}{2 C_{1} Co R_{1} \\left(R_{2} + Ro\\right)} - \\frac{\\sqrt{C_{1}^{2} R_{1}^{2} + 2 C_{1} Co R_{1}^{2} - 2 C_{1} Co R_{1} R_{2} - 2 C_{1} Co R_{1} Ro + Co^{2} R_{1}^{2} + 2 Co^{2} R_{1} R_{2} + 2 Co^{2} R_{1} Ro + Co^{2} R_{2}^{2} + 2 Co^{2} R_{2} Ro + Co^{2} Ro^{2}}}{2 C_{1} Co R_{1} \\left(R_{2} + Ro\\right)}\\)\n\n\nWhat are the compenets that determine Z1?\n\nprint('the compenets that determine Z1 are: {:s} '.format(str(H_sym_Z1.free_symbols)))\n\nthe compenets that determine Z1 are: {R2, Ro, R1, C1, Co} \n\n\nWhat is the locations of Z1?\n\nprint('location of zero: {:.2f} Hz'.format(N(H_sym_Z1.subs(nominal_component_value))/(2*np.pi)))\n\nlocation of zero: -506.73 Hz\n\n\nZ1 is the zero at 500 Hz and is one of the RIAA time constants.\nWe can fine the sensitivity of Z1 to C1 with the following operation.\n\nS_C1_H_sym_Z1 = (C1/H_sym_Z1)*(H_sym_Z1.diff(C1))\nS_C1_H_sym_Z1\n\n\\(\\displaystyle \\frac{C_{1} \\left(- \\frac{1}{2 C_{1} Co \\left(R_{2} + Ro\\right)} - \\frac{C_{1} R_{1}^{2} + Co R_{1}^{2} - Co R_{1} R_{2} - Co R_{1} Ro}{2 C_{1} Co R_{1} \\left(R_{2} + Ro\\right) \\sqrt{C_{1}^{2} R_{1}^{2} + 2 C_{1} Co R_{1}^{2} - 2 C_{1} Co R_{1} R_{2} - 2 C_{1} Co R_{1} Ro + Co^{2} R_{1}^{2} + 2 Co^{2} R_{1} R_{2} + 2 Co^{2} R_{1} Ro + Co^{2} R_{2}^{2} + 2 Co^{2} R_{2} Ro + Co^{2} Ro^{2}}} + \\frac{C_{1} R_{1} + Co R_{1} + Co R_{2} + Co Ro}{2 C_{1}^{2} Co R_{1} \\left(R_{2} + Ro\\right)} + \\frac{\\sqrt{C_{1}^{2} R_{1}^{2} + 2 C_{1} Co R_{1}^{2} - 2 C_{1} Co R_{1} R_{2} - 2 C_{1} Co R_{1} Ro + Co^{2} R_{1}^{2} + 2 Co^{2} R_{1} R_{2} + 2 Co^{2} R_{1} Ro + Co^{2} R_{2}^{2} + 2 Co^{2} R_{2} Ro + Co^{2} Ro^{2}}}{2 C_{1}^{2} Co R_{1} \\left(R_{2} + Ro\\right)}\\right)}{- \\frac{C_{1} R_{1} + Co R_{1} + Co R_{2} + Co Ro}{2 C_{1} Co R_{1} \\left(R_{2} + Ro\\right)} - \\frac{\\sqrt{C_{1}^{2} R_{1}^{2} + 2 C_{1} Co R_{1}^{2} - 2 C_{1} Co R_{1} R_{2} - 2 C_{1} Co R_{1} Ro + Co^{2} R_{1}^{2} + 2 Co^{2} R_{1} R_{2} + 2 Co^{2} R_{1} Ro + Co^{2} R_{2}^{2} + 2 Co^{2} R_{2} Ro + Co^{2} Ro^{2}}}{2 C_{1} Co R_{1} \\left(R_{2} + Ro\\right)}}\\)\n\n\nEvaluating this numerically with the component values, we get get the sensitivity of Z1 to C1.\n\nprint('the sensitivity of Z1 to C1 is: {:.2f}'.format(N(S_C1_H_sym_Z1.subs(nominal_component_value))))\n\nthe sensitivity of Z1 to C1 is: -1.00\n\n\nDoing the math with SymPy, we can get the sensitivity of Z1 to the other components.\n\nS_R1_H_sym_Z1 = (R1/H_sym_Z1)*(H_sym_Z1.diff(R1))\nprint('the sensitivity of Z1 to R1 is: {:.2f}'.format(N(S_R1_H_sym_Z1.subs(nominal_component_value))))\n\nthe sensitivity of Z1 to R1 is: -0.10\n\n\n\nS_R2_H_sym_Z1 = (R2/H_sym_Z1)*(H_sym_Z1.diff(R2))\nprint('the sensitivity of Z1 to R2 is: {:.2f}'.format(N(S_R2_H_sym_Z1.subs(nominal_component_value))))\n\nthe sensitivity of Z1 to R2 is: -0.85\n\n\n\nS_Co_H_sym_Z1 = (Co/H_sym_Z1)*(H_sym_Z1.diff(Co))\nprint('the sensitivity of Z1 to Co is: {:.4f}'.format(N(S_Co_H_sym_Z1.subs(nominal_component_value))))\n\nthe sensitivity of Z1 to Co is: -0.0002\n\n\n\nS_Ro_H_sym_Z1 = (Ro/H_sym_Z1)*(H_sym_Z1.diff(Ro))\nprint('the sensitivity of Z1 to Ro is: {:.2f}'.format(N(S_Ro_H_sym_Z1.subs(nominal_component_value))))\n\nthe sensitivity of Z1 to Ro is: -0.05\n\n\nLater, we we are doing the worst case analsys, we can ignore Co. \n\n\nZ2\nThe third zero of the transfer function is Z2.\n\nH_sym_Z2 = H_sym_zeros[2]\nH_sym_Z2\n\n\\(\\displaystyle - \\frac{C_{1} R_{1} + Co R_{1} + Co R_{2} + Co Ro}{2 C_{1} Co R_{1} \\left(R_{2} + Ro\\right)} + \\frac{\\sqrt{C_{1}^{2} R_{1}^{2} + 2 C_{1} Co R_{1}^{2} - 2 C_{1} Co R_{1} R_{2} - 2 C_{1} Co R_{1} Ro + Co^{2} R_{1}^{2} + 2 Co^{2} R_{1} R_{2} + 2 Co^{2} R_{1} Ro + Co^{2} R_{2}^{2} + 2 Co^{2} R_{2} Ro + Co^{2} Ro^{2}}}{2 C_{1} Co R_{1} \\left(R_{2} + Ro\\right)}\\)\n\n\n\nprint('the compenets that determine Z2 are: {:s} '.format(str(H_sym_Z2.free_symbols)))\n\nthe compenets that determine Z2 are: {R2, Ro, R1, C1, Co} \n\n\n\nprint('Z2: {:.3e} Hz'.format(N(H_sym_Z2.subs(nominal_component_value))/(2*np.pi)))\n\nZ2: -8.885e-3 Hz\n\n\nThe zero Z2, evaluates to a system zero at DC and is not one the of RIAA time constants.\n\nS_C1_H_sym_Z2 = (C1/H_sym_Z2)*(H_sym_Z2.diff(C1))\nprint('the sensitivity of Z2 to C1 is: {:.3e}'.format(N(S_C1_H_sym_Z2.subs(nominal_component_value))))\n\nthe sensitivity of Z2 to C1 is: -1.580e-4\n\n\n\nS_Co_H_sym_Z2 = (Co/H_sym_Z2)*(H_sym_Z2.diff(Co))\nprint('the sensitivity of Z2 to Co is: {:.2f}'.format(N(S_Co_H_sym_Z2.subs(nominal_component_value))))\n\nthe sensitivity of Z2 to Co is: -1.00\n\n\n\nS_R1_H_sym_Z2 = (R1/H_sym_Z2)*(H_sym_Z2.diff(R1))\nprint('the sensitivity of Z2 to R1 is: {:.2f}'.format(N(S_R1_H_sym_Z2.subs(nominal_component_value))))\n\nthe sensitivity of Z2 to R1 is: -0.90\n\n\n\nS_R2_H_sym_Z2 = (R2/H_sym_Z2)*(H_sym_Z2.diff(R2))\nprint('the sensitivity of Z2 to R2 is: {:.2f}'.format(N(S_R2_H_sym_Z2.subs(nominal_component_value))))\n\nthe sensitivity of Z2 to R2 is: -0.09\n\n\n\nS_Ro_H_sym_Z2 = (Ro/H_sym_Z2)*(H_sym_Z2.diff(Ro))\nprint('the sensitivity of Z2 to Ro is: {:.3f}'.format(N(S_Ro_H_sym_Z2.subs(nominal_component_value))))\n\nthe sensitivity of Z2 to Ro is: -0.006\n\n\n\n\nPoles\nHow many poles are there in the preamp transfer function?\n\nprint('there are {:d} poles in the transfer function'.format(len(H_sym_poles)))\n\nthere are 4 poles in the transfer function\n\n\n\n\nP0\nThe first pole is:\n\nH_sym_P0 = H_sym_poles[0]\nH_sym_P0\n\n\\(\\displaystyle - \\frac{1}{C_{1} R_{1}}\\)\n\n\nThe pole P0 evaluates to one of the RIAA time constants.\n\nprint('P0: {:.2f}Hz'.format(N(H_sym_P0.subs(nominal_component_value))/(2*np.pi)))\n\nP0: -50.63Hz\n\n\n\nS_C1_H_sym_P0 = (C1/H_sym_P0)*(H_sym_P0.diff(C1))\nprint('the sensitivity of P0 to C1 is: {:.2f}'.format(N(S_C1_H_sym_P0.subs(nominal_component_value))))\n\nthe sensitivity of P0 to C1 is: -1.00\n\n\n\nS_R1_H_sym_P0 = (R1/H_sym_P0)*(H_sym_P0.diff(R1))\nprint('the sensitivity of P0 to R1 is: {:.2f}'.format(N(S_R1_H_sym_P0.subs(nominal_component_value))))\n\nthe sensitivity of P0 to R1 is: -1.00\n\n\n\n\nP1\nThe second pole is:\n\nH_sym_P1 = H_sym_poles[1]\nH_sym_P1\n\n\\(\\displaystyle - \\frac{1}{Co Ro}\\)\n\n\nThe pole P1 evaluates to a frequency near DC.\n\nprint('P1: {:.2f}Hz'.format(N(H_sym_P1.subs(nominal_component_value))/(2*np.pi)))\n\nP1: -1.59Hz\n\n\n\nS_Co_H_sym_P1 = (Co/H_sym_P1)*(H_sym_P1.diff(Co))\nprint('the sensitivity of P1 to Co is: {:.2f}'.format(N(S_Co_H_sym_P1.subs(nominal_component_value))))\n\nthe sensitivity of P1 to Co is: -1.00\n\n\n\nS_Ro_H_sym_P1 = (Ro/H_sym_P1)*(H_sym_P1.diff(Ro))\nprint('the sensitivity of P1 to Ro is: {:.2f}'.format(N(S_Ro_H_sym_P1.subs(nominal_component_value))))\n\nthe sensitivity of P1 to Ro is: -1.00\n\n\n\n\nP2\nThe 3rd pole is:\n\nH_sym_P2 = H_sym_poles[2]\nH_sym_P2\n\n\\(\\displaystyle \\frac{- C_{3} R_{3} - C_{4} R_{3} - C_{4} R_{6} - \\sqrt{C_{3}^{2} R_{3}^{2} + 2 C_{3} C_{4} R_{3}^{2} - 2 C_{3} C_{4} R_{3} R_{6} + C_{4}^{2} R_{3}^{2} + 2 C_{4}^{2} R_{3} R_{6} + C_{4}^{2} R_{6}^{2}}}{2 C_{3} C_{4} R_{3} R_{6}}\\)\n\n\n\nprint('the compenets that determine P2 are: {:s} '.format(str(H_sym_P2.free_symbols)))\n\nthe compenets that determine P2 are: {R3, R6, C4, C3} \n\n\n\nprint('P2: {:.2f}Hz'.format(N(H_sym_P2.subs(nominal_component_value))/(2*np.pi)))\n\nP2: -2122.88Hz\n\n\n\nS_C3_H_sym_P2 = (C3/H_sym_P2)*(H_sym_P2.diff(C3))\nprint('the sensitivity of P2 to C3 is: {:.2f}'.format(N(S_C3_H_sym_P2.subs(nominal_component_value))))\n\nthe sensitivity of P2 to C3 is: -1.00\n\n\n\nS_C4_H_sym_P2 = (C4/H_sym_P2)*(H_sym_P2.diff(C4))\nprint('the sensitivity of P2 to C4 is: {:.3e}'.format(N(S_C4_H_sym_P2.subs(nominal_component_value))))\n\nthe sensitivity of P2 to C4 is: -2.829e-5\n\n\n\nS_R3_H_sym_P2 = (R3/H_sym_P2)*(H_sym_P2.diff(R3))\nprint('the sensitivity of P2 to R3 is: {:.2f}'.format(N(S_R3_H_sym_P2.subs(nominal_component_value))))\n\nthe sensitivity of P2 to R3 is: -0.96\n\n\n\nS_R6_H_sym_P2 = (R6/H_sym_P2)*(H_sym_P2.diff(R6))\nprint('the sensitivity of P2 to R6 is: {:.3f}'.format(N(S_R6_H_sym_P2.subs(nominal_component_value))))\n\nthe sensitivity of P2 to R6 is: -0.041\n\n\n\n\nP3\nThe 4th pole is:\n\nH_sym_P3 = H_sym_poles[3]\nH_sym_P3\n\n\\(\\displaystyle \\frac{- C_{3} R_{3} - C_{4} R_{3} - C_{4} R_{6} + \\sqrt{C_{3}^{2} R_{3}^{2} + 2 C_{3} C_{4} R_{3}^{2} - 2 C_{3} C_{4} R_{3} R_{6} + C_{4}^{2} R_{3}^{2} + 2 C_{4}^{2} R_{3} R_{6} + C_{4}^{2} R_{6}^{2}}}{2 C_{3} C_{4} R_{3} R_{6}}\\)\n\n\n\nprint('the compenets that determine P3 are: {:s} '.format(str(H_sym_P3.free_symbols)))\n\nthe compenets that determine P3 are: {R3, R6, C4, C3} \n\n\n\nprint('P3: {:.2f}Hz'.format(N(H_sym_P3.subs(nominal_component_value))/(2*np.pi)))\n\nP3: -1.39Hz\n\n\n\nS_C3_H_sym_P3 = (C3/H_sym_P3)*(H_sym_P3.diff(C3))\nprint('the sensitivity of P3 to C3 is: {:.2e}'.format(N(S_C3_H_sym_P3.subs(nominal_component_value))))\n\nthe sensitivity of P3 to C3 is: -2.83e-5\n\n\n\nS_C4_H_sym_P3 = (C4/H_sym_P3)*(H_sym_P3.diff(C4))\nprint('the sensitivity of P3 to C5 is: {:.2f}'.format(N(S_C4_H_sym_P3.subs(nominal_component_value))))\n\nthe sensitivity of P3 to C5 is: -1.00\n\n\n\nS_R3_H_sym_P3 = (R3/H_sym_P3)*(H_sym_P3.diff(R3))\nprint('the sensitivity of P3 to R3 is: {:.2f}'.format(N(S_R3_H_sym_P3.subs(nominal_component_value))))\n\nthe sensitivity of P3 to R3 is: -0.04\n\n\n\nS_R6_H_sym_P3 = (R6/H_sym_P3)*(H_sym_P3.diff(R6))\nprint('the sensitivity of P3 to R6 is: {:.2f}'.format(N(S_R6_H_sym_P3.subs(nominal_component_value))))\n\nthe sensitivity of P3 to R6 is: -0.96\n\n\nIn the worst case analysis below, the componets that have sensitivites greater than 0.05 are the ones that factor into the worst case analysis.\n\n\n\nComponent selection\nThe table below list each of the components used in the preamp along with a link to the Digikey pages for each of the components in the preamp. Digikey is a larger distributor of electronic components in the US. All the components have operating temperature ranges that exceed the normal household envirment.\nThe resistors chosen are all 1% Metal film type resistors. Metal film resistors possess good noise characteristics and low non-linearity due to a low voltage coefficient. They are also beneficial due to long-term stability.\nThe capacitors are all polypropylene Film capacitor types. Polystyrene or polypropylene are considered the best for audio applications.\nThe Op Amp, LM833N, is a dual bipolar low noise (\\(\\frac {4.5nV}{\\sqrt{Hz}}\\)), wide bandwidth (16 MHz) audio operational amplifier from Texas Instrments.\n\n\n\nRef\nValue\nDescription\nDigikey PN\n\n\n\n\nRo\n499\n±1% 1/4W Metal Film\nRNF14FTD499RCT-ND\n\n\nRp\n47k\n±1% 1/4W Metal Film\n13-MFR-25FTE52-47KTB-ND\n\n\nR1\n80.6k\n±1% 1/4W Metal Film\n80.6KXBK-ND\n\n\nR2\n58.45k\n±1% 1/4W Metal Film\nRNF14FTD8K45CT-ND\n\n\nR3\n2.37k\n±1% 1/4W Metal Film\n13-MFR-25FBF52-2K37-ND\n\n\nR4\n2k\n±1% 1/4W Metal Film\n13-MFR-25FRF52-2KCT-ND\n\n\nR5\n4.3k\n±1% 1/4W Metal Film\nS4.3KCACT-ND\n\n\nR6\n54.9k\n±1% 1/4W Metal Film\nRNF14FTD54K9CT-ND\n\n\nCo\n200\\(\\mu\\)\n10% Film Capacitor 450V Polypropylene\n283-EFDKS45K207F064DH-ND\n\n\nCp\n100p\n10% Film Capacitor 250V Polypropylene\n399-RSBEC0100ZA00M-ND\n\n\nC1\n0.039\\(\\mu\\)\n2% Film Capacitor 25V 63V Polypropylene\nBC2066-ND\n\n\nC3\n0.033\\(\\mu\\)\n1% Film Capacitor 63V 100V Polypropylene\n399-PHE426DJ5330FR17T0CT-ND\n\n\nC4\n2\\(\\mu\\)\n10% Film Capacitor 305V 630V Polypropylene\n495-B32923P3205K000-ND\n\n\nU1, U2\nLM833N\nAudio op amp\n296-44419-5-ND\n\n\n\nThe parts in this list are considered good choices for a first pass at the bill of materials. The size of the production run and the piece part cost are also a factors which must be considered if the preamp is going to be built. One thing to notice is that Co, the 200 \\(\\mu\\) F capacitor is expensive. The use of a polypropylene film capacitor for this component is consistent with the advice of keeping all capacitors in the audio path polystyrene or polypropylene.\n\n\nMonte Carlo simulation\nIn this analysis the circuit equations are solved after assigning random element values from within the tolerance band to the components. This simulates building a large number of circuits with components chosen at random from bins or reals of components during the board stuffing process. All the components are required to met their specifications, but are allowed to have some varaition accorting to theier tolerance. For example a 1% 2k resistor can range from 1980 to 2020 \\(\\Omega\\). In addition to the components initial tolerance, the temperature coefficient and aging of paramters can also be included.\nIn this simulation, I’m only including the initial tolerances of parameters and I’m assuming the distritution is uniform. The Numpy function random.uniform is used to generate the random values within the tolerance range, however, for this function, the hight end-point value may or may not be included in the range depending on floating-point rounding, so if this is important, some adjustments to the code are required. The Numpy function random.seed is used to re-seed the random number generator.\n\nnum = 20 # number of simulations to run\nnew_x_axis_range = np.logspace(1, 5.5, 100, endpoint=True)*2*np.pi\n\n# make some arrays to the hold the results of each run\nmag_ans = np.zeros(shape=(num,len(new_x_axis_range)))\nphase_ans = np.zeros(shape=(num,len(new_x_axis_range)))\n\ncomponent_values_tol = nominal_component_value.copy() # makde a copy\n\nrandom.seed(a=None, version=2) # re-seed the random number generator\n\nThe following takes about 15 seconds to run on for num=20 on an i3 machine.\n\nfor i in range(0,num):\n\n    component_values_tol[Ro] = random.uniform(nominal_component_value[Ro]-nominal_component_value[Ro]*0.01,nominal_component_value[Ro]+nominal_component_value[Ro]*0.01)\n    component_values_tol[Rp] = random.uniform(nominal_component_value[Rp]-nominal_component_value[Rp]*0.01,nominal_component_value[Rp]+nominal_component_value[Ro]*0.01)\n\n    component_values_tol[R1] = random.uniform(nominal_component_value[R1]-nominal_component_value[R1]*0.01,nominal_component_value[R1]+nominal_component_value[R1]*0.01)\n    component_values_tol[R2] = random.uniform(nominal_component_value[R2]-nominal_component_value[R2]*0.01,nominal_component_value[R2]+nominal_component_value[R2]*0.01)    \n    component_values_tol[R3] = random.uniform(nominal_component_value[R3]-nominal_component_value[R3]*0.01,nominal_component_value[R3]+nominal_component_value[R3]*0.01)\n    component_values_tol[R4] = random.uniform(nominal_component_value[R4]-nominal_component_value[R4]*0.01,nominal_component_value[R4]+nominal_component_value[R4]*0.01)    \n    component_values_tol[R5] = random.uniform(nominal_component_value[R5]-nominal_component_value[R5]*0.01,nominal_component_value[R5]+nominal_component_value[R5]*0.01)\n    component_values_tol[R6] = random.uniform(nominal_component_value[R6]-nominal_component_value[R6]*0.01,nominal_component_value[R6]+nominal_component_value[R6]*0.01)    \n\n    component_values_tol[Co] = random.uniform(nominal_component_value[Co]-nominal_component_value[Co]*0.1,nominal_component_value[Co]+nominal_component_value[Co]*0.1)\n    component_values_tol[Cp] = random.uniform(nominal_component_value[Cp]-nominal_component_value[Cp]*0.1,nominal_component_value[Cp]+nominal_component_value[Cp]*0.1)\n    component_values_tol[C1] = random.uniform(nominal_component_value[C1]-nominal_component_value[C1]*0.02,nominal_component_value[C1]+nominal_component_value[C1]*0.02)\n    component_values_tol[C3] = random.uniform(nominal_component_value[C3]-nominal_component_value[C3]*0.01,nominal_component_value[C3]+nominal_component_value[C3]*0.01)\n    component_values_tol[C4] = random.uniform(nominal_component_value[C4]-nominal_component_value[C4]*0.1,nominal_component_value[C4]+nominal_component_value[C4]*0.1)\n\n    # enter the element values\n    preamp_equ_tol = preamp_equ_sym.subs(component_values_tol)\n\n    U_preamp_tol = solve(preamp_equ_tol,X)\n\n    H_preamp_tol = U_preamp_tol[v2]/U_preamp_tol[v1]\n\n    # Extract the numerator and denominator polynomials so that the system can be defined in SciPy.\n    H_preamp_tol_num, H_preamp_tol_denom = fraction(H_preamp_tol) #returns numerator and denominator\n\n    # convert symbolic to numpy polynomial\n    a2 = np.array(Poly(H_preamp_tol_num, s).all_coeffs(), dtype=float)\n    b2 = np.array(Poly(H_preamp_tol_denom, s).all_coeffs(), dtype=float)\n    preamp_sys_tol = signal.TransferFunction(a2,b2)\n\n    w_preamp_sys_tol, mag_preamp_sys_tol, phase_preamp_sys_tol = preamp_sys_tol.bode(w=new_x_axis_range)\n    \n    # save the results from each run\n    mag_ans[i] = mag_preamp_sys_tol\n    phase_ans[i] = phase_preamp_sys_tol\n\n\n\nPreamplifier deviation from RIAA response\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nfor i in range(0,num):\n    plt.semilogx(w_RIAA/(2*np.pi), (mag_RIAA-RIAA_gain_1kHz) + (mag_ans[i]-preamp_gain_1kHz),'-k')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\nax1.set_ylim((-0.3,0.3))\nplt.grid()\nplt.axvspan(20, 20e3, color='y', alpha=0.3)\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:blue'\n\nfor i in range(0,num):\n    plt.semilogx(w_RIAA/(2*np.pi), phase_RIAA+phase_ans[i],':',color=color)  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n\nplt.title('Monte Carlo runs showing deviation from RIAA curve')\nplt.show()\n\n\n\n\nThe Monte Carlo simulation shows that the preamp amplitude response deviatin from the RIAA curve can very from -0.2 to 0.3 dB over the audio band. If the performance requirement for this preamp was to be within \\(\\pm\\) 0.1 dB of the RIAA curve, then some redesign or tighter component tolerancing is required.\n\n\nWorst case analysis\nIn a worst case analysis, we would look at:\n\nminimum and maximum values of the initial component tolerance\n\nmaximum or minumum temperature coefficients of the parameters\n\nmaximum aging or drift of parameter values\n\nSince we usually can’t tell by inspection which combination of minimum and maximum values will give the worst case, we can run a number of simulations in which all combination of minum and maximium variations are included. From the family of results we can look for the worst case.\nHow resistors and capactors in the preamp circuit?\n\nprint('number of components: {:d}'.format(len(nominal_component_value)))\n\nnumber of components: 14\n\n\nHow many combinations min and max combinations?\n\nprint('number of min and max combinations: {:,d}'.format(2**14))\n\nnumber of min and max combinations: 16,384\n\n\n16 thousand simulation runs to too many. From the sensitivity analysis above, only R1, R2, R3, R6, Ro, C1 and C3 are sensitive. Running all combinations of the min and max tolerance for this set is reasonable and is \\(2^7=128\\) combinations.\nThe tolerances for each of the componts is defined below:\n\nTol = {Ro:0.01,R1:0.01,R2:0.01,R3:0.01,R6:0.01,C1:0.02,C3:0.01}\n\nOnly C1 has a tolerance other than 1%.\nThe array ‘run’ is created that consists of a binary count, with leading zeros from 0 to 127. Then the zero values are replaced with -1.\n\nrun = []\nfor i in range(0,2**(len(Tol))):\n    temp = list('{:07b}'.format(i)) # include leading zeros\n    for j in range(len(temp)):\n        temp[j] = int(temp[j])\n    run.append(temp)\n\nrun = np.asarray(run)\nrun = np.where(run == 0, -1, run)\n\nThe first row of run is:\n\nrun[0]\n\narray([-1, -1, -1, -1, -1, -1, -1])\n\n\nIn the for loop below, at i = 0, run[0] would be all -1’s and this could apply the low tolerance range to the nominal component values.\n\nrun[-1]\n\narray([1, 1, 1, 1, 1, 1, 1])\n\n\nThe last time through the for loop, where i = 127, run[-1] is all 1’s and this would apply the high tolerance range to the nominal component values. Between i = 0 and i = 127, all combinations of minumum and maximum tolerance is appled.\n\nnew_x_axis_range = np.logspace(1, 5.5, 100, endpoint=True)*2*np.pi\n\n# make some arrays to hold the results\nmag_ans = np.zeros(shape=(len(run),len(new_x_axis_range)))\nphase_ans = np.zeros(shape=(len(run),len(new_x_axis_range)))\n\nThe following cell takes about 90 seconds to run on an i3 machine.\n\nfor i in range(len(run)):\n    component_values_tol[Ro] = nominal_component_value[Ro]*(1+run[i][0]*Tol[Ro])\n    component_values_tol[Rp] = nominal_component_value[Rp]\n\n    component_values_tol[R1] = nominal_component_value[R1]*(1+run[i][1]*Tol[R1])\n    component_values_tol[R2] = nominal_component_value[R2]*(1+run[i][2]*Tol[R2]) \n    component_values_tol[R3] = nominal_component_value[R3]*(1+run[i][3]*Tol[R3])\n\n    component_values_tol[R4] = nominal_component_value[R4] \n    component_values_tol[R5] = nominal_component_value[R5]\n    component_values_tol[R6] = nominal_component_value[R6]*(1+run[i][4]*Tol[R6])\n\n    component_values_tol[Co] = nominal_component_value[Co]\n    component_values_tol[Cp] = nominal_component_value[Cp]\n    component_values_tol[C1] = nominal_component_value[C1]*(1+run[i][5]*Tol[C1])\n    component_values_tol[C3] = nominal_component_value[C3]*(1+run[i][6]*Tol[C3])\n    component_values_tol[C4] = nominal_component_value[C4]\n    \n    # enter the element values\n    preamp_equ_tol = preamp_equ_sym.subs(component_values_tol)\n\n    U_preamp_tol = solve(preamp_equ_tol,X)\n\n    H_preamp_tol = U_preamp_tol[v2]/U_preamp_tol[v1]\n\n    # Extract the numerator and denominator polynomials so that the system can be defined in SciPy.\n    H_preamp_tol_num, H_preamp_tol_denom = fraction(H_preamp_tol) #returns numerator and denominator\n\n    # convert symbolic to numpy polynomial\n    a2 = np.array(Poly(H_preamp_tol_num, s).all_coeffs(), dtype=float)\n    b2 = np.array(Poly(H_preamp_tol_denom, s).all_coeffs(), dtype=float)\n    preamp_sys_tol = signal.TransferFunction(a2,b2)\n\n    w_preamp_sys_tol, mag_preamp_sys_tol, phase_preamp_sys_tol = preamp_sys_tol.bode(w=new_x_axis_range)\n    mag_ans[i] = mag_preamp_sys_tol\n    phase_ans[i] = phase_preamp_sys_tol\n\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nfor i in range(0,num):\n    plt.semilogx(w_RIAA/(2*np.pi), (mag_RIAA-RIAA_gain_1kHz) + (mag_ans[i]-preamp_gain_1kHz),'-k')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\nax1.set_ylim((-0.3,0.4))\nplt.grid()\nplt.axvspan(20, 20e3, color='y', alpha=0.3)\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:blue'\n\nfor i in range(0,num):\n    plt.semilogx(w_RIAA/(2*np.pi), phase_RIAA+phase_ans[i],':',color=color)  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n\nplt.title('Worst Case deviation from RIAA curve')\nplt.show()\n\n\n\n\nThe results above show that the worst case tolerance conditions yeild deviations of -0.19 to 0.3 dB from the RIAA curve."
  },
  {
    "objectID": "Two amplifier RIAA Phono Preamp.html#summary",
    "href": "Two amplifier RIAA Phono Preamp.html#summary",
    "title": "Two amplifier RIAA phono preamp",
    "section": "Summary",
    "text": "Summary\nThe circuit presented in this analysis is just one example of many circuits that can be found on line. For each design to be evaluated, some type of side by side analysis should be used used to down select. This notebook can be used as template for any compartivite analysis.\nThe circuit in the app note appears to have low frequency group delay that might be an issue. Also the deviation from the RIAA curve using normal component tolerances does not meet the 0.1 dB requirement. The worst case analysis also confirms this. The circuit employs an expesive 200\\(\\mu\\) Farad film capacitor. The circuit is missing a subsonic filter."
  },
  {
    "objectID": "Source free series circuit.html",
    "href": "Source free series circuit.html",
    "title": "Source free series circuit",
    "section": "",
    "text": "Last update: 9 May 2022\nIn this notebook, the Python modules SymPy and SciPy are used to solve for currents and voltages in a series R, L and C circuit from the characteristic differential equation.\nA source free series RLC circuit consists of a resistor, capacitor and inductor connected in series with some initial energy stored either in the inductor, capacitor or both. Since the circuit is a single loop, the current flowing around the loop is the same current in each component. Both parallel and series connected circuits are usually presented in electrical circuit analysis classes. The two textbooks I used while in college presented the parallel connected circuit in some detail then kind of glossed over the series connect circuit since it is the dual of the other. The circuit to be analyzed is shown below and was drawn using EasyEDA, the link to the schematic is here.\nNotice that the component values are not shown, nor is there a reference node indicated. Once we write the circuit equation we can do the math symbolically with SymPy. Or we can can solve the differential equation numerically with SciPy. This notebook will look at SymPy and SciPy based solutions for the voltage and currents in the circuit. Initial conditions will be either the voltage on the capacitor or the current in the loop.\nThe following Python libraries are used. NumPy is the fundamental package for scientific computing in Python. SciPy is a collection of mathematical algorithms and convenience functions built on the NumPy extension of Python. Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. SymPy is a Python library for symbolic mathematics. The init_printing() function will allow the Jupyter notebook to render equations in the best format.\nimport numpy as np\nfrom scipy.integrate import odeint\nimport matplotlib.pyplot as plt\nfrom sympy import *\ninit_printing()"
  },
  {
    "objectID": "Source free series circuit.html#sympy-solution-given-it1-and-it0-at-t0",
    "href": "Source free series circuit.html#sympy-solution-given-it1-and-it0-at-t0",
    "title": "Source free series circuit",
    "section": "SymPy solution, given \\(i(t)=1\\) and \\(i'(t)=0\\) at \\(t=0\\)",
    "text": "SymPy solution, given \\(i(t)=1\\) and \\(i'(t)=0\\) at \\(t=0\\)\nThe current at t=0 is 1 amp and the derivative of the current at t=0 is 0. The initial conditions are needed to solve for the constants as shown later. By use of Kirchhoff’s current law, the current around the loop is the same current flowing in each component and the sum of the potential differences (voltages) around the loop is zero.\nWithin SymPy we can declare variable, t, for time and tell Sympy that t is only positive. Also, we can define the function i(t) as the current in the loop by using the class sympy.core.function.Function.\n\nt = symbols('t',positive=True)  # t &gt; 0\ni = Function('i')(t)\n\nThe symbols L, R and C are used for inductance, resistance and capacitance.\n\nL, R, C = symbols('L R C')\n\nThe voltage current relation of the components in the circuit is defined as follows: The voltage across the resistor is defined by Ohm’s law and is \\(Ri(t)=v(t)\\). An inductor is an electrical component that stores energy in a magnetic field when electric current flows through it. The voltage across the terminals is the derivative of the current, \\(L\\frac{d}{dt}i(t)=v(t)\\) A capacitor is a device that stores energy in an electric field. The terminal voltage is defined by \\(\\frac{1}{C}\\int_0^{\\tau}{i(t)}dt=v(t)\\).\nThe sum of voltages on each of the components around the loop is equal to zero and the equation is written as: \\(L\\frac{d}{dt}i(t)+Ri(t)+\\frac{1}{C}\\int_0^{\\tau}{i(t)}dt=0\\)\nWhen both sides are differentiated with respect to time the result is a homogeneous differential equation. The unknown function is the current i(t) and the solution to the differential equation is the natural response.\nThe variable zero is defined as a constant so that it can be included in the SymPy equation and differentiated without SymPy returning an error.\n\nzero = symbols('zero',constant = True)\n\nloop_voltage is the differential equation that says the sum of the voltages around the loop is equal to zero.\n\nloop_voltage = Eq((L*i.diff(t)+R*i+Integral(i,t)/C).diff(t),(zero).diff(t))\nloop_voltage\n\nEq(L*Derivative(i(t), (t, 2)) + R*Derivative(i(t), t) + i(t)/C, 0)\n\n\nThe SymPy function dsolve is used to find the solution to most kinds of ordinary differential equations and systems of ordinary differential equations. Except for the evaluation of the constants, C1 and C2, the current in the loop as a function of time is shown below.\n\nloop_current = dsolve(loop_voltage,i)\nloop_current\n\n\\[i{\\left (t \\right )} = C_{1} e^{\\frac{t \\left(- R - \\sqrt{R^{2} - \\frac{4 L}{C}}\\right)}{2 L}} + C_{2} e^{\\frac{t \\left(- R + \\sqrt{R^{2} - \\frac{4 L}{C}}\\right)}{2 L}}\\]\n\n\nAt t=0, the loop current is 1 amp as stated above. The following line of Python code evaluates the right hand side (using args[1]) of the loop current equation at time zero (subs({t:0})) and equates it to 1 amp.\n\nEq(loop_current.args[1].subs({t:0}),1)\n\n\\[C_{1} + C_{2} = 1\\]\n\n\nThe derivative of the loop current is evaluated at t=0 and the result is set equal to zero. This is written as an equation using the following python code.\n\nEq(loop_current.args[1].diff(t).subs({t:0}),0)\n\n\\[\\frac{C_{1} \\left(- R - \\sqrt{R^{2} - \\frac{4 L}{C}}\\right)}{2 L} + \\frac{C_{2} \\left(- R + \\sqrt{R^{2} - \\frac{4 L}{C}}\\right)}{2 L} = 0\\]\n\n\nThe symbols C1 and C2 are declared for the constants in the solution for the differential equation and evaluation of the loop current equation at t=0 and the derivative at t=0 are put into matrix form as shown below.\n\nC1, C2 = symbols('C1 C2')\nMatrix(\n    [[Eq(loop_current.args[1].subs({t:0}),1)],\n    [Eq(loop_current.args[1].diff(t).subs({t:0}),0)]])\n\n\\[\\left[\\begin{matrix}C_{1} + C_{2} = 1\\\\\\frac{C_{1} \\left(- R - \\sqrt{R^{2} - \\frac{4 L}{C}}\\right)}{2 L} + \\frac{C_{2} \\left(- R + \\sqrt{R^{2} - \\frac{4 L}{C}}\\right)}{2 L} = 0\\end{matrix}\\right]\\]\n\n\nThe Sympy function solve can be used to find a solution to these two equations and two unknowns.\n\nconst = solve(Matrix(\n    [[Eq(loop_current.args[1].subs({t:0}),1)],\n    [Eq(loop_current.args[1].diff(t).subs({t:0}),0)]]),[C1, C2])\n\nconst\n\n\\[\\left \\{ C_{1} : - \\frac{R}{2 \\sqrt{R^{2} - \\frac{4 L}{C}}} + \\frac{1}{2}, \\quad C_{2} : \\frac{R}{2 \\sqrt{R^{2} - \\frac{4 L}{C}}} + \\frac{1}{2}\\right \\}\\]\n\n\nSubstituting the constants back into the loop current equation we get the solution in symbolic form.\n\nloop_current.subs(const)\n\n\\[i{\\left (t \\right )} = \\left(- \\frac{R}{2 \\sqrt{R^{2} - \\frac{4 L}{C}}} + \\frac{1}{2}\\right) e^{\\frac{t \\left(- R - \\sqrt{R^{2} - \\frac{4 L}{C}}\\right)}{2 L}} + \\left(\\frac{R}{2 \\sqrt{R^{2} - \\frac{4 L}{C}}} + \\frac{1}{2}\\right) e^{\\frac{t \\left(- R + \\sqrt{R^{2} - \\frac{4 L}{C}}\\right)}{2 L}}\\]\n\n\nGoing future requires some numeric values for R, L and C. If the values of R, L and C produce a negative square root, then the natural response is called underdamped because the current waveform is a decaying sinusoidal wave. Values of R, L and C that don’t have an imaginary exponent produce a natural response that is called overdamped. Examples of the underdamped and overdamped responses are shown below.\nEngineering text books usually define the resonant frequency and the damping coefficient, then define the over, under and critically damped conditions in terms of the resonant frequency and the damping coefficient as follows:\nResonant frequency: \\(\\omega_o=\\frac {1} {\\sqrt{LC}}\\), the frequency at which a system tends to oscillate in the absence of a driving force.\nDamping coefficient: \\(\\alpha = \\frac {R}{2RL}\\), a measure describing how oscillations in a system decay.\nOverdamped: \\(\\alpha &gt; \\omega_o\\), the response is the sum of two decaying exponentials with no oscillation.\nCritically damped: \\(\\alpha = \\omega_o\\), the response sits on the border between the overdamped and underdamped cases.\nUnder damped: \\(\\alpha &lt; \\omega_o\\), the response is a decaying sinusoidal wave and has a frequency of \\(\\omega_d = \\sqrt{\\omega_o^2 + \\alpha^2}\\)"
  },
  {
    "objectID": "Source free series circuit.html#over-damped-response",
    "href": "Source free series circuit.html#over-damped-response",
    "title": "Source free series circuit",
    "section": "Over damped response",
    "text": "Over damped response\nThe condition for the overdamped response is satisfied by, \\(\\frac{R}{2L}&gt;\\frac{1}{\\sqrt{LC}}\\). Choosing R=6, L=4 and C=1 will produce an overdamped response. The element values are assigned to the symbols with the dictionary data type, which are used to store data values in key:value pairs.\n\nele_values = {R:6,L:4,C:1}\n\nUsing the SymPy operator evalf() to convert to floating point.\n\nalpha = (R/(2*L)).subs(ele_values)\nprint('alpha = {:.2f}'.format(alpha.evalf()))\n\nw_o = (1/sqrt(L*C)).subs(ele_values)\nprint('natural frequency = {:.2f}, rad/s'.format(w_o.evalf()))\n\nprint('alpha &gt; natural frequency = {:s}'.format(str(alpha &gt; w_o)))\n\nalpha = 0.75\nnatural frequency = 0.50, rad/s\nalpha &gt; natural frequency = True\n\n\nSubstituting the constants and the component values we get the following equation.\n\nloop_current.subs(const).subs(ele_values)\n\n\\[i{\\left (t \\right )} = \\left(- \\frac{3 \\sqrt{5}}{10} + \\frac{1}{2}\\right) e^{\\frac{t \\left(-6 - 2 \\sqrt{5}\\right)}{8}} + \\left(\\frac{1}{2} + \\frac{3 \\sqrt{5}}{10}\\right) e^{\\frac{t \\left(-6 + 2 \\sqrt{5}\\right)}{8}}\\]\n\n\nThe module lambdify is used to transform SymPy expressions to lambda functions which can be used to calculate numerical values.\n\nfunc_current = lambdify(t, loop_current.subs(const).subs(ele_values).args[1]) \n\nThe plot below shows the loop current in amps versus time. The response is a smoothly decreasing function from the initial condition of 1 amp. The lambdify function returns complex numbers and both real and imaginary parts are plotted. The imaginary values are zero.\n\nx = np.linspace(0, 20, 2000, endpoint=True)\n#dx = 0.1\n#x = np.arange(0,20,dx)\n\nplt.title('Loop current vs time')\n\n#plt.plot(x, func_current(x))\nplt.plot(x, np.real(func_current(x)),label='Re i(t)')\nplt.plot(x, np.imag(func_current(x)),label='Im i(t)')\n\nplt.ylabel('i(t), amps')\nplt.xlabel('time, sec')\nplt.ylim((-0.1,1.1))\nplt.xlim((-1,20))\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "Source free series circuit.html#under-damped-response",
    "href": "Source free series circuit.html#under-damped-response",
    "title": "Source free series circuit",
    "section": "Under damped response",
    "text": "Under damped response\nChoosing R=0.5, L=1 and C=0.1 will produce an under damped response.\n\\(\\frac{R}{2L}&lt;\\frac{1}{\\sqrt{LC}}\\)\nThe component values are assigned below.\n\nele_values = {R:0.5,L:1.0,C:0.1}\n\n\nalpha = (R/(2*L)).subs(ele_values)\nprint('alpha = {:.2f}'.format(alpha.evalf()))\n\nw_o = (1/sqrt(L*C)).subs(ele_values)\nprint('natural frequency = {:.2f}, rad/s'.format(w_o.evalf()))\n\nprint('alpha &lt; natural frequency = {:s}'.format(str(alpha &lt; w_o)))\n\nalpha = 0.25\nnatural frequency = 3.16, rad/s\nalpha &lt; natural frequency = True\n\n\nSubstitute the constants and component values we get the following equation.\n\nloop_current.subs(const).subs(ele_values)\n\n\\[i{\\left (t \\right )} = \\left(\\frac{1}{2} + 0.0396525792859072 i\\right) e^{0.5 t \\left(-0.5 - 6.30476010645925 i\\right)} + \\left(\\frac{1}{2} - 0.0396525792859072 i\\right) e^{0.5 t \\left(-0.5 + 6.30476010645925 i\\right)}\\]\n\n\n\nfunc_current = lambdify(t, loop_current.subs(const).subs(ele_values).args[1]) \n\nThe plot below shows the loop current in amps versus time. The response is a decaying sinusoidal waveform from the initial condition of 1 amp. The lambdify function returns complex numbers and both real and imaginary parts are plotted. The imaginary values are zero.\n\nplt.title('Loop current vs time')\n\nplt.plot(x, np.real(func_current(x)),label='Re i(t)')\nplt.plot(x, np.imag(func_current(x)),label='Im i(t)')\n\nplt.ylabel('i(t), amps')\nplt.xlabel('time, sec')\nplt.ylim((-1.1,1.1))\nplt.xlim((-1,20))\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nThe plot above was verified by comparing the results with LTspice simulation of the same circuit."
  },
  {
    "objectID": "Source free series circuit.html#solving-for-other-circuit-variables",
    "href": "Source free series circuit.html#solving-for-other-circuit-variables",
    "title": "Source free series circuit",
    "section": "Solving for other circuit variables",
    "text": "Solving for other circuit variables\nThe voltages across each of the components can be computed since the current has been solved for. The initial condition for the current at t=0 is 1 amp. Also the derivative of the current at t=0 is 0, which will be used when calculating the voltage across the inductor. At t=0, the current flowing through the capacitor is 1 amp, which causes a voltage across the resistor of 0.5 volts.\nThe voltages around the loop must sum to zero, so this means one of the components must have a negative voltage with respect to the others for the sum to be zero. Since the inductor’s voltage is related to the derivative of the current and at t=0, i’(0)=0, the inductor has no voltage across its terminals. The capacitor can have an initial voltage on its terminals, which has not been defined, but since the voltages must sum to zero, the initial voltage on the capacitor must be -0.5 volts.\n\n# voltage across the resistor\nR_voltage = ele_values[R]*np.real(func_current(x))\n\nplt.title('Voltage across R')\nplt.plot(x,R_voltage)\n\nplt.ylabel('Volts')\nplt.xlabel('time, sec')\n\n#plt.ylim((-1,1))\nplt.xlim((-0.5,20))\nplt.yticks(np.arange(-0.5, 0.75, 0.25))\n#plt.xticks(np.arange(0, 20+1, 2.0))\n#plt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nVoltage across the inductor is calculated by: \\(L\\frac{d}{dt}i(t)=v(t)\\) There is no initial voltage because i’(0)=0.\nNumPy does not provide general functionality to compute derivatives so we will use the gradient function and supply the spacing between the samples.\n\nL_voltage = ele_values[L]*np.gradient(np.real(func_current(x)),x[1])\nL_voltage[0] = 0 # make v(0) = 0 \n\nplt.title('Voltage across L')\nplt.plot(x, L_voltage)\n\nplt.ylabel('Volts')\nplt.xlabel('time, sec')\n\nplt.ylim((-3,3))\nplt.xlim((-0.5,20))\n#plt.yticks(np.arange(-2.5, 3.1, 0.5))\n#plt.xticks(np.arange(0, 20+1, 2.0))\n#plt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nThe voltage across the capacitor is given by: \\(\\frac{1}{C}\\int_0^{\\tau}{i(t)}dt+v(0)=v(t)\\). As described above, \\(v(0)=-0.5\\)\nThe code below calculates the integral of the current using the numpy function trapz, which uses the composite trapezoidal rule to approximate integration.\n\nintegral_of_current = np.zeros(len(x))\nfor k in range(len(x)):\n    integral_of_current[k] = np.trapz(np.real(func_current(x[0:k+1])),x=None,dx=x[1])\n\nC_voltage = integral_of_current/ele_values[C] -  ele_values[R]*np.real(func_current(0))\n\nplt.title('Voltage across C')\n\nplt.plot(x, C_voltage )\n\nplt.ylabel('volts')\nplt.xlabel('time, sec')\nplt.ylim((-3,3))\nplt.xlim((-0.5,20))\n#plt.yticks(np.arange(-2.5, 3, .5))\n#plt.xticks(np.arange(0, 20+1, 2.0))\n#plt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "Source free series circuit.html#sum-the-voltages-around-the-loop",
    "href": "Source free series circuit.html#sum-the-voltages-around-the-loop",
    "title": "Source free series circuit",
    "section": "Sum the voltages around the loop",
    "text": "Sum the voltages around the loop\nTo check the results we can sum the voltages around the loop and the result should be zero, or approximately zero.\n\nplt.title('Sum of Voltage around the loop')\n\nplt.plot(x, R_voltage + L_voltage + C_voltage)\n\nplt.ylabel('volts')\nplt.xlabel('time, sec')\n#plt.ylim((-.1,.1))\nplt.xlim((-1,10))\n#plt.yticks(np.arange(-2.5, 3, .5))\n#plt.xticks(np.arange(0, 20+1, 2.0))\n#plt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nThe sum of the voltages are almost zero, but not quite, probably because of round off errors in the integration and differentiation. Investigation of the error could be the subject of a different effort or maybe a future update to this notebook."
  },
  {
    "objectID": "Source free series circuit.html#scipy-solution",
    "href": "Source free series circuit.html#scipy-solution",
    "title": "Source free series circuit",
    "section": "SciPy solution",
    "text": "SciPy solution\nUsing SciPy to numerically solve for the current in the series circuit. The differential equation needs to be re-formulated because SciPy can only solve first order equations. The differential equation \\(𝐿\\frac{𝑑^2}{𝑑𝑡^2}𝑖(𝑡)+𝑅\\frac{𝑑}{𝑑𝑡}𝑖(𝑡)+\\frac{𝑖(𝑡)}{𝐶}=0\\) is rewritten by multiplying the terms by C, \\(𝐿C\\frac{𝑑^2}{𝑑𝑡^2}𝑖(𝑡)+𝑅C\\frac{𝑑}{𝑑𝑡}𝑖(𝑡)+𝑖(𝑡)=0\\).\nThe shorthand notation for the derivative of the current is used: \\(𝐿Ci''+𝑅Ci'+𝑖=0\\).\nRearrange the equation to put the second derivative of i on the left hand side: \\(i''=\\frac {-Ri'-i}{CL}\\)\nA new variable \\(z\\) is introduced \\(z = i'\\) along with its derivative \\(z'=i''\\)\nSubstituting the expression for z and z’ back into the original equation, we get the following two equations.\n\\(z' =\\frac {-RCz-i}{CL}\\)\n\\(z=i'\\)\nThese are then written into a Python function called circuit_model.\n\ndef circuit_model(I, t):\n    R = 0.5 # resistor value\n    L = 1.0 # inductor value\n    C = 0.1 # capacitor value\n    i = I[0] # the current variable\n    z = I[1] # \n    didt = z # the equation from above\n    dzdt = (-C*R*z-i)/(L*C) # \n    return [didt, dzdt]\n\nThe initial conditions\n\nt0 = 0 # t starts at zero\ni0 = 1 # current at t=0\nz0 = 0\nI0 = [i0, z0]\n\nUsing the SciPy function scipy.integrate.odeint to solve the differential equation. Odeint computes the derivative of the function at t.\n\nt = np.linspace(0, 20,num=2000)\nloop_current = odeint(circuit_model, I0, t)\n\nplt.title('loop current')\nplt.plot(t, loop_current[:,0], label='i(t)')\nplt.ylabel('current, amps')\nplt.xlabel('time, sec')\nplt.ylim((-1,1.5))\nplt.xlim((-1,20))\n#plt.yticks(np.arange(-2.5, 3, .5))\n#plt.xticks(np.arange(0, 20+1, 2.0))\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nThe SciPy solution is compared against the SymPy solution by taking the difference in the calculated loop current. The difference is shown below.\n\nplt.title('SymPy solution minus SciPy solution for current')\nplt.plot(t, (loop_current[:,0]-np.real(func_current(x)))*1e6)\nplt.ylabel('current, micro amps')\nplt.xlabel('time, sec')\nplt.ylim((-0.1,0.1))\nplt.xlim((0,20))\n#plt.yticks(np.arange(-0.1, 0.1, .05))\n#plt.xticks(np.arange(0, 20+1, 2.0))\n#plt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nThe difference between the computed currents is less than 0.1 micro amps, which is probably small enough to ignore."
  },
  {
    "objectID": "Source free series circuit.html#sympy-solution-given-vct1-and-vct0-at-t0",
    "href": "Source free series circuit.html#sympy-solution-given-vct1-and-vct0-at-t0",
    "title": "Source free series circuit",
    "section": "SymPy solution, given \\(Vc(t)=1\\) and \\(Vc'(t)=0\\) at \\(t=0\\)",
    "text": "SymPy solution, given \\(Vc(t)=1\\) and \\(Vc'(t)=0\\) at \\(t=0\\)\nIf the initial condition for the series circuit has the voltage on the capacitor given, then the circuit equations can be solved as follows. Declare the variable, t, for time and define it to be positive. Define a function, v, for the voltage across the capacitor and make it time dependent.\n\nt = symbols('t',positive=True)  # t &gt; 0\nv = Function('v')(t)\n\nThe symbols L, R and C used for inductance, resistance and capacitance were declared above.\nThe current through the capacitor is equal to the derivative of the voltage, \\(C\\frac{d}{dt}v(t)=i(t)\\). The voltage across the series L and R is \\(L\\frac{d}{dt}i(t)+Ri(t)=-v(t)\\), where v(t) is the voltage across the capacitor. Since the current is the same in the loop, the capacitor current can be substituted in the equation to give: \\(CL\\frac{d^2}{dt^2}v(t)+CR\\frac{d}{dt}v(t)=-v(t)\\)\nThe right hand side is negative since the voltage around the loop must sum to zero. The equation can be expressed in SymPy as:\n\nEq(L*C*v.diff(t,t) + R*((C*v.diff(t))),  -v)\n\n\\[C L \\frac{d^{2}}{d t^{2}} v{\\left (t \\right )} + C R \\frac{d}{d t} v{\\left (t \\right )} = - v{\\left (t \\right )}\\]\n\n\nUsing the SymPy function dsolve to find the capacitor voltage, we get:\n\nC_voltage = dsolve(Eq(L*C*v.diff(t,t) + R*C*v.diff(t), -v))\nC_voltage\n\n\\[v{\\left (t \\right )} = C_{1} e^{\\frac{t \\left(- R - \\frac{\\sqrt{C \\left(C R^{2} - 4 L\\right)}}{C}\\right)}{2 L}} + C_{2} e^{\\frac{t \\left(- R + \\frac{\\sqrt{C \\left(C R^{2} - 4 L\\right)}}{C}\\right)}{2 L}}\\]\n\n\nat t = 0, voltage on capacitor is 1\n\nEq(C_voltage.args[1].subs({t:0}),1)\n\n\\[C_{1} + C_{2} = 1\\]\n\n\nderivative of the capacitor voltage is 0\n\nEq(C_voltage.args[1].diff(t).subs({t:0}),0)\n\n\\[\\frac{C_{1} \\left(- R - \\frac{\\sqrt{C \\left(C R^{2} - 4 L\\right)}}{C}\\right)}{2 L} + \\frac{C_{2} \\left(- R + \\frac{\\sqrt{C \\left(C R^{2} - 4 L\\right)}}{C}\\right)}{2 L} = 0\\]\n\n\nUsing the two initial conditions, the constants can be solved for.\n\nconst = solve(Matrix(\n    [[Eq(C_voltage.args[1].subs({t:0}),1)],\n    [Eq(C_voltage.args[1].diff(t).subs({t:0}),0)]]),[C1, C2])\nconst\n\n\\[\\left \\{ C_{1} : - \\frac{C R}{2 \\sqrt{C^{2} R^{2} - 4 C L}} + \\frac{1}{2}, \\quad C_{2} : \\frac{C R}{2 \\sqrt{C^{2} R^{2} - 4 C L}} + \\frac{1}{2}\\right \\}\\]"
  },
  {
    "objectID": "Source free series circuit.html#under-damped-response-1",
    "href": "Source free series circuit.html#under-damped-response-1",
    "title": "Source free series circuit",
    "section": "Under damped response",
    "text": "Under damped response\nAs described above the underdamped case is defined as \\(\\frac {R}{2RL} &gt; \\frac {1} {\\sqrt{LC}}\\). Choosing R=0.5, L=1 and C=0.1 will produce an under damped response.\nEvaluating the equation numerically, the voltage on the capacitor is:\n\nC_voltage.subs(const).subs(ele_values)\n\n\\[v{\\left (t \\right )} = \\left(\\frac{1}{2} + 0.0396525792859072 i\\right) e^{0.5 t \\left(-0.5 - 6.30476010645925 i\\right)} + \\left(\\frac{1}{2} - 0.0396525792859072 i\\right) e^{0.5 t \\left(-0.5 + 6.30476010645925 i\\right)}\\]\n\n\nUsing the SymPy function lambdify to turn the expression into a function:\n\nfunc_C_voltage = lambdify(t, C_voltage.subs(const).subs(ele_values).args[1]) \n\nFind the current in C by differentiating the voltage, \\(C\\frac{d}{dt}v(t)=i(t)\\)\n\nx = np.linspace(0, 20, 2000, endpoint=True)\ncurrent_in_C = ele_values[C]*np.gradient(np.real(func_C_voltage(x)),x[1])\ncurrent_in_C[0] = 0 # set the current in the capacitor to 0 at t=0\n\nFind the voltage across the resistor, \\(Ri(t)\\) and the voltage across the inductor, \\(L\\frac{d}{dt}i(t)\\). At t=0, there is no current flowing, so the initial voltage on the resistor is zero. The initial voltage on the capacitor of 1 volt, must be balanced by an initial voltage on the inductor.\n\nR_voltage = ele_values[R]*current_in_C\nL_voltage = ele_values[L]*np.gradient(current_in_C,x[1])\nL_voltage[0] = -1 # set the voltage on the inductor at t=0\n\n\nplt.title('volts across C')\n\nplt.plot(x, np.real(func_C_voltage(x)),label='volts across C')\nplt.plot(x, current_in_C, label='current in C')\nplt.plot(x, R_voltage, label='volts across R')\nplt.plot(x, L_voltage, label='volts across L')\n\nplt.ylabel('voltage')\nplt.xlabel('time, sec')\n#plt.ylim((-3,3))\nplt.xlim((0,5))\n#plt.yticks(np.arange(-2.5, 3, .5))\n#plt.xticks(np.arange(0, 20+1, 2.0))\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nChecking to see that the voltage sum around the loop is zero.\n\nplt.title('Sum of the component voltages')\n\nplt.plot(x, np.real(func_C_voltage(x)) + R_voltage + L_voltage)\n\nplt.ylabel('voltage')\nplt.xlabel('time, sec')\n#plt.ylim((-3,3))\nplt.xlim((-1,20))\n#plt.yticks(np.arange(-2.5, 3, .5))\n#plt.xticks(np.arange(0, 20+1, 2.0))\n#plt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nThe sum of the voltages almost is zero. Investigating the source of the error is a different topic."
  },
  {
    "objectID": "Source free series circuit.html#summary",
    "href": "Source free series circuit.html#summary",
    "title": "Source free series circuit",
    "section": "Summary",
    "text": "Summary\nThis notebook walked through the steps of using SymPy and SciPy to solve a 2nd order ordinary differential equation for a series connected LRC circuit. Both symbolic and numerical solutions were generated and the results were plotted. The SymPy and SciPy libraries are very versatile and make these types of computations very easy.\nElectrical engineers don’t normally solve circuit analysis problems by writing differential equations and solving them, Laplace transforms are used instead.\n\nLaplace transform\nThe Laplace transform is named after Pierre-Simon Laplace. Skipping the mathematical justification, engineers can analyze circuits by using a transform that converts a function of a real variable t, to a function of a complex variable s. The resistors, capacitors and inductors can be replaced by the transformed Laplace elements. Replace the resistors in the circuit with \\(RI(s)\\), replace the inductors with \\(L[I(s)s+i(0)]\\) and the capacitors with \\([\\frac{I(s)}{sC} + \\frac{v(0)}{s}]\\)\nDefining the SymPy variable for time and the complex variable s.\n\ns = symbols('s')\nt = symbols('t',positive=True)  # t &gt; 0\n\nThe symbols L, R and C are used for inductance, resistance and capacitance. L_i_zero (also i(0)) is the initial current through the inductor and C_v_zero (also v(0)) is the initial voltage on the capacitor. The symbol s is used for the Laplace variable.\n\nL, R, C, L_i_zero, C_v_zero, s= symbols('L R C L_i_zero C_v_zero s')\n\n\n\nSeries circuit\nReplacing each element with the transformed Laplace elements, write the sum of the voltages around the loop:\n\\(LsI(s)-Li(0)+RI(s)+\\frac{I(s)}{sC}+\\frac{v(0)}{s}=0\\)\nCollecting the \\(I(s)\\) terms on the right:\n\\(I(s)=\\frac{i(0)}{Ls+R+\\frac{1}{Cs}+\\frac{v(0)}{s}}\\)\nDefining the element values:\n\nele_values = {R:0.5,L:1.0,C:0.1,L_i_zero:1,C_v_zero:0}\n\nWhere L_i_zero is the initial current in the inductor and C_v_zero is the initial voltage on the capacitor, we can write the expression in SymPy:\n\nexp = L_i_zero/(L*s+R+1/(C*s)+C_v_zero/s)\nexp\n\n\\[\\frac{L_{i zero}}{\\frac{C_{v zero}}{s} + L s + R + \\frac{1}{C s}}\\]\n\n\nThe symbolic solution inverse transform takes a long time and I had to interrupt the kernel and kill the process.\n\n#inverse_laplace_transform(exp, s, t)\n\nDoing the inverse transform numerically is much quicker.\n\ninverse_laplace_transform(exp.subs(ele_values), s, t)\n\n\\[1.0 \\left(- 0.0793051585718144 \\sin{\\left (3.15238005322962 t \\right )} + 1.0 \\cos{\\left (3.15238005322962 t \\right )}\\right) e^{- 0.25 t}\\]\n\n\n\nfunc_current1 = lambdify(t, inverse_laplace_transform(exp.subs(ele_values), s, t)) \n\nThe plot below shows the loop current in amps versus time.\n\n#x = np.linspace(0, 20, 2000, endpoint=True)\nplt.title('Loop current vs time')\n\nplt.plot(x, np.real(func_current1(x)),label='Re i(t)')\nplt.plot(x, np.imag(func_current1(x)),label='Im i(t)')\n\nplt.ylabel('i(t), amps')\nplt.xlabel('time, sec')\nplt.ylim((-1.1,1.1))\nplt.xlim((-1,20))\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nNearly the same result obtained by solving the differential equations above. The difference is plotted below. Investigating the source of the error is a different topic.\n\nplt.title('SymPy solution minus Laplace solution for current')\nplt.plot(x, (np.real(func_current1(x))-np.real(func_current(x)))*1e0)\nplt.ylabel('current, amps')\nplt.xlabel('time, sec')\n#plt.ylim((-0.1,0.1))\n#plt.xlim((0,20))\n#plt.yticks(np.arange(-0.1, 0.1, .05))\n#plt.xticks(np.arange(0, 20+1, 2.0))\n#plt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "NCLHv1 analysis.html",
    "href": "NCLHv1 analysis.html",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "",
    "text": "Last update: 9 Nov 2022\nannual report: 2021\n10Q: Q3 of 2022\nshare price data from: 8/28/2022, \\$13.72\n\\(\\large{\\color {red} {\\text{Work in progress notes, 11-18-2022: still working on updates in quarterly data section.}}}\\)\n\\(\\large{\\color {red} {\\text{Concerns about declining equity and ability to fill ships back to 107% level.}}}\\)\n\\(\\large{\\color {red} {\\text{Frank's said he would not dicount to fill, but will acheive historic levels by 2 quarter of 2023.}}}\\)\nNeed to look at impact of future bookings (since this amount is a liability on the balance sheet) to debt ratios."
  },
  {
    "objectID": "NCLHv1 analysis.html#abstract",
    "href": "NCLHv1 analysis.html#abstract",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Abstract",
    "text": "Abstract\nThis notebook was developed to analyze the financial performance of NCLH. The analysis presented primarily uses financial data prior to fiscal year 2019. Most of which is irrelevant now, since NCLH’s consolidated financial sheets are dramatically different following the shock of the pandemic. From a financial perspective, it’s not really possible to compare the finances of post pandemic NCL to the pre pandemic NCL. On account of the large discontinuity in operations, the company’s pre and post pandemic financials need to be considered separately. Click the link here to jump to my analysis of recent quarterly performance."
  },
  {
    "objectID": "NCLHv1 analysis.html#bottom-line-up-front",
    "href": "NCLHv1 analysis.html#bottom-line-up-front",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Bottom line up front",
    "text": "Bottom line up front\nThe years 2020 to present are omitted from the DCF and NAIC analysis since NCLH and the other cruise lines suspended operations in March of 2020 due to the pandemic. My analysis of recent quarterly performance shows that NCLH will run out of cash sometime in 2023 if the company is not able to increase occupancy level close to 100%. Follow the link to the Conclusion."
  },
  {
    "objectID": "NCLHv1 analysis.html#introduction",
    "href": "NCLHv1 analysis.html#introduction",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Introduction",
    "text": "Introduction\nNorwegian Cruise Line Holdings Ltd., NCLH together with its subsidiaries, operates as a cruise company in North America, Europe, the Asia-Pacific, and internationally. The company operates the Norwegian Cruise Line, Oceania Cruises, and Regent Seven Seas Cruises brands.\nSector(s): Consumer Cyclical Industry: Travel Services"
  },
  {
    "objectID": "NCLHv1 analysis.html#company-description",
    "href": "NCLHv1 analysis.html#company-description",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Company description",
    "text": "Company description\nNorwegian Cruise Line Holdings operates a global cruise company with a portfolio of three brands: Norwegian Cruise Line, Oceania Cruises and Regent Seven Seas Cruises. Norwegian commenced operations from Miami in 1966, launching the modern cruise industry by offering weekly departures from Miami to the Caribbean. It is the third-largest cruise line in the world by passengers, controlling about 8.7% of the total worldwide share of the cruise market by passengers as of 2018.\nThe cruise line was founded as Norwegian Caribbean Line in 1966 by the Norwegian Knut Kloster and the Israeli Ted Arison, with the 8,666-ton, 140-m cruise ship/car ferry, Sunward, which in 1966 operated as a car ferry between Southampton UK and Gibraltar, for that one short season only. Arison soon left to form Carnival Cruise Lines, while Kloster acquired additional ships for Caribbean service. Norwegian pioneered many firsts in the cruise industry, such as the first Out Island Cruise, the first combined air-sea program (marketed as “Cloud 9 Cruises”), which combined low-cost air fares with the cruise, and first shipline to develop new ports in the Caribbean, such as Ocho Rios in Jamaica.\nNorwegians’s second and third ships, the Starward and Skyward, were the first newly built ships designed for the cruise line. Like the original Sunward of 1966, they had the capability to carry automobiles through a well-concealed stern door. Later, this area was turned into cabins and a two-deck movie theater, later to be used as a casino. Norwegian was responsible for many of the cruise innovations that have now become standard throughout the industry.\nNorwegian acquired Orient Lines in 1998. After talks, Norwegian itself was acquired in 2000 by Star Cruises, a subsidiary of Genting Hong Kong, part of the Malaysia-based Genting Group. In 2007, Star Cruises sold Orient Line’s Marco Polo to Transocean Tours, and Orient Lines ceased operations in early 2008.\nIn August 2007, Star Cruises sold 50% of Norwegian for \\$1 billion to US-based Apollo Management to strengthen Norwegian’s financial position.\nIn January 2008, the Apollo Funds acquired 50% of the outstanding ordinary share capital of NCLC. As part of this investment, the Apollo Funds assumed control of NCLC’s Board of Directors. Also, in January 2008, the TPG Viking Funds acquired, in the aggregate, 12.5% of NCLC’s outstanding share capital from the Apollo Funds.\nIn February 2011, NCLH, a Bermuda limited company, was formed.\nIn January 2013, NCLH completed its IPO and the ordinary shares of NCLC were exchanged for the ordinary shares of NCLH, and NCLH became the owner of 100% of the ordinary shares and parent company of NCLC (the “Corporate Reorganization”). In August 2013 and December 2013, NCLH completed the Secondary Offerings.\nIn November 2014, NCLH completed the acquisition of Prestige (Oceania and Regent brands). Frank J. Del Rio (founded Oceania in October 2002), became President and Chief Executive Officer of NCLH. Prior to that Kevin M. Sheehan served as the President and Chief Executive Officer of the Company since August 2010.\nSegment Reporting\nWe have concluded that our business has a single reportable segment. Each brand, Oceania, Regent and Norwegian, constitutes a business for which discrete financial information is available and management regularly reviews the operating results and, therefore, each brand is considered an operating segment. Our operating segments have similar economic characteristics, including similar margins and similar products and services; therefore, we aggregate all of the operating segments into one reportable segment.\nAs of December 31, 2020, NCLH had 28 ships with approximately 59,150 Berths and had orders for nine additional ships to be delivered through 2027. NCLH has nine ships on order across our portfolio of brands. For the Norwegian brand, Project Leonardo will introduce six additional ships with expected delivery dates from 2022 through 2027. For Regent Seven Seas Cruises, NCLH has one Explorer Class Ship on order for delivery in 2023. For Oceania Cruises, NCLH has two Allura Class Ships on order for delivery in 2023 and 2025.\nCOVID-19\nBeginning on March 13, 2020, NCLH suspended all cruise voyages in response to COVID-19. This suspension has been extended through May 31, 2021.\nThe resumption of operations will be dependent, in part, on NCLH’s ability to comply with various governmental regulations, the severity and duration of the COVID-19 pandemic, the lifting of various travel restrictions and travel bans issued by various countries and communities around the world, as well as port availability. NCLH expects a gradual phased relaunch of our ships after the voyage suspension period, with our ships initially operating at reduced occupancy levels. Our selection of itineraries in the short-term will be predicated by port availability and the safety of the destinations we visit.\nSince March 2020, NCLH has launched a series of capital markets transactions to bolster its financial position during the voyage suspension period, which in aggregate raised approximately $5.6 billion.\nNCLH has also taken several additional measures to improve their liquidity through deferring certain ship milestone payments, deferring certain debt amortization payments and extending certain maturities under our debt agreements, including under our agreements with export credit agencies (“ECAs”) and related governments. NCLH has also undertaken several proactive cost reduction and cash conservation measures to mitigate the financial and operational impacts of COVID-19, through the reduction of capital expenditures as well as reductions in operating expenses, including ship operating expenses and selling, general and administrative expenses.\nOn May 5, 2020, in a filing with the Securities and Exchange Commission, Norwegian Cruise Line Holdings (NCLH) said there is “substantial doubt” about its ability to continue as a “going concern” as it faces a liquidity crisis over the next twelve months.\nBy the next day, NCLH was able to secure over \\$2.2 billion of additional liquidity in oversubscribed capital markets transactions, but at a price:\n\n\\$400 million in common stock at \\$11 per share;\n\\$675 million in senior secured notes due 2024 at a 12.25% interest rate;\n\\$750 million in exchangeable notes due 2024 at 6% interest rate, and exchangeable at any time into common shares at \\$13.75; and\n\\$400 million private investment from a global private equity firm.\n\nOn May 7, 2020, NCLH CEO declared that the company has secured enough liquidity to get through potentially 18 months of zero revenues and may resume cruising later in 2020.\nOn July 25, 2021, Norwegian Jade set sail from Athens sailing the Greek Isles as the first ship in the Company’s fleet to resume sailing since the global suspension of voyages in March 2020. August 29 Oceania’s Marina sets sail from Stockholm, the first ship of the Oceania fleet.\nOn May 7 2022 NCL became the first major cruise operator to return the full fleet to service.\n}— Document revision history\n- 1/10/2022: Copied from VZ notebook and reorganized - Feb 2022: updated quick look, reorganized flow of calculations, corrected usage of financial rates, organized end sections - 23 Mar 2022: Cleaning up financial data spreadsheet. Removed NAIC tab. Removed duplicate revenue data. - 27 Mar 2022: MFG template copied from BMY - 3 May 2022: replaced np.linalg.lstsq with np.polyfit in NAIC forecast, added Future forecast based on historical data notes, Dilution notes, decision model - 6/19/2022: copied from MFG template, old NCLH files given v0 or OLD as filenames. - summer 2022: started looking at quarterly results to determine liquidity and default risk. }—"
  },
  {
    "objectID": "NCLHv1 analysis.html#analysis",
    "href": "NCLHv1 analysis.html#analysis",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Analysis",
    "text": "Analysis\nThe following sections of this notebook contain the financial analysis for the company.\nContents \n\nStock screener results\n\nLoad financial spreadsheet\n\nDiscounted cash flow analysis, baseline\n\nDCF Scenarios\n\nNACI stock selection guide analysis\n\nFuture stock price\n\nDividend payout\n\nManagement performance\n\nDecision model\n\nConclusion\n\nNotes\n\nReferences\n\nNew section: Recent quarterly performance\n\n\nCode\nfrom datetime import datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\nfrom dateutil.parser import parse\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom dateutil import parser\nimport os\nfrom pandas.plotting import register_matplotlib_converters\nimport np_financial\nregister_matplotlib_converters()"
  },
  {
    "objectID": "NCLHv1 analysis.html#stock-screener-results",
    "href": "NCLHv1 analysis.html#stock-screener-results",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "1) Stock screener results ",
    "text": "1) Stock screener results \nThis company was selected for analysis because I own 100 shares purchased solely to take part in the shareholder benefit as described below in the dividend section. Also, on occasion, I have large deposits with NCLH for future cruises and in the event of bankruptcy, those deposits may be at risk.\nCurrent news\nA review of the financial news sites from yahoo and google showed the following:\n- Recent news concerns cruise line debt and return to operations. In June 2022, Carnival provided a 2nd Quarter 2022 business update. The results seemed to be playing in the news as negative for Carnival and by extension the other cruise lines. - Low stock price is in the news. - On March 12, 2020, a class action complaint, Eric Douglas v. Norwegian Cruise Lines, Frank J. Del Rio and Mark A. Kempa, Case No. 1:20-CV-21107, was filed in the United States District Court for the Southern District of Florida, naming the Company, Frank J. Del Rio, the Company’s President and Chief Executive Officer, and Mark A. Kempa, the Company’s Executive Vice President and Chief Financial Officer, as defendants. Subsequently, two similar class action complaints were also filed in the United States District Court for the Southern District of Florida naming the same defendants. - On July 31, 2020, a consolidated amended class action complaint was filed by lead plaintiff’s counsel. The complaint asserts claims, purportedly brought on behalf of a class of shareholders, under Sections 10(b) and 20(a) of the Securities Exchange Act of 1934, and Rule 10b-5 promulgated thereunder, and allege that the Company made false and misleading statements to the market and customers about COVID-19. The complaint seeks unspecified damages and an award of costs and expenses, including reasonable attorneys’ fees, on behalf of a purported class of purchasers of our ordinary shares between February 20, 2020 and March 10, 2020.\nReview quarterly results\nQuarterly reports dating back to March 31, 2019 are analyzed below. D/E, Book value and liquidity levels indicate NCLH is a distressed company as a result of the pandemic.\nAverage daily volume\nAverage daily volume: 21,067,488\nDividend yield\nForward dividend yield: NA\nNotes from Crystal Cruises Bankruptcy\nHas anyone filed a claim with CFA Travel Insured?\nCF Travel Insured has acknowledged our claim and admit that our claim is valid. However, they are taking the position that they won’t pay any claims until they determine how much we will have received from the bankruptcy.\nThank you both. I did file a claim with ABC. I read over the entire policy and it says nothing about waiting for a bankruptcy settlement. They keep saying “we only pay for unreimbursed expenses and won’t know what is unreimbursed until the bankruptcy settlement is complete.”\nJune 2nd final date for claiming refunds via the assignee\nPersonally for those who are able to use it, the credit card dispute process is the best course of action - AMEX having proven their worth over and over again over the last 5 months\nThose that had a cruise departing from a US port need to submit through the ABC claims process. Your FMC surety bond is among the assets being held and payment for those claims will be made through the assignee.\nThe ABC claims process is “Assignment for Benefit of Creditors”, a liquidation process under Florida law, that Crystal has undertaken. All foreign flag cruise operators who have cruises leaving from the US (so only the WC would qualify) must post a surety bond of \\$32 million with the Federal Maritime Commission. That surety bond, for non-performance, is an asset of Crystal cruises that is part of the ABC liquidation process.\nOther than this surety bond, Crystal cruises has very few assets, and other creditors have more priority to those assets, while the surety bond can only be used to pay passengers back.\nCrystal Refund Roll Call\nDoes anyone have info. On Federal Maritime Commission procedure for making a claim…. presumably the FMC will cover up to \\$5,000 of non refunded money\nThe surety bond is an asset being managed by the ABC (Assignment for the Benefit of Creditors) group. You need to file a claim with ABC by 11 June. Make sure you state the departure port (as it has to be US the FMC bond to be valid). FMC stated this in an email to me.\nDoes anyone have info. On Federal Maritime Commission procedure for making a claim…. presumably the FMC will cover up to \\$5,000 of non refunded money"
  },
  {
    "objectID": "NCLHv1 analysis.html#load-financial-spreadsheet",
    "href": "NCLHv1 analysis.html#load-financial-spreadsheet",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "2) Load financial spreadsheet ",
    "text": "2) Load financial spreadsheet \nData from consolidated financial statements and annual reports was collected and entered into a spreadsheet. All numerical data is converted from thousands or millions of dollars to dollars. The stock share price history was obtained from yahoo and is included as a tab in the spreadsheet. Other tabs in the spreadsheet are various worksheets.\n\n\nCode\nticker = 'NCLH' # company ticker symbol\nos.chdir('/home/jeff32/Documents/Dividend Investing/DCF data/')\n\nfile_name = ticker+'_Financials.xlsx'\ndf_dcf_sheet = pd.read_excel(file_name,sheet_name='DCF data')\ndf_metrics_sheet = pd.read_excel(file_name,sheet_name='metrics')\ndf_price_history = pd.read_excel(file_name,sheet_name='Historical Prices')\n\n# load quarterly data into dataframe\ndf_qrt_sheet = pd.read_excel(file_name,sheet_name='QRT data')\n\n# change the working director back to the Jupyter folder\nos.chdir('/home/jeff32/Documents/JupyterLab/Discount Cash Flow Analysis/')\n\n\n\n\nCode\n# convert dates from string to datetime format in stock price history\nprice_date_list = []\nfor i in range(len(df_price_history)):\n    price_date_list.append(datetime.strptime(str(df_price_history['Date'][i]), '%Y-%m-%d'))\n\ndf_price_history.insert(0, 'datetime', price_date_list)  # insert a new column with datetime data\ndf_price_history.sort_values(by=['datetime'], inplace=True) # sort data frame by datetime\n\ndf_price_history.set_index('datetime',inplace=True)\n\n#df_price_history.head()\n\n\n\n2.1) Format data frame \nGenerate a new data frame that holds the financial data needed for the DCF model. Data from financial statements is copied into a spreadsheet which contains the data used in the analysis. The data in the DCF_data tab is in a consistent format for ease of use by this notebook. Standard names are used for the rows and columns.\n\n\nCode\n#column names: fiscal years \nfy_data = df_dcf_sheet.columns[1:].values.astype('datetime64[Y]')-1970\n#line 0: Total revenue  \nrevenue_data = df_dcf_sheet.iloc[0].to_numpy()[1:].astype('float')\n#line 1: Cost of goods sold\nCost_of_goods_sold_data = df_dcf_sheet.iloc[1].to_numpy()[1:].astype('float')\n#line 2: General and administrative\nGeneral_and_administrative_data = df_dcf_sheet.iloc[2].to_numpy()[1:].astype('float')\n#line 3: Research and development\nResearch_and_development_data = df_dcf_sheet.iloc[3].to_numpy()[1:].astype('float')\n#line 4: Depreciation and amortization\nDepreciation_and_amortization_data = df_dcf_sheet.iloc[4].to_numpy()[1:].astype('float')\n#line 5: Investment\nInvestment_data = df_dcf_sheet.iloc[5].to_numpy()[1:].astype('float')\n# line 6: Income before income taxes\nIncome_before_income_taxes_data = df_dcf_sheet.iloc[6].to_numpy()[1:].astype('float')\n# line 7: Income tax\nIncome_tax_data = df_dcf_sheet.iloc[7].to_numpy()[1:].astype('float')\n# line 8: Accounts receivable\nAccounts_receivable_data = df_dcf_sheet.iloc[8].to_numpy()[1:].astype('float')\n# line 9: Inventories\nInventories_data = df_dcf_sheet.iloc[9].to_numpy()[1:].astype('float')\n# line 10: Accounts payable\nAccounts_payable_data = df_dcf_sheet.iloc[10].to_numpy()[1:].astype('float')\n# line 11: Current assets\nCurrent_assets_data = df_dcf_sheet.iloc[11].to_numpy()[1:].astype('float')\n# line 12: Current liabilities\nCurrent_liabilities_data = df_dcf_sheet.iloc[12].to_numpy()[1:].astype('float')\n# line 13: Long term debt\nLong_term_debt_data = df_dcf_sheet.iloc[13].to_numpy()[1:].astype('float')\n# line 14: Shares outstanding\nShares_outstanding_data = df_dcf_sheet.iloc[14].to_numpy()[1:].astype('float')\n\n\n\n\nCode\n# make a new data frame to store selected financial data\ndf_dcf_data = pd.DataFrame(data={\n    'FY':fy_data[::-1],\n    'revenue':revenue_data[::-1],\n    'cost_of_goods_sold':Cost_of_goods_sold_data[::-1],\n    'general_and_administrative':General_and_administrative_data[::-1],\n    'research_and_development':Research_and_development_data[::-1],\n    'depreciation':Depreciation_and_amortization_data[::-1],\n    'investment':Investment_data[::-1],\n    'income_before_income_taxes':Income_before_income_taxes_data[::-1],\n    'income_tax':Income_tax_data[::-1],\n    'accounts_receivable':Accounts_receivable_data[::-1],\n    'inventories':Inventories_data[::-1],\n    'accounts_payable':Accounts_payable_data[::-1], \n    'current_assets':Current_assets_data[::-1],\n    'current_liabilities':Current_liabilities_data[::-1],\n    'long_term_debt':Long_term_debt_data[::-1],\n    'shares_outstanding':Shares_outstanding_data[::-1]\n    })\n\n#df_dcf_data"
  },
  {
    "objectID": "NCLHv1 analysis.html#discounted-cash-flow-analysis-baseline",
    "href": "NCLHv1 analysis.html#discounted-cash-flow-analysis-baseline",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "3) Discounted cash flow analysis, baseline ",
    "text": "3) Discounted cash flow analysis, baseline \nDiscounted cash flow (DCF) is a valuation method used to estimate the value of an investment based on its expected future cash flows. DCF analysis attempts to figure out the value of an investment today, based on projections of how much money it will generate in the future. In finance, discounted cash flow (DCF) analysis is a method of valuing a security, project, company, or asset using the concepts of the time value of money. The DCF method used in this notebook follows [1].\nThe value of any financial investment equals the present value of the expected future cash flows, discounted for risk and timing of these cash flows. The DCF method to value stocks is a four step process.\n1. Develop a set of future free cash flows for the corporation based on revenue growth, net operating profit margin, income tax rate and fix and working capital requirements. 2. Estimate the discount rate for the cash flows based on expected timing and risk. 3. Discount the cash flows and total them to calculate the value for the corporation as a whole. 4. Subtract the debt, preferred stock value and other claims and divide by the number of shares outstanding to get the intrinsic value.\nSections - Revenue growth rate - Net operating profit margin - Tax rate - Depreciation Rate - Investment Rate - Working Capital Rate - Current Assets - Current Liabilities - Value of Debt Outstanding - Current stock price - Shares outstanding - 10 year treasury bond yield - Bond yield spread to treasury - Preferred stock yield - Equity risk premium - Company specific beta - DCF model inputs - Future cash flows\n\nFuture forecast based on historical data\nThe DCF model uses historical financial data to estimate future cash flows. However, future changes are largely unpredictable, so we assume that the past record can be used as a rough guide to the future. The more questionable this assumption is, the less valuable is the analysis. So the DCF model is more useful when applied to stable well established companies, since companies with stable earnings are easier to forecast.\n\n\nComments about DCF analysis of a distressed company\nThe DCF analysis presented below was performed for years 2019 and prior, which are the pre-covid years. Companies with negative earnings are difficult to evaluate with DCF. NCLH is suffering from economic distress from strategic problems from the pandemic. As a result, there is financial distress where income, cash flow and the accumulation of large amounts of debt relative to equity weigh heavily on the company’s future viability. The consequent result of near term low or negative earnings and high debt load may make it difficult to access new debt.\nDCF Scenario 2 is presented below, which assumes that NCLH returns to pre-pandemic earnings and values the company in light of higher interest rates and the large debt accumulated.\n\n\nRevenue growth rate \nThe revenue growth rate (also sometimes called net sales) of the corporation plus any other revenues associated with the main operations of the business. It does not include dividends, interest income or non-operating income. Historic revenue data is obtained from consolidated income statements. The year over year change in revenue is calculated and converted to a percent, then an average revenue growth rate is calculated.\nAdjustments\nNo adjustments for this company.\n\n\nCode\n# calculate the percent change in revenue\npcr = np.zeros(len(df_dcf_data['revenue'].to_numpy())) # percent change in revenue\nfor i in range(len(df_dcf_data['revenue'].to_numpy()[0:-1])):\n    pcr[i+1] = ((df_dcf_data['revenue'].to_numpy()[i+1] - df_dcf_data['revenue'].to_numpy()[i])/\n                df_dcf_data['revenue'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Revenue, $B')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['revenue']/1e9, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcr,'+-g')\n    \nax2.set_ylabel('% Change in revenue',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,50))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Revenue')\nplt.show()\n\n\n\n\n\nObservation:\nIn November 2014, NCLH added Oceania and Regent brands, so 2015 and forward, revenue includes these ships, and Frank J. Del Rio (founded Oceania in October 2002) became President and Chief Executive Officer of NCLH. The acquisition of Oceania and Regent is probably the reason for the spike in % change in revenue in 2015. Afterwards, the revenue growth rate returns to the historic mean.\nClearly seen is the dramatic reduction in revenue in years 2020 and 2021 due to the suspension of operations in March of 2020\nCalculate the average revenue growth rate using the years 2016 to 2019.\n\n\nCode\nrgr_avg = pcr[-6:-2].mean()/100 #  years 2016 to 2019\nprint('average revenue growth rate pre Covid: {:.2f}%'.format(rgr_avg*100))\n\n\naverage revenue growth rate pre Covid: -94.51%\n\n\n\n\nNet operating profit margin \nNet Operating Profit should reflect the future revenue generating ability and expense requirements of the operating business that comprise the ongoing operations of the company.\n\\(\\text{NOPM} = \\frac{\\text{Revenue} - \\text{Expenses}}{\\text{Revenue}}\\)\n\\(\\text{Expenses} = \\text{Cost of Goods Sold (CGS)} + \\text{General and Administrative (G&A)} + \\text{Research and Development (R&D)}\\)\nGeneral and Administrative (G&A) is also called Sales, General and Administrative (SG&A)\nAdjustments\nExpenses are the following:\n1. Cost of Goods Sold (CGS) - Commissions, transportation and other - Onboard and other - Payroll and related - Fuel - Food - Other 2. Selling, General & Administrative (SGA)\nNCLH calls GSA, Marketing, general and administrative. The marketing, general and administrative expense also covers the following items: - Non-cash share-based compensation expenses related to equity awards - Severance payments related to restructuring costs - Expenses related to the redeployment of Norwegian Joy from Asia to the U.S. and the closing of the Shanghai office\nThe cruise line does not report any R&D expenses. Impairment loss for 2020 was 1,607,797. This has not been included in the spreadsheet as a line item.\nOnboard and other revenue primarily consists of revenue from gaming, beverage sales, shore excursions, specialty dining, retail sales, spa services and photo services. Our onboard revenue is derived from onboard activities we perform directly or that are performed by independent concessionaires, from which we receive a share of their revenue.\n\n\nCode\n# NOP = (Revenue - Expenses)\nnop = (df_dcf_data['revenue'].to_numpy() - \\\n    (df_dcf_data['cost_of_goods_sold'].to_numpy() + \\\n    df_dcf_data['general_and_administrative'].to_numpy() + \\\n    df_dcf_data['research_and_development'].to_numpy()) )\n\n# net operating profit margin as percent of revenue\nnopm = nop/df_dcf_data['revenue'].to_numpy()\n\n# plot as four grouped bar chart with labels on right and working capital rate on left\n# calculate position of bars\nx1_bar_position = []\nx2_bar_position = []\nx3_bar_position = []\nx4_bar_position = []\nfor i in df_dcf_data['FY']:\n    x1_bar_position.append(i-relativedelta(months=3))\n    x2_bar_position.append(i-relativedelta(months=1))\n    x3_bar_position.append(i+relativedelta(months=1))\n    x4_bar_position.append(i+relativedelta(months=3))\n    \nwidth = 40  # the width of the bars\n    \n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Net operating profit, \\\\$B')\n\nax1.bar(x1_bar_position,df_dcf_data['cost_of_goods_sold'].to_numpy()/1e9, width,label='CGS')\nax1.bar(x2_bar_position,df_dcf_data['general_and_administrative'].to_numpy()/1e9, width,label='G&A')\nax1.bar(x3_bar_position,df_dcf_data['research_and_development'].to_numpy()/1e9, width,label='R&D')\nax1.bar(x4_bar_position,nop/1e9, width,label='NOP')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,4))\nax1.legend()\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:cyan'\n\nax2.plot(df_dcf_data['FY'],nopm*100,'+-c')\n    \nax2.set_ylabel('% NOPM',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,40))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Net Operating Profit')\nplt.show()\n\n\n\n\n\nObservation\nPrior to the suspension of operations, the NOPM was above 25%. Calculate the average nopm for pre covid years.\n\n\nCode\n#Average net operating profit margin\nnopm_avg = nopm[-6:-2].mean() #  years 2016 to 2019\nprint('average net operating profit margin pre Covid: {:.2f}%'.format(nopm_avg*100))\n\n\naverage net operating profit margin pre Covid: -1.00%\n\n\n\n\nTax rate \nTax payments are taken from the consolidated income statement, provision for income taxes. The effect of taxes on profits is accounted for.\n\\(\\text{Tax rate} = \\frac{\\text{Income taxes}}{\\text{Income before income taxes}}\\)\nAdjustments\nIn February 2011, NCLH, a Bermuda limited company, was formed. Under current Bermuda law, NCLH is not subject to tax on income and capital gains. NCLH has received from the Minister of Finance under The Exempted Undertakings Tax Protection Act 1966, as amended, an assurance that, in the event that Bermuda enacts legislation imposing tax computed on profits, income, any capital asset, gain or appreciation, or any tax in the nature of estate duty or inheritance, then the imposition of any such tax shall not be applicable to NCLH or to any of our operations or shares, debentures or other obligations, until March 31, 2035.\nNCLH and NCLC are tax residents of the U.K. and are subject to normal U.K. corporation tax. During 2013, NCLH implemented a restructuring plan to provide a global tax platform for international expansion. As part of the plan, the Company became a tax resident of the U.K. As such, it qualifies for relief from U.S. Branch Profits taxes under the U.S.-U.K. Tax Treaty. In addition, the restructuring resulted in additional interest and depreciation which reduced the Company’s overall income tax expense.\nNCLH under Section 883 of the Code and the related regulations, as a foreign corporation is exempt from U.S. federal income taxation on its U.S. source income derived from the international operation of ships (“shipping income”)\nNCLH believes and has taken the position that substantially all of NCLH’s income, including the income of its ship-owning subsidiaries, is properly categorized as shipping income, and that we do not have a material amount of non-qualifying income.\nEconomic Substance Requirements\nNCLH and NCLC are exempted companies formed under the laws of Bermuda and some of their subsidiaries have been formed in Bermuda, Guernsey, Isle of Man, British Virgin Islands, Cayman Islands or the Bahamas. In June 2018, the European Union issued a scoping paper which set out economic substance requirements that targeted international financial centers, including the jurisdictions listed above, were required to adopt before 2019 with regard to relevant entities based in those jurisdictions.\nPursuant to the legislation passed in each jurisdiction, entities subject to each jurisdiction’s laws that carry out relevant activities as specified in such laws, are required to demonstrate substantial economic substance in that jurisdiction. In general terms, substantial economic substance means:\n- the entity is actually directed and managed in the jurisdiction;\n- core income-generating activities relating to the applicable relevant activity are performed in the jurisdiction; - - there are adequate employees in the jurisdiction;\n- the entity maintains adequate physical presence in the jurisdiction; and\n- there is adequate operating expenditure in the jurisdiction.\nNCLH has evaluated their activities and their subsidiaries and have concluded that in some cases, those activities are ‘relevant activities’ for the purposes of the applicable economic substance laws and that, consequently, certain entities within our organization will be required to demonstrate compliance with these economic substance requirements. NCLH may be subject to increased costs and our management team may be required to devote significant time to satisfying economic substance requirements in certain of these jurisdictions. If such entities cannot establish compliance with these requirements, we may be liable for penalties and fines in the applicable jurisdictions and/or required to re-domicile such entities to different jurisdictions.\n\n\nCode\n# plot as Grouped bar chart with labels on right and tax rate on left\n# calculate position of bars\nx1_bar_position = []\nx2_bar_position = []\nfor i in df_dcf_data['FY']:\n    x1_bar_position.append(i-relativedelta(months=1))\n    x2_bar_position.append(i+relativedelta(months=1))\n\n# calculate tax rate\ntax_rate = df_dcf_data['income_tax']/df_dcf_data['income_before_income_taxes']\n\nwidth = 50  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, \\\\$M')\n\nrects1 = ax1.bar(x1_bar_position,df_dcf_data['income_before_income_taxes']/1e6, width,\n    label='Income before income taxes')\nrects2 = ax1.bar(x2_bar_position,df_dcf_data['income_tax']/1e6, width,\n    label='Income taxes')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((-20,20))\nax1.legend()\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],tax_rate * 100,'+-g')\n    \nax2.set_ylabel('% Tax rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-40,100))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Tax Rate')\nplt.show()\n\n\n\n\n\nObservation:\nNCLH in all but one year, 2019 received an income tax benefit. There is a lot of information in the annual and quarterly reports on NCLH’s tax situation. The acquisition of Prestige in 2014 included deferred tax assets. Certain foreign corporations are exempt from U. S. federal income or branch profits tax on U.S.-source income derived from or incidental to the international operation of ships.\nCalculate average tax rate for pre covid years.\n\n\nCode\n# Average tax rate\ntax_rate_avg = tax_rate[-6:-2].mean()\nprint('average tax rate: {:.2f}%'.format(tax_rate_avg*100))\n\n\naverage tax rate: -0.13%\n\n\n\n\nDepreciation Rate \nThe depreciation rate is used to project the future net investment cash flows. The effect is to reduce the amount of FCFF. Depreciation amounts are from the Consolidated Statement of Cash Flows, Depreciation and Amortization.\n\\(\\text{Depreciation Rate}=\\frac{\\text{Depreciation and Amortization}}{\\text{Revenues}}\\)\nDepreciation is the write off or expensing of a percentage of the historical cost of an asset over the asset’s useful life. Property, plant and equipment (PP&E) are long term or non current assets owned or controlled by the company and used to manufacture and or sell the company’s products. The balance sheet typically shows all categories of PP&E grouped together, net of accumulated depreciation. Depreciation represents wear and tear on an asset or the fact that an asset gets used up over time. Companies record depreciation expense in the income statement every year for all depreciable assets in service or used by the company during the year. The difference between GAAP and Tax Accounting methods is handled through deferred taxes.\nAmortization is the write off or expensing of the cost of a financial instrument or an intangible asset over the shorter of its useful life or legal life. Amortization is similar to depreciation and reflects the declining useful life and value of the intangible asset over time. Companies in research and development intensive fields typically have many patents. Such industries include high technology, pharmaceuticals and chemicals.\n\n\nCode\n# depreciation rate\ndepreciation_rate = df_dcf_data['depreciation'] / df_dcf_data['revenue'].to_numpy()\n\n# plot depreciation on left and rate on right\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $B')\n\nax1.bar(df_dcf_data['FY'],df_dcf_data['depreciation']/1e9, width=100,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:Blue'\n\nax2.plot(df_dcf_data['FY'],depreciation_rate*100,'+-')\n    \nax2.set_ylabel('% Depreciation rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((0,30))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Depreciation')\nplt.show()\n\n\n\n\n\nObservation:\nDepreciation of ships is computed on a straight-line basis over the weighted average useful lives of primarily 30 years after a 15% reduction for the estimated residual value of the ship.\nShip improvement costs that NCLH believes add value to our ships are capitalized to the ship and depreciated over the shorter of the improvements’ estimated useful lives or the remaining useful life of the ship. When they record the retirement of a ship component included within the ship’s cost basis, they estimate the net book value of the component being retired and remove it from the ship’s cost basis.\nRepairs and maintenance activities are charged to expense as incurred. We account for Dry-dock costs under the direct expense method which requires us to expense all Dry-dock costs as incurred.\nNCLH determines the weighted average useful lives of their ships based primarily on our estimates of the useful lives of the ships’ major component systems on the date of acquisition, such as cabins, main diesels, main electric, superstructure and hull. The useful lives of ship improvements are estimated based on the economic lives of the new components. In addition, to determine the useful lives of the ship or ship components, we consider the impact of the historical useful lives of similar assets, manufacturer recommended lives and anticipated changes in technological conditions.\nCalculate average depreciation rate for pre-covid years.\n\n\nCode\n# average depreciation rate\ndepreciation_rate_avg = depreciation_rate[-6:-2].mean()\nprint('average depreciation rate: {:.2f}%'.format(depreciation_rate_avg*100))\n\n\naverage depreciation rate: 21.20%\n\n\n\n\nInvestment Rate \nTaken from Consolidated Statement of Cash Flows, Cash used for investing activities. Net investment in the dollar amount needed to support the growth of the firm. Included investments in properties, plant equipment in excess of the depreciation expenses associated with past investments. Net investment decreases the amount of money available to the stockholders. Investment in property, plant and equipment is necessary to both maintain service and sales and also to grow revenues and profits. Investment amounts should include capital expenditures and research and development.\n\\(Ir=\\frac {\\text {Capital Expenditures}}{\\text{Revenues}}\\)\nFor this company, the yearly investment amounts are taken from the Consolidated Statements of Cash Flows, Net Cash Used in Investing Activities.\nAdjustments\nSee 2020 annual report pdf page 125 for Ship Construction Contracts\n2018 annual report:\nSophisticated and efficient maintenance and operations systems support the technical superiority and modern look of our fleet. In addition to routine repairs and maintenance performed on an ongoing basis and in accordance with applicable requirements, each of our ships is generally taken out of service, approximately every 24 to 60 months, for a period of one or more weeks for scheduled maintenance work, repairs and improvements performed in Dry-dock.\nDry-dock interval is a statutory requirement controlled under IMO requirements reflected in chapters of the International Convention of the Safety of Life at Seas (“SOLAS”) and to some extent the International Load Lines Convention. Under these regulations, it is required that a passenger ship Dry-dock once in five years (depending on age of vessel), twice in 5 years (depending on flag state and age of vessel) and the maximum interval between each Dry-dock cannot exceed 3 years (depending age of vessel and flag state).\nHowever, most of our international ships qualify under a special exemption provided by the Bahamas and/or Marshall Islands (flag state), as applicable, after meeting certain criteria set forth by the ship’s flag state to Dry-dock once every 5 years.\nFuture capital commitments consist of contracted commitments, including ship construction contracts, and future expected capital expenditures necessary for operations as well as our ship refurbishment projects. As of December 31, 2018, anticipated capital expenditures were \\$1.6 billion, \\$1.2 billion and \\$0.7 billion for the years ending December 31,2019, 2020 and 2021, respectively. We have export credit financing in place for the anticipated expenditures related to ship construction contracts of \\$0.6 billion, \\$0.5 billion and \\$0.2 billion for the years ending December 31, 2019, 2020 and 2021, respectively. These future expected capital expenditures will significantly increase our depreciation and amortization expense as we take delivery of the ships.\nProject Leonardo will introduce an additional six ships, each approximately 140,000 Gross Tons with approximately 3,300 Berths, with expected delivery dates from 2022 through 2027, subject to certain conditions. We have a Breakaway Plus Class Ship, Norwegian Encore, with approximately 168,000 Gross Tons with 4,000 Berths, on order for delivery in the fall of 2019. For the Regent brand, we have orders for two Explorer Class Ships, Seven Seas Splendor and an additional ship, to be delivered in 2020 and 2023, respectively. Each of the Explorer Class Ships will be approximately 55,000 Gross Tons and 750 Berths. For the Oceania Cruises brand, we have orders for two Allura Class Ships to be delivered in 2022 and 2025. Each of the Allura Class Ships will be approximately 67,000 Gross Tons and 1,200 Berths.\nThe combined contract prices of the 11 ships on order for delivery was approximately €7.9 billion, or \\$9.1 billion based on the euro/U.S. dollar exchange rate as of December 31, 2018. We have obtained export credit financing which is expected to fund approximately 80% of the contract price of each ship, subject to certain conditions. We do not anticipate any contractual breaches or cancellations to occur. However, if any such events were to occur, it could result in, among other things, the forfeiture of prior deposits or payments made by us and potential claims and impairment losses which may materially impact our business, financial condition and results of operations.\nCapitalized interest for the years ended December 31, 2018, 2017 and 2016 was \\$30.4 million, \\$29.0 million and \\$33.7 million, respectively, primarily associated with the construction of our newbuild ships.\nSee 2018 annual report pdf page 48 for Contractual Obligations,\nSee 2018 annual report pdf page 80 for Property and Equipment, Net, includes anounts for\n- Ships - Ships improvements - Ships under construction - Land and land improvements - Other\nShips under construction include progress payments to the shipyard, planning and design fees and other associated costs. Capitalized interest costs which were primarily associated with the construction or revitalization of ships amounted to \\$30.4 million, \\$29.0 million and \\$33.7 million for the years ended December 31, 2018, 2017 and 2016, respectively.\nSee 2018 annual report pdf page 93 for Ship Construction Contracts, for minimum annual payments for non-cancelable ship construction contracts.\n\n\nCode\n# investment rate\ninvestment_rate = df_dcf_data['investment'] / df_dcf_data['revenue'].to_numpy()\n\n# plot investment on left and rate on right\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, \\\\$B')\n\nax1.bar(df_dcf_data['FY'],df_dcf_data['investment']/1e9, width=100,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:Blue'\n\nax2.plot(df_dcf_data['FY'],investment_rate*100,'+-')\n    \nax2.set_ylabel('% New Investment Rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-10,40))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('New Investment')\nplt.show()\n\n\n\n\n\nObservation:\nCapital expenditures for new ships is NCLH’s largest investment expense. NCLH pays for this expense by financing activities, where, typically 80% of the construction cost is financed. Looking at the years 2016 to 2019, investments are increasing at a rate of about 25% of revenue.\nCalculate average investment rate for pre-covid years.\n\n\nCode\n# average investment rate\ninvestment_rate_avg = investment_rate[-6:-2].mean()\nprint('average investment rate: {:.2f}%'.format(investment_rate_avg*100))\n\n\naverage investment rate: 38.26%\n\n\n\n\nWorking Capital Rate \nWorking capital is needed to support the corporate sales effort of any company. Often a company’s incremental change in net working capital either positive or negative is approximately proportional to its change in revenue.\n\\(\\text{Working capital} = \\text{Accounts Receivable} + \\text{Inventories} - \\text{Accounts Payable}\\)\nWorking capital is a company’s net investment in its accounts receivable and its inventories (cash outflows), minus its accounts payable (a cash inflow). Working capital and taxes are cash outflows from the corporation that are not available to pay debts and stockholders.\nAdjustments\nNo adjustments for this company.\n\n\nCode\n# plot as four grouped bar chart with labels on right and working capital rate on left\n# calculate position of bars\nx1_bar_position = []\nx2_bar_position = []\nx3_bar_position = []\nx4_bar_position = []\nfor i in df_dcf_data['FY']:\n    x1_bar_position.append(i-relativedelta(months=3))\n    x2_bar_position.append(i-relativedelta(months=1))\n    x3_bar_position.append(i+relativedelta(months=1))\n    x4_bar_position.append(i+relativedelta(months=3))\n\n# calculate working capital rate\nworking_capital = (df_dcf_data['accounts_receivable'] + df_dcf_data['inventories']) - \\\n    df_dcf_data['accounts_payable']\nworking_capital_rate = working_capital / df_dcf_data['revenue']\n\nwidth = 40  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, \\\\$B')\n\nrects1 = ax1.bar(x1_bar_position,df_dcf_data['accounts_receivable']/1e9, width,\\\n    label='Accounts Receivable')\nrects2 = ax1.bar(x2_bar_position,df_dcf_data['inventories']/1e9, width, label='Inventory')\n\nrects2 = ax1.bar(x3_bar_position,df_dcf_data['accounts_payable']/1e9, width, label='Accounts Payable')\nrects2 = ax1.bar(x4_bar_position,working_capital/1e9, width, label='Working Capital')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((-50,200))\nax1.legend()\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:Blue'\n\nax2.plot(df_dcf_data['FY'],working_capital_rate * 100,'+-')\n    \nax2.set_ylabel('% Working Capital Rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,50))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Working Capital')\nplt.show()\n\n\n\n\n\nObservation:\nThe working capital amounts for some years is negative because the accounts payable is much higher. A review of the 2018 didn’t provide a clear indication for the large accounts payable amount.\nThe average working capital rate is calculated for the pre-covid years of 2016 to 2019.\n\n\nCode\n# average working capital rate\nworking_capital_rate_avg = working_capital_rate[-6:-2].mean()\nprint('average working capital rate: {:.2f}%'.format(working_capital_rate_avg*100))\n\n\naverage working capital rate: 0.94%\n\n\n\n\nCurrent assets \nTotal Current Assets from the most recent balance sheet statement of the company. Current assets include inventory, cash and accounts receivables.\nAdjustments\nNone for this company.\n\n\nCode\n# plot Short Term Assets\nwidth = 100  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nplt.bar(df_dcf_data['FY'],df_dcf_data['current_assets']/1e9, width)\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Current assets')\nplt.ylabel('dollars, \\\\$B')\n\nplt.grid()\nplt.show()\n\n\n\n\n\nObservation:\nIn 2020, NCLH increased cash on hand to pay for pandemic pause in activities.\nUse 2019 data for current assets.\n\n\nCode\nsta = df_dcf_data['current_assets'].iloc[-3]\nprint('current assets: ${:.2f}B'.format(sta/1e9))\n\n\ncurrent assets: $3.56B\n\n\n\n\nCurrent liabilities \nTotal Current Liabilities from the most recent balance sheet consolidated statement.\nAdjustments\nNone for this company.\n\n\nCode\n# plot Short Term Liabilities\n\nwidth = 100  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nplt.bar(df_dcf_data['FY'],df_dcf_data['current_liabilities']/1e9, width)\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Current liabilities')\nplt.ylabel('dollars, \\\\$B')\n\nplt.grid()\nplt.show()\n\n\n\n\n\nObservation:\nDuring the pandemic pause, NCLH has decreased their short term liabilities by scaling back operations in 2020. Then with the return to service the current liabilities increased to the 2019 level.\nUse average of 2016 to 2019 data for current liabilities.\n\n\nCode\nstl = df_dcf_data['current_liabilities'].iloc[6:-2].mean()\nprint('Average of current liabilities pre Covid: ${:.2f}B'.format(stl/1e9))\n\n\nAverage of current liabilities pre Covid: $2.88B\n\n\n\n\nValue of Debt Outstanding \nAmount of debt outstanding from the most recent balance sheet of the company.\nAdjustments\nNone for this company.\n\n\nCode\n# calculate the percent change in debt, pcd\npcd = np.zeros(len(df_dcf_data['long_term_debt'].to_numpy())) # percent change in debt\nfor i in range(len(df_dcf_data['long_term_debt'].to_numpy()[0:-1])):\n    pcd[i+1] = ((df_dcf_data['long_term_debt'].to_numpy()[i+1] - df_dcf_data['long_term_debt'].to_numpy()[i])/\n                df_dcf_data['long_term_debt'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars, $B')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['long_term_debt']/1e9, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcd,'+-g')\n    \nax2.set_ylabel('% Change in debt',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-40,100))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('debt')\nplt.show()\n\n\n\n\n\nObservation:\nNCLH has increased long term debt on account of Covid-19 in order to pay for activities during the pause.\nUse average for years 2016 to 2019 data for long term debt.\n\n\nCode\nvod = df_dcf_data['long_term_debt'].iloc[6:-2].mean()\nprint('average total long term debt and other pre Covid: ${:.2f}B'.format(vod/1e9))\n\n\naverage total long term debt and other pre Covid: $8.27B\n\n\n\n\nCurrent stock price \nMost recent stock price for the company. The current stock price is used to calculate the market value of the firm. Use the market value when looking at market capitalization for common stock.\n\n\nCode\ncsp = 13.72 # current stock price\nprint('current stock price: ${:,.2f}'.format(csp))\n\n\ncurrent stock price: $13.72\n\n\n\n\nShares outstanding \nThe number of shares outstanding is used to calculate the intrinsic stock value.\nUse 2019 data for shares outstanding.\n\n\nCode\nso = df_dcf_data['shares_outstanding'].iloc[-3] # shares outstanding\nprint('shares outstanding, basic: {:,.0f}'.format(so))\n\n\nshares outstanding, basic: 254,728,932\n\n\n\n\nCode\n# calculate the percent change in shares outstanding, pcso\npcso = np.zeros(len(df_dcf_data['shares_outstanding'].to_numpy())) # percent change in debt\nfor i in range(len(df_dcf_data['shares_outstanding'].to_numpy()[0:-1])):\n    pcso[i+1] = ((df_dcf_data['shares_outstanding'].to_numpy()[i+1] - df_dcf_data['shares_outstanding'].to_numpy()[i])/\n                df_dcf_data['shares_outstanding'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('shares outstanding, M')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['shares_outstanding']/1e6, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcso,'+-g')\n    \nax2.set_ylabel('% Change in shares outstanding',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-5,25))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Shares outstanding')\nplt.show()\n\n\n\n\n\n\n\nCode\nprint('average shares outstanding growth rate: {:.2f}%'.format(pcso[1:].mean()))\nprint('shares outstanding pre-Covid: {:,.0f}'.format(df_dcf_data['shares_outstanding'].iloc[-3]))\n\n\naverage shares outstanding growth rate: 7.67%\nshares outstanding pre-Covid: 254,728,932\n\n\nObservation:\nThe number of shares outstanding comes from the 2020 annual report, see page 50 and Consolidated Statements, pdf page 87. This number needs to be researched a bit more to and should include any dilution from convertible bonds. As of 2020 the basic shares were 254,728,932 and the diluted shares were 254,728,932.\n\n\nDilution\nDilution occurs when a company issues new shares that result in a decrease in existing stockholders’ ownership percentage of that company. Stock dilution can also occur when holders of stock options, such as company employees, or holders of other optionable securities exercise their options. When the number of shares outstanding increases, each existing stockholder owns a smaller, or diluted, percentage of the company, making each share less valuable.\nInvestigate why there is a historic growth trend in the number of shares outstanding. Search annual report for dilutive actions: - share sales - convertible debt - employee options\nSearch results:\nRecent common share issuance has been the most dilutive, almost doubling the number of shares outstanding. For example, in March 2021, NCLH completed an equity offering that resulted in 52,577,947 ordinary shares being issued for gross proceeds of $1.6 billion.\nA relatively smaller number of shares were granted for employee share based compensation, for example, in March 2022, NCLH granted 4.8 million time-based restricted share unit awards to our employees, which primarily vest in substantially equal installments over three years. Additionally, in March 2022, NCLH granted 1.9 million performance-based restricted share units to certain members of our management team, which vest upon the achievement of certain pre-established performance targets established through 2024 and the satisfaction of an additional time-based vesting requirement that generally requires continued employment through March 1, 2025.\nJune 16, 2022 Form 8K: Among other things, the Restated 2013 Plan reflects amendments to: i. increase the number of the Company’s ordinary shares that may be delivered pursuant to all awards granted under the Restated 2013 Plan by an additional 7,000,000 shares, from 32,375,106 shares to a new maximum aggregate limit of 39,375,106 shares;\n\n\nCode\nprint('value ordinary shares authorized for awards: ${:,.2f}B'.format(39375106*csp/1e9))\n\n\nvalue ordinary shares authorized for awards: $0.54B\n\n\n\n\n10 year treasury bond yield \nThe 10 year treasury yield is used as a measure of the risk free rate.\nYield: 3.0350%\niShares 7-10 Year Treasury Bond ETF (IEF)\nAverage Yield to Maturity: 3.04%\n\n\nCode\ntby = (3.035+3.04)/2/100  # 10 year treasury bond yield, average of data from sources listed above\nprint('10 year treasury bond yield: {:,.2f}%'.format(tby*100))\n\n\n10 year treasury bond yield: 3.04%\n\n\n\n\nBond yield spread to treasury \nThe spread to treasury implies that all corporate debt will have a higher yield than yields associated with comparable maturity US Treasury Bonds. The best way to determine default risk is to see how a particular company’s debt is trading in the market and compare it on a spread basis with comparable maturity yields.\nLook at the following or use a default rating system that is published by the three major rating agencies, Standards and Poors Corp, Moody’s Investor Services and Fitch & Company.\nPIMCO Active Bond Exchange-Traded Fund (BOND)\nYield: 2.95%\niShares 5-10 Year Investment Grade Corporate Bond ETF (IGIB)\nAverage Yield to Maturity: 4.74%\niShares 10+ Year Investment Grade Corporate Bond ETF (IGLB)\nAverage Yield to Maturity: 5.08%\nWeb resources: - http://www.standardpoor.com/\n- http://bond.yahoo.com/rates.html\n- http://www.moodys.com/cust/default.asp\n- http://www.fitchibca.com/corporate/index.cfm\n\n\nCode\nbystt = ((2.95+4.74+5.08)/3-tby)/100           # bond yield spread (average) to treasury spread\nprint('Bond yield spread to treasury: {:,.2f}%'.format(bystt*100))\n\n\nBond yield spread to treasury: 4.23%\n\n\n\n\nPreferred stock yield \nAmount of preferred stock outstanding from the most recent balance sheet of the company.\n\n\nCode\npsy = 0/100  # preferred stock yield\nprint('preferred stock yield: {:,.2f}%'.format(psy*100))\n\nvps = 0 # value of preferred stock\nprint('value of preferred stock: {:,.2f}'.format(vps))\n\n\npreferred stock yield: 0.00%\nvalue of preferred stock: 0.00\n\n\n\n\nEquity risk premium \nThe expected excess return a hypothetical average investor would require of a diversified portfolio of stock (assumed beta = 1.0) over the yield on the 10-year Treasury Bond. The equity risk premium has been going down over the years.\n- 1926 to 1990: 5.5%\n- 1962 to 1990: 3.25%\n- 1981 to 1990: 0.19%\nIn times of sustained economic growth the risk premium demanded by investors generally declines.\nI’m going to use 3% as the equity risk premium.\n\n\nCode\neq_rp = 3.0/100             # equity risk premium\nprint('Equity risk premium: {:,.2f}%'.format(eq_rp*100))\n\n\nEquity risk premium: 3.00%\n\n\n\n\nCompany specific beta \nThe Beta used is Beta of Equity. Beta is the monthly price change of a particular company relative to the monthly price change of the S&P 500. The time period for Beta is 5 years when available. This value can be obtained at yahoo finance.\nA measure of risk of an individual stock. It measures volatility of return - a higher beta means a higher risk. A financial model that uses Beta as its sole measure of risk (signal factor model) is called a Capital Asset Pricing Model (CAPM).\n\n\nCode\nbeta = 2.47 # company specific beta\nprint('Company specific beta: {:,.2f}'.format(beta))\n\n\nCompany specific beta: 2.47\n\n\n\n\nDCF model inputs \nBelow are the DCF model inputs. These values were calculated above.\n\n\nCode\n# various rates\nrgr = rgr_avg              # revenue growth rate\nprint('revenue growth rate: {:,.2f}%'.format(rgr*100))\nnopm = nopm_avg             # net operating profit margin\nprint('net operating profit margin: {:,.2f}%'.format(nopm*100))\ntr = tax_rate_avg               # tax rate\nprint('tax rate: {:,.2f}%'.format(tr*100))\ndr = depreciation_rate_avg              # depreciation rate (% of revenue)\nprint('depreciation rate: {:,.2f}%'.format(dr*100))\nir = investment_rate_avg              # investment rate (% of revenue)\nprint('investment rate: {:,.2f}%'.format(ir*100))\nwcr = working_capital_rate_avg            # working capital rate (% of revenue)\nprint('working capital rate: {:,.2f}%'.format(wcr*100))\n\n\nrevenue growth rate: -94.51%\nnet operating profit margin: -1.00%\ntax rate: -0.13%\ndepreciation rate: 21.20%\ninvestment rate: 38.26%\nworking capital rate: 0.94%\n\n\nExcess return period\nThe excess return period is based on a judgment call. The authors of [1] use the 1-5-7-10 rule. They group companies into one of four general categories and excess return periods. They use a 10 year excess return period to calculate what they would consider the maximum value. They use a more conservative 1 year, 5 year or 7 year return period to calculate a more reasonable or minimum value.\n- 1 year: Boring companies that operate in a highly competitive, low margin industry in which they have nothing particular going for them. - 5 year: Decent companies that have a recognizable name and decent reputation and perhaps a regulatory benefit (utility company), but can’t control pricing or growth. - 7 year: Good companies with good brand names, large companies of scale, good marketing channels and consumer identification (e.g. McDonald’s) - 10 year: Great companies with great growth potential, tremendous marketing power, band names and in-place benefits (e.g. Intel, Microsoft, Coca Cola, Disney)\nThe excess return period used for the base case is ten years, which should lead to a higher calculated intrinsic value.\nUse 2019 for starting revenue amount.\n\n\nCode\n# General Inputs\nfy_start = df_dcf_data['FY'].iloc[-1].year # fiscal year to start excess return period\nerp = 10 # excess return period, years\nrev_start = df_dcf_data['revenue'].to_numpy()[-3] # starting revenues for excess return period\nprint('starting year: {:.0f}'.format(fy_start))\nprint('excess return period: {:.0f} years'.format(erp))\nprint('starting revenues: ${:,.2f}B'.format(rev_start/1e9))\nprint('shares outstanding: {:,.0f}'.format(so))\n\n\nstarting year: 2022\nexcess return period: 10 years\nstarting revenues: $1.28B\nshares outstanding: 254,728,932\n\n\n\n\nCode\nps_mv = vps               # preferred stock, market value \nprint('preferred stock, market value : ${:,.2f}B'.format(ps_mv/1e9))\ncs_mv = csp*so            # common stock, market value \nprint('common stock, market value: ${:,.2f}B'.format(cs_mv/1e9))\n\n\npreferred stock, market value : $0.00B\ncommon stock, market value: $3.49B\n\n\nLong Term Debt, Market Value, ltd_mv\nUse the book value for long term debt. Various online resources can be used to research this item. These include, Bondsonline and Bloomberg. The book value of debt and preferred stock is an accounting measure that relates to how much money was raised by the company when each security was issued. The market value of debt and the preferred and common stock is the price that specific obligations would trade at in today’s market.\nLong term debt for firms can take one of two forms. It can be a long-term loan from a bank or other financial institution or it can be a long-term bond issued to financial markets, in which case the creditors are the investors in the bond. Firms often have long term obligations that are not captured in the long term debt item. These include obligations to lessors on assets that firms have leased, to employees in the form of pension fund and health care benefits yet to be paid, and to the government in the form of taxes deferred. In the last two decades, accountants have increasingly moved towards quantifying these liabilities and showing them as long term liabilities.\n\n\nCode\nltd_mv = vod              # market value of long term debt\ntmv = ltd_mv+ps_mv+cs_mv  # total market value \nprint('total market value: ${:,.2f}B'.format(tmv/1e9))\n\n\ntotal market value: $11.76B\n\n\nCost of Common Equity, cce\nThe expected excess return a hypothetical average investor would require of a diversified portfolio of stock (assumed beta = 1.0) over the yield on the 10-year Treasury Bond. The annual rate of return that an investor expects to earn when investing in shares of a company is known as the cost of common equity. It includes dividends and increases in the market value.\n\n\nCode\ncce = tby+beta*eq_rp      # cost of common equity or the expected return for the stock\nprint('cost of common equity: {:,.2f}%'.format(cce*100))\n\n\ncost of common equity: 10.45%\n\n\nLong Term Debt, Average Yield, ltd_ay\nThe total cost of long term debt.\n\n\nCode\nltd_ay = tby+bystt        # long term debt average yield\nprint('long term debt average yield: {:,.2f}%'.format(ltd_ay*100))\n\n\nlong term debt average yield: 7.26%\n\n\nLong Term Debt, After Tax Yield, ltd_aty\nThe tax benefits of long term debt. Interest payments are tax deductible for the company.\n\n\nCode\nltd_aty = ltd_ay*(1-tr)   # long term debt after tax yield\nprint('long term debt after tax yield: {:,.2f}%'.format(ltd_aty*100))\n\nltd_pc = vod/tmv          # weight for long term debt \nltd_ate = ltd_aty*ltd_pc  # after tax effect of long term debt \nps_ay = psy               # preferred stock, average yield \nps_aty = ps_ay            # preferred stock, average yield \nprint('preferred stock, average yield: {:,.2f}%'.format(ps_aty*100))\n\nps_pc = ps_mv/tmv         # preferred stock, % capital \nps_ate = ps_aty*ps_pc     # preferred stock, after tax effect \ncs_ay = cce               # common stock, average yield \ncs_aty = cce              # common stock, after tax yield \nprint('common stock, after tax yield: {:,.2f}%'.format(cs_aty*100))\n\ncs_pc = cs_mv/tmv         # common stock, % capital \ncs_ate = cs_aty*cs_pc     # common stock, after tax effect \nprint('common stock, after tax effet: {:,.2f}%'.format(cs_ate*100))\n\ntate = ltd_ate+ps_ate+cs_ate # total after tax effect \nprint('total after tax effect: {:,.2f}%'.format(tate*100))\ntpc = ltd_pc+ps_pc+cs_pc     # total % Capital\nprint('total % Capital: {:,.2f}%'.format(tpc*100))\n\n\nlong term debt after tax yield: 7.27%\npreferred stock, average yield: 0.00%\ncommon stock, after tax yield: 10.45%\ncommon stock, after tax effet: 3.10%\ntotal after tax effect: 8.22%\ntotal % Capital: 100.00%\n\n\nWeighted average cost of capital\nA company’s weighted average cost of capital (WACC) is the weighted average of the company’s current cost of debt and equity calculated by using current debt, preferred stock and common stock market values. The WACC of the company, calculated after tax, is the discount rate used in the DCF valuation procedures. The WACC, which is the cost of the different components of financing used by the firm, weighted by their market value proportions. These include debt, preferred stock, and common stock.\nWACC: Weighted Average Cost of Capital, the rate used to discount cash flows, based on the following three factors. 1. Base rate of return. 2. Expected return based on debt and preferred stock. 3. Expected return on common stock and Beta.\nAll adjusted for the tax advantage of interest payments and the percentage of debt, preferred stock and common stock.\n\n\nCode\nwacc = tate\nprint('weighted average cost of capital: {:.1f}%'.format(wacc*100))\n\n\nweighted average cost of capital: 8.2%\n\n\n\n\nFuture cash flows \nThe future cash flows to the firm are projected based on revenue growth. The cash flows are then discounted using the WACC and the ISV is calculated.\n\n\nCode\n# make a list of the fiscal years in excess return period \nfy = np.zeros(erp+1)\nfy[0] = fy_start\nfor i in range(1,erp+1): \n    fy[i]=fy_start+i\n\nrev = np.zeros(len(fy))\nciwc = np.zeros(len(fy))\nrev[0] = rev_start  #*rgr+rev_start   # find the future revenue using constant revenue growth rate \n\nfor i in range(1,len(fy)): \n    rev[i] = rev[i-1]*rgr+rev[i-1]  # find the future revenue \n    ciwc[i] = (rev[i]-rev[i-1])*wcr  # find the change in working capital \n\nnet_op = np.zeros(len(fy)) # net operating profit\nadj_taxes = np.zeros(len(fy))\nnopat = np.zeros(len(fy))\ninvest = np.zeros(len(fy))\ndepre = np.zeros(len(fy))\nnet_invest = np.zeros(len(fy))\nfcff = np.zeros(len(fy))\ndisc_fact = np.zeros(len(fy))\ndisc_fcff = np.zeros(len(fy))                \n\n# calculate values in table \nfor i in range(1,len(fy)):\n    net_op[i] = rev[i]*nopm # net operating profit margin\n    adj_taxes[i] = net_op[i]*tr # net operating profit adjusted for taxes\n    nopat[i] = net_op[i]-adj_taxes[i] # after tax net operating profit\n    invest[i] = rev[i]*ir # future investments\n    depre[i] = rev[i]*dr # future depreciations\n    net_invest[i] = invest[i]-depre[i] # net investments\n    fcff[i] = nopat[i]-net_invest[i]-ciwc[i] # free cash flow to the firm\n    disc_fact[i] = 1/((1+wacc)**i) # discount factor\n    disc_fcff[i] = disc_fact[i]*fcff[i] # discounted free cash flow to the firm\n    \ndcrv = nopat[-1]/wacc*disc_fact[-1] # discounted corporate residual value\nderp_fcff = disc_fcff.sum() # discounted excess return period FCFF\n\ntcv = derp_fcff+dcrv+sta # total corporate value\ntvce = tcv-vod-vps-stl # total value of common equity\nisv = tvce/so # intrinsic stock value\n\n# print cash flows in a table\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format('Year','Rev','NOP','AdjTaxes',\n    'NOPAT','Invest.','Deprec.','dInvest.','dWC','FCFF','DF','DF*FCFF'))\nfor i in range(len(fy)):\n    print('{:4.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.4f}{:10,.0f}'.format(fy[i],\n        rev[i]/1e6,net_op[i]/1e6,adj_taxes[i]/1e6,nopat[i]/1e6,invest[i]/1e6,depre[i]/1e6,(invest[i]-depre[i])/1e6,ciwc[i]/1e6,\n        fcff[i]/1e6,disc_fact[i],disc_fcff[i]/1e6))\n\n\nYear       Rev       NOP  AdjTaxes     NOPAT   Invest.   Deprec.  dInvest.       dWC      FCFF        DF   DF*FCFF\n2022     1,280         0         0         0         0         0         0         0         0    0.0000         0\n2023        70        -1         0        -1        27        15        12       -11        -1    0.9241        -1\n2024         4        -0         0        -0         1         1         1        -1        -0    0.8539        -0\n2025         0        -0         0        -0         0         0         0        -0        -0    0.7891        -0\n2026         0        -0         0        -0         0         0         0        -0        -0    0.7292        -0\n2027         0        -0         0        -0         0         0         0        -0        -0    0.6738        -0\n2028         0        -0         0        -0         0         0         0        -0        -0    0.6227        -0\n2029         0        -0         0        -0         0         0         0        -0        -0    0.5754        -0\n2030         0        -0         0        -0         0         0         0        -0        -0    0.5317        -0\n2031         0        -0         0        -0         0         0         0        -0        -0    0.4913        -0\n2032         0        -0         0        -0         0         0         0        -0        -0    0.4540        -0\n\n\n\n\nCode\n# Intrinsic Value\nprint('discounted excess return period FCFF: ${:,.2f}B'.format(derp_fcff/1e9))\nprint('discounted corporate residual value: ${:,.2f}B'.format(dcrv/1e9))\nprint('total corporate value: ${:,.2f}B'.format(tcv/1e9))\nprint('total value of common equity: ${:,.2f}B'.format(tvce/1e9))\ntvce_baseline = tvce # save value as baseline case\nisv_baseline = isv # save the isv for the baseline case\nprint('intrinsic stock value, baseline case: ${:,.2f}'.format(isv_baseline))\nprint('current stock price: ${:,.2f}'.format(csp))\n\n\ndiscounted excess return period FCFF: $-0.00B\ndiscounted corporate residual value: $-0.00B\ntotal corporate value: $3.56B\ntotal value of common equity: $-7.60B\nintrinsic stock value, baseline case: $-29.82\ncurrent stock price: $13.72\n\n\nObservation:\nThe base line DCF analysis produces an intrinsic stock value of \\$80.\n\n\nList of all inputs to the DCF model\nThe following print statements format the inputs to the model similar to how they are presented on the Valuepro page.\n\n\nCode\nprint('{:&gt;35s} {:&lt;10.0f} {:&gt;35s} {:,.3f}'.format('Excess return period, years:',erp,'Depreciation rate, %:',dr*100))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.3f}'.format('Starting revenues, $B:',\n    rev_start/1e9,'Investment rate, %:',ir*100))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:,.3f}'.format('Revenue growth rate, %:',\n    rgr*100,'Working capital rate, %:',wcr*100))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:,.3f}'.format('Net operating profit margin, %:',\n    nopm*100,'Current assets, $B:',sta/1e9))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:.3f}'.format('Tax rate, %:',\n    tr*100,'Current liabilities, $B:',stl/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.2f}'.format('Current stock price, $:',\n    csp,'Equity risk premium, %:',eq_rp*100))\nprint('{:&gt;35s} {:&lt;10,.0f} {:&gt;35s} {:,.2f}'.format('Shares outstanding, basic, M:',\n    so/1e6,'Company specific beta:',beta))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:.3f}'.format('10 year treasury bond yield, %:',\n    tby*100,'Total long term debt and other, $B:',vod/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.3f}'.format('Bond yield spread to treasury, %:',\n    bystt*100,'Value of preferred stock, $B:',vps/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f}'.format('Preferred stock yield, %:',psy*100))\n\n\n       Excess return period, years: 10                       Depreciation rate, %: 21.200\n             Starting revenues, $B: 1.28                       Investment rate, %: 38.264\n            Revenue growth rate, %: -94.514               Working capital rate, %: 0.935\n    Net operating profit margin, %: -1.000                     Current assets, $B: 3.558\n                       Tax rate, %: -0.126                Current liabilities, $B: 2.883\n            Current stock price, $: 13.72                  Equity risk premium, %: 3.00\n      Shares outstanding, basic, M: 255                     Company specific beta: 2.47\n    10 year treasury bond yield, %: 3.04       Total long term debt and other, $B: 8.269\n  Bond yield spread to treasury, %: 4.23             Value of preferred stock, $B: 0.000\n          Preferred stock yield, %: 0.00      \n\n\n\n\nCode\n# weighted average cost of capital inputs\nprint('Weighted Average Cost of Capital')\nprint('Cost of common equity')\nprint('{:s}'.format('-'*37))\nprint('{:&gt;32s} {:,.2f}'.format('10 year treasury bond yield, %:',tby*100))\nprint('{:&gt;32s} {:,.2f}'.format('Company specific beta:',beta))\nprint('{:&gt;32s} {:,.2f}'.format('Equity risk premium, %:',eq_rp*100))\nprint('{:s}'.format('-'*37))\nprint('{:&gt;32s} {:,.2f}'.format('Cost of common equity, %:',cce*100))\nprint()\n\nprint('Market Capitalization and After-Tax Weighted Average Cost of Capital')\nprint()\nprint('{:s}{:^10s}{:^10s}{:^10s}{:^15s}{:^15s}'.format(' '*20,'Current','After-Tax','Market','%','Weighted After-'))\nprint('{:s}{:^10s}{:^10s}{:^10s}{:^15s}{:^15s}'.format(' '*20,'Yield','Yield','Value','Capitalization','Tax Yield'))\n\nprint('{:s}'.format('-'*80))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Long term debt',\n    ltd_ay*100,(tby+eq_rp)*(1-tr)*100,vod/1e9,ltd_pc*100,ltd_ate*100))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Preferred stock',\n     psy*100,ps_ate*100,vps/1e9,ps_pc*100,ps_ate*100))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Common stock',\n     cs_ay*100,cs_aty*100,cs_mv/1e9,cs_pc*100,cs_aty*100))\nprint('{:s}'.format('-'*80))\nprint('{:&lt;37s}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('',tmv/1e9,tpc*100,wacc*100))\n\n\nWeighted Average Cost of Capital\nCost of common equity\n-------------------------------------\n 10 year treasury bond yield, %: 3.04\n          Company specific beta: 2.47\n         Equity risk premium, %: 3.00\n-------------------------------------\n       Cost of common equity, %: 10.45\n\nMarket Capitalization and After-Tax Weighted Average Cost of Capital\n\n                     Current  After-Tax   Market         %       Weighted After-\n                      Yield     Yield     Value   Capitalization    Tax Yield   \n--------------------------------------------------------------------------------\nLong term debt         7.26      6.05         8       70.29           5.11\nPreferred stock        0.00      0.00         0        0.00           0.00\nCommon stock          10.45     10.45         3       29.71          10.45\n--------------------------------------------------------------------------------\n                                             12      100.00           8.22"
  },
  {
    "objectID": "NCLHv1 analysis.html#dcf-scenarios",
    "href": "NCLHv1 analysis.html#dcf-scenarios",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "4) DCF Scenarios ",
    "text": "4) DCF Scenarios \nThe following adjustments were made to various model parameters. - excess return period was adjusted to a more conservative 5 years - revenue growth rate was adjusted to 9% (base case = 9.428%) - net operating profit margin was adjusted to 25% (base case = 28.593%) - tax rate was adjusted to 1% (base case = -0.486%) - depreciation rate was adjusted to 10% (base case = 9.397%) - investment rate was adjust to 25% (base case = 25.003%) - working capital rate was set to an even 1% (base case = 0.979%) - weighted average cost of capital was adjusted up by 2% to reflect higher interest rates and provide a margin of safety (base case = 3.8%)\n\n\nCode\nprint('adjusted DCF input values and rates')\nerp = 5\nprint('excess return period: {:,.0f} years'.format(erp))\nrgr = 9/100\nprint('revenue growth rate: {:,.1f}%'.format(rgr*100))\nnopm = isv_s1_nopm = 25/100  # save nopm rate for NAIC preferred method\nprint('net operating profit margin: {:.2f}%'.format(nopm*100))\ntr = isv_s1_tr = 1/100 #  # save tax rate for NAIC preferred method\nprint('tax rate: {:.2f}%'.format(tr*100))\ndr = 10/100\nprint('depreciation rate: {:,.2f}%'.format(dr*100))\nir = 25/100              # investment rate (% of revenue)\nprint('investment rate: {:,.2f}%'.format(ir*100))\nwcr = 1/100\nprint('working capital rate: {:,.1f}%'.format(wcr*100))\nwacc_adj = (wacc+0.02) # weighted average cost of capital, increased by 2%\nprint('weighted average cost of capital: {:.1f}%'.format(wacc_adj*100))\n\n\nadjusted DCF input values and rates\nexcess return period: 5 years\nrevenue growth rate: 9.0%\nnet operating profit margin: 25.00%\ntax rate: 1.00%\ndepreciation rate: 10.00%\ninvestment rate: 25.00%\nworking capital rate: 1.0%\nweighted average cost of capital: 10.2%\n\n\n\n\nCode\n# make a list of the fiscal years in excess return period \nfy = np.zeros(erp+1)\nfy[0] = fy_start\nfor i in range(1,erp+1): \n    fy[i]=fy_start+i\n\nrev = np.zeros(len(fy))\nciwc = np.zeros(len(fy))\nrev[0] = rev_start  #*rgr+rev_start   # find the future revenue using constant revenue growth rate \n\nfor i in range(1,len(fy)): \n    rev[i] = rev[i-1]*rgr+rev[i-1]  # find the future revenue \n    ciwc[i] = (rev[i]-rev[i-1])*wcr  # find the change in working capital \n\nnet_op = np.zeros(len(fy))\nadj_taxes = np.zeros(len(fy))\nnopat = np.zeros(len(fy))\ninvest = np.zeros(len(fy))\ndepre = np.zeros(len(fy))\nnet_invest = np.zeros(len(fy))\nfcff = np.zeros(len(fy))\ndisc_fact = np.zeros(len(fy))\ndisc_fcff = np.zeros(len(fy))                \n\n# calculate values in table \nfor i in range(1,len(fy)): \n    net_op[i] = rev[i]*nopm # net operating profit\n    adj_taxes[i] = net_op[i]*tr # net operating profit adjusted for taxes\n    nopat[i] = net_op[i]-adj_taxes[i] # after tax net operating profit\n    invest[i] = rev[i]*ir # future investments\n    depre[i] = rev[i]*dr # future depreciations\n    net_invest[i] = invest[i]-depre[i] # net investments\n    fcff[i] = nopat[i]-net_invest[i]-ciwc[i] # free cash flow to the firm\n    disc_fact[i] = 1/((1+wacc_adj)**i) # discount factor\n    disc_fcff[i] = disc_fact[i]*fcff[i] # discounted free cash flow to the firm\n    \ndcrv = nopat[-1]/wacc*disc_fact[-1] # discounted corporate residual value\nderp_fcff = disc_fcff.sum() # discounted excess return period FCFF\n\ntcv = derp_fcff+dcrv+sta # total corporate value\ntvce = tcv-vod-vps-stl # total value of common equity\nisv = tvce/so # intrinsic stock value\n\n# print cash flows in a table\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format(\n    'Year','Rev','NOP','AdjTaxes',\n    'NOPAT','Invest.','Deprec.','dInvest.','dWC','FCFF','DF','DF*FCFF'))\nfor i in range(len(fy)):\n    print('{:4.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.4f}{:10,.0f}'.\n        format(fy[i],rev[i]/1e6,net_op[i]/1e6,adj_taxes[i]/1e6,nopat[i]/1e6,invest[i]/1e6,depre[i]/1e6,\n        (invest[i]-depre[i])/1e6,ciwc[i]/1e6,fcff[i]/1e6,disc_fact[i],disc_fcff[i]/1e6))\n\n\nYear       Rev       NOP  AdjTaxes     NOPAT   Invest.   Deprec.  dInvest.       dWC      FCFF        DF   DF*FCFF\n2022     1,280         0         0         0         0         0         0         0         0    0.0000         0\n2023     1,395       349         3       345       349       140       209         1       135    0.9073       122\n2024     1,521       380         4       376       380       152       228         1       147    0.8232       121\n2025     1,658       414         4       410       414       166       249         1       160    0.7469       120\n2026     1,807       452         5       447       452       181       271         1       175    0.6777       118\n2027     1,969       492         5       487       492       197       295         2       190    0.6149       117\n\n\n\n\nCode\n# Intrinsic Value\nprint('discounted excess return period FCFF: ${:,.2f}B'.format(derp_fcff/1e9))\nprint('discounted corporate residual value: ${:,.2f}B'.format(dcrv/1e9))\nprint('total corporate value: ${:,.2f}B'.format(tcv/1e9))\nprint('total value of common equity: ${:,.2f}B'.format(tvce/1e9))\ntvce_S1 = tvce # save value as scenario 1\nisv_S1 = isv # save the isv for scenario 1 case\nprint('intrinsic stock value, scenario 1 case: ${:,.2f}'.format(isv_S1))\nprint('current stock price: ${:,.2f}'.format(csp))\n\n\ndiscounted excess return period FCFF: $0.60B\ndiscounted corporate residual value: $3.65B\ntotal corporate value: $7.80B\ntotal value of common equity: $-3.35B\nintrinsic stock value, scenario 1 case: $-13.15\ncurrent stock price: $13.72\n\n\nThe DCF model calculates with adjustments an intrinsic stock value of \\$75\n\nScenario #2\nRun the DCF model again with current debt and current number of shares out standing.\n\n\nCode\nvod = 12563518*1000 #df_dcf_data['long_term_debt'].iloc[-1]\nprint('Total long term debt and other: ${:.2f}B'.format(vod/1e9))\n#print('Total long term debt and other pre-Covid: ${:.2f}B'.format(df_dcf_data['long_term_debt'].iloc[-3]/1e9))\n\n\nTotal long term debt and other: $12.56B\n\n\n\n\nCode\nltd_mv = vod              # market value of long term debt\ntmv = ltd_mv+ps_mv+cs_mv  # total market value \nprint('total market value: ${:,.2f}B'.format(tmv/1e9))\n\n\ntotal market value: $16.06B\n\n\n\n\nCode\ncce = tby+beta*eq_rp      # cost of common equity or the expected return for the stock\nprint('cost of common equity: {:,.2f}%'.format(cce*100))\n\n\ncost of common equity: 10.45%\n\n\n\n\nCode\nltd_ay = tby+bystt        # long term debt average yield\nprint('long term debt average yield: {:,.2f}%'.format(ltd_ay*100))\n\n\nlong term debt average yield: 7.26%\n\n\n\n\nCode\nltd_aty = ltd_ay*(1-tr)   # long term debt after tax yield\nprint('long term debt after tax yield: {:,.2f}%'.format(ltd_aty*100))\n\nltd_pc = vod/tmv          # weight for long term debt \nltd_ate = ltd_aty*ltd_pc  # after tax effect of long term debt \nps_ay = psy               # preferred stock, average yield \nps_aty = ps_ay            # preferred stock, average yield \nprint('preferred stock, average yield: {:,.2f}%'.format(ps_aty*100))\n\nps_pc = ps_mv/tmv         # preferred stock, % capital \nps_ate = ps_aty*ps_pc     # preferred stock, after tax effect \ncs_ay = cce               # common stock, average yield \ncs_aty = cce              # common stock, after tax yield \nprint('common stock, after tax yield: {:,.2f}%'.format(cs_aty*100))\n\ncs_pc = cs_mv/tmv         # common stock, % capital \ncs_ate = cs_aty*cs_pc     # common stock, after tax effect \nprint('common stock, after tax effet: {:,.2f}%'.format(cs_ate*100))\n\ntate = ltd_ate+ps_ate+cs_ate # total after tax effect \nprint('total after tax effect: {:,.2f}%'.format(tate*100))\ntpc = ltd_pc+ps_pc+cs_pc     # total % Capital\nprint('total % Capital: {:,.2f}%'.format(tpc*100))\n\n\nlong term debt after tax yield: 7.19%\npreferred stock, average yield: 0.00%\ncommon stock, after tax yield: 10.45%\ncommon stock, after tax effet: 2.27%\ntotal after tax effect: 7.90%\ntotal % Capital: 100.00%\n\n\n\n\nCode\nwacc = tate\nprint('weighted average cost of capital: {:.1f}%'.format(wacc*100))\n\n\nweighted average cost of capital: 7.9%\n\n\n\n\nCode\nso = 417734591 # df_dcf_data['shares_outstanding'].iloc[-1] # shares outstanding\nprint('shares outstanding, basic: {:,.0f}'.format(so))\n\n\nshares outstanding, basic: 417,734,591\n\n\n\n\nCode\nprint('adjusted DCF input values and rates')\nerp = 5\nprint('excess return period: {:,.0f} years'.format(erp))\nrgr = 9/100\nprint('revenue growth rate: {:,.1f}%'.format(rgr*100))\nnopm = isv_s1_nopm = 25/100  # save nopm rate for NAIC preferred method\nprint('net operating profit margin: {:.2f}%'.format(nopm*100))\ntr = isv_s1_tr = 1/100 #  # save tax rate for NAIC preferred method\nprint('tax rate: {:.2f}%'.format(tr*100))\ndr = 10/100\nprint('depreciation rate: {:,.2f}%'.format(dr*100))\nir = 25/100              # investment rate (% of revenue)\nprint('investment rate: {:,.2f}%'.format(ir*100))\nwcr = 1/100\nprint('working capital rate: {:,.1f}%'.format(wcr*100))\nwacc_adj = (wacc+0.02) # weighted average cost of capital, increased by 2%\nprint('weighted average cost of capital: {:.1f}%'.format(wacc_adj*100))\n\n\nadjusted DCF input values and rates\nexcess return period: 5 years\nrevenue growth rate: 9.0%\nnet operating profit margin: 25.00%\ntax rate: 1.00%\ndepreciation rate: 10.00%\ninvestment rate: 25.00%\nworking capital rate: 1.0%\nweighted average cost of capital: 9.9%\n\n\n\n\nCode\n# make a list of the fiscal years in excess return period \nfy = np.zeros(erp+1)\nfy[0] = fy_start\nfor i in range(1,erp+1): \n    fy[i]=fy_start+i\n\nrev = np.zeros(len(fy))\nciwc = np.zeros(len(fy))\nrev[0] = rev_start  #*rgr+rev_start   # find the future revenue using constant revenue growth rate \n\nfor i in range(1,len(fy)): \n    rev[i] = rev[i-1]*rgr+rev[i-1]  # find the future revenue \n    ciwc[i] = (rev[i]-rev[i-1])*wcr  # find the change in working capital \n\nnet_op = np.zeros(len(fy))\nadj_taxes = np.zeros(len(fy))\nnopat = np.zeros(len(fy))\ninvest = np.zeros(len(fy))\ndepre = np.zeros(len(fy))\nnet_invest = np.zeros(len(fy))\nfcff = np.zeros(len(fy))\ndisc_fact = np.zeros(len(fy))\ndisc_fcff = np.zeros(len(fy))                \n\n# calculate values in table \nfor i in range(1,len(fy)): \n    net_op[i] = rev[i]*nopm # net operating profit\n    adj_taxes[i] = net_op[i]*tr # net operating profit adjusted for taxes\n    nopat[i] = net_op[i]-adj_taxes[i] # after tax net operating profit\n    invest[i] = rev[i]*ir # future investments\n    depre[i] = rev[i]*dr # future depreciations\n    net_invest[i] = invest[i]-depre[i] # net investments\n    fcff[i] = nopat[i]-net_invest[i]-ciwc[i] # free cash flow to the firm\n    disc_fact[i] = 1/((1+wacc_adj)**i) # discount factor\n    disc_fcff[i] = disc_fact[i]*fcff[i] # discounted free cash flow to the firm\n    \ndcrv = nopat[-1]/wacc*disc_fact[-1] # discounted corporate residual value\nderp_fcff = disc_fcff.sum() # discounted excess return period FCFF\n\ntcv = derp_fcff+dcrv+sta # total corporate value\ntvce = tcv-vod-vps-stl # total value of common equity\nisv = tvce/so # intrinsic stock value\n\n# print cash flows in a table\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format(\n    'Year','Rev','NOP','AdjTaxes',\n    'NOPAT','Invest.','Deprec.','dInvest.','dWC','FCFF','DF','DF*FCFF'))\nfor i in range(len(fy)):\n    print('{:4.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.4f}{:10,.0f}'.\n        format(fy[i],rev[i]/1e6,net_op[i]/1e6,adj_taxes[i]/1e6,nopat[i]/1e6,invest[i]/1e6,depre[i]/1e6,\n        (invest[i]-depre[i])/1e6,ciwc[i]/1e6,fcff[i]/1e6,disc_fact[i],disc_fcff[i]/1e6))\n\n\nYear       Rev       NOP  AdjTaxes     NOPAT   Invest.   Deprec.  dInvest.       dWC      FCFF        DF   DF*FCFF\n2022     1,280         0         0         0         0         0         0         0         0    0.0000         0\n2023     1,395       349         3       345       349       140       209         1       135    0.9099       123\n2024     1,521       380         4       376       380       152       228         1       147    0.8280       122\n2025     1,658       414         4       410       414       166       249         1       160    0.7534       121\n2026     1,807       452         5       447       452       181       271         1       175    0.6855       120\n2027     1,969       492         5       487       492       197       295         2       190    0.6238       119\n\n\n\n\nCode\n# Intrinsic Value\nprint('discounted excess return period FCFF: ${:,.2f}B'.format(derp_fcff/1e9))\nprint('discounted corporate residual value: ${:,.2f}B'.format(dcrv/1e9))\nprint('total corporate value: ${:,.2f}B'.format(tcv/1e9))\nprint('total value of common equity: ${:,.2f}B'.format(tvce/1e9))\ntvce_S1 = tvce # save value as scenario 1\nisv_S1 = isv # save the isv for scenario 1 case\nprint('intrinsic stock value, scenario 2 case: ${:,.2f}'.format(isv_S1))\nprint('current stock price: ${:,.2f}'.format(csp))\n\n\ndiscounted excess return period FCFF: $0.60B\ndiscounted corporate residual value: $3.85B\ntotal corporate value: $8.01B\ntotal value of common equity: $-7.44B\nintrinsic stock value, scenario 2 case: $-17.80\ncurrent stock price: $13.72\n\n\nIn scenario 2, the CDF model, values NCLH at 22 per share."
  },
  {
    "objectID": "NCLHv1 analysis.html#naci-stock-selection-guide-analysis",
    "href": "NCLHv1 analysis.html#naci-stock-selection-guide-analysis",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "5) NACI stock selection guide analysis ",
    "text": "5) NACI stock selection guide analysis \nThis analysis follows the NAIC stock selection guide (SSG) [2]. The SSG relates revenue growth, EPS and share price history and makes a prediction about the future share price.\nThe National Association of Investors Clubs (NAIC) is a nonprofit organization dedicated to educating individual investors and investment clubs to become successful lifelong investors. NAIC’s Stock Selection Guide (SSG) is used in the following cells to analyze the company’s growth and whether the stock is selling at a reasonable price.\nThe SSG was originally developed in the 1950s as a paper worksheet by the not-for-profit National Association of Investors Corporation (NAIC). The SSG aims to aid individual investors in the fundamental analysis and selection of common stocks by reviewing components of a company’s growth, quality, and value.\n\nComments about NAIC analysis of a distressed company\nThe NAIC analysis presented below was performed for years 2019 and prior, which are the pre-covid years. Companies with negative earnings are difficult to evaluate with this method.\n\n\nLoad data from metrics sheet\n\n\nCode\n# column names: fiscal years \nfy_data = df_metrics_sheet.columns[1:].values.astype('datetime64[Y]')-1970\n# line 0: Net income\nnet_income_data = df_metrics_sheet.iloc[0].to_numpy()[1:].astype('float')\n# line 1: Shareholder equity\nshareholder_equity_data =  df_metrics_sheet.iloc[1].to_numpy()[1:].astype('float')\n# line 2: Total liabilities\ntotal_liabilities_data = df_metrics_sheet.iloc[2].to_numpy()[1:].astype('float')\n# line 3: Free cash flow, Net cash provided by operating activities \nfree_cash_flow_data =  df_metrics_sheet.iloc[3].to_numpy()[1:].astype('float')\n# line 4: Dividends\ndividends_data =  df_metrics_sheet.iloc[4].to_numpy()[1:].astype('float')\n# line 5: Total assets\ntotal_assets_data = df_metrics_sheet.iloc[5].to_numpy()[1:].astype('float')\n# line 6: Earnings per share\neps_data = df_metrics_sheet.iloc[6].to_numpy()[1:].astype('float')\n# line 7: Dividends per share  \ndps_data = df_metrics_sheet.iloc[7].to_numpy()[1:].astype('float')\n# line 8: Total tangible assets\ntotal_tangible_assets_data = df_metrics_sheet.iloc[8].to_numpy()[1:].astype('float')\n# line 9: Liabilities w/o deposits\nliabilities_wo_deposits_data = df_metrics_sheet.iloc[9].to_numpy()[1:].astype('float')\n# line 10: Provision for credit losses\nprovision_for_credit_losses_data = df_metrics_sheet.iloc[10].to_numpy()[1:].astype('float')\n# line 11: Short-term borrowings\nshort_term_borrowings_data = df_metrics_sheet.iloc[11].to_numpy()[1:].astype('float')\n# line 12: Preferred stock\npreferred_stock_data = df_metrics_sheet.iloc[12].to_numpy()[1:].astype('float')\n# line 13: Net cash used in investing activities \nnet_cash_used_in_investing_activities_data = df_metrics_sheet.iloc[13].to_numpy()[1:].astype('float')\n\n\n\n\nCode\n# make a new data frame to store data from metrics sheet\ndf_metrics_data = pd.DataFrame(data={\n    'FY':fy_data[::-1],\n    'net_income':net_income_data[::-1],\n    'shareholder_equity':shareholder_equity_data[::-1],\n    'total_liabilities':total_liabilities_data[::-1],\n    'free_cash_flow':free_cash_flow_data[::-1],\n    'dividends':dividends_data[::-1],\n    'total_assets':total_assets_data[::-1],\n    'eps':eps_data[::-1],    \n    'dps':dps_data[::-1],\n    'total_tangible_assets':total_tangible_assets_data[::-1],\n    'liabilities_wo_deposits':liabilities_wo_deposits_data[::-1],    \n    'provision_for_credit_losses':provision_for_credit_losses_data[::-1],\n    'short_term_borrowings':short_term_borrowings_data[::-1], \n    'preferred_stock':preferred_stock_data[::-1],\n    'net_cash_used_in_investing_activities':net_cash_used_in_investing_activities_data[::-1]\n    })\n\n#df_metrics_data\n\n\ncheck for matching years in both data frames\n\n\nCode\nif all(df_dcf_data['FY'] == df_metrics_data['FY']) != True:\n    print('error, years in data frame don\\'t match')\n    stop # this is not python code, so jupyterlab will throw an error\nelse:\n    print('OK, years in data frame match')\n\n\nOK, years in data frame match\n\n\n\n\nNAIC section 1: Visual analysis\nHigh and low price history for each year\nFrom the daily price history obtained from yahoo finance, the high and low closing price for each is obtained and the data saved to the financial data frame as new columns.\n\n\nCode\n#column names: fiscal years \nyears_list = df_metrics_sheet.columns[1:].values.astype('str')[::-1]\n\n# convert years to datetime format\nyear_ended_list = []\nfor i in years_list:\n    year_ended_list.append(datetime.strptime(i, '%Y'))\n\n# make emnpy lists to store open, close, high and low price data for each fiscal year\nfy_open = []\nfy_close = []\nfy_high = []\nfy_low = []\n\nfor i in year_ended_list:\n    start = i\n    end = i + relativedelta(years=1)\n    p1 = df_price_history.truncate(before=start, after=end)\n    if len(p1) == 0:\n        fy_open.append(np.nan)\n        fy_close.append(np.nan)        \n        fy_high.append(np.nan)\n        fy_low.append(np.nan)\n    else:\n        fy_open.append(p1['Open'].iloc[0])\n        fy_close.append(p1['Close'].iloc[-1])        \n        fy_high.append(p1['Close'].max())\n        fy_low.append(p1['Close'].min())\n\n# convert from list to numpy array\nfy_open = np.asarray(fy_open)\nfy_close = np.asarray(fy_close)\nfy_high = np.asarray(fy_high)\nfy_low = np.asarray(fy_low)\n\n\nPlotting the data\nThe annual sales, EPS and the high and low share price is plotted on a semilog plot. A consistent percentage change in the data will plot on the semi-log chart as a straight line.\nThe stock price is plotted separately from the sales and earnings for clarity.\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\n# figsize() function to adjust the size\nplt.subplots(1,2,figsize=(15, 5))\n\n# using subplot function and creating\n# plot one\nplt.subplot(1, 2, 1)\nwidth = 3  # the width of the bars\n#plt.bar(year_ended_list,fy_high-fy_low, width,bottom=fy_low,label='price')\nj = 0\nfor i in year_ended_list:\n    color = 'green'\n    if fy_open[j] &gt; fy_close[j]: color= 'red'\n    # high/low lines\n    plt.plot([i,i],[fy_low[j],fy_high[j]],color=color, linewidth=width)\n    # open marker\n    plt.plot([i,i-relativedelta(months=1)], [fy_open[j],fy_open[j]], color=color, linewidth=width)\n    # close marker\n    plt.plot([i,i+relativedelta(months=1)], [fy_close[j],fy_close[j]], color=color, linewidth=width)\n    j += 1\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.ylim((0,70))\nplt.title('Yearly stock high and low price range')\nplt.ylabel('stock price, $')\n#plt.legend()\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n\nplt.plot(df_metrics_data['FY'],df_dcf_data['revenue']/1e9,'+-',label='revenue, $B')\nplt.plot(df_metrics_data['FY'],df_metrics_data['eps'],'+-',label='EPS, $')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\n#plt.yscale('log')\n#plt.yticks([0.1,1,10,100,1000,10000],['0.1','1','10','100','1000','10000'])\n#plt.ylim((0.1,1000))\nplt.title('Revenue and EPS')\nplt.ylabel('Revenue and EPS')\nplt.legend()\n\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\nObservation:\nShare price has been trading in the 400 to 60 dollar range the years 2015 to 2019, indicating that the market is not impressed with the company. Average EPS (ignoring 2020) has been increasing along with revenues for the years 2015 to 2019.\n\n\nNAIC section 3, Price earnings history\nSection 3 of the SSG is the Price-Earnings history. The following table is built from the high and low prices each year and the earnings per share. The high and low Price/Earnings ratios are calculated for each year and are listed in the columns labeled h-per and l-per.\n\n\nCode\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format('year','high','low','eps',\n    'h-per','l-per'))\nfor i in range(len(year_ended_list)):\n    print('{:s}{:10,.2f}{:10,.2f}{:10,.2f}{:10,.2f}{:10,.2f}'.format(year_ended_list[i].strftime(\"%Y\"),\n        fy_high[i], fy_low[i],df_metrics_data['eps'][i],\n        fy_high[i]/df_metrics_data['eps'][i],\n        fy_low[i]/df_metrics_data['eps'][i]))\n\n\nyear      high       low       eps     h-per     l-per\n2012       nan       nan      0.95       nan       nan\n2013     35.47     24.79      0.50     70.94     49.58\n2014     48.03     29.65      1.64     29.29     18.08\n2015     63.76     42.93      1.89     33.74     22.71\n2016     57.99     34.40      2.79     20.78     12.33\n2017     59.46     42.79      3.33     17.86     12.85\n2018     60.93     39.55      4.28     14.24      9.24\n2019     59.56     40.71      4.33     13.76      9.40\n2020     59.65      7.77    -15.75     -3.79     -0.49\n2021     33.71     17.79    -12.33     -2.73     -1.44\n2022     23.72     10.38     -5.41     -4.38     -1.92\n\n\nAverage high and P/E for select years\nThe average price to earning ratio based on high and low stock prices is calculated.\nUse data from 2014 to 2019.\n\n\nCode\n#Average high P/E for years \npe_avg_high = (fy_high/df_metrics_data['eps'])[1:-2].mean()\nprint('average high P/E {:.2f}'.format(pe_avg_high))\n#Average low P/E for years \npe_avg_low = (fy_low/df_metrics_data['eps'])[2:-2].mean()\nprint('average low P/E {:.2f}'.format(pe_avg_low))\n\n\naverage high P/E 24.60\naverage low P/E 12.02\n\n\n\nEstimate future EPS\nUse polyfit to get EPS slope and intercept of a least square fit.\nUse data from 2013 to 2019.\n\n\nCode\ny = df_metrics_data['eps'][:-2]\nx = np.arange(len(y))\nm, c = np.polyfit(x, y, 1)\nprint('EPS slope: {:.2f}'.format(m))\nprint('EPS intercept: {:.2f}'.format(c))\nlstsq_fit = m*x + c  # data points for each year\n\n\nEPS slope: -0.81\nEPS intercept: 3.68\n\n\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('EPS')\n\nax1.plot(df_metrics_data['FY'],df_metrics_data['eps'], 'o',label='EPS')\nax1.plot(df_metrics_data['FY'][:-2],lstsq_fit, '-',label='least squares fit')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,4))\nax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('EPS and least squares fit')\nplt.show()\n\n\n\n\n\nUsing the equation for the best fit line, find the y value for the eps point at five years in the future.\n\n\nCode\n# estimated eps in 5 years\neps_5yr_est = m*(x[-1]+5) + c\nprint('estimated eps in 5 years: {:.1f}'.format(eps_5yr_est))\n\n\nestimated eps in 5 years: -6.8\n\n\nUsing the high and low price to earning ratio from above and the projected eps, calculate the range of stock price in five years.\n\n\nCode\nnaic_price_eps_low = eps_5yr_est*pe_avg_low\nnaic_price_eps_high = eps_5yr_est*pe_avg_high\nprint('estimated price range in 5 years: ${:.2f} to ${:.2f}'.format(naic_price_eps_low,naic_price_eps_high))\n\n\nestimated price range in 5 years: $-82.30 to $-168.48\n\n\nThis is the estimated price range of the stock based on projected EPS and is a guide for what the stock price might be if conditions remain the same. Since the slope of the EPS history is negative, the projected stock price is negative.\n\n\nNAIC section 3: 5 year estimated EPS, preferred method\nSee page 87 and figure 10-1, Need the following data:\n- estimate sales in 5 years based on sales growth - NOPM - Tax rate - shares outstanding\nNet Operating Profit should reflect the future revenue generating ability and expense requirements of the operating business that comprise the ongoing operations of the company.\n\\(\\text{NOPM} = \\frac{\\text{Revenue} - \\text{Expenses}}{\\text{Revenue}}\\)\nTax payments are taken from the consolidated income statement, provision for income taxes. The effect of taxes on profits is accounted for.\n\\(\\text{Tax rate} = \\frac{\\text{Income taxes}}{\\text{Income before income taxes}}\\)\nTo get future EPS\n\\(\\text{future EPS} = \\frac {\\text{future revenue} \\times \\text{NOPM} \\times \\text{(1-tax rate)}}{\\text{number of shares}}\\)\nUse polyfit to get revenue least square fit\nUse data from 2013 to 2019.\n\n\nCode\ny = df_dcf_data['revenue'][:-2]/1e6\nx = np.arange(len(y))\nm, c = np.polyfit(x, y, 1)\nprint('revenue slope: {:.2f}'.format(m))\nprint('revenue intercept: {:.2f}'.format(c))\nlstsq_fit = m*x + c  # data points for each year\n\n\nrevenue slope: 243.34\nrevenue intercept: 3069.45\n\n\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $M')\n\nax1.plot(df_metrics_data['FY'],df_dcf_data['revenue']/1e6, 'o',label='revenue')\nax1.plot(df_metrics_data['FY'][:-2],lstsq_fit, '-',label='least squares fit')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,4))\nax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Revenue and least squares fit')\nplt.show()\n\n\n\n\n\nUsing the equation for the best fit line, find the y value for the EPS point at five years in the future.\n\n\nCode\n# estimated revenue in 5 years\nrev_5yr_est = m*(x[-1]+5) + c\nprint('estimated rev in 5 years: ${:,.1f}M'.format(rev_5yr_est))\n\n\nestimated rev in 5 years: $6,232.9M\n\n\nneed to include estimate of number of shares outstanding in 5 years\n\n\nCode\nprint('starting revenues: ${:,.2f}'.format(rev_start/1e9))\n\n\nstarting revenues: $1.28\n\n\nUsing the adjusted NOPM and tax rate from scenario 1.\n\n\nCode\npm_nopm = isv_s1_nopm # use nopm from scenario 1\npm_tax_rate = isv_s1_tr # use tr from scenario 1\n\npm_eps_5yr_est = rev_5yr_est*pm_nopm*(1-pm_tax_rate)*1e6/df_dcf_data['shares_outstanding'].iloc[-1] \n#pm_eps_5yr_est = rev_5yr_est*nopm_avg*1e6/df_dcf_data['shares_outstanding'].iloc[-1] \nprint('using preferred method: estimated eps in 5 years: ${:.2f}'.format(pm_eps_5yr_est))\n\n\nusing preferred method: estimated eps in 5 years: $3.66\n\n\nUsing the high and low price to earning ratio from above and the projected EPS, calculate the range of stock price in five years.\n\n\nCode\nnaic_price_pm_low = pm_eps_5yr_est*pe_avg_low\nnaic_price_pm_high = pm_eps_5yr_est*pe_avg_high\nprint('estimated price range in 5 years from preferred method: {:.2f} to {:.2f}'.format(\n    naic_price_pm_low,naic_price_pm_high))\n\n\nestimated price range in 5 years from preferred method: 43.94 to 89.94\n\n\nObservation: Based on revenue growth, the projected stock price is a bit higher than the current price. However, based on price history, the stock is not expected to appreciate."
  },
  {
    "objectID": "NCLHv1 analysis.html#future-stock-price",
    "href": "NCLHv1 analysis.html#future-stock-price",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "6) Future stock price ",
    "text": "6) Future stock price \nThe projected future stock price is estimated from the results shown in this notebook based on DCF intrinsic stock value, the NAIC method or a combination of both. The DCF method does not consider market sentiment or popularity of the stock, whereas the NAIC method looks at the PE and EPS to develop the historical consensus that the market has put on the price of the stock. Both the NAIC and the DCF valuation should be considered. The DCF valuation is of the current ISV which is used as an indication of the future value, since it is assumed that the market price will converge eventually to the intrinsic value.\nThe estimated future stock price considers the following:\n- base case ISV - Senario ISV - NAIC EPS growth - NAIC preferred method\nUsing 5 year NAIC as a conservative estimate for the 10 year value and the analysis results, a judgment call is made concerning the price to put on the future value of the stock.\n\n\nCode\nprint('estimated price range in 5 years from EPS: ${:.2f} to ${:.2f}'.format(naic_price_eps_low,naic_price_eps_high))\nprint('estimated price range in 5 years from preferred method: ${:.2f} to ${:.2f}'.format(\n    naic_price_pm_low,naic_price_pm_high))\n\nprint('intrinsic stock value, baseline case: ${:,.2f}'.format(isv_baseline))\nprint('intrinsic stock value, scenario 1 case: ${:,.2f}'.format(isv_S1))\n\nprint('current stock price: ${:,.2f}'.format(csp))\n\n\nestimated price range in 5 years from EPS: $-82.30 to $-168.48\nestimated price range in 5 years from preferred method: $43.94 to $89.94\nintrinsic stock value, baseline case: $-29.82\nintrinsic stock value, scenario 1 case: $-17.80\ncurrent stock price: $13.72\n\n\nThe estimated price range in 5 years from the preferred method is \\$86.14 to \\$116.85. However, this no longer applies as a result of the pandemic.\nThe estimated price range in 5 years from the preferred method is \\$86.14 to \\$116.85. Taking the average and using that value on the IRR calculations.\nUsing the average of:\n- low estimated price from EPS and the low - estimated price from the preferred method - intrinsic stock value, scenario 1 case\nuse average of NAIC low price\n\n\nCode\n#fsp = (naic_price_eps_low + naic_price_pm_low + csp)/3 # estimated future stock price\nfsp = (naic_price_eps_low) # estimated future stock price\nprint('estimated future stock price: ${:,.2f}'.format(fsp))\n\n\nestimated future stock price: $-82.30"
  },
  {
    "objectID": "NCLHv1 analysis.html#dividend-payout",
    "href": "NCLHv1 analysis.html#dividend-payout",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "7) Dividend payout ",
    "text": "7) Dividend payout \nNCLH does not pay a dividend.\n\nShareholder benefit\nNCLH provides a shareholder’s benefit which provides \\$100 of onboard credit for a 7 night cruise and \\$250 onboard credit per stateroom on sailings of 15 days or more.\nI purchased 100 shares on Feb 24, 2020 at \\$42.9050 per share. My total cost basis is \\$4,290.50 and so far I’ve received shareholder benefits of \\$300. The current stock price is \\$13.72. The plan is to use the shareholder benefit many times and then sell the stock for the same or higher price. Well, given the disruption to the cruise industry and the dilution of NCLH, getting the original price in the future may not be possible.\nFor this analysis assume the following: - two cruises per year, totaling \\$200 in shareholder benefit which will be called a dividend.\nCalculate shareholder’s benefit per 100 shares and update the df_metrics_data[‘dps’] data.\n\n\nCode\ndf_metrics_data['dps'] = np.ones(len(df_metrics_data['dps']))*200/100 # shareholder benefit per year per share owned\n\n\n\n\nCode\n# calculate the percent change in dividends\npcd = np.zeros(len(df_metrics_data['dps'])) # percent change in dividend\nfor i in range(len(df_metrics_data['dps'][0:-1])):\n    pcd[i+1] = ((df_metrics_data['dps'][i+1] - df_metrics_data['dps'][i])/\n                df_metrics_data['dps'][i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dividend per share, $')\n\n# plot revenue as single bar\nplt.bar(df_metrics_data['FY'],df_metrics_data['dps'], width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(year_ended_list,pcd,'+-g')\n    \nax2.set_ylabel('% Change in dividend',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,20))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Dividend history per share')\nplt.show()\n\n\n\n\n\n\n\nCode\nadgr = pcd[-6:].mean() #last 6 years\nprint('average dividend growth rate: {:.2f}%'.format(adgr))\n\n\naverage dividend growth rate: 0.00%\n\n\n\n\nShareholder’s benefit as dividend yield\nDividend yield equals the annual dividend per share divided by the stock’s price per share. The plot below shows the history of dividend yield over the evaluation period.\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nwidth = 50  # the width of the bars\nplt.bar(df_metrics_data['FY'],(df_metrics_data['dps']/fy_high-df_metrics_data['dps']/fy_low)*100, \n        width,bottom=df_metrics_data['dps']/fy_low*100,label='yield')\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.ylim((0,30))\nplt.title('Range of dividend yield each year')\nplt.ylabel('dividend yield, %')\n#plt.legend()\nplt.grid()\n\n# show plot\nplt.show()\n\n\n\n\n\n\n\nInternal Rate of Return (IRR) calculations\nThe internal rate of return (IRR) is the discount rate that makes the net present value (NPV) of all cash flows equal to zero in a discounted cash flow analysis. Generally speaking, the higher an internal rate of return, the more desirable an investment is to undertake.\nAs explained above, the stock price has not changed by much over the years, even though the revenue and dividends have been increasing. The final stock price is set equal to the current price.\nUsing the average dividend growth rate calculated above, a series of estimated future dividend payments are generated.\n\n\nCode\nfdp = np.zeros(len(df_metrics_data['dps'])) # future dividend payments\nfdp[0] = df_metrics_data['dps'].iat[-1]\nfor i in range(len(df_metrics_data['dps'][0:-1])):\n    fdp[i+1] = fdp[i]+fdp[i]*adgr/100\n\n\n\n\nCode\nprint('current stock price: ${:,.2f}'.format(csp))\n\nfsp = 20 #100 #csp #500 #(csp + 102.05 + 138.82)/3 # final stock price, $\nprint('final stock price: ${:,.2f}'.format(fsp))\n\n\ncurrent stock price: $13.72\nfinal stock price: $20.00\n\n\n\n\nCode\nest_cf = np.copy(fdp) # make a copy of the estimated cash flow\n\n# cash flows, initial purchase, dividend payments and final sale\nest_cf[0] = est_cf[0] - 42.9 # subtract purchase price from the first dividend payment\nest_cf[-1] = est_cf[-1] + fsp # include the sale price with the final dividend payment\n\n\n\n\nCode\ndividend_irr = np_financial.irr(est_cf)\nprint('IRR: {:.2f}%'.format(dividend_irr*100))\n\n\nIRR: -0.29%\n\n\nA negative IRR indicates that my shareholders benefit will not break even with a final stock price of \\$20."
  },
  {
    "objectID": "NCLHv1 analysis.html#management-performance",
    "href": "NCLHv1 analysis.html#management-performance",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "8) Management performance ",
    "text": "8) Management performance \nThe following analysis somewhat follows the Warren Buffett strategy as outlined in [3]. This strategy is essentially value investing where companies are chosen that meet a set of criteria and who’s stock price is below the intrinsic value plus a margin of safety. These investments are usually held for the long term.\n\nFinancial metrics\nThe following analysis looks at financial ratios over the evaluation period. Financial ratios can be used to judge management performance. Consistent favorable trends are an indication that management is taking care of the company.\nTotal liabilities to total assets ratio\nDebt to equity and debt to NOP ratios\nFinancial ratios: RoE, RoA and PM\nNAIC section 2: Evaluating management\nNormalized data from consolidated statements\nMarket metrics\nOne dollar premise\nShare price vs EPS\nMarket capitalization\nQualitative metrics\nSimple and understandable business model\nFavorable long term prospects\nCommodity reliance\nConsistent operating history\nrationality:\n\nfocus on core aspects\nonly invest in high ROE businesses\nfocus on shareholder equity\n\n\n\nFinancial metrics \nThe following financial metrics are examined over the evaluation period. We are looking for favorable trends and evidence of consistent operations. Some red flags will also be evident in the plots.\nRed flags:\n- Shrinking gross profit margin - Receivables growing faster than sales - Rising debt-to-equity ratio - Several years of revenue trending down - Unsteady cash flow - Rising accounts receivable or inventory in relation to sales - Rising outstanding share count - Consistently higher liabilities than assets - Decreasing gross profit margin - Increasing revenue while cash flow remains the same - Unusual changes in key financial ratios\n\nTotal liabilities to total assets ratio\nThe ratio of liabilities to assets is plotted over the evaluation period. For most companies examined the liabilities are the total liabilities and the ratio is calculated using total assets and total tangible assets. Total tangible assets have goodwill and intangibles removed from the total. The ratio gives an indication of how much the company is worth versus how much the company owes. Ideally the ratio of liabilities to assets should be less than one.\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('ratio')\n\n# plot revenue as single bar\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['total_assets'], '-+',\n    label='total liabilities to total assets')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['total_tangible_assets'], '-*',\n    label='total liabilities to total tangible assets')\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,1))\nax1.legend(bbox_to_anchor=(1.8, 1))\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\n#ax2.plot(year_ended_list,pcd,'+-g')\nax2.plot(df_metrics_data['FY'],\n    (df_metrics_data['total_assets']-df_metrics_data['total_tangible_assets'])/df_metrics_data['total_assets']*100,\n    ':',color=color,label='intangible assets to total assets')\n    \nax2.set_ylabel('% intangible assets',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,100))\nax2.legend(bbox_to_anchor=(1.7, 0))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Total liabilities to total assets ratio')\nplt.show()\n\n\n\n\n\nFor the years 2020 and 2021, the ratio of total liabilities to total tangible assets has risen on account of borrowing during the pandemic.\nThe value assigned to goodwill and intangibles is about \\$?? billion.\nThe percentage of intangible assets to total assets is also plotted. The ratio indicates the company has taken on a lot of debt relative to assets and is something of concern.\nThe level of intangible assets are low.\n\n\nDebt to equity and debt to NOP ratios\nThe debt-to-equity ratio (D/E) is another key characteristic Buffett considers carefully. Buffett prefers to see a small amount of debt so that earnings growth is being generated from shareholders’ equity as opposed to borrowed money. The D/E ratio is calculated as follows:\n\\(\\text{Debt-to-Equity Ratio} = \\frac {\\text{Total Liabilities}} {\\text{Shareholders' Equity}} \\text{  OR  } \\frac {\\text{Long term debt}} {\\text{Shareholders' Equity}}\\)\nThis ratio shows the proportion of equity and debt the company uses to finance its assets, and the higher the ratio, the more debt—rather than equity—is financing the company. A high debt level compared to equity can result in volatile earnings and large interest expenses. For a more stringent test, investors sometimes use only long-term debt instead of total liabilities in the calculation above.\nD/E is the traditional way to look at a company’s debt. Some rules of thumb say that the D/E should not be above 2 or 3. However the D/E company’s typically vary by industry. The ratio of LT debt to NOP gives the number of years it would take the company to pay back debt from NOP, the lower the number the shorter amount of time.\n\\(\\text{Debt-to-NOP Ratio} = \\frac {\\text{Total Liabilities}} {\\text{NOP}}\\)\n\n\nCode\ntangible_equity = df_metrics_data['total_tangible_assets'] - df_metrics_data['total_liabilities']\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('ratio')\n\nax1.plot(df_metrics_data['FY'],df_dcf_data['long_term_debt']/df_metrics_data['shareholder_equity'],\n    '-^',label='(LT debt)/Equity')\n#ax1.plot(year_ended_list,df_dcf_data['long_term_debt']/tangible_equity, '-',label='(LT debt)/(Tangible Equity)')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['shareholder_equity'],\n    '-*',label='(total liabilities)/Equity')\n#ax1.plot(year_ended_list,total_liabilities/BV, '-^',label='(total liabilities)/BV')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/nop, '-+',label='(total liabilities)/NOP')\n#ax1.plot(year_ended_list,total_liabilities/net_income, '-+',label='(total liabilities)/(net income)')\n#ax1.plot(year_ended_list,df_dcf_data['current_liabilities']/nop, '-*',label='(current liabilities)/NOP')\n#ax1.plot(year_ended_list,Liabilities_wo_deposits/nop, '-+',label='(Liabilities w/o deposits)/NOP')\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,10))\n#ax1.legend()\nax1.legend(bbox_to_anchor=(1.6, 1))\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Various debt ratios')\nplt.show()\n\n\n\n\n\n(LT debt)/Equity is plotted and prior to 2019 was less than the threshold of 2.\nA threshold of 2 is traditionally the upper limit for a reasonable amount of debt that a company should carry.\n(total liabilities)/Equity is plotted and prior to 2019 was less than the threshold of 2.\n(total liabilities)/NOP to is plotted for each year in the evaluation period and prior to 2019 was less than the threshold of 10.\nA value of 10 has been chosen as the threshold for this ratio and indicates how many years it would take the company to pay off total liabilities from the NOP generated each year. A threshold of ten seems like a reasonable level of debt measured against NOP.\n\n\nFinancial ratios\nVarious ratios can be used to judge management performance. Consistent favorable trends are an indication that management is taking care of the company.\nReturn on equity\nSometimes return on equity (RoE) is referred to as stockholder’s return on investment. It reveals the rate at which shareholders earn income on their shares. Buffett always looks at RoE to see whether a company has consistently performed well compared to other companies in the same industry. RoE is calculated as follows:\n\\(\\text{Return on Equity} = \\frac {\\text{Net Income}} {\\text{Shareholder's Equity}}\\)\nLooking at the RoE in just the last year isn’t enough. The investor should view the RoE from the past five to 10 years to analyze historical performance.\n\\(\\text{Shareholders’ Equity} = \\text{Total Assets} − \\text{Total Liabilities}\\)\nFor this company, this method of getting Shareholders’ Equity gives negative values. On the Consolidated Balance Sheets, there is a line for Total stockholders’ equity, which is used.\nReturn on Assets\nReturn on assets is a profitability ratio that provides how much profit a company is able to generate from its assets. In other words, return on assets (RoA) measures how efficient a company’s management is in generating earnings from their economic resources or assets on their balance sheet.\n\\(\\text{Return on assets} = \\frac {\\text{Net Income}} {\\text{Total Assets}}\\)\nCalculating the RoA of a company can be helpful in comparing a company’s profitability over multiple quarters and years as well as comparing to similar companies. However, it’s important to compare companies of similar size and industry.\nFor example, banks tend to have a large number of total assets on their books in the form of loans, cash, and investments. A large bank could easily have over \\$2 trillion in assets while putting up a net income that’s similar to companies in other industries. Although the bank’s net income or profit might be similar to an unrelated company and the bank might have high-quality assets, the bank’s RoA will be lower. The larger number of total assets must be divided into the net income, creating a lower RoA for the bank.\nSimilarly, auto manufacturing requires huge facilities and specialized equipment. A lucrative software company that sells downloadable programs online may generate the same net profits, but it could have a significantly higher RoA than its more asset-heavy counterparts. When utilizing this metric to compare productivity across businesses, it’s important to take into account what types of assets are required to function in a given industry, rather than simply comparing the figures.\nProfit Margin\nA company’s profitability depends not only on having a good profit margin, but also on consistently increasing it. This margin is calculated by dividing net income by net sales. For a good indication of historical profit margins, investors should look back at least five years. A high-profit margin indicates the company is executing its business well, but increasing margins mean management has been extremely efficient and successful at controlling expenses.\n\\(\\text{Profit Margin} = \\frac {\\text{Net Income}} {\\text{Revenue}}\\)\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('percent')\n\nax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_metrics_data['shareholder_equity']*100,\n    '-+',label='RoE')\nax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_metrics_data['total_assets']*100,\n    '-*',label='RoA')\n#ax1.plot(df_metrics_data['FY'],total_liabilities/shareholder_equity, '-^',label='D/E')\nax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_dcf_data['revenue']*100,\n    '-^',label='Profit margin')\n\nax1.tick_params(axis='y')\nax1.set_ylim((-10,20))\n#ax1.legend()\nax1.legend(bbox_to_anchor=(1.05, 1))\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Various ratios')\nplt.show()\n\n\n\n\n\nObservation:\nThe trends for RoE, RoA and profit margin are shown above. The effect of the pandemic caused the ratios to turn negative and are no longer a meaningful metric.\n\n\n\nNAIC section 2: Evaluating management\nSee page 86, figure 9-1.\n- % pretax profit on sales, (net before taxes)/rev - % earned on equity (another way of saying RoE, using calculated equity)\nPercent earned on equity is a measure of financial performance calculated by dividing net income by equity. Because equity is equal to a company’s assets minus its debt, percent earned on equity is considered the return on net assets. Percent earned on equity is considered a gauge of a corporation’s profitability and how efficient it is in generating profits.\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('percent')\n#ax1.plot(year_ended_list,net_income, '-+',label='net income')\nax1.plot(df_metrics_data['FY'],df_dcf_data['income_before_income_taxes']/df_dcf_data['revenue']*100, '-+',\n         label='income before taxes/rev')\n#ax1.plot(year_ended_list,df_dcf_data['revenue'], '-+',label='revenue')\n#ax1.plot(year_ended_list,free_cash_flow, '-*',label='free cash flow')\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,20))\n#ax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('% pretax profit on sales')\nplt.show()\n\n\n\n\n\nOver the years 2016 to 2020, pretax profit on sales has a downward trend. Ideally this trend should be increasing or at least flat.\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('percent')\n\n#ax1.plot(year_ended_list,shareholder_equity/df_dcf_data['revenue']*100, '-+k',\n#        label='shareholder equity/rev')\n#ax1.plot(year_ended_list,net_income/shareholder_equity*100, '-+',label='RoE')\nax1.plot(df_metrics_data['FY'],\n    df_metrics_data['net_income']/(df_metrics_data['total_assets']-df_metrics_data['total_liabilities'])*100,\n    '-+',label='RoE')\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,20))\n#ax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('% earned on equity')\nplt.show()\n\n\n\n\n\nPercent earned on equity (another way of saying RoE). Percent earned on equity trend has been flat up to 2015, then became erratic.\nPercent earned on equity is a measure of financial performance calculated by dividing net income by equity. Because equity is equal to a company’s assets minus its debt, percent earned on equity is considered the return on net assets. Percent earned on equity is considered a gauge of a corporation’s profitability and how efficient it is in generating profits.\n\n\nPlot normalized data from consolidated statements\nThe following charts examine data from the consolidated financial statements and compare normalized trends over the evaluation period. The first chart plots normalized revenue along with normalized EPS, NOP and free cash flow. All values are normalized to the starting value in the series. Change relative to the normalized starting value can be seen over the evaluation period. Ideally the normalized parameters plotted should track revenue. Any large departures indicate an area of concern.\n\nNormalized consolidated statement of income\nThe following chart shows normalized revenue plotted with normalized parameters from the consolidated statement of income.\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\n# set look back range, left_yr is the index into the date range\nleft_yr = -10\n\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['revenue'][left_yr:]-df_dcf_data['revenue'].iloc[left_yr])/np.abs(df_dcf_data['revenue'].iloc[left_yr])*100,\n    '^-',label='Revenue')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['eps'][left_yr:]-df_metrics_data['eps'].iloc[left_yr])/np.abs(df_metrics_data['eps'].iloc[left_yr])*100,\n    '-.',label='EPS')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (nop[left_yr:]-nop[left_yr])/np.abs(nop[left_yr])*100,\n    '-.',label='NOP')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['free_cash_flow'][left_yr:]-df_metrics_data['free_cash_flow'].iloc[left_yr])/np.abs(df_metrics_data['free_cash_flow'].iloc[left_yr])*100,\n    '-.',label='Free cash flow')\n# net income\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['net_income'][left_yr:]-df_metrics_data['net_income'].iloc[left_yr])/np.abs(df_metrics_data['net_income'].iloc[left_yr])*100,\n    '-.',label='Net income')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n# Changes x-axis range\nplt.gca().set_xbound(year_ended_list[left_yr], year_ended_list[-1])\n\nplt.ylim((-100,500))\nplt.title('Normalized income statement data')\nplt.ylabel('percent change')\n#plt.legend()\nplt.legend(bbox_to_anchor=(1.6, 1))\n\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\n\n\nNormalized income statement 5 year look back\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\n# set look back range, left_yr is the index into the date range\nleft_yr = -6\n\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['revenue'][left_yr:]-df_dcf_data['revenue'].iloc[left_yr])/np.abs(df_dcf_data['revenue'].iloc[left_yr])*100,\n    '^-',label='Revenue')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['eps'][left_yr:]-df_metrics_data['eps'].iloc[left_yr])/np.abs(df_metrics_data['eps'].iloc[left_yr])*100,\n    '-.',label='EPS')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (nop[left_yr:]-nop[left_yr])/np.abs(nop[left_yr])*100,\n    '-.',label='NOP')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['free_cash_flow'][left_yr:]-df_metrics_data['free_cash_flow'].iloc[left_yr])/np.abs(df_metrics_data['free_cash_flow'].iloc[left_yr])*100,\n    '-.',label='Free cash flow')\n# net income\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['net_income'][left_yr:]-df_metrics_data['net_income'].iloc[left_yr])/np.abs(df_metrics_data['net_income'].iloc[left_yr])*100,\n    '-.',label='Net income')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n# Changes x-axis range\nplt.gca().set_xbound(year_ended_list[left_yr], year_ended_list[-1])\n\nplt.ylim((-100,100))\nplt.title('Normalized income statement data')\nplt.ylabel('percent change')\n#plt.legend()\nplt.legend(bbox_to_anchor=(1.6, 1))\n\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\nThe plot above shows a large departure of revenue in 2020.\n\n\nNormalized consolidated balance sheet\nThe following chart shows normalized revenue plotted with normalized parameters from the consolidated balance sheet.\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\n# set look back range, left_yr is the index into the date range\nleft_yr = -10\n\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['revenue'][left_yr:]-df_dcf_data['revenue'].iloc[left_yr])/np.abs(df_dcf_data['revenue'].iloc[left_yr])*100,\n    '^-',label='Revenue')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['total_liabilities'][left_yr:]-df_metrics_data['total_liabilities'].iloc[left_yr])/np.abs(df_metrics_data['total_liabilities'].iloc[left_yr])*100,\n    '-.',label='Total liabilities')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['total_assets'][left_yr:]-df_metrics_data['total_assets'].iloc[left_yr])/np.abs(df_metrics_data['total_assets'].iloc[left_yr])*100,\n    '-.',label='Total assets')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['total_tangible_assets'][left_yr:]-df_metrics_data['total_tangible_assets'].iloc[left_yr])/np.abs(df_metrics_data['total_tangible_assets'].iloc[left_yr])*100,\n    '-.',label='Total tangible assets')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['long_term_debt'][left_yr:]-df_dcf_data['long_term_debt'].iloc[left_yr])/np.abs(df_dcf_data['long_term_debt'].iloc[left_yr])*100,\n    '-.',label='Long term debt')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['current_liabilities'][left_yr:]-df_dcf_data['current_liabilities'].iloc[left_yr])/np.abs(df_dcf_data['current_liabilities'].iloc[left_yr])*100,\n    '-.',label='Current liabilities')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['depreciation'][left_yr:]-df_dcf_data['depreciation'].iloc[left_yr])/np.abs(df_dcf_data['depreciation'].iloc[left_yr])*100,\n    '-.',label='Depreciation & amortization')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n# Changes x-axis range\nplt.gca().set_xbound(year_ended_list[left_yr], year_ended_list[-1])\n\n#plt.ylim((0,4))\nplt.title('Normalized balance statement data')\nplt.ylabel('percent change')\n#plt.legend()\nplt.legend(bbox_to_anchor=(1.6, 1))\n\nplt.grid()\n\n# show plot\nplt.show()\n\n\n\n\n\nObservation\nBalance sheet items generally followed revenue up to 2020, then as revenue tanked, balance sheet items remained generally on the same trajectory.\n\n\nNormalized balance statement 5 year look back\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\n# set look back range, left_yr is the index into the date range\nleft_yr = -6\n\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['revenue'][left_yr:]-df_dcf_data['revenue'].iloc[left_yr])/np.abs(df_dcf_data['revenue'].iloc[left_yr])*100,\n    '^-',label='Revenue')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['total_liabilities'][left_yr:]-df_metrics_data['total_liabilities'].iloc[left_yr])/np.abs(df_metrics_data['total_liabilities'].iloc[left_yr])*100,\n    '-.',label='Total liabilities')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['total_assets'][left_yr:]-df_metrics_data['total_assets'].iloc[left_yr])/np.abs(df_metrics_data['total_assets'].iloc[left_yr])*100,\n    '-.',label='Total assets')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['total_tangible_assets'][left_yr:]-df_metrics_data['total_tangible_assets'].iloc[left_yr])/np.abs(df_metrics_data['total_tangible_assets'].iloc[left_yr])*100,\n    '-.',label='Total tangible assets')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['long_term_debt'][left_yr:]-df_dcf_data['long_term_debt'].iloc[left_yr])/np.abs(df_dcf_data['long_term_debt'].iloc[left_yr])*100,\n    '-.',label='Long term debt')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['current_liabilities'][left_yr:]-df_dcf_data['current_liabilities'].iloc[left_yr])/np.abs(df_dcf_data['current_liabilities'].iloc[left_yr])*100,\n    '-.',label='Current liabilities')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['depreciation'][left_yr:]-df_dcf_data['depreciation'].iloc[left_yr])/np.abs(df_dcf_data['depreciation'].iloc[left_yr])*100,\n    '-.',label='Depreciation & amortization')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n# Changes x-axis range\nplt.gca().set_xbound(year_ended_list[left_yr], year_ended_list[-1])\n\n#plt.ylim((0,4))\nplt.title('Normalized balance statement data')\nplt.ylabel('Percent change')\n#plt.legend()\nplt.legend(bbox_to_anchor=(1.6, 1))\n\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\n\n\n\nMarket metrics \nThe share price is determined by the market. The value is determined by the analyst.\n\nOne dollar premise\nThis is a financial test that shows the strength of the business and how well management has rationality allocated to the company’s business.\nFrom a company’s income, subtract all dividends paid to shareholders. What is left over is the company’s retained earnings. Now add the company’s retained earnings over a 10 year period. Next determine the difference between the company’s current market value and its market value 10 years ago. If the business has employed retained earnings unproductively over this ten year period, the market eventually catches up and will set a lower price on the business.\nOnly use pre covid data.\n\n\nCode\nretained_earnings = df_metrics_data['net_income'][-6:-2].sum() - df_metrics_data['dividends'][-6:-2].sum()\nprint('retained earnings: ${:,.2f}B'.format(retained_earnings/1e9))\n\n\nretained earnings: $-1.37B\n\n\n\n\nCode\n# Current market value, share price multiplied by number of shares\ncmv_high = df_dcf_data['shares_outstanding'].iloc[-3]*fy_high[-3]\ncmv_low = df_dcf_data['shares_outstanding'].iloc[-3]*fy_low[-3]\nprint('Current market value: ${:,.2f}B to ${:,.2f}B'.format(cmv_low/1e9,cmv_high/1e9))\n\n\nCurrent market value: $1.98B to $15.19B\n\n\n\n\nCode\n# Past market value, share price multiplied by number of shares\npmv_high = df_dcf_data['shares_outstanding'].iloc[1]*fy_high[1]\npmv_low = df_dcf_data['shares_outstanding'].iloc[1]*fy_low[1]\nprint('Past market value: ${:,.0f}B to ${:,.0f}B'.format(pmv_low/1e9,pmv_high/1e9))\n\n\nPast market value: $5B to $7B\n\n\n\n\nCode\nprint('Difference in market value: ${:,.0f}B to ${:,.0f}B'.format((cmv_low-pmv_low)/1e9,(cmv_high-pmv_high)/1e9))\n\n\nDifference in market value: $-3B to $8B\n\n\nThis difference in market value is greater than the retained earnings.\n\n\nShare price vs EPS\nLooking at the one dollar premise in terms of share price and EPS.\nThe one dollar premise: one dollar of earning should translate into one dollar of market value - this seems the same as a plot of EPS versus share price.\n\n\nCode\n# plotting the eps data points\nax = plt.bar(df_metrics_data['eps'][1:-2],fy_high[1:-2]-fy_low[1:-2],width = .05,bottom=fy_low[1:-2])\nplt.grid()\nplt.ylim((20,80))\nplt.ylabel('high and low share price range')\nplt.xlabel('EPS')\nplt.title('years from 2010 to 2021')\n\nrects = ax.patches\n\n# Make some labels.\nlabels = [year_ended_list[i].strftime(\"%Y\") for i in range(len(year_ended_list))]\nfor rect, label in zip(rects, labels):\n    y_top =  rect.get_y() + rect.get_height()\n    plt.text(rect.get_x(), y_top+1, label, rotation=90,va='bottom')    \n\nplt.show()\n\n\n\n\n\nObservations:\nThe range in share price is roughly the same across the range of EPS. This means that investors are not valuing the company’s EPS.\n\n\n\nQualitative metrics \nBeyond the numbers in the financial statements, there are metrics that are qualitative in nature that are important to the investor. These are subjective measures of business and management operations that influence value. In this section a few qualitative metrics are discussed below.\n\nSimple and understandable business model\nThe business is to provide a vacation at sea.\nFavorable long term prospects\nPoor, with the threat of inflation, higher fuel prices, higher interest rates and low ticket demand.\nCommodity reliance\nNCL provides a service which is easily reproducible by a competitor.\nConsistent operating history\nSeemed consistent prior to the pandemic, but unknown going forward.\nRationality\n\n\nFocus on core aspects\nYes, cruising is the only business NCL operates.\nOnly invest in high ROE businesses\nNA\nFocus on shareholder equity\nSurvival of the company during and after the pandemic has been the focus.\n\n\nMarketing Strategy\nNCLH significantly reduced their marketing activities in 2020 due to the suspension of cruise voyages as a result of the COVID-19 pandemic. Sales and marketing activities have increased in an attempt to further drive demand. Additionally, they continue a deliberate approach on marketing and sales outreach to guests with future cruise credits, as a result of suspended sailings, to encourage redemption of cruise credits towards future sailings. Building customer loyalty among past guests is an important element of marketing strategy. Past guests create a cost-effective means of attracting business, particularly to new ships and itineraries as they are familiar with the brands, products and services and often return to cruise with NCLH.\nFleet Expansion\nFor the Norwegian brand, Project Leonardo will introduce six additional ships, each ranging from approximately 140,000 to 156,300 Gross Tons with approximately 3,300 to 3,550 Berths, with expected delivery dates from 2022 through 2027. For the Oceania Cruises brand, NCLH has orders for two Allura Class Ships to be delivered in 2023 and 2025. Each of the Allura Class Ships will be approximately 67,000 Gross Tons and 1,200 Berths. For the Regent brand, NCLH has one Explorer Class Ship on order to be delivered in 2023, which will be approximately 55,000 Gross Tons and 750 Berths.\n\n\n\n\n\n\n\n\nNorwegian\nOceania\nRegent\n\n\n\n\nPrima Class: 6 ships 2022 to 2027\nAllura Class: 2 ships 2023 and 2025\nExplorer Class: 1 ship 2023\n\n\nNorwegian Encore\nOceania Riviera\nSeven Seas Splendor\n\n\nNorwegian Bliss\nOceania Marina\nSeven Seas Explorer\n\n\nNorwegian Joy\nOceania Nautica\nSeven Seas Voyager\n\n\nNorwegian Escape\nOceania Sirena\nSeven Seas Mariner\n\n\nNorwegian Getaway\nOceania Regatta\nSeven Seas Navigator\n\n\nNorwegian Breakaway\nOceania Insignia\n\n\n\nNorwegian Epic\n\n\n\n\nNorwegian Gem\n\n\n\n\nNorwegian Jade\n\n\n\n\nNorwegian Pearl\n\n\n\n\nNorwegian Jewel\n\n\n\n\nPride of America\n\n\n\n\nNorwegian Dawn\n\n\n\n\nNorwegian Star\n\n\n\n\nNorwegian Sun\n\n\n\n\nNorwegian Sky\n\n\n\n\nNorwegian Spirit\n\n\n\n\n\nShips represent the most significant assets, and NCLH records them at cost less accumulated depreciation. Depreciation of ships is computed on a straight-line basis over the weighted average useful lives of primarily 30 years after a 15% reduction for the estimated residual value of the ship. Ship improvement costs that NCLH believes add value to our ships are capitalized to the ship and depreciated over the shorter of the improvements’ estimated useful lives or the remaining useful life of the ship. When they record the retirement of a ship component included within the ship’s cost basis, they estimate the net book value of the component being retired and remove it from the ship’s cost basis. Repairs and maintenance activities are charged to expense as incurred. NCLH accounts for Dry-dock costs under the direct expense method which requires us to expense all Dry-dock costs as incurred.\nNCLH determines the weighted average useful lives of our ships based primarily on our estimates of the useful lives of the ships’ major component systems on the date of acquisition, such as cabins, main diesels, main electric, superstructure and hull. The useful lives of ship improvements are estimated based on the economic lives of the new components. In addition, to determine the useful lives of the ship or ship components, thay consider the impact of the historical useful lives of similar assets, manufacturer recommended lives and anticipated changes in technological conditions. Given the large and complex nature of our ships, their accounting estimates related to ships and determinations of ship improvement costs to be capitalized require judgment and are uncertain. Should certain factors or circumstances cause them to revise their estimate of ship service lives or projected residual values, depreciation expense could be materially lower or higher. In 2020, one ship had significant improvements that extended the remaining weighted average useful life of the vessel. Accordingly, They have updated our estimate of both its useful life and residual value based on the new weighted average useful life of its current components. The impact of the change in estimate is accounted for on a prospective basis and is not material.\nIf circumstances cause NCLH to change their assumptions in making determinations as to whether ship improvements should be capitalized, the amounts they expense each year as repairs and maintenance costs could increase, partially offset by a decrease in depreciation expense. If they reduced their estimated weighted average 30-year ship service life by one year, depreciation expense for the year ended December 31, 2020 would have increased by \\$19.8 million. In addition, if their ships were estimated to have no residual value, depreciation expense for the same period would have increased by \\$99.6 million. We believe their estimates for ship accounting are reasonable and their methods are consistently applied. They believe that depreciation expense is based on a rational and systematic method to allocate their ships’ costs to the periods that benefit from the ships’ usage.\nBack to Contents"
  },
  {
    "objectID": "NCLHv1 analysis.html#decision-model",
    "href": "NCLHv1 analysis.html#decision-model",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "9) Decision model ",
    "text": "9) Decision model \nThe decision model establishes thresholds that are to be used in the purchase decision. There are three hard decision thresholds in this model which are:\n1. Intrinsic value 2. Debt 3. Dividend payout ratio 4. Dividend IIR\nThe first threshold is based on the intrinsic value of the company as calculated by the DCF model scenario 1. Recognizing that absolute intrinsic value is an elusive concept, judgment, justified by facts (assets, earnings, dividends, debt and cash flow), establishes the value by adjusting various rates, based on judgment and using a five year forward projection period. This should give an intrinsic value that is based on the historical data, modified by judgment.\nI’m using a threshold of the intrinsic value calculated in scenario 1 (isv_S1) that is greater than 70% of the current stock price, provided that the NAIC valuation is above the current stock price. This accounts for the inadequacy or incorrectness of the data, the uncertainties of the future, and considers the behavior of the market.\nThe second threshold is the level of debt. The ratios of (LT debt)/Equity, (total liabilities)/Equity and (total liabilities)/NOP are plotted for the evaluation period. Over the evaluation period the (LT debt)/Equity and (total liabilities)/Equity should be less than 2 and stable. A threshold of 2 has been discussed in the literature as a level of debt that a company can reasonably take on.\nThe threshold for (total liabilities)/NOP is set at 10. This means that the company can pay off all the liabilities with ten years worth of NOP, which seems like a reasonable time frame for an established and stable company.\nThe third threshold is the dividend payout ratio and is a relative measure of how much the company is paying to shareholders in dividends compared to the metrics of NOP and free cash flow (Net cash provided by operating activities). The payout ratio is useful for assessing a dividend’s sustainability. Payout ratio for a REIT is established by tax law and not used as an evaluation criteria. For other industries a threshold of 50% has been set as the limit.\nThe dividend IRR threshold is the internal rate of return for investor dividend cash flow (divident_irr) should be greater than 10 year treasury bond yield (tby) plus the equity risk premium (eq_rp). Otherwise, other investment opportunities should be looked at.\nIn the decision model there are soft thresholds based on judgment. Soft thresholds are a collection of ratios and analysis that taken together tell a story of the performance of the company and management’s ability to run the company and support dividends over the long term. Use judgment and make an evaluation.\nThe third criteria is a collection of ratios and analysis that taken together tell a story of the performance of the company and management’s ability to run the company and support dividends over the long term. Use judgment and make an evaluation. These are the following:\n1. Financial metrics 2. Market metrics 3. Qualitative metrics\nThe soft thresholds are discussed in section 10. These metrics only look at data prior to 2019.\n\nCheck DCF and NAIC value thresholds\n\n\nCode\n# check DCF scenario 1\ndcf_score = isv_S1/csp #ratio of isv to csp\ndcf_threshold = 0.7\nif dcf_score &lt; 0.7:\n    print('FAIL, DCF score is less than {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\nelse:\n    print('PASS, DCF score is above {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\n\n# check NAIC\nnaic_score = np.array([naic_price_eps_low,naic_price_pm_low]).min()/csp\nnaic_threshold = 1\nif naic_score &lt; 1:\n    print('FAIL, NAIC score is less than {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\nelse:\n    print('PASS, NAIC score is above {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\n\n# check both scores\nif naic_score &lt; 1 or dcf_score &lt; 0.7:\n    print('One or both DCF and NAIC scores failed')\nelse:\n    print('Both DCF and NAIC scores pass')\n\n\nFAIL, DCF score is less than 0.7 at -1.3\nFAIL, NAIC score is less than 1.0 at -6.0\nOne or both DCF and NAIC scores failed\n\n\n\n\nCheck debt thresholds\n\n\nCode\ndebt_lookback = 6\navg_LT_debt2EQ = df_dcf_data['long_term_debt'][-debt_lookback:-2].mean()/df_metrics_data['shareholder_equity'][-debt_lookback:-2].mean()\navg_TLiability2EQ = df_metrics_data['total_liabilities'][-debt_lookback:-2].mean()/df_metrics_data['shareholder_equity'][-debt_lookback:-2].mean()\navg_TLiability2NOP = df_metrics_data['total_liabilities'][-debt_lookback:-2].mean()/nop[-debt_lookback:-2].mean()\n\nprint('long term debt to shareholder equity ratio = {:.2f}'.format(avg_LT_debt2EQ))\nprint('total liabilities to shareholder equity ratio = {:.2f}'.format(avg_TLiability2EQ))\nprint('total liabilities to NOP ratio = {:.2f}'.format(avg_TLiability2NOP))\n\nif (avg_LT_debt2EQ &gt; 2) or (avg_TLiability2EQ &gt; 2) or (avg_TLiability2NOP &gt; 10):\n    print('FAILED one of the debt threshold limits')\nelse:\n    print('passed debt threshold limits')\n\n\nlong term debt to shareholder equity ratio = 1.36\ntotal liabilities to shareholder equity ratio = 1.85\ntotal liabilities to NOP ratio = 10.44\nFAILED one of the debt threshold limits\n\n\n\n\nCheck dividend payout and IIR thresholds\n\n\nCode\n# check dividend payout ratio average the last three years\nprint('Dividends are paid at {:.1f}% of cash flow'.format(\n    (df_metrics_data['dividends']/df_metrics_data['free_cash_flow'])[-3:].mean()*100))\nprint('Dividends are paid at {:.1f}% of NOP'.format((df_metrics_data['dividends']/nop)[-3:].mean()*100))\n\nif ((df_metrics_data['dividends']/nop)[-3:].mean() or (df_metrics_data['dividends']/df_metrics_data['free_cash_flow'])[-3:].mean()) &gt; 0.5:\n    print('FAIL, dividend payout ratio too high')\n\n\nDividends are paid at 0.0% of cash flow\nDividends are paid at 0.0% of NOP\n\n\n\n\nCode\n# Check dividend IRR limit\nif dividend_irr &lt; (tby+eq_rp):\n    print('FAIL, dividend IRR is less than {:.2f} at {:.2f}'.format((tby+eq_rp)*100,dividend_irr*100))\nelse:\n    print('PASS, dividend IRR is above {:.2f} at {:.2f}'.format((tby+eq_rp)*100,dividend_irr*100))\n\n\nFAIL, dividend IRR is less than 6.04 at -0.29"
  },
  {
    "objectID": "NCLHv1 analysis.html#recent-quarterly-performance",
    "href": "NCLHv1 analysis.html#recent-quarterly-performance",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Recent quarterly performance ",
    "text": "Recent quarterly performance \nQuarterly financial reports were obtained from the company’s investor relations web page. These reports are available in pdf, spreadsheet or XBRL format. Data from the NCL’s spreadsheets was copied and organized into a new spreadsheet to be used in this notebook.\nThe python code below reads the new excel spreadsheet and converts the data into a pandas dataframe. Dollar amounts are converted from thousands of dollars to straight dollars. Dates are converted from a character string to datetime format.\nThe code below reads the data from the excel sheet which has already been converted into a pandas dataframe and puts the data into temporary variables.\n\n\nCode\n#column names: dates for ending of quarter \nqrtr_dates = df_qrt_sheet.columns[1:].values.astype('datetime64[D]')\n\n# load spreadsheet row data from sheet into temp variables\nrevenue_qrtr_data = df_qrt_sheet.iloc[0].to_numpy()[1:].astype('float')\ncruise_operating_expense_qrtr_data = df_qrt_sheet.iloc[1].to_numpy()[1:].astype('float')\nmarketing_general_and_administrative_qrtr_data = df_qrt_sheet.iloc[2].to_numpy()[1:].astype('float')\ndepreciation_qrtr_data = df_qrt_sheet.iloc[3].to_numpy()[1:].astype('float')\nother_operating_expense_qrtr_data = df_qrt_sheet.iloc[4].to_numpy()[1:].astype('float')\noperating_income_qrtr_data = df_qrt_sheet.iloc[5].to_numpy()[1:].astype('float')\ncash_and_cash_equivalents_qrtr_data = df_qrt_sheet.iloc[6].to_numpy()[1:].astype('float')\naccounts_receivable_qrtr_data = df_qrt_sheet.iloc[7].to_numpy()[1:].astype('float')\ninventories_qrtr_data = df_qrt_sheet.iloc[8].to_numpy()[1:].astype('float')\nprepaid_expenses_and_other_assets_qrtr_data = df_qrt_sheet.iloc[9].to_numpy()[1:].astype('float')\ntotal_current_assets_qrtr_data = df_qrt_sheet.iloc[10].to_numpy()[1:].astype('float')\nproperty_and_equipment_qrtr_data = df_qrt_sheet.iloc[11].to_numpy()[1:].astype('float')\ngoodwill_qrtr_data = df_qrt_sheet.iloc[12].to_numpy()[1:].astype('float')\ntrade_names_qrtr_data = df_qrt_sheet.iloc[13].to_numpy()[1:].astype('float')\nother_long_term_assets_qrtr_data = df_qrt_sheet.iloc[14].to_numpy()[1:].astype('float')\ntotal_assets_qrtr_data = df_qrt_sheet.iloc[15].to_numpy()[1:].astype('float')\ncurrent_portion_of_long_term_debt_qrtr_data = df_qrt_sheet.iloc[16].to_numpy()[1:].astype('float')\naccounts_payable_qrtr_data = df_qrt_sheet.iloc[17].to_numpy()[1:].astype('float')\naccrued_expenses_and_other_liabilities_qrtr_data = df_qrt_sheet.iloc[18].to_numpy()[1:].astype('float')\nadvance_ticket_sales_qrtr_data = df_qrt_sheet.iloc[19].to_numpy()[1:].astype('float')\ntotal_current_liabilities_qrtr_data = df_qrt_sheet.iloc[20].to_numpy()[1:].astype('float')\nlong_term_debt_qrtr_data = df_qrt_sheet.iloc[21].to_numpy()[1:].astype('float')\nother_long_term_liabilities_qrtr_data = df_qrt_sheet.iloc[22].to_numpy()[1:].astype('float')\ntotal_liabilities_qrtr_data = df_qrt_sheet.iloc[23].to_numpy()[1:].astype('float')\nnet_loss_qrtr_data = df_qrt_sheet.iloc[24].to_numpy()[1:].astype('float')\nadditions_to_property_and_equipment_qrtr_data = df_qrt_sheet.iloc[25].to_numpy()[1:].astype('float')\nnet_cash_provided_by_investing_activities_qrtr_data = df_qrt_sheet.iloc[26].to_numpy()[1:].astype('float')\nrepayments_of_long_term_debt_qrtr_data = df_qrt_sheet.iloc[27].to_numpy()[1:].astype('float')\nproceeds_from_long_term_debt_qrtr_data = df_qrt_sheet.iloc[28].to_numpy()[1:].astype('float')\ncommon_share_issuance_proceeds_qrtr_data = df_qrt_sheet.iloc[29].to_numpy()[1:].astype('float')\nearly_redemption_premium_qrtr_data = df_qrt_sheet.iloc[30].to_numpy()[1:].astype('float')\ndeferred_financing_fees_qrtr_data = df_qrt_sheet.iloc[31].to_numpy()[1:].astype('float')\nnet_cash_provided_by_financing_activities_qrtr_data = df_qrt_sheet.iloc[32].to_numpy()[1:].astype('float')\nnet_increase_in_cash_and_cash_equivalents_qrtr_data = df_qrt_sheet.iloc[33].to_numpy()[1:].astype('float')\ncash_and_cash_equivalents_at_beginning_of_period_qrtr_data = df_qrt_sheet.iloc[34].to_numpy()[1:].astype('float')\ncash_and_cash_equivalents_at_end_of_period_qrtr_data = df_qrt_sheet.iloc[35].to_numpy()[1:].astype('float')\nweighted_average_shares_outstanding_basic_qrtr_data = df_qrt_sheet.iloc[36].to_numpy()[1:].astype('float')\nfuture_cruise_credits_qrtr_data = df_qrt_sheet.iloc[37].to_numpy()[1:].astype('float')\n\n\nThe code below takes the temporary variables, converts thousands of dollars to actual dollars, by multiplying by 1000, reverses the order of the data and stores the data back into a new dataframe.\n\n\nCode\n# make a new data frame to store selected quarterly data\ndf_qrtr_data = pd.DataFrame(data={\n    'QTR':qrtr_dates[::-1],\n    'revenue':revenue_qrtr_data[::-1]*1000,\n    'cruise_operating_expense':cruise_operating_expense_qrtr_data[::-1]*1000,\n    'marketing_general_and_administrative':marketing_general_and_administrative_qrtr_data[::-1]*1000,\n    'depreciation':depreciation_qrtr_data[::-1]*1000,\n    'other_operating_expense':other_operating_expense_qrtr_data[::-1]*1000,\n    'operating_income':operating_income_qrtr_data[::-1]*1000,\n    'cash_and_cash_equivalents':cash_and_cash_equivalents_qrtr_data[::-1]*1000,\n    'accounts_receivable':accounts_receivable_qrtr_data[::-1]*1000,\n    'inventories':inventories_qrtr_data[::-1]*1000,\n    'prepaid_expenses_and_other_assets':prepaid_expenses_and_other_assets_qrtr_data[::-1]*1000,\n    'total_current_assets':total_current_assets_qrtr_data[::-1]*1000,\n    'property_and_equipment':property_and_equipment_qrtr_data[::-1]*1000,\n    'goodwill':goodwill_qrtr_data[::-1]*1000,\n    'trade_names':trade_names_qrtr_data[::-1]*1000,\n    'other_long_term_assets':other_long_term_assets_qrtr_data[::-1]*1000,\n    'total_assets':total_assets_qrtr_data[::-1]*1000,\n    'current_portion_of_long_term_debt':current_portion_of_long_term_debt_qrtr_data[::-1]*1000,\n    'accounts_payable':accounts_payable_qrtr_data[::-1]*1000,\n    'accrued_expenses_and_other_liabilities':accrued_expenses_and_other_liabilities_qrtr_data[::-1]*1000,\n    'advance_ticket_sales':advance_ticket_sales_qrtr_data[::-1]*1000,\n    'total_current_liabilities':total_current_liabilities_qrtr_data[::-1]*1000,\n    'long_term_debt':long_term_debt_qrtr_data[::-1]*1000,\n    'other_long_term_liabilities':other_long_term_liabilities_qrtr_data[::-1]*1000,\n    'total_liabilities':total_liabilities_qrtr_data[::-1]*1000,\n    'net_loss':net_loss_qrtr_data[::-1]*1000,\n    'additions_to_property_and_equipment':additions_to_property_and_equipment_qrtr_data[::-1]*1000,\n    'net_cash_provided_by_investing_activities':net_cash_provided_by_investing_activities_qrtr_data[::-1]*1000,\n    'repayments_of_long_term_debt':repayments_of_long_term_debt_qrtr_data[::-1]*1000,\n    'proceeds_from_long_term_debt':proceeds_from_long_term_debt_qrtr_data[::-1]*1000,\n    'common_share_issuance_proceeds':common_share_issuance_proceeds_qrtr_data[::-1]*1000,\n    'early_redemption_premium':early_redemption_premium_qrtr_data[::-1]*1000,\n    'deferred_financing_fees':deferred_financing_fees_qrtr_data[::-1]*1000,\n    'net_cash_provided_by_financing_activities':net_cash_provided_by_financing_activities_qrtr_data[::-1]*1000,\n    'net_increase_in_cash_and_cash_equivalents':net_increase_in_cash_and_cash_equivalents_qrtr_data[::-1]*1000,\n    'cash_and_cash_equivalents_at_beginning_of_period':cash_and_cash_equivalents_at_beginning_of_period_qrtr_data[::-1]*1000,\n    'cash_and_cash_equivalents_at_end_of_period':cash_and_cash_equivalents_at_end_of_period_qrtr_data[::-1]*1000,\n    'weighted_average_shares_outstanding_basic':weighted_average_shares_outstanding_basic_qrtr_data[::-1],\n    'future_cruise_credits':future_cruise_credits_qrtr_data[::-1]\n    })\n\n\n\nRevisions to Previously Reported Quarterly Financial Statements, page 10 of Sept 2022 10Q\nDuring the fourth quarter of 2021, the Company identified an error in its consolidated balance sheet as of September 30, 2021 and consolidated statement of cash flows for the nine months ended September 30, 2021. Based on their nature, certain amounts shown as cash and cash equivalents should have been classified as short-term investments. We have determined that these errors were not material to the previously issued interim financial statements for the period ended September 30, 2021.\nAs a result of the error, the amounts previously reported as cash and cash equivalents have been reclassified to cash flows used in investing activities in the consolidated statement of cash flows for the nine months ended September 30, 2021 as follows (in thousands):\n\n\nNews for Nov 2022\nNCL to Pay Travel Agent Partners Commission on Non-Commissionable Fares\nNorwegian Cruise Line To Pay Travel Advisors Commission on Non-Commissionable Fares\nNCL To Pay Travel Agents Commission on NCFs"
  },
  {
    "objectID": "NCLHv1 analysis.html#income",
    "href": "NCLHv1 analysis.html#income",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Income",
    "text": "Income\nThe plot below shows the recent quarterly income plotted from the first quarter of 2019 to the second quarter of 2022. Revenue, operating income, advance ticket sales and cost of operations are plotted. Markers are included on the zero dollar line that indicate the dates for operations suspended, first ship to return to service and full fleet in operation. The dates for operations suspended (March 13, 2020), 1st ship return to service (July 25, 2021) and full fleet operating (May 7, 2022) are indicated on the plot with markers.\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $B')\n\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['revenue']/1e9,'-+b',label='revenue')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['operating_income']/1e9,'-+r',label='operating income')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['advance_ticket_sales']/1e9,'-+g',label='advance ticket sales')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['cruise_operating_expense']/1e9,'-+c',label='cruise operating expense')\n\nax1.plot(pd.Timestamp(datetime(2020,3,13)),0,'^k',label='operations suspended, March 13') # March 13, 2020, operations suspended\nax1.plot(pd.Timestamp(datetime(2021,7,25)),0,'ob',label='1st ship return to service, July 25') # July 25, 2021, 1st ship return to service\nax1.plot(pd.Timestamp(datetime(2022,5,7)),0,'&gt;k',label='full fleet operating, May 7') # May 7 2022, full fleet operating\n\nax1.tick_params(axis='y')\nax1.set_ylim((-2,3))\nplt.legend(bbox_to_anchor=(1.7, 1))\n#ax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Recent quarterly income items')\nplt.show()\n\n\n\n\n\nRevenue prior to the cruise industry shutdown was between one and two billion dollars per quarter (depending on the season), with revenue in the third quarter of 2019 at about 2 billion dollars. Advance ticket sales were running just under about two billion per quarter. Since the restart of cruise, advanced ticket sales (which include future cruise credits) have returned to pre-pandemic levels. Cruise line operating expenses were about one billion dollars per quarter and have returned to pre-pandemic levels as the full fleet is back online. During the current quarter revenue has exceeded operating expenses, a positive indicator. Operating income has increased during the current quarter.\nIn the plot below, quarterly data for revenue, cruise operating expense, marketing general and administrative and net operating profit (NOP) are shown. The dates for operations suspended (March 13, 2020), 1st ship return to service (July 25, 2021) and full fleet operating (May 7 2022) are indicated on the plot with markers.\n\n\nCode\nnop_qrtr = (df_qrtr_data['revenue'].to_numpy() - \\\n    (df_qrtr_data['cruise_operating_expense'].to_numpy() + \\\n    df_qrtr_data['marketing_general_and_administrative'].to_numpy()  ))\n\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $B')\n\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['revenue']/1e9,'-+b',label='revenue')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['cruise_operating_expense']/1e9,'-+c',label='cruise operating expense')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['marketing_general_and_administrative']/1e9,'-+m',label='marketing general & administrative')\nax1.plot(df_qrtr_data['QTR'],nop_qrtr/1e9,'-+r',label='NOP')\n#ax1.plot(df_qrtr_data['QTR'],df_qrtr_data['advance_ticket_sales']/1e9,'-+g',label='advance ticket sales')\n\nax1.plot(pd.Timestamp(datetime(2020,3,13)),0,'^k',label='operations suspended, March 13') # March 13, 2020, operations suspended\nax1.plot(pd.Timestamp(datetime(2021,7,25)),0,'ob',label='1st ship return to service, July 25') # July 25, 2021, 1st ship return to service\nax1.plot(pd.Timestamp(datetime(2022,5,7)),0,'&gt;k',label='full fleet operating, May 7') # May 7 2022, full fleet operating\n\nax1.tick_params(axis='y')\nax1.set_ylim((-1,2))\nplt.legend(bbox_to_anchor=(1.1, 1))\n#ax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('NOP and supporting data')\nplt.show()\n\n\n\n\n\nNet operating profit (NOP) reflects revenue levels and expense requirements of the operating business.\n\\(\\text{NOPM} = \\frac{\\text{Revenue} - \\text{Expenses}}{\\text{Revenue}}\\)\n\\(\\text{Expenses} = \\text{Cost of Goods Sold (CGS)} + \\text{General and Administrative (G&A)} + \\text{Research and Development (R&D)}\\)\nGeneral and Administrative (G&A) is also called Sales, General and Administrative (SG&A)\nNOP is used to analyze the performance of a company’s core operations without the costs of the capital structure and tax expenses impacting profit. It is a more concise measure of corporate performance since it is able to show earnings before the influence of accounting and financial deductions, also used above in the DCF model. NOP as defined here is roughly equivalent to EBIT (earnings before interest and taxes). What is concerning is that NOP has remained flat at a level of 0.7 billion dollars per quarter for the last four quarters and has not trended up with increasing revenue."
  },
  {
    "objectID": "NCLHv1 analysis.html#statement-of-income",
    "href": "NCLHv1 analysis.html#statement-of-income",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Statement of income",
    "text": "Statement of income\n\nPercentage change in quarterly statement of income\nThe following chart shows percentage change in quarterly consolidated statement of income, looking back over the last ten quarters or so. This can be used to investigate recient trends.\n\\(\\large{\\color{red}{\\text{update this section as analysis of balance sheet}}}\\)\npercent change along with values\nfix colors, make the same b,g,r,c,m,y,k\n\n\nCode\n# Set the locator\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\n# figsize() function to adjust the size\nplt.subplots(1,2,figsize=(15, 5))\n\n# set look back range, left_qtr is the index into the date range\nleft_qtr = -6\n\n# using subplot function and creating two side by side plots\n# plot one\nplt.subplot(1, 2, 1)\n\nplt.plot(df_qrtr_data['QTR'][left_qtr:],df_qrtr_data['revenue'][left_qtr:]/1e9,'-+b',label='revenue')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],df_qrtr_data['cruise_operating_expense'][left_qtr:]/1e9,'-+g',label='cruise operating expense')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],df_qrtr_data['marketing_general_and_administrative'][left_qtr:]/1e9,'-+r',label='marketing general & administrative')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],df_qrtr_data['other_operating_expense'][left_qtr:]/1e9,'-+c',label='other_operating_expense')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],df_qrtr_data['operating_income'][left_qtr:]/1e9,'-+m',label='operating income')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\n# Changes x-axis range\n#plt.gca().set_xbound(year_ended_list[left_qtr], year_ended_list[-1])\n#plt.gca().set_xbound(df_qrtr_data['QTR'].iloc[left_qtr], df_qrtr_data['QTR'].iloc[-1])\n\n#plt.ylim((-100,500))\nplt.title('Recent quarterly income items')\nax1.set_ylabel('dollars, $B')\nplt.legend()\n\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['revenue'][left_qtr:]-df_qrtr_data['revenue'].iloc[left_qtr])/np.abs(df_qrtr_data['revenue'].iloc[left_qtr])*1,\n    '^-b',label='revenue/100')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['cruise_operating_expense'][left_qtr:]-df_qrtr_data['cruise_operating_expense'].iloc[left_qtr])/np.abs(df_qrtr_data['cruise_operating_expense'].iloc[left_qtr])*10,\n    '^-g',label='cruise operating expense/10')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['marketing_general_and_administrative'][left_qtr:]-df_qrtr_data['marketing_general_and_administrative'].iloc[left_qtr])/np.abs(df_qrtr_data['marketing_general_and_administrative'].iloc[left_qtr])*100,\n    '^-r',label='marketing general & administrative')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['other_operating_expense'][left_qtr:]-df_qrtr_data['other_operating_expense'].iloc[left_qtr])/np.abs(df_qrtr_data['other_operating_expense'].iloc[left_qtr])*100,\n    '^-c',label='other operating expense')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['operating_income'][left_qtr:]-df_qrtr_data['operating_income'].iloc[left_qtr])/np.abs(df_qrtr_data['operating_income'].iloc[left_qtr])*100,\n    '^-m',label='operating income')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Percentage change in quarterly income')\nplt.ylabel('percent change')\nplt.legend()\n\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\nThe plot above scales revenue by 100 and cruise operating expense by 10 inorder to get the plots within a simular range. Revenue, cruise operating expense, marketing general & administrative and other operating expense have been increasing over the period as NCL has resumed operations. In the last two quarters operating income reversed its negative trend."
  },
  {
    "objectID": "NCLHv1 analysis.html#balance-sheet",
    "href": "NCLHv1 analysis.html#balance-sheet",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Balance sheet",
    "text": "Balance sheet\n\nPercentage change in quarterly balance sheet\nThe following chart shows percentage change in quarterly consolidated balance sheet, looking back over the last ten quarters or so. The first graph show the current assets and liabilities.\n\\(\\large{\\color{red}{\\text{plot current along side of total assets and liabilities}}}\\)\n\\(\\large{\\color{red}{\\text{organized balance sheet analysis into assets and liabilities, long and short, percent change and magnitude}}}\\)\nSeems like advance ticket sales could be driving quick ratio.\nThink about removing advance ticket sales from debt calculations.\n\n\nCode\n# Set the locator\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\n# figsize() function to adjust the size\nplt.subplots(1,2,figsize=(15, 5))\n\n# set look back range, left_qtr is the index into the date range\nleft_qtr = -6\n\n# using subplot function and creating two side by side plots\n# plot one\nplt.subplot(1, 2, 1)\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['total_current_assets'][left_qtr:]-df_qrtr_data['total_current_assets'].iloc[left_qtr])/np.abs(df_qrtr_data['total_current_assets'].iloc[left_qtr])*100,\n    '^-g',label='total_current_assets')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['total_current_liabilities'][left_qtr:]-df_qrtr_data['total_current_liabilities'].iloc[left_qtr])/np.abs(df_qrtr_data['total_current_liabilities'].iloc[left_qtr])*100,\n    '^-r',label='total_current_liabilities')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\n# Changes x-axis range\n#plt.gca().set_xbound(year_ended_list[left_qtr], year_ended_list[-1])\n#plt.gca().set_xbound(df_qrtr_data['QTR'].iloc[left_qtr], df_qrtr_data['QTR'].iloc[-1])\n\n#plt.ylim((0,20))\nplt.title('Percentage change in current assets and liabilities')\nplt.ylabel('percent change')\nplt.legend()\n\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n# color sequence b,g,r,c,m,y,k\n\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['total_assets'][left_qtr:]-df_qrtr_data['total_assets'].iloc[left_qtr])/np.abs(df_qrtr_data['total_assets'].iloc[left_qtr])*100,\n    '^-g',label='total assets')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['total_liabilities'][left_qtr:]-df_qrtr_data['total_liabilities'].iloc[left_qtr])/np.abs(df_qrtr_data['total_liabilities'].iloc[left_qtr])*100,\n    '^-r',label='total_liabilities')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Percentage change in total assets and liabilities')\nplt.ylabel('percent change')\nplt.legend()\n\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\nCurrent liabilites has been increasing over the past 9 quarters while the current assets have declined. This is probably due to NCL spening its cash reserves as it resumes operations."
  },
  {
    "objectID": "NCLHv1 analysis.html#debt-analysis",
    "href": "NCLHv1 analysis.html#debt-analysis",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Debt analysis",
    "text": "Debt analysis\n\\(\\large{\\color{red}{\\text{update this section, remove principal payments}}}\\)\nThe current long term amounts were obtained from the quarterly reports. Since March 2020, Moody’s has downgraded NCLH’s long-term issuer rating to B2, the senior secured rating to B1 and the senior unsecured rating to Caa1. Since April 2020, S&P Global has downgraded the issuer credit rating to B, lowered the issue-level rating on the \\$875 million Revolving Loan Facility and \\$1.5 billion Term Loan A Facility to BB-, the issue-level rating on the other senior secured notes to B+ and the senior unsecured rating to B-. If the credit ratings were to be further downgraded, or general market conditions were to ascribe higher risk to NCLH’s rating levels, the cruise industry, NCLH’s access to capital and the cost of any debt or equity financing will be further negatively impacted. NCLH also has significant capacity to incur additional indebtedness under the debt agreements and may issue additional ordinary shares from time to time, subject to the authorized number of ordinary shares.\nNCLH may be required to pledge additional collateral and/or post additional cash reserves or take other actions that may reduce the liquidity.\nThe Principal Payout Schedule was obtained from a pdf file on the NCLH investor web page and a new dataframe was created for this data.\n\nUpdated from 2nd qtr reports\nPrincipal Payout Schedule (in U.S. dollars, thousands) As of September 30, 2022\n\n\n\nDate\nDebt\n\n\n\n\nQ4 2022\n331,440\n\n\n2023\n934,497\n\n\n2024\n3,683,554\n\n\n2025\n1,067,735\n\n\n2026\n1,971,347\n\n\n2027\n3,022,242"
  },
  {
    "objectID": "NCLHv1 analysis.html#debt-to-equity-ratio",
    "href": "NCLHv1 analysis.html#debt-to-equity-ratio",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Debt to equity ratio",
    "text": "Debt to equity ratio\nThe debt to equity (D/E) ratio is used to evaluate a company’s financial leverage and is calculated by dividing a company’s total liabilities by its shareholder equity. Higher ratios tend to indicate a company with higher risk to shareholders. When using the D/E ratio, it is very important to consider the industry in which the company operates. Because different industries have different capital needs and growth rates, a relatively high D/E ratio may be common in one industry, while a relatively low D/E may be common in another. Generally speaking, a D/E ratio below 1.0 would be seen as relatively safe, whereas ratios of 2.0 or higher would be considered risky.\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars, $B, or ratio')\n\n# plot revenue, current assets and liabilities\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['revenue']/1e9,'-+',label='revenue')\n#ax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_assets']/1e9,'-+m',label='total assets')\n#ax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_liabilities']/1e9,'-+c',label='total liabilities')\nax1.plot(df_qrtr_data['QTR'],(df_qrtr_data['total_assets']-df_qrtr_data['total_liabilities'])/1e9,'-+',label='equity')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['long_term_debt']/(df_qrtr_data['total_assets']-df_qrtr_data['total_liabilities']),'-+',label='D/E')\n\nax1.plot(pd.Timestamp(datetime(2020,3,13)),0.2,'^k',label='operations suspended, March 13') # March 13, 2020, operations suspended\nax1.plot(pd.Timestamp(datetime(2021,7,25)),0.2,'ob',label='1st ship return to service, July 25') # July 25, 2021, 1st ship return to service\nax1.plot(pd.Timestamp(datetime(2022,5,7)),0.2,'&gt;k',label='full fleet operating, May 7') # May 7 2022, full fleet operating\n\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,14))\nax1.legend(bbox_to_anchor=(1.1, 1))\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\n#ax2 = ax1.twinx()\n#color = 'tab:green'\n\n#ax2.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_assets']/df_qrtr_data['total_current_liabilities'],'-.g',label='current ratio')\n \n#ax2.set_ylabel('current ratio',color=color)\n#ax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((0,3))\n#ax2.legend(bbox_to_anchor=(1.45, 0))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Debt to equity')\nplt.show()\n\n\n\n\n\nThe D/E ratio for NCL was historically less than two before the pandemic, however, presently the D/E ratio is above 30. The company’s survival took precedence over maintaining a healthy balance sheet.\nNov 13, 2022 yahoo finance reported cruise line D/E as follows: - Royal Caribbean Cruises Ltd. (RCL): D/E = 7.47 - Carnival Corporation & plc (CCL): D/E = 4.21 - Norwegian Cruise Line Holdings Ltd. (NCLH): D/E = 36.63\n\nCurrent Ratio\nThe current ratio is a liquidity ratio that measures a company’s ability to pay short-term obligations or those due within one year. In theory, the higher the current ratio, the more capable a company is of paying its obligations because it has a larger proportion of short-term asset value relative to the value of its short-term liabilities. The current ratio can be a useful measure of a company’s short-term solvency when it is placed in the context of what has been historically normal for the company and its peer group. It also offers more insight when calculated repeatedly over several periods.\nThe plot below shows revenue, current assets, current liabilities and the current ratio. Markers are also shown for the dates when operations were suspended, the first NCL ship returned to service and when the full fleet was back in operation.\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars, $B, or current ratio')\n\n# plot revenue, current assets and liabilities\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['revenue']/1e9,'-+b',label='revenue')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_assets']/1e9,'-+m',label='current assets')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_liabilities']/1e9,'-+c',label='current liabilities')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_assets']/df_qrtr_data['total_current_liabilities'],'-.g',label='current ratio')\n\nax1.plot(pd.Timestamp(datetime(2020,3,13)),0.1,'^k',label='operations suspended, March 13') # March 13, 2020, operations suspended\nax1.plot(pd.Timestamp(datetime(2021,7,25)),0.1,'ob',label='1st ship return to service, July 25') # July 25, 2021, 1st ship return to service\nax1.plot(pd.Timestamp(datetime(2022,5,7)),0.1,'&gt;k',label='full fleet operating, May 7') # May 7 2022, full fleet operating\n\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,6))\nax1.legend(bbox_to_anchor=(1.1, 1))\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\n#ax2 = ax1.twinx()\n#color = 'tab:green'\n\n#ax2.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_assets']/df_qrtr_data['total_current_liabilities'],'-.g',label='current ratio')\n \n#ax2.set_ylabel('current ratio',color=color)\n#ax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((0,3))\n#ax2.legend(bbox_to_anchor=(1.45, 0))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Current ratio')\nplt.show()\n\n\n\n\n\nPrior to the pandemic shut down, NCLH was operating with a current ratio of about 0.25, a historical normal level for the company. During the shut down, NCL increased cash holding to sustain the company while cruise operations were suspended and no revenue was being generated. Now that cruise operations have started to ramp up, we see a reversion of the current ratio back to below a ratio of one. Since the company has negative earnings, a ratio below one is of concern, since it indicates the company might not have sufficient cash to sustain operations.\nThe plot below shows cash & cash equivalents and current liabilities less advance ticket sales. Markers are also shown for the dates when operations were suspended, the first NCLH ship returned to service and when the full fleet was back in operation. An additional marker shows the date of March 31, 2023, when the \\$1B commitment available will end. The expiration of the Commitment Facility was extended through March 31, 2023.\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars, $B, or ratio')\n\n# plot revenue, current assets and liabilities\n#ax1.plot(df_qrtr_data['QTR'],df_qrtr_data['advance_ticket_sales']/1e9,'-+',label='advance ticket sales')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['cash_and_cash_equivalents']/1e9,'-+',label='cash & cash equivalents')\n#ax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_liabilities']/1e9,'-+c',label='current liabilities')\nax1.plot(df_qrtr_data['QTR'],(df_qrtr_data['total_current_liabilities']-df_qrtr_data['advance_ticket_sales'])/1e9,'-+m',label='current liabilities less advance ticket sales')\n#ax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_assets']/1e9,'-+',label='current assets')\n\nax1.plot(pd.Timestamp(datetime(2020,3,13)),0.1,'^k',label='operations suspended, March 13') # March 13, 2020, operations suspended\nax1.plot(pd.Timestamp(datetime(2021,7,25)),0.1,'ob',label='1st ship return to service, July 25') # July 25, 2021, 1st ship return to service\nax1.plot(pd.Timestamp(datetime(2022,5,7)),0.1,'&gt;k',label='full fleet operating, May 7') # May 7 2022, full fleet operating\nax1.plot(pd.Timestamp(datetime(2023,3,31)),1.0,'*r',label='$1B commitment available ending, Mar 31') # March 31, 2023, $1B commitment available ending\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,4))\nax1.legend(bbox_to_anchor=(1.1, 1))\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\n#ax2 = ax1.twinx()\n#color = 'tab:green'\n\n#ax2.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_assets']/df_qrtr_data['total_current_liabilities'],'-.g',label='current ratio')\n \n#ax2.set_ylabel('current ratio',color=color)\n#ax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((0,3))\n#ax2.legend(bbox_to_anchor=(1.45, 0))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Liquidity')\nplt.show()\n\n\n\n\n\nOne of the components of current liabilities is advance ticket sales, which are deposits made by customers on future cruises or conversion of refunds owed from canceled cruises to future cruise credits. If we think of advance ticket sales as future revenue not yet classified as revenue, we can subtract that amount from current liabilities to get an indication of actual current liabilities.\nAs of December 31, 2021, NCLH had advance ticket sales of \\$1.8 billion, including the long-term portion, which included approximately \\$0.7 billion of future cruise credits. NCLH also has agreements with credit card processors that, as of December 31, 2021, governed approximately \\$1.3 billion in advance ticket sales that had been received by the Company relating to future voyages. These agreements allow the credit card processors to require under certain circumstances, including the existence of a material adverse change, excessive chargebacks and other triggering events, that the Company maintain a reserve which would be satisfied by posting collateral.\nAlthough the agreements vary, these requirements may generally be satisfied either through a percentage of customer payments withheld or providing cash funds directly to the card processor. Any cash reserve or collateral requested could be increased or decreased. As of December 31, 2021, NCLH had cash collateral reserves of approximately \\$1.2 billion with credit card processors recognized in accounts receivable, net or other long-term assets\nComparing current liabilities less advance ticket sales to cash and cash equivalents, we see that current liabilities less advance ticket sales has recently exceeded cash and cash equivalents, which is of concern."
  },
  {
    "objectID": "NCLHv1 analysis.html#book-value",
    "href": "NCLHv1 analysis.html#book-value",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Book value",
    "text": "Book value\nThe book value of a company is the net difference between that company’s total assets and total liabilities, where book value reflects the total value of a company’s assets that shareholders of that company would receive if the company were to be liquidated. It serves as the total value of the company’s assets that shareholders would theoretically receive if a company was liquidated. This assumes that the liquidation value of assets is equal to the value claimed by the company.\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars, $B')\n\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_assets']/1e9,'-+',label='total assets')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_liabilities']/1e9,'-+',label='total liabilities')\nax1.plot(df_qrtr_data['QTR'],(df_qrtr_data['total_assets']-df_qrtr_data['total_liabilities'])/1e9,'-+',label='book value')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,20))\nplt.legend(bbox_to_anchor=(1.6, 1))\n#ax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Book value')\nplt.show()\n\n\n\n\n\nAs shown in the plot above, the difference between total assets and total liabilities has been narrowing over the past ten quarters as NCL has been raising cash with loans and by selling shares. If NCLH takes the \\$1 billion dollar commitment, which would increase liabilities by the same amount, the book value of the company would be about \\$0.5 billion dollars.\n\\(\\large{\\color{red}{\\text{update comments about book value}}}\\)\n\n\nCode\nprint('book value MRQ = ${:.2f}B'.\\\n    format(df_qrtr_data['total_assets'].iloc[-1]/1e9 - (df_qrtr_data['total_liabilities'].iloc[-1]/1e9)))\n\n\nbook value MRQ = $0.07B\n\n\n\n\nCode\nprint('book value with $1 billion dollar commitment = ${:.2f}B'.\\\n    format(df_qrtr_data['total_assets'].iloc[-1]/1e9 - (df_qrtr_data['total_liabilities'].iloc[-1]/1e9 + 1.0)))\n\n\nbook value with $1 billion dollar commitment = $-0.93B\n\n\n\nWeighted average of outstanding shares\nThe weighted average of outstanding shares is a calculation that incorporates any changes in the number of a company’s outstanding shares over a reporting period. The reporting period usually coincides with a company’s quarterly or annual reports. NCL calculates the weighted average of outstanding shares and reports the number. Shares outstanding refers to the amount of stock held by shareholders, including restrictive shares held by company insiders. The plot below shows the number of weighted average of outstanding shares each quarter.\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('basic shares outstanding, M')\n\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['weighted_average_shares_outstanding_basic']/1e6,'-+',label='weighted_average_shares_outstanding_basic, M')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,20))\n#plt.legend(bbox_to_anchor=(1.6, 1))\n#ax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Weighted average shares outstanding basic')\nplt.show()\n\n\n\n\n\nThe plot above shows that weighted average shares outstanding basic at the end of each quarter. NCLH has been raising cash by selling shares and as can be seen, there are almost double the number of shares outstanding.\n\n\nBook value per share\nBook value per share is a method to calculate the per-share book value of a company based on common shareholders’ equity in the company. Should the company dissolve, the book value per common share indicates the dollar value remaining for common shareholders after all assets are liquidated and all debtors are paid. A company’s share price usually trades at many multiples of the book value, so book value can serve as a reference point. A declining book value would be a red flag for an investor.\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars')\n\nax1.plot(df_qrtr_data['QTR'],(df_qrtr_data['total_assets']-df_qrtr_data['total_liabilities'])/df_qrtr_data['weighted_average_shares_outstanding_basic'],'-+',label='book value per share')\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,35))\n#plt.legend(bbox_to_anchor=(1.6, 1))\n#ax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Book value per weighted average share')\nplt.show()\n\nprint('book value per weighted average share, MRQ = ${:.2f}'.\\\n    format((df_qrtr_data['total_assets'].iloc[-1]-df_qrtr_data['total_liabilities'].iloc[-1])/df_qrtr_data['weighted_average_shares_outstanding_basic'].iloc[-1]))\n\n\n\n\n\nbook value per weighted average share, MRQ = $0.16\n\n\nThe Book value per weighted average share at the end of the second quarter of 2022 was less than \\$5 dollars. This measure of value indicates that investors have had their investments diluted by about a factor of 10 compared to pre pandemic share prices. The current share price reflects some optimism in the ability of the company to regain at least some positive cash flow in the near future.\nNovember 13, 2022 yahoo finance reported cruise line Book Value per Share (BVS) as follows: - Royal Caribbean Cruises Ltd. (RCL): BVS = \\$12.56 - Carnival Corporation & plc (CCL): BVS = \\$6.66 - Norwegian Cruise Line Holdings Ltd. (NCLH): BVS = \\$0.95\n\n\nBookings\nNCLH generates the majority of revenue from ticket sales. NCLH has stated that NCLH will not discount tickets to fill their ships as this would hurt the brand.\nBooking trends for full year 2023 remain positive with cumulative booked position in line with a record 2019 inclusive of the Company’s 20% increase in capacity. Pricing continues to be significantly higher than that of 2019 at a similar point in time and thus at record levels for full year 2023.\nThe future cruise credits are not contracts, and therefore, guests who elected this option are excluded from our contract liability balance; however, the credit for the original amount paid is included in advance ticket sales.\nThe future cruise credits issued under these programs are generally valid for any sailing through December 31, 2022, and we may extend the length of time these future cruise credits may be redeemed. The use of such credits may prevent us from garnering certain future cash collections as staterooms booked by guests with such credits will not be available for sale, resulting in less cash collected from bookings to new guests. We may incur incremental commission expense for the use of these future cruise credits.\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $B')\n\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['advance_ticket_sales']/1e9,'-+g',label='advance ticket sales')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['future_cruise_credits']/1e9,'-+b',label='future cruise credits')\nax1.plot(df_qrtr_data['QTR'],(df_qrtr_data['advance_ticket_sales']-df_qrtr_data['future_cruise_credits'])/1e9,'-.',label='ticket sales less future cruise credits')\n\nax1.plot(pd.Timestamp(datetime(2020,3,13)),0,'^k',label='operations suspended, March 13') # March 13, 2020, operations suspended\nax1.plot(pd.Timestamp(datetime(2021,7,25)),0,'ob',label='1st ship return to service, July 25') # July 25, 2021, 1st ship return to service\nax1.plot(pd.Timestamp(datetime(2022,5,7)),0,'&gt;k',label='full fleet operating, May 7') # May 7 2022, full fleet operating\n\nax1.tick_params(axis='y')\nax1.set_ylim((-0.1,2.5))\nplt.legend(bbox_to_anchor=(1.7, 1))\n#ax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Ticket sales')\nplt.show()\n\n\n\n\n\nAny value in looking at Passengers carried, Passenger Cruise Days, berths and Capacity Days?\nWhy the difference between advanced ticket sales and ticket sales less FCC? Seems like during the pause, all ticket sales should have been converted to FCC. Ticket sales less future cruise credits should be new bookings, which are almost \\$2B.\n\nFrom Sep 2022 10Q\nDilution from value-add FCCs issued during the pandemic will not carry over into 2023 as the bonus portion of these FCCs expire at YE2022. (from slide 13 of Third Quarter 2022 Earnings Conference Call, November 8, 2022)"
  },
  {
    "objectID": "NCLHv1 analysis.html#liquidity-model",
    "href": "NCLHv1 analysis.html#liquidity-model",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Liquidity model",
    "text": "Liquidity model\nA liquidity model was developed to estimate future cash on hand and ability to pay future debt. The liquidity model starts with a projected occupancy profile and number of berths available. Revenue is calculated from estimated passenger cruise days. Operating expense is estimated from capacity days. Future cash flow is estimated from revenue less operating expenses and marketing, general and administrative expenses. Liquidity is the running total of future cash flows (which might be negative) and the present cash and cash equivalents less debt payments.\nThe calculations for future liquidity follow these steps: - estimate future occupancy percentage, currently at 80% and rising to final value of 107% - data for future berths - estimate capacity days - estimate passenger cruise days - estimate rev per quarter - estimate operating expense per quarter - calculate future cash flow - calculate cash on hand and subtract debt payments\nDuring the Second Quarter 2022 Earnings Conference Call, August 9, 2022, NCLH stated that they want to reach historical occupancy levels by Q2 2023. The occupancy percentage is currently at 80%.\nLooking out 11 quarters to the \\$3.68 billion dollar principle payment due in 2024, which is 11 quarters from now, make a list of the dates for each quarter and calculate the values in the model at each date.\n\\(\\large{\\color{red}{\\text{check calculations in spreadsheet}}}\\)\nmistake - future berths, start with date of June 2022, need to start with MRQ\n\nFuture berths\nNCLH has nine new ships on order and scheduled to be delivered through 2027. Future berths are contained in the dataframe declared below. Updates from Third Quarter 2022 Earnings Conference Call, November 8, 2022, estimated placement by quarter.\n\n\n\nDate\nShip\nBerths Added\n\n\n\n\n3rd quarter 2022\nPrima\n3,100\n\n\n2nd quarter 2023\nViva\n3,100\n\n\n2nd quarter 2023\nVista\n1,200\n\n\n4th quarter 2023\nGrandeur\n750\n\n\n2024\ndeliveries delayed\n0\n\n\n1st quarter 2025\nPrima+A\n3,550\n\n\n2nd quarter 2025\nAllura Class\n1,200\n\n\n4th quarter 2025\nPrima+B\n3,550\n\n\n4th quarter 2026\nPrima+C\n3,550\n\n\n4th quarter 2027\nPrima+D\n3,550\n\n\n\n\n\nShip Construction Contracts\nFor the Norwegian brand, the first Prima Class Ship, Norwegian Prima, at approximately 143,500 Gross Tons and with 3,100 Berths, was delivered in July 2022.\n\n\nCode\ndf_ship_construction_contracts = pd.DataFrame(data={\n    'date':np.array(['2022-12-30','2023-12-30','2024-12-30','2025-12-30','2026-12-30','2027-12-30']).astype('datetime64[D]'),\n    'amount':np.array([25047,2178120,249655,1463558,1648375,761282])*1000    \n})\n\n\n\n\nCode\n# Set the locator\nlocator = mdates.AutoDateLocator()\n#locator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\n#fmt = mdates.DateFormatter('%b %Y')\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars, $B')\n\n# plot Ship construction contracts\n#ax1.bar(df_material_cash_requirements['date'],df_material_cash_requirements['long term debt']/1e9, width=50,label='long term debt, principal+interest payments')\nax1.bar(df_ship_construction_contracts['date'],df_ship_construction_contracts['amount']/1e9, width=50,label='ship construction')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,14))\n#ax1.legend(bbox_to_anchor=(1.4, 1))\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\n#ax2 = ax1.twinx()\n#color = 'tab:green'\n\n#ax2.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_assets']/df_qrtr_data['total_current_liabilities'],'-.g',label='current ratio')\n \n#ax2.set_ylabel('current ratio',color=color)\n#ax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((0,3))\n#ax2.legend(bbox_to_anchor=(1.45, 0))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Ship construction contracts')\nplt.show()\n\n\n\n\n\n\nUpdate from 10Q 2022\nThe impacts of COVID-19 on the shipyards where our ships are under construction (or will be constructed), Russia’s ongoing invasion of Ukraine and/or other macroeconomic events, have already resulted in some delays in expected ship deliveries. These impacts along with other potential modifications the Company may make to its newbuilds, including potential initiatives to improve environmental sustainability, are expected to result in additional delays in ship deliveries in the future, which may be prolonged.\nThe combined contract prices of the eight ships on order for delivery as of September 30, 2022 was approximately €6.7 billion, or \\$6.6 billion based on the euro/U.S. dollar exchange rate as of September 30, 2022. We have obtained export credit financing which is expected to fund approximately 80% of the contract price of each ship, subject to certain conditions. We do not anticipate any contractual breaches or cancellations to occur. However, if any such events were to occur, it could result in, among other things, the forfeiture of prior deposits or payments made by us and potential claims and impairment losses which may materially impact our business, financial condition and results of operations.\n\n\nCode\n# data from 2nd qrt, as of June 30, 2022, data presentation\ndf_future_berths = pd.DataFrame(data={\n    'date':np.array(['2022-06-30','2022-09-30','2022-12-31','2023-03-31','2023-06-30','2023-09-30','2023-12-31',\n    '2024-03-31','2024-06-30','2024-09-30','2024-12-31','2025-03-31','2025-06-30','2025-09-30',\n    '2025-12-31','2026-03-31','2026-06-30','2026-09-30','2026-12-31','2027-03-31','2027-06-30',\n    '2027-09-30','2027-12-31','2028-03-31','2028-06-30','2028-09-30','2028-12-31']).astype('datetime64[D]'),\n    'berths':np.array([59150,63000,63000,63000,64200,67300,68050,68050,68050,68050,\n        68050,71600,72800,72800,76350,76350,79900,79900,83450,83450,83450,83450,\n        87000,87000,87000,87000,87000]),\n    'ships':np.array([28,29,29,29,30,31,32,32,32,32,32,33,34,34,35,35,36,36,37,\n        37,37,37,38,38,38,38,38])    \n})\n\n\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots(figsize=(10, 4))\nax1.set_ylabel('Berths, thousands')\n\nax1.plot(df_future_berths['date'],df_future_berths['berths']/1e3,'-^',label='berths')\n\nax1.tick_params(axis='y')\nax1.set_ylim((55,90))\nplt.legend(bbox_to_anchor=(1.2, 1))\n#ax1.legend()\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:red'\n\n#ax2.plot(year_ended_list,pcd,'+-g')\nax2.plot(df_future_berths['date'],df_future_berths['ships'],'-+',color=color,label='ships')    \nax2.set_ylabel('# of ships',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((25,40))\nax2.legend(bbox_to_anchor=(1.2, 0))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Future berths and ships, all NCLH brands')\nplt.show()\n\n\n\n\n\nThe plot above shows future berths and number of ships. Future berths are used to estimate future revenue by using capacity days, passenger cruise days as shown below."
  },
  {
    "objectID": "NCLHv1 analysis.html#conclusion",
    "href": "NCLHv1 analysis.html#conclusion",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "10) Conclusion ",
    "text": "10) Conclusion \nThe following is a summary of the results described above:\n- Current news: Recent news concerns cruise line debt and return to operations. In June 2022, Carnival provided a 2nd Quarter 2022 business update. The results are playing in the news as negative for Carnival and by extension the other cruise lines. - Low cruise line stock price is in the news. - Review quarterly results: Quarterly reports dating back to March 31, 2019 are analyzed below. D/E, Book value and liquidity levels indicate NCLH is a distressed company as a result of the pandemic. - Average daily volume: 21,067,488 - Dividend yield: NA - Discounted cash flow analysis: The DCF analysis presented was performed for years 2019 and prior, which are the pre-covid years. Companies with negative earnings are difficult to evaluate with DCF. NCLH is suffering from economic distress from strategic problems from the pandemic. As a result, there is financial distress where income, cash flow and the accumulation of large amounts of debt relative to equity weigh heavily on the company’s future viability. The consequent result of near term low or negative earnings and high debt load may make it difficult to access new debt. - DCF Scenarios: DCF scenario 2 is presented, which assumes that NCLH returns to pre-pandemic earnings and values the company in light of higher interest rates and the large debt accumulated. - NACI stock selection guide analysis: The NAIC analysis presented below was performed for years 2019 and prior, which are the pre-covid years. Companies with negative earnings are difficult to evaluate with this method. - Dividend payout: NCLH provides a shareholder benefit which provides \\$100 of onboard credit for a 7 night cruise and \\$250 onboard credit per stateroom on sailings of 15 days or more. Shareholder benefit cash flow IRR is negative. - Management performance: Financial metrics such as liabilities to assets and return on investment were calculated. These don’t have much meaning at this point in the company’s history since the pandemic followed by a recession is looking like it will be a fatal blow to the industry. What we see now is a company on life support, getting massive infusions of debt and a continued negative earnings which is trending in the wrong direction.\nConcerns: D/E above 10, and negative earnings.\nSummary: The pandemic followed by a recession is looking like it will be a fatal blow to the industry. The company is on live support, getting massive infusions of debt and continued negative earnings trend that cannot support near term debt payments.\nRecommendation: Do not buy the stock and do not leave large cruise deposits for cruises more than 180 days out."
  },
  {
    "objectID": "NCLHv1 analysis.html#notes",
    "href": "NCLHv1 analysis.html#notes",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "11) Notes ",
    "text": "11) Notes \nThe following notes outline the changes to the DCF model for financial and REIT companies.\nValuing a REIT\nNotes from Valuepro Book, page 237\n\nNOPM: To calculate operating income take rental revenue and subtracted total real estate expenses and G&A expenses. To arrive at the NOPM divide the adjusted income from real estate by real estate rental revenue. For the REIT, take income from real estate, which includes depreciation and amortization, and subtract GSA. Exclude other income, gains on sale of real estate and interest expenses.\nREIT has no traditional R&D costs\n\nREIT is not taxed at the corporate level, tax rate: should be near zero.\nDepreciation and capital expenditures are significantly higher for REITs than in other companies.\nNew property acquisitions are not directly accounted for in the DCF model for a REIT.\n\nWorking capitol: accounts payable, rents and security deposits\nShort term assets: cash, rents and other receivables and prepaid expenses\nShort term liabilities: accounts payable, advance rents security deposits\n\nWorking capital is almost zero, which is similar to other financial companies.\nThe consolidated balance sheet lists the assets as: - Real estate held for investment, at cost: - Land - Buildings and improvements - Total real estate held for investment, at cost - Less accumulated depreciation and amortization - Real estate held for investment, net - Real estate and lease intangibles held for sale, net - Cash and cash equivalents &lt;- current asset - Accounts receivable, net &lt;- current asset - Lease intangible assets, net - Other assets, net\nThe line items indicated above have been taken to be the current assets. Intangibles and long term items have been excluded.\nThe consolidated balance sheet lists the liabilities as: - Distributions payable &lt;- current liabilities - Accounts payable and accrued expenses &lt;- current liabilities - Lease intangible liabilities, net - Other liabilities - Line of credit payable and commercial paper &lt;- current liabilities - Term loans, net - Mortgages payable, net &lt;- current liabilities - Notes payable, net\nThe line items indicated above have been taken to be the current liabilities.\nValuing a financial company\nNotes from Valuepro Book, page 206\n\nTotal revenue comes from the total interest and dividend income line on the income statement. The calculation of operating income is more inclusive for a financial company than for an industrial or high tech company. For financial companies, operating revenue includes all normal revenue items plus interest income, dividends received and other investment income.\nCost of Goods Sold (CGS) comes from the Total interest expense line on the statement of income.\nGeneral and Administrative (G&A) are set to zero since they are included in the interest expense line\nA financial company has no traditional R&D costs\n\\(\\text{Cost of Goods Sold (CGS)} = \\text{Total interest expense} + \\text{Total non-interest expense}\\)\n\nGeneral and Administrative (G&A) are set to zero since they are included in the interest expense line\nA financial company has no traditional R&D costs\nDepreciation and amortization of premises and equipment from Consolidated Statements of Cash Flows.\n\nAmortization of other acquisition-related intangible assets is not included.\nNew investment and Depreciation: Property, plant and equipment expenditures and depreciation charges are significantly lower for a financial company. A typical manufacturing company, in order to grow its business, invests a significant portion of its revenues in plant, property and equipment (PPE). Financial companies invest very little in the way of PPE. However, software, risk management systems and acquisitions of other businesses, need to be included.\n\nFrom the Consolidated Statements of Cash Flows, under Cash Flows from Investing Activities - Purchases of premises and equipment - Purchases of leased equipment, net\n\nWorking capital supports manufacturing and service activities of nonfinancial companies. For financial companies, their principal liabilities and assets are financial claims that take the place of working capital. Because there is no differentiation between current and long term assets and liabilities for a financial company, we adjust working capital charges to zero. A financial company generally invests all of its funds in other financial assets, which have characteristics of current assets rather than PP&E.\n\\(\\text{Accounts Receivable} = 0\\)\n\\(\\text{Inventories} = 0\\)\n\\(\\text{Accounts Payable} = 0\\)\n\\(\\text{working capital} = 0\\)\nShort term assets: The balance sheets of most financial companies do not separate assets and liabilities into current and long term categories. When calculating the short term assets take the total assets and subtract goodwill and intangible assets also subtract other assets of questionable value. Subtract long term assets such as PP&E from total assets.\n\n\\(\\text{Short term assets} = \\text{Total assets} - \\text{good will and others of questionable value} - \\text{Premises and equipment}\\)\n\nA financial company’s principal liabilities are deposits, Federal funds purchased, trading account liabilities, insurance policy and claims reserves, contract holder funds and short term borrowing. To be consistent with the treatment of interest and an operating expense for financial companies, include long term debt in the short term liability category.\n\nShort term liabilities: Include long term debt.\n\n\\(\\text{Long term debt} = 0\\)\nExcess return period\nThe excess return period is based on a judgment call. The authors of [2] use the 1-5-7-10 rule. They group companies into one of four general categories and excess return periods. They use a 10 year excess return period to calculate what they would consider the maximum value. They use a more conservative 1 year, 5 year or 7 year return period to calculate a more reasonable or minimum value.\n- 1 year: Boring companies that operate in a highly competitive, low margin industry in which they have nothing particular going for them.\n- 5 year: Decent companies that have a recognizable name and decent reputation and perhaps a regulatory benefit (utility company), but can’t control pricing or growth.\n- 7 year: Good companies with good brand names, large companies of scale, good marketing channels and consumer identification (e.g. McDonald’s)\n- 10 year: Great companies with great growth potential, tremendous marketing power, band names and in-place benefits (e.g. Intel, Microsoft, Coca Cola, Disney)\nNotes about negative working capital\nThe company has a negative working capital rate. Negative working capital describes a situation where a company’s current liabilities exceed its current assets as stated on the firm’s balance sheet. In other words, there is more short-term debt than there are short-term assets.\nNegative working capital most often arises when a business generates cash very quickly because it can sell products to its customers before it has to pay the bills to its vendors for the original goods or raw materials. In this way, the company is effectively using the vendor’s money to grow.\nDividend Aristocrat, Achiever & Champion\nThis company was selected for analysis because it is on the Dividend Aristocrat list and passes the quick look tests. This notebook will be used as a template when analyzing other companies.\n\nAristocrat: S&P 500 Dividend Aristocrats is designed to measure the performance of S&P 500 index constituents that have followed a policy of consistently increasing dividends every year for at least 25 consecutive years.\nAchiever: The Broad Dividend Achievers Index. Eligible companies must be incorporated in the U.S. or its territories, trade on the NYSE, NASDAQ or AMEX, and have increased its annual regular dividend payments for the last 10 or more consecutive years.\nhttps://dividendvaluebuilder.com/dividend-achievers-list/\nhttps://www.marketbeat.com/dividends/achievers/\nChampion: This list includes companies that had increased their dividend for at least 25 consecutive years, and includes additional companies that had paid higher dividends without having increased the payout in every calendar year.\nhttps://dividendvaluebuilder.com/dividend-champions-list/\nhttps://www.dividendgrowthinvestor.com/p/dividend-champions-list.html"
  },
  {
    "objectID": "NCLHv1 analysis.html#references",
    "href": "NCLHv1 analysis.html#references",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "12) References ",
    "text": "12) References \n\nGray, Gary, et al. Streetsmart Guide to Valuing a Stock: the Savvy Investors Key to Beating the Market. McGraw-Hill, 2004.\nO’Hara, Thomas E., and Ken Janke. Starting and Running a Profitable Investment Club: the Official Guide from the National Association of Investors Corporation. Times Business, 1998.\nRobert G. Hagstrom, The Warren Buffett Way, Wiley, 2013"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is a collection of Jupyter Notebooks describing problems solved with the aid of Python and the SciPy, SymPy, NumPy, Pandas and Matplotlib libraries. Jupyter notebooks are documents that allow code and markdown text to be contained in the same document and provides an excellent way to document the analysis and results of the problem solving effort.\nThis site was built using JupyterLab notebooks and Quarto, which is an open-source scientific and technical publishing system."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A collection of Jupyter Notebooks",
    "section": "",
    "text": "Note\n\n\n\nLast update: 28 Nov 2023\nThis page is a test of Quarto. The Jupyter notebooks are rough drafts. So far I like how Quarto renders JupyterLab notebooks to HTML.\nGrammer, typo and spelling checks are needed."
  },
  {
    "objectID": "index.html#electrical-engineering-problems",
    "href": "index.html#electrical-engineering-problems",
    "title": "A collection of Jupyter Notebooks",
    "section": "Electrical Engineering Problems",
    "text": "Electrical Engineering Problems\nThe following notebooks cover various topics in electrical engineering. Electrical engineering is an engineering discipline concerned with electricity, electronics and electromagnetism. My intrest is in circuits and systems, which covers theory, analysis, design, practical implementations of circuits and the application of circuit techniques to systems and to signal processing.\n\nCircuit Analysis\n\nSymbolic Modified Nodal Analysis using Python\nThis is website is an online version of my book: Symbolic Modified Nodal Analysis using Python. Symbolic circuit analysis is a circuit analysis method that derives network equations with the circuit elements represented by symbols.\n\n\nSource free series circuit\nIn this notebook, the Python modules SymPy and SciPy are used to solve for currents and voltages in a series R, L and C circuit from the characteristic differential equation. A source free series RLC circuit consists of a resistor, capacitor and inductor connected in series with some initial energy stored either in the inductor, capacitor or both. Since the circuit is a single loop, the current flowing around the loop is the same current in each component. Both parallel and series connected circuits are usually presented in electrical circuit analysis classes. The two textbooks I used while in college presented the parallel connected circuit in some detail then kind of glossed over the series connect circuit since it is the dual of the other. The circuit to be analyzed is shown below and was drawn using EasyEDA, the link to the schematic is here.\n\n\nSciPy analog filter design\nThis notebook describes the use of filter functions in the SciPy library which can be used to design analog filters. Filters are one of the basic building blocks of signal processing. In general terms, filters used for signal processing can be divided into two groups, analog and digital filters. Analog filters, the topic of this notebook, are filters which can be described with linear differential equations and implemented in circuits which operate on continuously varying signals. A digital filter is a system that performs operations on a sampled, discrete-time signal with numerical operations. The first part of this notebook uses SciPy functions to calculate analog filter polynomials based on the classical Butterworth, Chebyshev, Elliptic and Bessel filters. The last section of this notebook walks through a Butterworth filter design and Cauer synthesis of an LC ladder type filter.\n\n\nAdd links from Blogger to Symbolic MNA book problems\nThese JupyterLab notebooks are going to be in the Symbolic MNA book as problems\n\nTwo port parameters\n\nCoupled inductor circuits\n\nNodal analysis of a RLC circuit with controlled sources\n\nThevenin equivalent circuit\n\nBridge-T network analysis\n\nPi filter transfer function\n\nNullors, Gyrators, NICs, Current Conveyors\n\nState variable filter: LP, HP, BP BR\nBi-Quad LP, HP, BP, BR\n\nSallen Key active filter\n\nWien bridge oscillator\n\nChua’s circuit\n\nSwitched mode power supply\n\nT-coil circuit\n\nRLC, initial conditions and inverse Laplace transform\n\n\n\n\nElectric Machines & Electromechanics\n\nOvershot waterwheel hydroelectric power system analysis\nThis notebook examines the construction of a small hydroelectric system built by Michael Loftis, a YouTuber, real estate and insurance broker, who had a hydro system built for his off-grid cabin on 50 acres in the Ozark Mountains. The cabin and hydro system are unique and look really interesting. The cabin is available for rent as an AirBnB and is located in Thornfield, Missouri. The cabin is now advertised as having some solar panels with a gas powered generator to supplement the solar. Michael’s hydroelectric project looked really interesting and I was curious about the technical details. In the YouTube videos, Did we MAKE HYDRO POWER Using Our Waterwheel for Self Reliant Off Grid Living? and Possible Hydro Power Solutions for our Off Grid Cabin? A BIG THANKS TO YOU!!, he discusses the status of the system. I was disappointed to hear that his system wasn’t working, because at first glance it looked very impressive. I decided to learn a little about small hydroelectric systems and document in this notebook what I’ve learned.\n\n\n\nLake District Overshot Waterwheel Project Review and Analysis\nThis notebook examines the design of a small hydroelectric water wheel built by Kris Harbour Natural Building. The water wheel is a 2.1 meter diameter wheel and he expected to generate between 1.5 and 1.7 kW. The project is documented in four YouTube videos, links are here: Part 1, Part 2, Part 3 and Part 4. The videos provide some technical details, but not enough to satisfy me. My JupyterLab notebook will cover the project description, the water wheel design, generator, flume, inverter and other topics he did not address such as instrumentation, safety, maintenance, reliability and economics. (this is a work in progress)\nOther future topics\n\nelectric power, motors, magnetics\n\nInductor and transformer design\n\nElectric propulsion\n\n\n\nHeat transfer\n\nHeat sink design: TPA6021A4 with 20-pin DIP anodized heat sink (Aavid Thermalloy 580400B00000, Digi-Key HS181-ND)\n\n\n\nRadio and RF design\n\nscikit-rf\n\nsignals and spectra, modulation and coding\n\ntopics in microwave engineering\n\nSmith chart plots\n\nantenna design\n\nantenna temperature\n\nreceiver design, analog FE for SDR\n\nLink Analysis\n\nstarlink example (LEO)\n\nDirect to cellphone satellite service\n\n\nHydrogen line receiver\n\nH1 line signal strength\n\nbasic design and feasibility\n\nantenna design, horn, other\n\nreceiver design\n\nfilter design\n\nmapping the sky\n\n\n\n\nInformation security\n\nPassword card\nThis notebook describes the use of a password card and the Python code that generates the password card. The password card is a grid of random letters, numbers and special characters that provides a convenient aid to generate and remember long and complex passwords. A basic description of the password card and instructions are provided along with the Python code used to generate the password card. A security analysis and an analysis of alternatives is provided.\n\n\n\nElectromagnetics\n\n\nSignal processing\n\nFourier transforms\nDiscrete Fourier transforms\nWindows\nFilters"
  },
  {
    "objectID": "index.html#discount-cash-flow-analysis-of-equities",
    "href": "index.html#discount-cash-flow-analysis-of-equities",
    "title": "A collection of Jupyter Notebooks",
    "section": "Discount Cash Flow Analysis of Equities",
    "text": "Discount Cash Flow Analysis of Equities\nThe following notebooks use discount cash flow analysis to determine if the stock of a company should be purchased. A discounted cash flow analysis requires making assumptions about a company’s sales growth, profit margins, depreation rate, investment rate, cost of capital, debt, dividends etc. Historical data is used to make these assumptions.\nHere are two notebooks posted as a test.\n\nFederal Realty (FRT)\nFederal Realty is a recognized leader in the ownership, operation and redevelopment of high-quality retail-based properties located primarily in major coastal markets from Washington, D.C. to Boston as well as San Francisco and Los Angeles. The company specializes in the ownership, management, and redevelopment of high quality retail and mixed-use properties located primarily in densely populated and affluent communities in strategically selected metropolitan markets in the Northeast and Mid-Atlantic regions of the United States, as well as in California and South Florida. As of December 31, 2020, the company owned or had a majority interest in community and neighborhood shopping centers and mixed-use properties which are operated as 101 predominantly retail real estate projects comprising approximately 23.4 million square feet.\n\n\nNorwegian Cruise Line Holdings (NCLH)\nThis notebook was developed to analyze the financial performance of NCLH. The analysis presented primarily uses financial data prior to fiscal year 2019. Most of which is irrelevant now, since NCLH’s consolidated financial sheets are dramatically different following the shock of the pandemic. From a financial perspective, it’s not really possible to compare the finances of post pandemic NCL to the pre pandemic NCL. On account of the large discontinuity in operations, the company’s pre and post pandemic financials need to be considered separately.\n\n\nHanesbrands Inc. (HBI)\nThis notebook presents analysis and commentary for HanesBrands (NYSE: HBI). The analysis presented is based on examination of the business fundamentals. A discount cash flow analysis is used to estimate the intrinsic value of the company. A second evaluation method based on earnings history and historical price to earnings ratio is calculated. Using some judgment calls, as explained in the analysis, an intrinsic stock value is calculated. Some shares of HBI were purchased based on dividend yield and the intrinsic stock value. As described in the analysis, HBI suspended the dividend in order to direct funds to pay down the debt. Since the company is not paying a dividend, does it make sense to hold the company as a value stock? The analysis concludes that there is some merit to think the company might be a value stock, but as a non-dividend paying stock, having HBI does not fit my investment goals of holding quality dividend paying stocks.\nAt the time of writing this report, some stock analysis were suggesting that HBI might be a value play, that is, buying this stock on the cheap and holding until the price recovers or the dividend is re-instated. See the articles here and here. My analysis shown below indicates that the even if NOP can be increased over time by 10%, the ratio of NOP to total liabilities remains above 7. (need to recalculate projected total liabilities and clean up the analysis). Historically the ratio was near 5 when HBI initiated their dividend.\n\n\nOther notebooks to be converted from Blogger\n\nU.S. Bancorp (USB)\n\nBristol-Myers Squibb Company (BMY)\n\nInternational Paper Company (IP)"
  },
  {
    "objectID": "index.html#equities-technical-analysis",
    "href": "index.html#equities-technical-analysis",
    "title": "A collection of Jupyter Notebooks",
    "section": "Equities Technical Analysis",
    "text": "Equities Technical Analysis\nA collection of notebooks analyzing the stockmarket statistical trends of price movement and volume."
  },
  {
    "objectID": "SciPy analog filter design.html",
    "href": "SciPy analog filter design.html",
    "title": "SciPy analog filter design",
    "section": "",
    "text": "Last update: 27 Aug 2022\nThis notebook describes the use of filter functions in the SciPy library which can be used to design analog filters. Filters are one of the basic building blocks of signal processing. In general terms, filters used for signal processing can be divided into two groups, analog and digital filters. Analog filters, the topic of this notebook, are filters which can be described with linear differential equations and implemented in circuits which operate on continuously varying signals. A digital filter is a system that performs operations on a sampled, discrete-time signal with numerical operations.\nThe first part of this notebook uses SciPy functions to calculate analog filter polynomials based on the classical Butterworth, Chebyshev, Elliptic and Bessel filters. The last section of this notebook walks through a Butterworth filter design and Cauer synthesis of an LC ladder type filter.\nThe Analog filter design can be approached in two ways, by synthesis or by cookbook methods. The SciPy filter design functions fall into the cookbook method of design and stop at circuit realization. SciPy has filter functions that generate the coefficients of the polynomials for Butterworth, Chebyshev, Elliptic and Bessel type filters. And for each type filter, lowpass, highpass, bandpass and bandstop can be specified. SciPy also has filter functions that can evaluate the frequency and transient response of the filters. For the Butterworth, Chebyshev and elliptic type filters there are functions which can estimate the filter order required to meet passband or stopband ripple and attenuation specifications. However there is no Bessel order specification function included in the SciPy libraries.\nThere are several important filter types such as Gaussian, Optimum “L” filter and Linkwitz–Riley filter that are not included in the SciPy signal processing module.\nThe SciPy filter functions provide the electrical engineer with the polynomial coefficients for the filter transfer function. Taking the filter polynomial coefficients and designing an analog filter circuit is not within the scope of what SciPy provides. At this point in the filter design process, the engineer can implement some types of filter networks by using design tables.\nExample calculations are provided below for synthesis of a low pass filter. In the example, I walk through the calculations for filter synthesis directly from the transfer function of a Butterworth lowpass filter. Butterworth lowpass filters are all pole filters with unity numerator in the transfer function which allows the driving point impedance to easly be found and the filter synthesis is accomplished by expanding the \\(Z_{11}\\) with partial fraction expansion.\nNotes on filter synthesis\nCurrently when designs require a ladder prototype, a designer completes the task with the aid of tables. The complex numerical methods involved in generating tables are far from general. Transfer functions with zeros require very complex and filter specific algorithms. The most common approach, Cauer’s algorithm, takes advantage of a particular filter’s properties. Each filter approximation technique needs its own specific algorithm [1]. For transfer functions with zeros, Cauer’s approach produces only one circuit implementation, where many are possible. For transfer functions without zeros, such as Butterworth, Chebyshev and Bessel-Thompson type filters, determining element values involves the classical continued fraction expansion of the input impedance.\nReference\n1. R. D. Koller and B. M. Wilamowski, A ladder prototype synthesis algorithm, Proceedings of the 35th Midwest Symposium on Circuits and Systems, 1992 2. Electronic Filter Design Handbook, 3rd Edition, Arthur B. Williams, Fred J. Taylor, McGraw Hill, 1995\nRelated information DSP references:\n- https://pysdr.org/index.html\n- https://medium.datadriveninvestor.com/designing-filters-made-easy-using-python-9c44d9064f94\nimport numpy as np\nfrom scipy import signal\nfrom sympy import *\nimport matplotlib.pyplot as plt\ninit_printing()"
  },
  {
    "objectID": "SciPy analog filter design.html#lowpass-filter-specification",
    "href": "SciPy analog filter design.html#lowpass-filter-specification",
    "title": "SciPy analog filter design",
    "section": "Lowpass filter specification",
    "text": "Lowpass filter specification\nThe SciPy functions used below can be used to find the order of the filter required based on the passband frequency, stopband frequency and allowable ripple or attenuation.\nSciPy has four functions that return the order of the filter satisfying the users filter specifications, which are in the buttord, cheb1ord, cheb2ord and ellipord. Depending on the type of filter wanted, uncomment the particular function.\n\nfp = 5e3 # passband edge frequency, Hz\ngpass = 3 # The maximum loss in the passband (dB)\n\nfs = 10e3 # stopband edge frequency, Hz\ngstop = 40 # The minimum attenuation in the stopband (dB)\n\n# uncomment filter type to use\nN, Wn = signal.buttord(fp*2*np.pi, fs*2*np.pi, gpass, gstop,analog=True)\n#N, Wn = signal.cheb1ord(fp*2*np.pi, fs*2*np.pi, gpass, gstop,analog=True)\n#N, Wn = signal.cheb2ord(fp*2*np.pi, fs*2*np.pi, gpass, gstop,analog=True)\n#N, Wn = signal.ellipord(fp*2*np.pi, fs*2*np.pi, gpass, gstop,analog=True)\n\nprint('order = {:.0f}, frequency = {:.2f} rad/sec'.format(N,Wn))\n\norder = 7, frequency = 31426.58 rad/sec\n\n\n\nFind filter coefficients\nUncomment the line depending on the type of filter desired. The functions return the numerator (b) and denominator (a), polynomials.\n\nsystem_lp = signal.butter(N, Wn, btype='lowpass', output='ba', analog=True)\n#system_lp = signal.cheby1(N, gpass, Wn, btype='lowpass', output='ba', analog=True)\n#system_lp = signal.cheby2(N, gstop, Wn, btype='lowpass', output='ba', analog=True)\n#system_lp = signal.ellip(N, gpass, gstop, Wn, btype='lowpass', output='ba', analog=True)\n#system_lp = signal.bessel(N, Wn, btype='lowpass', output='ba', analog=True, norm='mag')\n\n\n\nplot filter specification limits\nThe grey areas are the keep out areas specified above.\n\nb, a = system_lp\nw, h = signal.freqs(b, a, 2000) # Compute frequency response of analog filter\nplt.semilogx(w/(2*np.pi), 20 * np.log10(abs(h)))\nplt.title('Filter frequency response and limits')\nplt.xlabel('Frequency, Hz')\nplt.ylabel('Amplitude [dB]')\nplt.grid(which='both', axis='both')\n\nplt.fill([0, fp,  fp,  0], [-gpass, -gpass, -100, -100], '0.9', lw=0) # passband\nplt.fill([fs, fs, 100e9, 100e9], [-gstop, 100, 100, -gstop], '0.9', lw=0) # stopband\nplt.xlim((w[0]/(2*np.pi),100e3))\nplt.ylim((-100,10))\nplt.show()\n\n\n\n\n\n\nplot poles and zeros\n\nz, p, k = signal.tf2zpk(system_lp[0],system_lp[1])\n\nplt.plot(np.real(z), np.imag(z), 'ob', markerfacecolor='none')\nplt.plot(np.real(p), np.imag(p), 'xr')\nplt.legend(['Zeros', 'Poles'], loc=2)\nplt.title('Pole / Zero Plot')\nplt.xlabel('Real')\nplt.ylabel('Imaginary')\nplt.grid()\nplt.show()\nprint('k = {:.1f}'.format(k))\n\n\n\n\nk = 30274733497480892041442126462976.0\n\n\n\n\nPlot the Bode magnitude and phase data\nUse the SciPy function bode to plot the magnitude and phase of the filter.\n\n#x = np.linspace(10*2*np.pi, 10e3*2*np.pi, 1000, endpoint=True)\nw, mag, phase = signal.bode(system_lp, w=w)\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nplt.semilogx(w/(2*np.pi), mag,'-k')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\nplt.grid(which='both', axis='both')\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:blue'\n\nplt.semilogx(w/(2*np.pi), phase,':',color=color)  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-5,25))\n\nplt.title('Bode plot')\nplt.show()\n\n\n\n\n\n\nPlot the impulse and step response\nUse the SciPy functions impulse2 and step2 to plot the impulse and step response of the system.\n\n# figsize() function to adjust the size\nplt.subplots(1,2,figsize=(15, 5))\n\n# using subplot function and creating\n# plot one\nplt.subplot(1, 2, 1)\n\n# impulse response\nt, y = signal.impulse2(system_lp,N=500)\nplt.plot(t/1e-3, y)\nplt.title('Impulse response')\nplt.ylabel('volts')\nplt.xlabel('time, msec')\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n\nt, y = signal.step2(system_lp,N=500)\nplt.plot(t/1e-3, y)\nplt.title('Step response')\nplt.ylabel('volts')\nplt.xlabel('time, msec')\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\nPlot the group delay.\nThe following python code calculates and plots group delay. Frequency components of a signal are delayed when passed through a circuit and the signal delay will be different for the various frequencies unless the circuit has the property of being linear phase. The delay variation means that signals consisting of multiple frequency components will suffer distortion because these components are not delayed by the same amount of time at the output of the device.\nGroup delay: \\(\\tau _{g}(\\omega )=-\\frac {d\\phi (\\omega )}{d\\omega }\\)\n\nplt.title('Filter group delay')\nplt.semilogx(w/(2*np.pi), -np.gradient(phase*np.pi/180)/np.gradient(w)/1e-3,'-',label='group delay')\n#plt.semilogx(w/(2*np.pi), -np.gradient(phase)/w/1e-3,'-',label='phase delay')\nplt.ylabel('Group delay, msec')\nplt.xlabel('Frequency, Hz')\nplt.ylim((0,0.3))\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "SciPy analog filter design.html#highpass-filter-specification",
    "href": "SciPy analog filter design.html#highpass-filter-specification",
    "title": "SciPy analog filter design",
    "section": "Highpass filter specification",
    "text": "Highpass filter specification\nUsing the same filter order selection function above, set the passband frequency limit above the stopband frequency limit.\n\nfp = 10e3 # passband edge frequency, Hz\ngpass = 3 # The maximum loss in the passband (dB)\n\nfs = 5e3 # stopband edge frequency, Hz\ngstop = 40 # The minimum attenuation in the stopband (dB)\n\n# uncomment filter type to use\n#N, Wn = signal.buttord(fp*2*np.pi, fs*2*np.pi, gpass, gstop,analog=True)\nN, Wn = signal.cheb1ord(fp*2*np.pi, fs*2*np.pi, gpass, gstop,analog=True)\n#N, Wn = signal.cheb2ord(fp*2*np.pi, fs*2*np.pi, gpass, gstop,analog=True)\n#N, Wn = signal.ellipord(fp*2*np.pi, fs*2*np.pi, gpass, gstop,analog=True)\n\nprint('order = {:.0f}, frequency = {:.2f} rad/sec'.format(N,Wn))\n\norder = 5, frequency = 62831.85 rad/sec\n\n\n\nFind filter coefficients\nUncomment the line depending on the type of filter desired. The functions return the numerator (b) and denominator (a), polynomials.\n\n#system_hp = signal.butter(N, Wn, btype='highpass', output='ba', analog=True)\nsystem_hp = signal.cheby1(N, gpass, Wn, btype='highpass', output='ba', analog=True)\n#system_hp = signal.cheby2(N, gstop, Wn, btype='highpass', output='ba', analog=True)\n#system_hp = signal.ellip(N, gpass, gstop, Wn, btype='highpass', output='ba', analog=True)\n#system_hp = signal.bessel(N, Wn, btype='highpass', output='ba', analog=True, norm='mag')\n\n\n\nplot filter specification limits\nThe grey areas are the keep out areas specified above.\n\nb, a = system_hp\nw, h = signal.freqs(b, a, 2000) # Compute frequency response of analog filter\nplt.semilogx(w/(2*np.pi), 20 * np.log10(abs(h)))\nplt.title('Filter frequency response and limits')\nplt.xlabel('Frequency, Hz')\nplt.ylabel('Amplitude [dB]')\nplt.grid(which='both', axis='both')\n\nplt.fill([100e9, fp,  fp,  100e9], [-gpass, -gpass, -100, -100], '0.9', lw=0) # passband\nplt.fill([fs, fs, 0, 0], [-gstop, 100, 100, -gstop], '0.9', lw=0) # stopband\nplt.xlim((100,100e3))\nplt.ylim((-100,10))\nplt.show()\n\n\n\n\n\n\nplot poles and zeros\n\nz, p, k = signal.tf2zpk(system_hp[0],system_hp[1])\n\nplt.plot(np.real(z), np.imag(z), 'ob', markerfacecolor='none')\nplt.plot(np.real(p), np.imag(p), 'xr')\nplt.legend(['Zeros', 'Poles'], loc=2)\nplt.title('Pole / Zero Plot')\nplt.xlabel('Real')\nplt.ylabel('Imaginary')\nplt.grid()\nplt.show()\n\n\n\n\n\n\nPlot the Bode magnitude and phase data\nUse the SciPy function bode to plot the magnitude and phase of the filter.\n\n#x = np.linspace(10*2*np.pi, 10e3*2*np.pi, 1000, endpoint=True)\nw, mag, phase = signal.bode(system_hp, w=w)\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nplt.semilogx(w/(2*np.pi), mag,'-k')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\nplt.grid(which='both', axis='both')\nax1.set_xlim((110,100e3))\nax1.set_ylim((-100,10))\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:blue'\n\nplt.semilogx(w/(2*np.pi), phase,':',color=color)  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-5,25))\n\nplt.title('Bode plot')\nplt.show()\n\n\n\n\n\n\nPlot the impulse and step response\nUse the SciPy functions impulse2 and step2 to plot the impulse and step response of the system.\n\n# figsize() function to adjust the size\nplt.subplots(1,2,figsize=(15, 5))\n\n# using subplot function and creating\n# plot one\nplt.subplot(1, 2, 1)\n\n# impulse response\nt, y = signal.impulse2(system_hp,N=500)\nplt.plot(t/1e-3, y)\nplt.title('Impulse response')\nplt.ylabel('volts')\nplt.xlabel('time, msec')\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n\nt, y = signal.step2(system_hp,N=500)\nplt.plot(t/1e-3, y)\nplt.title('Step response')\nplt.ylabel('volts')\nplt.xlabel('time, msec')\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\nPlot the group delay.\nThe following python code calculates and plots group delay. Frequency components of a signal are delayed when passed through a circuit and the signal delay will be different for the various frequencies unless the circuit has the property of being linear phase. The delay variation means that signals consisting of multiple frequency components will suffer distortion because these components are not delayed by the same amount of time at the output of the device.\nGroup delay: \\(\\tau _{g}(\\omega )=-\\frac {d\\phi (\\omega )}{d\\omega }\\)\n\nplt.title('Filter group delay')\nplt.semilogx(w/(2*np.pi), -np.gradient(phase*np.pi/180)/np.gradient(w)/1e-3,'-',label='group delay')\n#plt.semilogx(w/(2*np.pi), -np.gradient(phase)/w/1e-3,'-',label='phase delay')\nplt.xlim((100,100e3))\nplt.ylim((0,0.3))\nplt.ylabel('Group delay, msec')\nplt.xlabel('Frequency, Hz')\nplt.legend()\nplt.grid(which='both', axis='both')\nplt.show()"
  },
  {
    "objectID": "SciPy analog filter design.html#bandpass-filter-specification",
    "href": "SciPy analog filter design.html#bandpass-filter-specification",
    "title": "SciPy analog filter design",
    "section": "Bandpass filter specification",
    "text": "Bandpass filter specification\nUsing the same filter order selection function above, set the passband frequency range as a two element array of band edge frequencies. Set the stopband frequencies as a two element array of band edge frequencies.\n\nf_lower = 8e3 # lower passband edge frequency, Hz\nf_upper = 12e3 # upper passband edge frequency, Hz\n\nwp = [f_lower*2*np.pi,f_upper*2*np.pi] # passband edge frequency, Hz\ngpass = 3 # The maximum loss in the passband (dB)\n\ntransition = 3e3 # amount of transition allowed, Hz\nws = [(f_lower-transition)*2*np.pi,(f_upper+transition)*2*np.pi] # stopband edge frequency, Hz\ngstop = 50 # The minimum attenuation in the stopband (dB)\n\n# uncomment filter type to use\n#N, Wn = signal.buttord(wp, ws, gpass, gstop,analog=True)\n#N, Wn = signal.cheb1ord(wp, ws, gpass, gstop,analog=True)\nN, Wn = signal.cheb2ord(wp, ws, gpass, gstop,analog=True)\n#N, Wn = signal.ellipord(wp, ws, gpass, gstop,analog=True)\n\nprint('order = {:.0f}'.format(N))\n\norder = 5\n\n\n\n#system_bp = signal.butter(N, [f_lower*(2*np.pi),f_upper*(2*np.pi)], btype='bandpass', output='ba', analog=True)\n#system_bp = signal.cheby1(N, gpass, [f_lower*(2*np.pi),f_upper*(2*np.pi)], btype='bandpass', output='ba', analog=True)\n#system_bp = signal.cheby2(N, gstop, [f_lower*(2*np.pi),f_upper*(2*np.pi)], btype='bandpass', output='ba', analog=True)\nsystem_bp = signal.ellip(N, gpass, gstop, [f_lower*(2*np.pi),f_upper*(2*np.pi)], btype='bandpass', output='ba', analog=True)\n#system_bp = signal.bessel(N, [f_lower*(2*np.pi),f_upper*(2*np.pi)], btype='bandpass', output='ba', analog=True, norm='mag')\n\n\nplot filter specification limits\nThe grey areas are the keep out areas specified above.\n\nb, a = system_bp\nw, h = signal.freqs(b, a, 2000) # Compute frequency response of analog filter\nplt.semilogx(w/(2*np.pi), 20 * np.log10(abs(h)))\nplt.title('Filter frequency response and limits')\nplt.xlabel('Frequency, Hz')\nplt.ylabel('Amplitude [dB]')\nplt.grid(which='both', axis='both')\n\nplt.fill([f_lower, f_upper,  f_upper,  f_lower], [-gpass, -gpass, -100, -100], '0.9', lw=0) # passband\nplt.fill([f_lower-transition, f_lower-transition, 0, 0], [-gstop, 100, 100, -gstop], '0.9', lw=0) # stopband\nplt.fill([f_upper+transition, f_upper+transition, 100e9, 100e9], [-gstop, 100, 100, -gstop], '0.9', lw=0) # stopband\nplt.xlim((1e3,1e5))\nplt.ylim((-100,10))\nplt.show()\n\n\n\n\n\n\nplot poles and zeros\n\nz, p, k = signal.tf2zpk(system_bp[0],system_bp[1])\n\nplt.plot(np.real(z), np.imag(z), 'ob', markerfacecolor='none')\nplt.plot(np.real(p), np.imag(p), 'xr')\nplt.legend(['Zeros', 'Poles'], loc=2)\nplt.title('Pole / Zero Plot')\nplt.xlabel('Real')\nplt.ylabel('Imaginary')\nplt.grid()\nplt.show()\n\n\n\n\n\n\nPlot the Bode magnitude and phase data\nUse the SciPy function bode to plot the magnitude and phase of the filter.\n\n#x = np.linspace(10*2*np.pi, 10e3*2*np.pi, 1000, endpoint=True)\nw, mag, phase = signal.bode(system_bp, w=w)\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nplt.semilogx(w/(2*np.pi), mag,'-k')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\nplt.grid(which='both', axis='both')\n\nax1.set_xlim((1e3,1e5))\nax1.set_ylim((-100,10))\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:blue'\n\nplt.semilogx(w/(2*np.pi), phase,':',color=color)  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-100,10))\n\nplt.title('Bode plot')\nplt.show()\n\n\n\n\n\n\nPlot the impulse and step response\nUse the SciPy functions impulse2 and step2 to plot the impulse and step response of the system.\n\n# figsize() function to adjust the size\nplt.subplots(1,2,figsize=(15, 5))\n\n# using subplot function and creating\n# plot one\nplt.subplot(1, 2, 1)\n\n# impulse response\nt, y = signal.impulse2(system_bp,N=500)\nplt.plot(t/1e-3, y)\nplt.title('Impulse response')\nplt.ylabel('volts')\nplt.xlabel('time, msec')\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n\nt, y = signal.step2(system_bp,N=500)\nplt.plot(t/1e-3, y)\nplt.title('Step response')\nplt.ylabel('volts')\nplt.xlabel('time, msec')\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\nPlot the group delay.\nThe following python code calculates and plots group delay. Frequency components of a signal are delayed when passed through a circuit and the signal delay will be different for the various frequencies unless the circuit has the property of being linear phase. The delay variation means that signals consisting of multiple frequency components will suffer distortion because these components are not delayed by the same amount of time at the output of the device.\nGroup delay: \\(\\tau _{g}(\\omega )=-\\frac {d\\phi (\\omega )}{d\\omega }\\)\n\nplt.title('Filter group delay')\nplt.semilogx(w/(2*np.pi), -np.gradient(phase*np.pi/180)/np.gradient(w)/1e-3,'-',label='group delay')\n#plt.semilogx(w/(2*np.pi), -np.gradient(phase)/w/1e-3,'-',label='phase delay')\nplt.xlim((1e3,100e3))\nplt.ylim((0,3))\n\nplt.ylabel('Group delay, msec')\nplt.xlabel('Frequency, Hz')\nplt.legend()\nplt.grid(which='both', axis='both')\nplt.show()"
  },
  {
    "objectID": "SciPy analog filter design.html#bandstop-filter-specification",
    "href": "SciPy analog filter design.html#bandstop-filter-specification",
    "title": "SciPy analog filter design",
    "section": "Bandstop filter specification",
    "text": "Bandstop filter specification\nUsing the same filter order selection function above, set the passband frequency range as a two element array of band edge frequencies. Set the stopband frequencies as a two element array of band edge frequencies.\n\nf_lower = 8e3 # lower passband edge frequency, Hz\nf_upper = 10e3 # upper passband edge frequency, Hz\n\nwp = [f_lower*2*np.pi,f_upper*2*np.pi] # passband edge frequency, Hz\ngpass = 1 # The maximum loss in the passband (dB)\n\ntransition = 1e3 # amount of transition allowed\nws = [(f_lower-transition)*2*np.pi,(f_upper+transition)*2*np.pi] # stopband edge frequency, Hz\ngstop = 60 # The minimum attenuation in the stopband (dB)\n\n# uncomment filter type to use\n#N, Wn = signal.buttord(wp, ws, gpass, gstop,analog=True)\n#N, Wn = signal.cheb1ord(wp, ws, gpass, gstop,analog=True)\n#N, Wn = signal.cheb2ord(wp, ws, gpass, gstop,analog=True)\nN, Wn = signal.ellipord(wp, ws, gpass, gstop,analog=True)\n\nprint('order = {:.0f}'.format(N))\n\norder = 5\n\n\n\n#system_bs = signal.butter(N, [f_lower*(2*np.pi),f_upper*(2*np.pi)], btype='bandstop', output='ba', analog=True)\n#system_bs = signal.cheby1(N, gpass, [f_lower*(2*np.pi),f_upper*(2*np.pi)], btype='bandstop', output='ba', analog=True)\n#system_bs = signal.cheby2(N, gstop, [f_lower*(2*np.pi),f_upper*(2*np.pi)], btype='bandstop', output='ba', analog=True)\nsystem_bs = signal.ellip(N, gpass, gstop, [f_lower*(2*np.pi),f_upper*(2*np.pi)], btype='bandstop', output='ba', analog=True)\n#system_bs = signal.bessel(N, [f_lower*(2*np.pi),f_upper*(2*np.pi)], btype='bandstop', output='ba', analog=True, norm='mag')\n\n\nplot filter specification limits\nThe grey areas are the keep out areas specified above.\n\nb, a = system_bs\nw, h = signal.freqs(b, a, 2000) # Compute frequency response of analog filter\nplt.semilogx(w/(2*np.pi), 20 * np.log10(abs(h)))\nplt.title('Filter frequency response and limits')\nplt.xlabel('Frequency, Hz')\nplt.ylabel('Amplitude [dB]')\nplt.grid(which='both', axis='both')\n\nplt.fill([f_lower, f_upper,  f_upper,  f_lower], [-gstop, -gstop, -100, -100], '0.9', lw=0) # passband\nplt.fill([f_lower-transition, f_lower-transition, 0, 0], [-gpass, -100, -100, -gpass], '0.9', lw=0) # stopband\nplt.fill([f_upper+transition, f_upper+transition, 100e9, 100e9], [-gpass, -100, -100, -gpass], '0.9', lw=0) # stopband\nplt.xlim((5e3,2e4))\nplt.ylim((-100,10))\nplt.show()\n\n\n\n\n\n\nplot poles and zeros\n\nz, p, k = signal.tf2zpk(system_bs[0],system_bs[1])\n\nplt.plot(np.real(z), np.imag(z), 'ob', markerfacecolor='none')\nplt.plot(np.real(p), np.imag(p), 'xr')\nplt.legend(['Zeros', 'Poles'], loc=2)\nplt.title('Pole / Zero Plot')\nplt.xlabel('Real')\nplt.ylabel('Imaginary')\nplt.grid()\nplt.show()\n\n\n\n\n\n\nPlot the Bode magnitude and phase data\nUse the SciPy function bode to plot the magnitude and phase of the filter.\n\n#x = np.linspace(10*2*np.pi, 10e3*2*np.pi, 1000, endpoint=True)\nw, mag, phase = signal.bode(system_bs, w=w)\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nplt.semilogx(w/(2*np.pi), mag,'-k')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\nplt.grid(which='both', axis='both')\n\nax1.set_xlim((5e3,2e4))\nax1.set_ylim((-100,10))\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:blue'\n\nplt.semilogx(w/(2*np.pi), phase,':',color=color)  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-5,25))\n\nplt.title('Bode plot')\nplt.show()\n\n\n\n\n\n\nPlot the impulse and step response\nUse the SciPy functions impulse2 and step2 to plot the impulse and step response of the system.\n\n# figsize() function to adjust the size\nplt.subplots(1,2,figsize=(15, 5))\n\n# using subplot function and creating\n# plot one\nplt.subplot(1, 2, 1)\n\n# impulse response\nt, y = signal.impulse2(system_bs,N=500)\nplt.plot(t/1e-3, y)\nplt.title('Impulse response')\nplt.ylabel('volts')\nplt.xlabel('time, msec')\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n\nt, y = signal.step2(system_bs,N=500)\nplt.plot(t/1e-3, y)\nplt.title('Step response')\nplt.ylabel('volts')\nplt.xlabel('time, msec')\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\nPlot the group delay.\nThe following python code calculates and plots group delay. Frequency components of a signal are delayed when passed through a circuit and the signal delay will be different for the various frequencies unless the circuit has the property of being linear phase. The delay variation means that signals consisting of multiple frequency components will suffer distortion because these components are not delayed by the same amount of time at the output of the device.\nGroup delay: \\(\\tau _{g}(\\omega )=-\\frac {d\\phi (\\omega )}{d\\omega }\\)\n\nplt.title('Filter group delay')\nplt.semilogx(w/(2*np.pi), -np.gradient(phase*np.pi/180)/np.gradient(w)/1e-3,'-',label='group delay')\n#plt.semilogx(w/(2*np.pi), -np.gradient(phase)/w/1e-3,'-',label='phase delay')\nplt.ylabel('Group delay, msec')\nplt.xlabel('Frequency, Hz')\nplt.xlim((1e3,100e3))\nplt.ylim((0,3))\nplt.legend()\nplt.grid(which='both', axis='both')\nplt.show()"
  },
  {
    "objectID": "SciPy analog filter design.html#bessel-lowpass-filter",
    "href": "SciPy analog filter design.html#bessel-lowpass-filter",
    "title": "SciPy analog filter design",
    "section": "Bessel Lowpass filter",
    "text": "Bessel Lowpass filter\nSciPy doesn’t have an order calculation function for the Bessel type filter, so I’ll just pick a filter order and cut off frequency and plot the results. The analog Bessel filter has maximally flat group delay and maximally linear phase response, with very little ringing in the step response.\n\nFind filter coefficients\nThe functions return the numerator (b) and denominator (a), polynomials. Critical frequency normalization parameter is set to norm = ‘mag’, so that the gain magnitude is -3 dB at angular frequency Wn.\n\nfp = 10e3 # passband edge frequency, Hz\nN = 5\nWn = fp*2*np.pi\nsystem_lp = signal.bessel(N, Wn, btype='lowpass', output='ba', analog=True, norm='mag')\n\n\n\nPlot the Bode magnitude and phase data\nUse the SciPy function bode to plot the magnitude and phase of the filter.\n\nw, mag, phase = signal.bode(system_lp, w=w)\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nplt.semilogx(w/(2*np.pi), mag,'-k')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\nplt.grid(which='both', axis='both')\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:blue'\n\nplt.semilogx(w/(2*np.pi), phase,':',color=color)  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-5,25))\n\nplt.title('Bode plot')\nplt.show()\n\n\n\n\n\n\nplot poles and zeros\n\nz, p, k = signal.tf2zpk(system_lp[0],system_lp[1])\n\nplt.plot(np.real(z), np.imag(z), 'ob', markerfacecolor='none')\nplt.plot(np.real(p), np.imag(p), 'xr')\nplt.legend(['Zeros', 'Poles'], loc=2)\nplt.title('Pole / Zero Plot')\nplt.xlabel('Real')\nplt.ylabel('Imaginary')\nplt.grid()\nplt.show()\nprint('k = {:.1f}'.format(k))\n\n\n\n\nk = 10980315993618949271977984.0\n\n\n\n\nPlot the impulse and step response\nUse the SciPy functions impulse2 and step2 to plot the impulse and step response of the system.\n\n# figsize() function to adjust the size\nplt.subplots(1,2,figsize=(15, 5))\n\n# using subplot function and creating\n# plot one\nplt.subplot(1, 2, 1)\n\n# impulse response\nt, y = signal.impulse2(system_lp,N=500)\nplt.plot(t/1e-3, y)\nplt.title('Impulse response')\nplt.ylabel('volts')\nplt.xlabel('time, msec')\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n\nt, y = signal.step2(system_lp,N=500)\nplt.plot(t/1e-3, y)\nplt.title('Step response')\nplt.ylabel('volts')\nplt.xlabel('time, msec')\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\nPlot the group delay.\nThe following python code calculates and plots group delay. Frequency components of a signal are delayed when passed through a circuit and the signal delay will be different for the various frequencies unless the circuit has the property of being linear phase. The delay variation means that signals consisting of multiple frequency components will suffer distortion because these components are not delayed by the same amount of time at the output of the device.\nGroup delay: \\(\\tau _{g}(\\omega )=-\\frac {d\\phi (\\omega )}{d\\omega }\\)\n\nplt.title('Filter group delay')\nplt.semilogx(w/(2*np.pi), -np.gradient(phase*np.pi/180)/np.gradient(w)/1e-3,'-',label='group delay')\n#plt.semilogx(w/(2*np.pi), -np.gradient(phase)/w/1e-3,'-',label='phase delay')\nplt.ylabel('Group delay, msec')\nplt.xlabel('Frequency, Hz')\nplt.ylim((0,0.05))\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nThe group delay graph shows that the delay through the filter is constant for frequencies below 10kHz."
  },
  {
    "objectID": "SciPy analog filter design.html#filter-synthesis-example",
    "href": "SciPy analog filter design.html#filter-synthesis-example",
    "title": "SciPy analog filter design",
    "section": "Filter synthesis example",
    "text": "Filter synthesis example\nThe following example walks through the filter design steps for a low pass filter using the SciPy analog filter functions. Specifications for the filter are determined and the order of the filter is calculated. The type of filter selected for implementation is a passive Butterworth filter implemented with capacitors and inductors in a ladder topology. At this point in the design process, filter design tables are used and the normalized element values for a ladder topology are obtained. In this example. I’m going to calculate the normalized values using partial fraction decomposition following the Cauer method.\n\nFilter specification\nThe following filter Specifications are derived from the system requirements. The engineer has determined that a low pass filter is needed and that a passive filter using capacitors and inductors with Butterworth characteristics will meet the system requirements and design goals.\nRequired: Design a low pass filter meeting the following specifications:\n- Low pass Butterworth filter, implemented with inductors and capacitors - 3dB attenuation frequency of 1kHz - 20dB minimum stop band attenuation at 2kHz - \\(R_{source}\\) = \\(R_{load}\\) = 1000 ohms\nUsing the Butterworth filter order selection function, determine the order of the filter required.\n\nfp = 1e3 # passband edge frequency, Hz\ngpass = 3 # The maximum loss in the passband (dB)\n\nfs = 2e3 # stopband edge frequency, Hz\ngstop = 20 # The minimum attenuation in the stopband (dB)\n\n# uncomment filter type to use\norder, Wn = signal.buttord(fp*2*np.pi, fs*2*np.pi, gpass, gstop,analog=True)\n\nprint('order = {:.0f}, -3dB cutoff frequency = {:.2f} Hz'.format(order,Wn/(2*np.pi)))\n\norder = 4, -3dB cutoff frequency = 1000.59 Hz\n\n\nThe filter order meeting the attenuation requirements is 4.\n\n\nPlot the Bode magnitude and phase data\nUse the SciPy function bode to plot the magnitude and phase of the filter. The gray areas are the keep out areas specified above.\n\nsystem_lp = signal.butter(order, Wn, btype='lowpass', output='ba', analog=True)\n\nw, mag, phase = signal.bode(system_lp, w=w)\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nplt.semilogx(w/(2*np.pi), mag,'-k')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\nplt.grid(which='both', axis='both')\n\nplt.fill([0, fp,  fp,  0], [-gpass, -gpass, -100, -100], '0.9', lw=0) # passband\nplt.fill([fs, fs, 100e9, 100e9], [-gstop, 100, 100, -gstop], '0.9', lw=0) # stopband\nplt.xlim((w[0]/(2*np.pi),10e3))\nplt.ylim((-80,10))\n\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:blue'\n\nplt.semilogx(w/(2*np.pi), phase,':',color=color)  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-5,25))\n\nplt.title('Bode plot')\nplt.show()\n\n\n\n\nThe plot above shows that a 4th order filter will meet the filter design requirements.\n\n\nSynthesis of filters from polynomials\nFilter design tables contain tables of transfer functions and the component values, so that design by synthesis is not the usual design method. However, in this example, I’m going to walk through the synthesis of the filter from the polynomial. One method of synthesis is to expand the driving point impedance. If the transfer function is all poles, the expression for \\(Z_{11}\\) is:\n\\(Z_{11} = \\frac {D(s)-s^n}{D(s)+s^n}\\)\nwhere\nD(s) is the denominator of the transfer function.\nThe expansion involves successive division and inversion of the ratio of the two polynomials. The final form contains a sequence of terms that are the values for the capacitors and inductors.\nButterworth, Cheby1 and Bessel have poles only, so the expression for \\(Z_{11}\\) given above can be used.\nUsing the SciPy function butter, calculate the numerator and denominator of the transfer function.\n\nnum, den = signal.butter(order, Wn, btype='lowpass', output='ba', analog=True)\n\ns = symbols('s') # declare the variable s\n\n# transfer function\nPoly(num, s)/Poly(den, s)\n\n\\(\\displaystyle \\frac{1.56225057575991 \\cdot 10^{15}}{1.0 s^{4} + 16428.50380806 s^{3} + 134947868.685722 s^{2} + 649341798364.839 s + 1.56225057575991 \\cdot 10^{15}}\\)\n\n\nThe transfer function contains terms that have large exponents, i.e. \\(10^{15}\\). Continuing with the calculations, these terms present a problem in the numerical calculations. The expression for \\(Z_{11}\\) is shown below.\n\nZ11 = (Poly(den, s)-s**order)/(Poly(den, s)+s**order)\nZ11\n\n\\(\\displaystyle \\frac{16428.50380806 s^{3} + 134947868.685722 s^{2} + 649341798364.839 s + 1.56225057575991 \\cdot 10^{15}}{2.0 s^{4} + 16428.50380806 s^{3} + 134947868.685722 s^{2} + 649341798364.839 s + 1.56225057575991 \\cdot 10^{15}}\\)\n\n\nThe filter is realized with the Cauer procedure. This involves removal of poles and zeros from the Z(s) and Y(s) functions in a continued fraction expansion. The SymPy function apart() can be employed to remove the poles at s=0 for \\(Z_{11}\\).\n\napart(1/Z11)\n\n\\(\\displaystyle 0.000121739631518895 s + \\frac{1.0 \\cdot \\left(3.57799435572491 \\cdot 10^{-8} s^{2} + 0.00029390546949122 s + 1.0\\right)}{1.05159211095627 \\cdot 10^{-11} s^{3} + 8.63804249968547 \\cdot 10^{-8} s^{2} + 0.000415645101010116 s + 1.0}\\)\n\n\nThe term, \\(0.000121739631518895s\\) , is the first element value in the filter, with the source resistance of 1 ohm. The value will need to be impedance scaled in the final design. Removing \\(0.000121739631518895s\\) from the expression by subtracting it, we get the following result.\n\napart(1/Z11) - 0.000121739631518895*s\n\n\\(\\displaystyle 3.79470760369927 \\cdot 10^{-19} s + \\frac{1.0 \\cdot \\left(3.57799435572491 \\cdot 10^{-8} s^{2} + 0.00029390546949122 s + 1.0\\right)}{1.05159211095627 \\cdot 10^{-11} s^{3} + 8.63804249968547 \\cdot 10^{-8} s^{2} + 0.000415645101010116 s + 1.0}\\)\n\n\nHere is where we run into a problem with the calculations. There are terms in the result that have small exponents. e.g. \\(10^{-19}\\) and for these terms it is not easy to determine whether these are actually zero or should be carried along in subsequent steps. To avoid this difficulty, the filter natural frequency is normalized to 1 rad/s, which avoids large exponents in the coefficients of the filter polynomial.\nRe-calculate the numerator and denominator polynomials with the normalized frequency of 1 rad/s.\n\norder = 4\nWn = 1\nnum, den = signal.butter(order, Wn, btype='lowpass', output='ba', analog=True)\n\nPoly(num, s)/Poly(den, s)\n\n\\(\\displaystyle \\frac{1.0}{1.0 s^{4} + 2.61312592975275 s^{3} + 3.41421356237309 s^{2} + 2.61312592975275 s + 1.0}\\)\n\n\nNow the driving point impedance for the filter can be written.\n\nZ11 = (Poly(den, s)-s**order)/(Poly(den, s)+s**order)\nZ11\n\n\\(\\displaystyle \\frac{2.61312592975275 s^{3} + 3.41421356237309 s^{2} + 2.61312592975275 s + 1.0}{2.0 s^{4} + 2.61312592975275 s^{3} + 3.41421356237309 s^{2} + 2.61312592975275 s + 1.0}\\)\n\n\n\napart(1/Z11)\n\n\\(\\displaystyle 0.76536686473018 s + \\frac{0.541196100146197 \\cdot \\left(0.765366864730178 s^{2} + 1.0 s + 0.541196100146198\\right)}{0.76536686473018 s^{3} + 1.0 s^{2} + 0.76536686473018 s + 0.292893218813452}\\)\n\n\nThe first value is 0.76536686473018*s, which we can save in an array called normalized_values\n\nnormalized_values = np.zeros(order)\nnormalized_values[0] = 0.76536686473018\n\nNow remove the term \\(0.76536686473018s\\) by subtracting it, we get:\n\nZa = apart(1/Z11) - 0.76536686473018*s\nZa\n\n\\(\\displaystyle - 4.44089209850063 \\cdot 10^{-16} s + \\frac{0.541196100146197 \\cdot \\left(0.765366864730178 s^{2} + 1.0 s + 0.541196100146198\\right)}{0.76536686473018 s^{3} + 1.0 s^{2} + 0.76536686473018 s + 0.292893218813452}\\)\n\n\nUnfortunately, Sympy numerical evaluation can’t tell an expression that is exactly zero apart from one that is merely very small. Using chop=True doesn’t help. I also tried .evalf(20) to get more digits displayed, then tried chop=True, and these still left small values that interfered with the next step.\nThe work around is to copy the terms needed by hand into the next step.\n\n# don't copy the small values\nZa = 0.541196100146197*(0.765366864730178*s**2 + 1.0*s + 0.541196100146198)/(0.76536686473018*s**3 + 1.0*s**2 + 0.76536686473018*s + 0.292893218813452)\nZa\n\n\\(\\displaystyle \\frac{0.414213562373094 s^{2} + 0.541196100146197 s + 0.292893218813453}{0.76536686473018 s^{3} + 1.0 s^{2} + 0.76536686473018 s + 0.292893218813452}\\)\n\n\nContinuing with the partial fraction expansion.\n\napart(1/Za)\n\n\\(\\displaystyle 1.84775906502258 s + \\frac{0.5411961001462 \\cdot \\left(0.76536686473018 s + 1.0\\right)}{0.765366864730178 s^{2} + 1.0 s + 0.541196100146198} - 6.43275715994145 \\cdot 10^{-15}\\)\n\n\nNext value is 1.84775906502258*s, copying the remainder by hand, leaving out small numbers\n\nnormalized_values[1] = 1.84775906502258\n\n\nZb = 0.5411961001462*(0.76536686473018*s + 1.0)/(0.765366864730178*s**2 + 1.0*s + 0.541196100146198)\nZb\n\n\\(\\displaystyle \\frac{0.414213562373098 s + 0.5411961001462}{0.765366864730178 s^{2} + 1.0 s + 0.541196100146198}\\)\n\n\n\napart(1/Zb)\n\n\\(\\displaystyle 1.84775906502256 s + 6.1647256116105 \\cdot 10^{-15} + \\frac{0.99999999999999}{0.76536686473018 s + 1.0}\\)\n\n\nThe next value is 1.84775906502256*s, copying the remainder by hand, leaving out small numbers\n\nnormalized_values[2] = 1.84775906502256\n\n\nZc = 0.99999999999999/(0.76536686473018*s + 1.0)\nZc\n\n\\(\\displaystyle \\frac{0.99999999999999}{0.76536686473018 s + 1.0}\\)\n\n\n\napart(1/Zc)\n\n\\(\\displaystyle 0.765366864730188 s + 1.0\\)\n\n\n\nnormalized_values[3] = 0.765366864730188\n\nNormalized 4th order values are:\n\nnormalized_values\n\narray([0.76536686, 1.84775907, 1.84775907, 0.76536686])\n\n\nThese values agree with the values in Table 11-2 of [2].\n\n\nFrequency and impedance scaling\nNow scale to correct frequency and source resistor value.\nFrequency scaling factor (FSF)\n\\(FSF = \\frac {desired frequency}{normalized frequency} \\text{ (in rad/s)}\\)\nDiving all reactive elements by FSF\nImpedance scaling factor\n\\(R' = Z \\times R\\)\n\\(L' = \\frac {Z \\times L}{FSF}\\)\n\\(C' = \\frac {C}{Z \\times FSF}\\)\nprimes are values after scaling\n\nFSF = fp*2*np.pi\nZ = 1000\n\n# shunt C\nC1 = normalized_values[0]/(Z*FSF)\nprint('C1 = {:.1f} nF'.format(C1/1e-9))\n# series L\nL1 = Z*normalized_values[1]/FSF\nprint('L1 = {:.1f} mH'.format(L1/1e-3))\n# shunt C\nC2 = normalized_values[2]/(Z*FSF)\nprint('C2 = {:.1f} nF'.format(C2/1e-9))\n# series L\nL2 = Z*normalized_values[3]/FSF\nprint('L2 = {:.1f} mH'.format(L2/1e-3))\n\nC1 = 121.8 nF\nL1 = 294.1 mH\nC2 = 294.1 nF\nL2 = 121.8 mH\n\n\n\n\nLTspice simulation results\nA schematic of the filter was drawn in LTspice and the AC frequency response was simulated.\n\nThe plot below agrees with the Bode plot obtained from SciPy.\n\n\n\nSummary\nThe SciPy filter functions are useful for determining the order of various canonical filter types and generating plots of the frequency and transient responses. The filter polynomial coefficients generated by the filter functions can be used to derive the component values for the capacitors and inductors, but this usually is not easy and SciPy doesn’t have any tools to help with circuit realization."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html",
    "href": "Overshot waterwheel hydroelectric power system analysis.html",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "",
    "text": "Last update: 20 July 2022"
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#introduction",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#introduction",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Introduction",
    "text": "Introduction\nThis notebook examines the construction of a small hydroelectric system built by Michael Loftis [1], a YouTuber, real estate and insurance broker, who had a hydro system built for his off-grid cabin on 50 acres in the Ozark Mountains. The cabin and hydro system are unique and look really interesting. The cabin is available for rent as an AirBnB and is located in Thornfield, Missouri. The cabin is now advertised as having some solar panels with a gas powered generator to supplement the solar.\nMichael’s hydroelectric project looked really interesting and I was curious about the technical details. In videos [6,7], which I watched first, he discusses the status of the system. I was disappointed to hear that his system wasn’t working, because at first glance it looked very impressive. I decided to learn a little about small hydroelectric systems and document in this notebook what I’ve learned."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#scope",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#scope",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Scope",
    "text": "Scope\nThe numbers used in my calculations were estimated by me by looking at relative dimensions in the videos, so my numbers are not the actual numbers, but should be close enough to get a general idea of how the system is working. My analysis should be considered a preliminary analysis and not a detailed design review, as such, I’ve covered a few topics that were mainly of interest to me. Other than a few comments about the dam, how to build a dam is outside the scope of what I want to cover."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#description",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#description",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Description",
    "text": "Description\nThe small dam, waterwheel and generator didn’t charge Michael’s house battery system, as he had hoped it would. He posted a series of seven videos describing the effort [2, 3, 4, 5, 6, 7, 8]. The videos cover the time frame from July 2020 to April 2021. His waterwheel was constructed using wheels from an old hay rake, which belonged to his grandmother. The dam captured water from a spring on his property and he tried to generate enough electrical power to keep his five 50Ah, 24 volt Lithium batteries charged. The batteries are \\$849.00 each. In video [7] he described his gas powered generator, a PREDATOR 3500, which he runs in the morning and evening to keep his batteries charged, since the hydro is not working.\n\nThe photo above was captured from [8] and shows a side view of the dam and waterwheel. The waterwheel is operated in an overshot configuration where the wheel is rotated by water entering buckets just past the top of the wheel. You can also see the flume, which brings water from the pond to the top of the waterwheel. In this photo, the gate to the flume is closed and the hydro system is not being operated.\n\nThe photo above from [7] shows the end of the flume and the waterwheel with a large amount of splash over. Also you can see the pulley system which when connected, steps up the rotations per minute to the generator. As described in video [6], the hydro system is not working as he had hoped.\nMichael thought the flow rate from the spring during the rainy season was 500 to 1000 gallons per minute (GPM) and 300 GPM at other times [2]. In one of his videos [3] he filled a five gallon bucket in what he said was in one second, but by my estimation was closer to two seconds. This would put the flow rate at 150 GPM, but there was some spillage and the total flow wasn’t captured since he was doing this with one hand while videoing. Over the course of his videos, it appears that the flow rate is seasonal. The dam is about 4 feet tall [8] and the waterwheel has a diameter of what looks like 7 feet [5]. His generator is a 2kW permanent magnet generator, from Missouri Wind and Solar and produces 24V at 500 rotations per minute (RPM). In [5] he said the gear ratio was 8:1, but was lower initially, since two belts were shown to have been installed in subsequent videos. During a time of heavy water flow from the spring, he showed [6] the display on the Midnite Classic 200 which indicated 40 Volts at the house and 40 Watts of power from the hydro system. The power cable running to the house is 220 ft from the waterwheel.\nMy main criticisms of his hydro system are that the dam appeared not to have been built to any minimum standards and that a feasibility study was not done. Most of the work and design was performed by Jerry, his builder, a very skilled craftsman. However, I don’t think Jerry has sufficient technical training to design a small concrete dam. A basic waterwheel is conceptually simple and a workable wheel can be constructed by almost anyone who has basic tools and carpentry skills. The design of the waterwheel was constrained by the size of the antique wheels that Jerry suggested could be used as a frame for the waterwheel [5]. Michael consulted with various vendors about the generator and battery system but didn’t realize that just buying recommended items from a shopping list is not a good way to engineer a hydroelectric project. A basic feasibility study would have shown (see calculations below) that about 200 Watts of power could be generated by the waterwheel, not the 1 kW he was hoping for.\nMichael responded to many comments posted on YouTube concerning his project. He deflected all concerns about the design and construction of the dam, saying that he has complete confidence and faith in Jerry. By this point in the video series, the dam was built and holding water, so there was not a lot that could be done to address the comments about the dam, even if he was concerned, which he didn’t seem to be. He was genuinely interested in seeking suggestions about what could be done with the electrical system, but major changes to the waterwheel itself or outright replacement were off the table. The old wheels have sentimental value to him and he likes the aesthetics of the dam and waterwheel as they are now."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#bottom-line-up-front",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#bottom-line-up-front",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Bottom line up front",
    "text": "Bottom line up front\nThe summary, recommendations and conclusion are normally placed at the end of a paper; but in this notebook, I’ve moved them here to serve as an executive summary."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#summary",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#summary",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Summary",
    "text": "Summary\nThe analysis covered the following topics, which are summarized here: - Household energy requirements: the household energy requirements are TBD, but the batteries are 6,000 VAhr and this is the energy the hydro system needs to replenish each day. - Water source: spring flow rate is seasonal and a range of flow rates were used in the calculations - The dam: comments were provided concerning the dam construction and spillway - Power production vs flow and head height was calculated and determined to be about 220 Watts at 300 GPM. - Flume flow rate was calculated and found to be about 70 GPM - Waterwheel performance calculations were made and the design needs improvements - Generator, pulley system and Electrical cable were examined and the generator/pulley system needs a do over. - The electrical cable is fine."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#conclusion",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#conclusion",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe waterwheel is a nice idea. Seems like at 300 GPM the hydro system could charge the batteries.\nDam and pond are a nice feature on the property, but the design and construction of the dam is suspect.\nA feasibility study using some basic math and science should have been done.\n\nMichael’s system was not working for the following reasons:\n1. flume was delivering insufficient water flow, 2. waterwheel RPM was too high and the buckets could not capture or hold the water and 3. a high generator RPM needed to produce 28 to 29 absorption volts is not sustained."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#recommendations",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#recommendations",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Recommendations",
    "text": "Recommendations\n\nGet the dam evaluated, is it worth putting any additional effort into the hydro system when the dam has issues?\nseepage, foundation uplifting, inadequate spillway capacity\nremove stumps in the pond and surrounding dead trees\nGet a better estimate of seasonal flow from the spring, this will guide any decisions about upgrades to the hydro system\nFix problems with the waterwheel and flume\nResize flume and waterwheel width\nLook at alternate bucket designs with additional buckets\nConsider and evaluate other generator, gear box or pulley options\nweather proof generator and drive train\n\n\nimport pint\nureg = pint.UnitRegistry()\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#units",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#units",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Units",
    "text": "Units\nIn the calculations that follow, I’m using the Python package, Pint, which allows mathematical operations with units and conversions to and from different units. Keeping track of the units will provide some error checking when performing the calculations. United States customary units are used to describe the hydro system in the narrative. All calculations are done using the International System (SI) of units.\nA list of units and conversions used by Pint is here.\n\nVariables and constants\nThere are a couple of variables and constants that will be used throughout the calculations. - Q is used for flow rate\n- tau or \\(\\tau\\) is used for torque - omega or \\(\\omega\\) is used for angular velocity - g is used to denote the gravity of Earth, standard gravity by definition is equal to 9.80665 \\(m/s^2\\) - rho or \\(\\rho\\) is used for the density of water, at 50F or 10C is 999.75 \\(kg/m^3\\) - eta or \\(\\eta\\) is used for energy conversion efficiency, accounts for the energy lost to heat\n\ng = 9.80665*ureg.meter/(ureg.sec**2) # \nrho = 999.75*ureg.kg/(ureg.meter**3) # density of water at 50F or 10C, kg/m^3"
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#electrical-energy-and-electrical-power",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#electrical-energy-and-electrical-power",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Electrical energy and electrical power",
    "text": "Electrical energy and electrical power\nElectricity generation is the process of generating electrical energy converted from other forms of energy. Electrical energy typically has units of Watt hours, which is the product of the power in watts multiplied by time in hours. Kilowatt hours is what you would commonly see on your monthly electric bill. The base units (in SI units) for electrical energy is: \\(kg \\cdot m^2 \\cdot s^{-2}\\). This is equal to the energy dissipated as heat when an electric current of one ampere passes through a resistance of one ohm for one second.\nPower is the amount of energy transferred per unit time. Electrical power typically has units of Watts. The base units (in SI units) for electrical power is: \\(kg \\cdot m^2 \\cdot s^{-3}\\). Power from electricity is the product of the current flowing through the element and of the voltage across the element. However, this gets a little complicated if the voltage and current have a phase difference, then the product of Volts x Amps is a vector product. The vector product, which can be expressed as a complex number, has units of Volt Amps and the real part of the product has units of Watts. The imaginary part is called reactive power and for this power, there is no net transfer of energy to the load. Voltage and current are usually measured with a multimeter and for alternating current (AC) measurements, the RMS values are usually measured. The product of the RMS voltage and the RMS current is called the apparent power, which has units of Watts. It’s almost always the apparent power that we are talking about when we talk about the Watts needed for the household.\nI’ll do my best not to mix up the usage of power and energy."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#household-energy-requirements",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#household-energy-requirements",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Household energy requirements",
    "text": "Household energy requirements\nThe energy requirements for Michael’s off grid cabin should have been the first topic to consider. Michael’s power requirements can be estimated from the battery and gas generator system he currently is using. The average household in Missouri uses 1,086 kWh Per Month according to this report and this web page. This is 36,200 Whr per day.\nMichael’s batteries are connected in parallel. So the total nominal battery voltage is 24 volts and the total current is the sum of the currents available from the batteries.\n\nbattery_voltage = 24*ureg.volts\nprint('battery voltage: {:s}'.format(f\"{battery_voltage:.0f~P}\"))\n\nnumber_of_batteries = 5\nbattery_rating = 50*ureg.amps*ureg.hours\nhouse_battery_capacity = number_of_batteries*battery_rating\nprint('house battery capacity: {:s}'.format(f\"{house_battery_capacity:.0f~P}\"))\nprint('house battery energy: {:s}'.format(f\"{battery_voltage*house_battery_capacity:,.0f~P}\"))\n\nbattery voltage: 24 V\nhouse battery capacity: 250 A·hr\nhouse battery energy: 6,000 A·V·hr\n\n\nThe batteries supply energy to the house during times when the generator is not running. The available battery energy for the house, in \\(volt \\cdot amp \\cdot hours\\), is the product of the nominal battery voltage and the rated capacity. For resistive loads, \\(volt \\cdot amp \\cdot hours\\) can be considered equivalent to Watt hours.\nSince hydro is not working, a gas powered generator, the Predator 3500, is used during the day to charge the batteries. The Predator 3500 has a maximum running watts of 3,000. Michael says he runs the generator about 10 hours per day (a few hours in the morning and then from dinner time until 10pm), the energy generated by the Predator 3500 is calculated below.\nPhotos on the AirBnB web page for the cabin show a photovoltaic installation of what looks like 20 panels. The solar panels probably are between 100 and 200 Watts each for a total of 2000 to 4000 Watts of solar power for the array. Solar is not included in this analysis.\n\n# generator is running 10 hours per day and rated to produce 3000 watts continuously\ngenerator_run_time = 10*ureg.hour\ngenerator_rating = 3000*ureg.watts\ngenerator_energy = generator_run_time*generator_rating\nprint('generator energy: {:s} per day'.format(f\"{generator_energy:,.0f~P}\"))\n\ngenerator energy: 30,000 W·hr per day\n\n\nThe generator produces 30,000 Whr and the fully charged batteries contain 6,000 VAhr. The amount of energy produced by the generator is the amount the family consumes. We can assume that the hydro system will run continuously whereas the gas powered generator is run for ten hours each day.\n\nBut…\n30,000 Whr per day seems really high, so I would question his generator run time per day. Or maybe my math is wrong.\nSetting aside his generator usage, he intended that the hydro system was going to charge his batteries. So, let’s assume that his daily energy needs can be satisfied from his batteries. His batteries hold energy of 6,000 VAhr (taking a short cut and equating Volt Amps to Watts); and 6,000 Whr per day for a very energy efficient family is doable. It follows that he needs about 6,000 Whr per day of hydro energy to match his installed batteries.\n\nprint('hydro energy needed per day: {:s}'.format(f\"{6000*ureg.watt*ureg.hour:,.1f~P}\"))\n\nhydro energy needed per day: 6,000.0 W·hr\n\n\nOver a 24 hour period, the power he needs from the hydro system is:\n\nprint('power needed from hydro generator: {:s}'.format(f\"{(6000*ureg.watt*ureg.hour)/(24*ureg.hour):,.1f~P}\"))\n\npower needed from hydro generator: 250.0 W"
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#battery-charging",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#battery-charging",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Battery charging",
    "text": "Battery charging\nThe batteries are 50Ah 24V LiFePO4 Deep Cycle batteries from Battle Born Batteries with a price tag of \\$849 each. They have an absorption voltage of 28.4V to 29.2V and the recommended charge rate is 50% of the battery or battery bank capacity or less. The absorption time is 15 minutes per 50Ah of capacity at a charging current of 25A per battery. Other specifications: - Float: 26.8V to 27.6V - Battery Charge Temperature Range: 25°F (-3°C) to 135°F (57.2°C) - Battery Discharge Temperature Range: -4°F (-20°C) to 135°F (57.2°C) - Cell Charge Temperature: 32°F (0°C) to 131°F (55°C) - Cell Discharge Temperature: 68°F (20°C) to 140°F (60°C)\nMichael has his batteries connected in parallel, so the current is the sum of the currents that each battery can produce. The hydro generator power curves are shown later, but for now let’s assume that the generator can produce 30 volts at 20 amps. The charging time is calculated below.\n\nbattery_charge_rate = house_battery_capacity/2\nprint('Max allowable battery charge rate: {:s}'.format(f\"{battery_charge_rate:,.1f~P}\"))\n\ngenerator_amps = 20*ureg.amp\nprint('generator charge current: {:s}'.format(f\"{generator_amps:,.1f~P}\"))\nprint('battery charge time: {:s}'.format(f\"{(battery_charge_rate/generator_amps).to('hour'):,.1f~P}\"))\n\nMax allowable battery charge rate: 125.0 A·hr\ngenerator charge current: 20.0 A\nbattery charge time: 6.2 hr\n\n\nIf the hydro system can produce enough energy to charge his batteries in 6 hours, the power produced during the remaining hours could be used to power other items in the house and in the winter keep his batteries from getting too cold.\nPlot battery charge time versus charging current.\n\ncharging_current = np.arange(2,25,0.5)*ureg.amp\n\nplt.plot(charging_current.to('amp'),(battery_charge_rate/charging_current).to('hour'))\n\nplt.grid()\nplt.xlim((1,25))                                          \nplt.ylabel('Charging time, hours')\nplt.xlabel('Charging current, amp')\n\nplt.title('Estimated battery charging time')\nplt.show()\n\n\n\n\nThe plot above shows that for charging currents of less than about 5.2 amps, the hydro generator can’t fully charge his batteries from the empty condition within one 24 hour period. Most likely the family is using battery energy while charging the batteries, so the hydro generator would need to produce about 8.3 amps."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#site-plan",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#site-plan",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Site plan",
    "text": "Site plan\nIn Michael’s videos about the hydro project, I got the impression that he did not have any measured elevation data for his property. He thought he had about 30 feet of elevation from the spring to his property line. This might be enough for a penstock and tail pipe driving a Pelton or inline turbine. For Michael’s waterwheel the head height is approximately the diameter of the waterwheel but, depending on the bucket design and release angle, could be a little less."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#water-source",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#water-source",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Water source",
    "text": "Water source\nThe water source for the dam comes from a spring located a short distance away from the cabin. The spring also provides water to the cabin’s holding tank for household use. While watching Michael’s videos, I did not hear that he has documented historical data for the water flow rate. Historical data is critical for determining the economic viability of the hydro project. Michael indicated that from February to June, there is lots of water flowing from the spring. Over the course of the video series, he commented how the spring wasn’t running like it was while they were building the dam. Since the flow rate seems to be seasonal, the analysis that follows will consider a range of flow rates and not use just one value.\nOne thing to note is that in [8], the water out of the spring was muddy, which could mean that a lot of surface water flows down the hill during periods of heavy rain and is mixing with the spring water. Surface water mixing in with the spring water could compromise the safety of the spring as a source of potable water."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#the-dam",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#the-dam",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "The dam",
    "text": "The dam\nThe purpose of the dam is to capture water from the spring and allow for the flume to sit at the correct height relative to the waterwheel. The dam’s reservoir is an attractive feature for the property and should be a source of enjoyment year around. He said the water was cold (50F), so in the warm summer months it should be refreshing. The dam structure is a concrete retaining wall, with a water depth of about 4.5 feet at the wall. This type of dam can be classified as a concrete gravity dam where the weight of the dam itself keeps it in place against the weight of the water. Videos [2,3] show some of the dam construction and the following observations and comments are provided:\n- The dam’s foundation did not appear to be adequately prepared. Concrete gravity dams are suitable for sites where there is a reasonably sound rock foundation, although a low dam can be built on other soil types. Michael said in the video that Jerry dug down to rock, but this was not apparent. Settling of the foundation and expansion from freezing water from seepage can lead to weakening of the foundation and wall. - The reservoir evacuation method of using a plug in the pipe could have been better thought out. An important consideration is the ability to evacuate (lower or drain) the reservoir in a timely manner. - Interrupting the pour between the footing and the wall created a cold joint, a possible source of leaks. - There was a conspicuous paucity of rebar visible in the construction videos. - The spillway is insufficient. - Head race and flume did not line up with the waterwheel.\nThere were questions in the comment section about the permit for the dam construction. Michael said he didn’t need a permit. I did a little bit of searching on the internet and he seems to be correct. If the water impounded has a surface area of less than fifteen acres, or the height of the dam is less than 35 feet, and is not on a navigable waterway, a permit is not required.\nA small spillway was incorporated into the dam in the center section. The spillway is a hydraulic structure that passes normal (operational) and/or flood flows in a manner that protects the structural integrity of the dam. The spillway appears to be sized incorrectly as evidenced by the dam being over topped in [8].\nThe spillway width is estimated to be 4 feet wide and the depth of the spillway looks like about 3 inches.\n\nspillway_width = 4*ureg.feet # 4 ft\nspillway_water_head = 3*ureg.inch # 3 inches\n\nThe rectangular weir formula is used to calculate the flow through the spillway.\n\\(Q= \\frac {2}{3} \\times C_d \\times (2g)^{1/2} \\times L \\times H^{3/2}\\)\nWhere:\nQ = the total discharge\n\\(C_d\\) = discharge coefficient\ng = acceleration due to gravity\nL = the effective crest length\nH = the total hydraulic head above the spillway\nReference sources for the weir formula are here, here and here.\n\nL = spillway_width.to('meter')\nH = spillway_water_head.to('meter')\nCd = 0.5 # discharge coefficient, a nominal value was chosen, typical values range from 0.4 to 0.6 and depends on a lot of factors.\n\n# need to remove units for calculation, then set units back to m^3/sec\nQ_spillway = ((2/3)*Cd*L.magnitude*2*(g.magnitude)**(1/2)*(H.magnitude)**(3/2))*ureg.meter**3/ureg.sec\nprint('spillway flow rate: {:s}'.format(f\"{Q_spillway.to('gal/min'):.1f~P}\"))\n\nspillway flow rate: 848.6 gal/min\n\n\nThe current spillway can accommodate about 850 GPM, but it should be increased by several inches in head height to accommodate 2000 GPM or more depending on the results of a hydrology study for the local drainage into the reservoir.\n\nUsing the weir formula to estimate flow rate\nThe wier formula can be used to estimate the water flow. In videos [6,7] there looks to be about a half inch deep of water flow through the spillway.\n\nspillway_water_head = 0.5*ureg.inch\n\nL = spillway_width.to('meter')\nH = spillway_water_head.to('meter')\nCd = 0.5 # discharge coefficient, a nominal value was chosen, typical values range from 0.4 to 0.6 and depends on a lot of factors.\n\n# need to remove units for calculation, then set units back to m^3/sec\nQ_spillway = ((2/3)*Cd*L.magnitude*2*(g.magnitude)**(1/2)*(H.magnitude)**(3/2))*ureg.meter**3/ureg.sec\nprint('low season flow rate: {:s}'.format(f\"{Q_spillway.to('gal/min'):.1f~P}\"))\n\nlow season flow rate: 57.7 gal/min\n\n\nTrying to estimate the flow rate from Michael’s videos is admittedly very inaccurate, however it’s the best we can do to substantiate Michael’s claims of 500 to 1000 gallons per minute (GPM) and 300 GPM at other times. Based on the weir calculation using 1/2 inch depth of flow, the flow rate is nowhere near what Michael claims it is during the low season."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#gravitational-power-of-water",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#gravitational-power-of-water",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Gravitational power of water",
    "text": "Gravitational power of water\nThe harvesting of the gravitational energy of water is a proven source of energy. Since a waterwheel is used to extract power from the water, the head height is approximately the diameter of the waterwheel. Water enters at the top of the wheel and the weight of the water causes the wheel to rotate until a point near the bottom of the wheel. So let’s use the diameter of the waterwheel. For the flow rate, we can use a value of 300 GPM since this is what Michael said was the flow rate in [2].\nThe gravitational power of water is given by:\n\\(P = \\rho \\times g \\times Q \\times H\\)\nwhere:\nP = gravitational power of water\nH = height of fall\nQ = flow rate\ng = acceleration due to gravity\n\\(\\rho\\) = density of water\nThe gravitation power of water is easily calculated and Michael should have run his numbers through this formula first thing. However, if he did the math and was overly optimistic about the efficiency of his waterwheel, he could have been deceived. If he looked at a range of efficiencies, he would have seen that things didn’t look so good at the lower end, which could have told him that an in depth and thorough preliminary design was needed. Reference sources for the hydroelectric power equation are here and here.\n\nhead_height = 7*ureg.feet # the estimated diameter of the waterwheel\nprint('head height: {:s}'.format(f\"{head_height:.1f~P}\"))\n# convert to meters and save value in variable H for use in calculations\nH = head_height.to_base_units()\nprint('head height in base units: {:s}'.format(f\"{H:.4f~P}\"))\n\nflow_rate = 300*ureg.gal/(1*ureg.min)\nprint('flow rate: {:s}'.format(f\"{flow_rate:.1f~P}\"))\nQ = flow_rate.to_base_units()\nprint('flow rate in base units: {:s}'.format(f\"{Q:.4f~P}\"))\n\nhead height: 7.0 ft\nhead height in base units: 2.1336 m\nflow rate: 300.0 gal/min\nflow rate in base units: 0.0189 m³/s\n\n\nThe maximum efficiency of overshot waterwheels is listed in [11] as being between 80 and 85% maximum, which is for an optimized design. Michael’s waterwheel has a number of issues, the most serious is the substantial amount of water splashing out of the bucket. So Michael’s waterwheel probably has an efficiency of around 60%. The efficiencies for the generator and charge controller were estimated from literature and catalogs. The efficiency for the generator is reduced to 95% to account for the low operating RPM compared to the rated RPM. The charge controller is assumed to be relatively efficient so a 97% value is used. - \\(\\eta\\) for waterwheel = 60% - \\(\\eta\\) for generator = 95%\n- \\(\\eta\\) for charge controller = 97%\n\n# Gravitational power of water\ngrav_pwr_of_water = (rho*g*Q*H) #.to_base_units().to('watt')\nprint('gravitational power of water: {:s}'.format(f\"{grav_pwr_of_water.to('watt'):.2f~P}\"))\n\neta_waterwheel = 0.6 # for a small waterwheel\neta_generator = 0.95\neta_charge_controller = 0.97\neta_total = eta_waterwheel*eta_generator*eta_charge_controller\nest_pwr = grav_pwr_of_water*eta_total\nprint('electrical power with efficiency factors: {:s}'.format(f\"{est_pwr.to('watt'):.1f~P}\"))\n\ngravitational power of water: 395.92 W\nelectrical power with efficiency factors: 218.9 W\n\n\nThe calculations above provide an indication of the power the hydro system is capable of producing. The estimated electrical power is about 219 watts. If the efficiency of the waterwheel could be improved to 0.7, then about 260 Watts could be generated, which might be sufficient to charge the batteries.\nThe calculations show that the expected power production is rather low and the ability to charge the house batteries is doubtful. Before proceeding with the project, one would need to look at other options. It sounded like Michael really wanted a dam and waterwheel, this being the case, a larger wheel diameter should have been considered in light of these calculations. Michael could have still used the old wheels, and Jerry could have built a larger outer frame to extend the radius. Also, the waterwheel is somewhat narrow compared to similar historical waterwheels, so Jerry could have increased the width."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#estimated-power-production-vs-flow-and-height",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#estimated-power-production-vs-flow-and-height",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Estimated power production vs flow and height",
    "text": "Estimated power production vs flow and height\nThe plot below shows the estimated production of electrical power for a range of flow rates and water head heights (in this case set by the diameter of the waterwheel). Mechanical and electrical efficiencies discussed above are included. The range of flow rates from the spring were talked about in the videos. In the plot below, I have flow rates ranging from 100 to 500 GPM and head height set by waterwheel diameters of 6, 7 and 8 feet.\n\nflow_rate = np.linspace(50,500,10)*ureg.gal/ureg.min # flow rate range in gallons per minute\nhead_height = np.array([6,7,8])*ureg.feet # head height, range in feet\n# convert to metric\nQ = flow_rate.to_base_units()\nH = head_height.to_base_units()\n\nfor i in range(len(head_height)):\n    plt.plot(flow_rate.to('gal/min'),(rho*g*Q*H[i]*eta_total).to('watt'),\n        label = 'water head: {:s}'.format(f\"{H[i].to('feet'):.0f~P}\"))\n\nplt.legend(bbox_to_anchor=(1.4, 1))\nplt.grid()\n\nplt.ylim((0,600))\nplt.xlim((100,500))                                          \nplt.ylabel('electrical power, W')\nplt.xlabel('water flow rate, GPM')\n\nplt.title('Estimated production of hydro power')\nplt.show()\n\n\n\n\nThe chart above clarifies that for small flow rates of less than 300 GPM and small diameter wheels, the possible power output is not sufficient to supply the household using the efficiencies discussed above. Families that conserve their use of electrical power need about 10 kWhr per day (or less, but definitely a lot more if air conditioning is used)."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#the-flume",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#the-flume",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "The flume",
    "text": "The flume\nThe flume carries water from the dam to the top of the waterwheel. By examining the videos, I made some estimates for the flume dimensions. The width of the flume was about the same width of the waterwheel, which makes sense, since you want a nice flow of water into the buckets. The side walls of the flume looked like 2 by 6 inch boards. Looking at [7] when Michael opens the gate, the depth of the water in the channel looks like 1.5 to 2 inches of flow.\nI can use the Manning equation for open channel flow to calculate the flow rate in the flume. Mannings equation is an empirical equation that applies to uniform flow in open channels and is a function of the channel velocity, flow area and channel slope.\n\\(V = \\frac {s^{1/2} \\times R^{2/3}} {n}\\)\nwhere:\nV - water velocity\nn - Manning’s roughness coefficient; unplaned wood = 0.013, wood sides and smooth sheet metal bottom\nR - The channel’s hydraulic radius, calculated by dividing the water flow’s cross-sectional area A by its wetted perimeter P\ns - Slope of the channel’s bottom surface\nExamining Manning’s equation, we can see that the area and slope are directly proportional to the water flow rate, which means an increase in area and slope would increase the water flow rate. On the other hand, the roughness coefficient and the wetted perimeter are inversely proportional to the water flow rate, meaning that increasing their values would decrease the water flow rate.\nThe slope of the flume was not apparent in any of the videos, so I’m using a value of 0.01.\n\nflume_width = 10*ureg.inch  # 10 inches\nwater_depth = 2*ureg.inch # 2 inches\n# convert to metric\nflume_width.ito('meter')\nwater_depth.ito('meter')\nwater_cross_section_area = water_depth*flume_width\nprint('water_cross_section_area = {:s}'.format(f\"{water_cross_section_area:.4f~P}\"))\nwetted_perimeter = 2*water_depth+flume_width\nprint('wetted_perimeter = {:s}'.format(f\"{wetted_perimeter:.4f~P}\"))\nhydraulic_radius = water_cross_section_area/wetted_perimeter\nprint('hydraulic_radius = {:s}'.format(f\"{hydraulic_radius:.4f~P}\"))\nroughness_coefficient = 0.01 # wood sides and smooth sheet metal bottom\nslope = 0.1/100 # slope of the flume\n\n# need to use magnitude of quantity when using fractional exponents\nwater_flume_velocity = (((hydraulic_radius.magnitude)**(2/3)*slope**(1/2))/roughness_coefficient)*ureg.meter/ureg.sec\n\nprint('water_flume_velocity = {:s}'.format(f\"{water_flume_velocity:.4f~P}\"))\nvolumetric_flow_rate = water_flume_velocity*water_cross_section_area\nprint('volumetric_flow_rate = {:s}'.format(f\"{volumetric_flow_rate:.6f~P}\"))\nprint('volumetric_flow_rate = {:s}'.format(f\"{volumetric_flow_rate.to('gal/min'):.2f~P}\"))\n\nwater_cross_section_area = 0.0129 m²\nwetted_perimeter = 0.3556 m\nhydraulic_radius = 0.0363 m\nwater_flume_velocity = 0.3466 m/s\nvolumetric_flow_rate = 0.004472 m³/s\nvolumetric_flow_rate = 70.89 gal/min\n\n\nManning’s equation tells us that about 70 GPM are delivered by the flume to the waterwheel, so we can conclude that the flume is not sized correctly and several more inches of flow depth and/or slope are needed. The Manning formula has accuracies of \\(\\pm \\text{10 to 20%}\\). Reference sources for the Manning’s equation formula are here, here and here.\nPlot flume flow rate over range of slopes and depths.\n\nwater_depth = np.linspace(0.1,6,10)*0.0254 # water depth in flume, in inches, then converted to meters\n\nflume_length = 12*12 # feet converted to inches\nflume_drop = np.array([0.5,2,4,6,8,12,18]) # total drop at far end of flume in inches\nslope = (flume_drop/flume_length)/100 # flume slope, percent\n\nfor i in range(len(slope)):\n    plt.plot(water_depth/0.0254,(((water_depth*flume_width)/(2*water_depth+flume_width))**(2/3)*slope[i]**(1/2))/roughness_coefficient*(water_depth*flume_width)*60/0.003785,\n        label = 'slope: {:.2f}%'.format(slope[i]*100))\n\nplt.legend(bbox_to_anchor=(1.1, 1))\nplt.grid()\n\n#plt.ylim((0,1))\nplt.ylabel('Q, gpm')\nplt.xlabel('flume water depth, inches')\n\nplt.title('range of flume flows vs. depth and slope')\nplt.show()\n\n\n\n\nAs we can see in the plot above, in order for the flume to deliver up to 300 GPM, modifications to the flume are required. The calculations above were done with the flume width set to 10 inches, which is the estimated width of the waterwheel. But to get 300 GPM in this channel would mean that water velocity is going to be too high for the operation of the wheel. I think the flume and waterwheel width are serious flaws in the current design that can’t be overcome by tweaks elsewhere in the system."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#flume-to-waterwheel-interface",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#flume-to-waterwheel-interface",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Flume to waterwheel interface",
    "text": "Flume to waterwheel interface\nThe level of the flume at the water exit point should be very near the top center of the waterwheel. Depending on the velocity of the water there is some amount of horizontal throw and the exact position and the corresponding angle of the entry to the wheel probably is best optimized by trial and error. One thing to note is that historical waterwheels seem to be much wider than Michael’s wheel and the water velocity in the flume is lower."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#the-waterwheel",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#the-waterwheel",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "The waterwheel",
    "text": "The waterwheel\nWaterwheels have a long history and they were a driving force behind the earliest stages of industrialization in Britain. John Smeaton’s scientific investigation of the water wheel led to significant increases in efficiency supplying much needed power for the Industrial Revolution. A historical overview of waterwheels in given in [11] along with a list of overshot waterwheels in current operation which are generating electrical power. Most of these operate with much higher flow rates, on average 6,000 gal/min, which is substantially greater than what Michael has available. The waterwheel operating with the smallest flow rate at 1268 gal/min, is a 13 foot diameter, 2.4 foot wide wheel, with 36 buckets turning at 8 RPM and generating 2.2kW of electrical power. This waterwheel is operated by PI Mitterfellner GMBH, located in Austria.\nMichael’s water wheel was built by Jerry and Michael posted a slide show of the construction in [10]. From an Aesthetic point of view, the waterwheel is very nice. The dimensions of the waterwheel were estimated from the videos and compared to the relative heights of Jerry and Michael’s Dad when they were off loading the wheel. The geometry of the water chamber or bucket was not shown in any detail. In the photo below, it looks like the paddles of the water chamber have a small radial section that connects to a longer board at an angle. The angle of the longer board doesn’t look correct, it needs more tilt.\n\nThe outer radius looks to be about 3.5 feet. The inner radius is about 3 feet. The bucket width looks to be approximately 10 inches wide. The number of buckets appears to be 12. The angles appear such that most of the water will exit the wheel at about 90 degrees measured from top center. Overshot waterwheels are relatively efficient mechanically and are easily maintained. The overshot waterwheel is a slow speed device and usually designed to operate between 5 and 15 RPM. This limits its usefulness as a power source for the generation of electricity because gearbox or pulleys are needed to step up the rotational speed.\nThe diagram below shows the components of the overshot waterwheel. The weight of the water on the downstream chambers causes a torque on the wheel making it rotate.\n\nDiagram of an overshot waterwheel by Malcolm Boura obtained from Wikipedia.\nHistorical waterwheels have a lot more buckets than Michael’s waterwheel. In the diagram above, the paddles that separate the water chambers are at an angle. Depending on the design, the angels vary between 20 and 30 degrees. Some waterwheel designs have a small flat section or have chambers with the scoops optimized to hold the water through a larger range of angles as the wheel rotates.\nThere is a large collection of waterwheels on YouTube. The Egloffstein Franconia waterwheel is connected to a 12kW generator and the Dalby Waterwheel was one of the first hydroelectric plants on the Hood Canal in WA, built in 1923.\nWe can do some simple calculations to estimate the volume of the buckets.\n\nnum_buckets = 12 # number of buckets on the waterwheel\nbucket_width = 10*ureg.inch\nouter_radius = 3.5*ureg.feet\ninner_radius = 3*ureg.feet\n\n# convert dimensions to meters\nbucket_width.ito('meter')\nouter_radius.ito('meter')\ninner_radius.ito('meter')\n\nbucket_vol = (np.pi*outer_radius**2 - np.pi*inner_radius**2)*bucket_width/num_buckets\nprint('bucket volume: {:s}'.format(f\"{bucket_vol.to('gallons'):.2f~P}\") )\n                                    \n# calculate chamber arc, \nbucket_arc = (360/num_buckets)*ureg.degree\nbucket_arc.ito('radian')                                    \nprint('bucket arc: {:s}'.format(f\"{bucket_arc.to('degree'):.1f~P}\") )\n\nbucket volume: 5.30 gal\nbucket arc: 30.0 deg"
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#bucket-fill-level-vs-rpm-for-various-flow-rates",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#bucket-fill-level-vs-rpm-for-various-flow-rates",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Bucket fill level vs RPM for various flow rates",
    "text": "Bucket fill level vs RPM for various flow rates\nThe number of buckets in the waterwheel is low compared to historical waterwheels. The following plot looks at bucket fill percentage versus waterwheel RPM at various flow rates.\n\nflow_rate = np.array([50,100,150,200,300])*ureg.gal/ureg.min # flow rate range in gallons per minute\n# convert to metric\nQ = flow_rate.to_base_units()\n\nwheel_revs_per_sec = np.linspace(2,10,100)*ureg.rpm\nomega = wheel_revs_per_sec.to_base_units()\n\nbucket_arc = (np.pi/num_buckets)*ureg.radian\n\nfor i in range(len(Q)):\n    plt.plot(omega.to('rpm'),((Q[i]*(bucket_arc/omega))/bucket_vol).magnitude*100,label = 'Q={:s}'.format(f\"{Q[i].to('gal/min'):.0f~P}\"))\n\nplt.legend(bbox_to_anchor=(1.5, 1))\nplt.grid()\n\n#plt.ylim((0,10))\nplt.ylabel('bucket % full')\nplt.xlabel('waterwheel RPM')\n\nplt.title('title')\nplt.show()\n\n\n\n\nThe calculations show that the buckets are not being filled to even 50% for RPM’s greater than 5. More buckets are needed to better utilize the range of flow rates expected. The pulley 8:1 system that Michael is using has the waterwheel rotating at 62 RPM, where only about 2% of the bucket capacity will be used at a flow rate of 300 gal/min."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#maximum-rotational-speed",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#maximum-rotational-speed",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Maximum rotational speed",
    "text": "Maximum rotational speed\nThe rotational speed of the waterwheel is limited by the centrifugal force acting on the water in the buckets. The centrifugal force is directed radially outwards from the waterwheel and if the rotational speed is sufficiently high, the water will be flung out of the buckets, since gravity acting towards the center of the earth can’t keep the water contained.\nThe centrifugal force can be calculated as follows:\n\\(F=m \\times \\omega ^{2} \\times r\\)\nwhere:\nF = the centrifugal force\nm = the mass of the object, in this case a unit mass of water\nr = the radius of rotation\n\\(\\omega\\) = the angular velocity\nThe plot below shows the force on a unit mass of water due to gravity compared to the centrifugal force versus rotational speed.\n\navg_water_radius = (outer_radius + inner_radius)/2\nwheel_revs_per_sec = np.linspace(2,40,10)*ureg.rpm\nomega = wheel_revs_per_sec.to_base_units()\n\nm = 1*ureg.kg\nr = avg_water_radius\nffg = np.ones(len(omega))*(m*g)\n\nplt.plot(omega.to('rpm'),ffg.to('force_pound'),label='force from gravity')\nplt.plot(omega.to('rpm'),(m*r*omega**2).to('force_pound'),label='force from rotation')\n\nplt.legend(bbox_to_anchor=(1.5, 1))\nplt.grid()\n\n#plt.ylim((0,10))\nplt.ylabel('force, lbs')\nplt.xlabel('waterwheel rpm')\n\nplt.title('Force of gravity and centrifugal force')\nplt.show()\n\n\n\n\nIn order to keep the water in the buckets, the rotational speed of the waterwheel should be kept to about 10 RPM. Later in this notebook, the pulley system and generator is analyzed. Michael thought he needed 500 RPM on the generator shaft and 62 RPM on the waterwheel. At this RPM there is a large centrifugal force on the water in the buckets and much of the water would be flung out."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#run-test",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#run-test",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Run test",
    "text": "Run test\nTwo frames were captured from [6] and combined into the image below. On the left is the waterwheel running with the pulleys connected to the generator and on the right is the display of the Midnite charge controller, which is indicating that the input is 40.3 volts at 40 Watts. There is a large amount of splash over, indicating not all of the water is being captured.\n\nI’m not familiar with the operation of the charge controller, so I can’t comment on what’s being shown on the display. Michael did say that his batteries were not being charged under this condition.\nLooking at the photo on the left, I would say that there are three things happening. The paddles are slapping the flow of water coming off the flume, the stream is misaligned and water is being flung out of the buckets. Clearly, some adjustments are needed. The waterwheel is running somewhat as a turbine, where the wheel is rotated by both the velocity of the flow and of the weight of the water in the buckets.\nThe flume water velocity and flow rate were calculated above:\n\nprint('water_flume_velocity = {:s}'.format(f\"{water_flume_velocity:.4f~P}\"))\nprint('volumetric_flow_rate = {:s}'.format(f\"{volumetric_flow_rate:.6f~P}\"))\n\nwater_flume_velocity = 0.3466 m/s\nvolumetric_flow_rate = 0.004472 m³/s\n\n\n\nV = water_flume_velocity\nQ = volumetric_flow_rate\n\nThe power from the velocity of the water in the flume is calculated below:\n\nprint('power in the water flow from the flume: {:s}'.format(f\"{(0.5*(rho*Q)*V**2).to('watt'):.2f~P}\"))\n\npower in the water flow from the flume: 0.27 W\n\n\nThere doesn’t appear to be much water captured by the buckets and some of it looks like it’s being flung out. So let’s say half of the flow is captured and drops a height of 2 feet before being forced out by centrifugal force. The power from gravity of the water in the bucket is:\n\nH = 2*ureg.foot\nQ = volumetric_flow_rate/2\nprint('power from gravity of water: {:s}'.format(f\"{(g*rho*Q*H).to('watt'):.2f~P}\"))\n\npower from gravity of water: 13.36 W\n\n\nTaking both of these together, it’s clear that not much power is being generated by the waterwheel when operated this way.\nIf I were to speculate as to why the charge controller is reading 40 Watts on the display, I would say the charge controller is hunting for the maximum power point and momentarily seeing a bit of energy stored in the rotational inertia of the wheel. When the electrical load is reduced and the wheel is allowed to spin faster, the charge controller sees the voltage rise and applies some load. When the load is applied, the wheel slows down and the voltage drops along with the power. The charge controller reduces the load and this oscillation continues. I don’t know where in this cycle the displayed power is read."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#generator-and-pulley-system",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#generator-and-pulley-system",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Generator and pulley system",
    "text": "Generator and pulley system\nThe generator used in this hydro system is a 2000 Watt Freedom II Hydro PMG Permanent Magnet Generator from Missouri Wind And Solar, with a price tag of \\$969. This generator has a 28 magnet dual rotor and there are two stator windings. So it’s actually two generators on the same shaft. There are two, three phase outputs from the generator (6 wires total) and two rectifier blocks are used in Michael’s system. Six conductors were used to bring the power from the generator to the house. It was not clear in any of the videos if Michael had the output of the rectifier blocks wired in series or parallel (I think a series connection would be the proper connection as the input to the charge controller). According to Missouri Wind And Solar, the generator will produce 12 volts at 266 RPM, 24 volts at 500 RPM and 48 volts at 900 RPM. A specification sheet for the generator is not available, but Missouri Wind And Solar provided answers to my questions via emails. The voltages versus RPM are voltages under load and the maximum power of 2000 Watts is produced between 1400 and 1500 RPM.\nThe plot below shows the under load voltage versus RPM, with an extrapolated data point at 1500 RPM of 80 volts and 2000 Watts. The calculations that follow assume that the generator can be made to run at 500 RPM. I don’t think this is possible, since the calculations above show the waterwheel is generating about 220 Watts of power and the flow rate in the flume is insufficient at only 70 GPM. I think the generator needs a closer look since it’s being operated at about 30% of the advertized full power speed. The Njorgen 105/34, for example, is a better choice for a waterwheel. The Njorgen 105/34 will produce 5kW at 400 Volts with 150 RPM, according to the datasheet. DIY’ers are making axial flux alternators for windmills and a tech savvy maker could design and construct a generator optimized for the low RPM of a waterwheel (an idea for a future Jupyter notebook).\n\ngen_volts = np.array([12,24,48])\ngen_rpm = np.array([226,500,900])\n\ny = gen_volts\nx = gen_rpm\nm, c = np.polyfit(x, y, 1)\nprint('generator voltage vs rpm slope: {:.2f}'.format(m))\nprint('generator voltage vs rpm intercept: {:.2f}'.format(c))\n\ngen_rpm_plt = np.linspace(200, 1500, 10)\nplt.plot(gen_rpm,gen_volts,'ok',label = 'data points provided by the vendor')\nplt.plot(gen_rpm_plt,m*gen_rpm_plt + c, '-b',label='least squares fit')\nplt.plot(1500,80,'or',label = '2000 Watts at 1500 RPM')\n\nplt.legend(bbox_to_anchor=(1.7, 1))\nplt.grid()\n\nplt.ylim((0,100))\nplt.ylabel('generator voltage, under load')\nplt.xlabel('RPM')\n\nplt.title('Generator voltage vs RPM')\nplt.show()\n\ngenerator voltage vs rpm slope: 0.05\ngenerator voltage vs rpm intercept: -1.21\n\n\n\n\n\nThe plot below is the estimated generator power as a function of RPM assuming power is linear with RPM and including a derating factor at lower RPM.\n\ngen_eff_high = 1.0 # derating value at high end of RPM range\ngen_eff_low = 0.7 # derating value at low end of RPM range\n\n# generate power derating vs RPM\nm1, c1 = np.polyfit([226,1500], [gen_eff_low,gen_eff_high], 1) # range of RPMs and derating\n\ngen_pwr = np.array([2000*226/1500,2000])\ngen_rpm = np.array([226,1500])\n\ny = gen_pwr\nx = gen_rpm\nm2, c2 = np.polyfit(x, y, 1)\n\ngen_rpm_plt = np.linspace(226, 1500, 10)\nplt.plot(gen_rpm_plt,(m1*gen_rpm_plt + c1)*(m2*gen_rpm_plt + c2), '-',label='generator pwr w/ rpm derating')\n\nplt.plot(1500,2000,'or',label = '2000 Watts at 1500 RPM')\nplt.plot(900,(m1*900 + c1)*(m2*900 + c2),'^',label = '{:.0f} Watts at 900 RPM'.format((m1*900 + c1)*(m2*900 + c2)))\nplt.plot(500,(m1*500 + c1)*(m2*500 + c2),'^',label = '{:.0f} Watts at 500 RPM'.format((m1*500 + c1)*(m2*500 + c2)))\nplt.plot(226,(m1*226 + c1)*(m2*226 + c2),'^',label = '{:.0f} Watts at 226 RPM'.format((m1*226 + c1)*(m2*226 + c2)))\n\nplt.legend(bbox_to_anchor=(1.7, 1))\nplt.grid()\n\n#plt.xlim((0,1600))\n#plt.ylim((0,2200))\nplt.ylabel('generator power, watts')\nplt.xlabel('RPM')\n\nplt.title('Generator power vs RPM')\nplt.show()\n\n\n\n\nDividing power by voltage we can get the current supplied to the load.\n\ngen_rpm_plt = np.linspace(226, 1500, 50)\n\nplt.plot(gen_rpm_plt,(m1*gen_rpm_plt+c1)*(m2*gen_rpm_plt+c2)/(m*gen_rpm_plt+c), '-',label='generator current, Amps')\n\nplt.plot(1500,(m1*1500+c1)*(m2*1500+c2)/(m*1500+c),'or',label = '{:.1f} Amps at 1500 RPM'.format((m1*1500+c1)*(m2*1500+c2)/(m*1500+c)))\nplt.plot(900,(m1*900+c1)*(m2*900+c2)/(m*900+c),'^',label = '{:.1f} Amps at 900 RPM'.format((m1*900+c1)*(m2*900+c2)/(m*900+c)))\nplt.plot(500,(m1*500+c1)*(m2*500+c2)/(m*500+c),'^',label = '{:.1f} Amps at 500 RPM'.format((m1*500+c1)*(m2*500+c2)/(m*500+c)))\nplt.plot(226,(m1*226+c1)*(m2*226+c2)/(m*226+c),'^',label = '{:.1f} Amps at 226 RPM'.format((m1*226+c1)*(m2*226+c2)/(m*226+c)))\n\nplt.legend(bbox_to_anchor=(1.7, 1))\nplt.grid()\n\nplt.ylim((19,26))\nplt.ylabel('generator current, under load')\nplt.xlabel('RPM')\n\nplt.title('Generator current vs RPM')\nplt.show()\n\n\n\n\nAssuming that the 500 RPM point is close to the operating point, 20 Amps is available for charging.\nAs described above, I assumed that generator power was linear with RPM and that generator current could then be calculated from the voltage and power with a derating for the lower RPM. I think these calculations are fine, given the limited performance data available for the generator. Also, I wonder about the test conditions for the 12 volts at 266 RPM, 24 volts at 500 RPM and 48 volts at 900 RPM data. 12, 24, and 48 are nominal battery voltages and perhaps the RPM’s quoted are the minimum RPM required to charge 12, 24, or 48 volt batteries.\nI started to derive a set of curves for voltage versus power at select RPM’s; and I found that I was making assumptions on top of assumptions in order to derive the performance curves. So the calculations above have the least amount of assumptions, which is only to assume the relations are linear. In my attempt to derive the generator performance curves, I got the feeling that this generator is not well suited for waterwheel applications. If I were actually doing this hydro project, and I already had the generator in hand having spent about a thousand dollars for it, before proceeding I would build a small generator test stand and collect some measured performance data. I suppose that the non-availability of a datasheet or performance curves should have been a red flag.\n\ngenerator_pwr = 510*ureg.watt\ngenenerator_rpm = 500*ureg.rpm\n\n# convert to base units\ngenerator_pwr.ito_base_units()\ngenenerator_rpm.ito_base_units()\n\n# calculate the generator shaft torque\ngen_tau = generator_pwr/genenerator_rpm\nprint('generator shaft torque, in base units: {:s}'.format(f\"{gen_tau.to_base_units():.1f~P}\"))\nprint('generator shaft torque: {:s}'.format(f\"{gen_tau.to('foot * force_pound'):.1f~P}\"))\n\ngenerator shaft torque, in base units: 9.7 kg·m²/rad/s²\ngenerator shaft torque: 7.2 ft·lbf\n\n\nAs calculated above the generator needs 7.2 ft·lbf of torque at 500 RPM to charge the house batteries."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#pulley-system",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#pulley-system",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Pulley system",
    "text": "Pulley system\nThe purpose of the pulley system is to transform the waterwheel rotations to the generator shaft. Michael said that Jerry did the calculations for the wheel sizes for the pulleys, which he said was a one to eight ratio. The pulley wheel diameters were estimated by looking at the video and doing a relative comparison against the final generator pulley which is advertised to be 3 inches in radius. The formula for pulleys is:\n\\(D_1 \\times RPM_1 = D_2 \\times RPM_2\\)\nwhere \\(D_1\\) and \\(D_2\\) are the pulley diameters and \\(RPM_1\\) and \\(RPM_2\\) are the rotations per minute.\nStarting at the generator, d1 and d2 are the pulleys on the generator belt and rpm1 is what is needed on the generator shaft.\n\nd1 = 3 # diameter of pulley on the generator\nd2 = 8 # diameter of large pulley on middle axle\nrpm1 = 500 # needed at the generator\nrpm2 = d1*rpm1/d2 # at the big wheel on 2nd shaft\nprint('RPM on the middle axle: {:0.1f}'.format(rpm2))\n\nRPM on the middle axle: 187.5\n\n\nThe middle axle has two pulleys, d2, from above and d3; d3 is connected by a second belt to the pulley on the waterwheel shaft, d4.\n\nd3 = 4 # diameter of small pulley on middle axle\nd4 = 12 # diameter of pulley on waterwheel\nrpm3 = d3*rpm2/d4\nprint('RPM on the waterwheel axle: {:0.1f}'.format(rpm3))\nprint('total pulley ratio: {:.1f}:1'.format((d2/d1)*(d4/d3)))\nprint('Torque on the waterwheel axle: {:s}'.format(f\"{(gen_tau*(d2/d1)*(d4/d3)).to('foot * force_pound'):.1f~P}\"))\n\nRPM on the waterwheel axle: 62.5\ntotal pulley ratio: 8.0:1\nTorque on the waterwheel axle: 57.5 ft·lbf\n\n\nThe total ratio for the pulleys is 8:1 and agrees with Michael’s comments in the video. A waterwheel RPM of 62 is needed to spin the generator at 500 RPM, which is too high for the waterwheel. Water will not stay in the buckets at this RPM. Let’s say 10 RPM is more appropriate for the waterwheel, then a pulley or gear ratio of 100 is needed. At a 100:1 ratio, gears are needed, so a planetary type gearbox might work."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#electrical-cable",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#electrical-cable",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Electrical cable",
    "text": "Electrical cable\nElectrical power cables are needed to run from the generator located at the waterwheel up to the cabin, where the charge controller and the batteries are located. The generator makes 3 phase AC and at 400 RPM, the generater produces, let’s say 300W. The cable according to Michael is 10-3, which I take to mean three conductors of 10 AWG wire. Two sets of cables were buried in the ground. Michael said he purchased the cable form Missouri Wind and Solar, but I could not find on their website any information about the type of cable he might have purchased.\nUsing cable specifications from Alpha Wire, M3903 was chosen as a type of cable that has specifications typical for this application. This cable is available in configurations of three or six conductors with multi stranded (7/.0385) 10 AWG bare copper conductors. The conductor DC resistance is 1.03 ohms/1000ft at 20°C (68°F), nominal. The cable has a voltage rating of 600 VRMS, is sunlight resistant and is suitable for direct burial.\nThe voltage drop per conductor for 220 feet of cable run, operating at 30 volts and carrying 500 Watts of power is calculated below. Since the AC frequency of the generator is low, I’m ignoring the AC impedance of the cable and using the RMS voltage to calculate the voltage drop due to the DC resistance in the cable. 30 volts was used in the calculations, since this is probably close to the operating point of the charge controller.\nThe generator frequency is: frequency = RPM * Number of magnetic poles (P) / 120.\n\n# generator AC frequency\nRPM = 500  # RPM\nP = 28*2 # 28 magnets, so number of poles is twice the number of magnets\nprint('generator AC frequency: {:.1f} Hz'.format(RPM*P/120))\n\ngenerator AC frequency: 233.3 Hz\n\n\nAs long as the AC frequency is under 500 Hz and the cable run is less than 500 ft, I would ignore the AC impedance when doing back of the envelope or rough order of magnitude type calculations that engineers like to do. As part of a detailed design, I definitely would look at the impedance of the cable (capacitance 32 pF/ft at 1 kHz and inductance 0.16 μH/ft) to calculate the voltage drop.\nThe resistance per phase for the 220 foot cable run is calculated below.\n\nresistance_per_phase = 1.03*ureg.ohms/(1000*ureg.foot)*220*ureg.feet\nprint('resistance per phase: {:s} at 20 degrees C for 220 ft'.format(f\"{resistance_per_phase:.3f~P}\") )\n\nvolts_per_phase = 30*ureg.volt # rms\nprint('volts per phase: {:s}rms'.format(f\"{volts_per_phase:.1f~P}\") )\n\ncurrent_per_phase = (500*ureg.watt/6/volts_per_phase).to_base_units() # using 6 since there are 6 wires in the system\nprint('current per phase: {:s}rms'.format(f\"{current_per_phase:.2f~P}\") )\n\nvoltage_drop_per_phase = (resistance_per_phase*current_per_phase).to('volts')\nprint('voltage drop per phase: {:s}rms'.format(f\"{voltage_drop_per_phase:.2f~P}\") )\n\nresistance per phase: 0.227 Ω at 20 degrees C for 220 ft\nvolts per phase: 30.0 Vrms\ncurrent per phase: 2.78 Arms\nvoltage drop per phase: 0.63 Vrms\n\n\nA voltage drop of about 0.6 volts to the house when the generator is developing 500 W, is acceptable."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#charge-controller",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#charge-controller",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Charge controller",
    "text": "Charge controller\nThe charge controller is a Midnite Classic 200 which has a price tage of about \\$650, depending on the vendor. The charge controller provides maximum power point tracking (MPPT). MPPT’ers are intelligent DC to DC converters and regulate current and voltage to safely charge batteries.\nI believe that the Midnite Classic 200 needs at least the battery absorption voltage from the generator to charge the batteries. I did not read in the manual that the Midnite Classic 200 was capable of boosting the voltage from the generator to the absorption voltage. This probably means that the generator RPM needs to be a bit higher than 500 RPM.\nI would talk to customer support at Battle Born Batteries and get recommendations for setting up the Midnite Classic 200 for their batteries. The set up for a waterwheel based hydroelectric system might be a little out of the ordinary. Based on the little I know about LiFePO batteries, I would think that you would set up the charge controller to charge in a constant current mode for the required absorption time which the controller would adjust based on available current and state of charge. Also the maximum power point tracking time constant should be set to sweep in intervals of tens of minutes not seconds like it would do for a solar system.\nOnce the hydro generator performance curves are obtained, either from the generator’s manufacture or measured from a test stand, a more detailed evaluation can be performed. I suspect that the operating point for Michael’s hydro system lays on the part of the curve which is above the maximum power point at the operating RPM, since his hydro system will struggle to make the absorption voltage, so MPPT would not be useful."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#after-thoughts",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#after-thoughts",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "After thoughts",
    "text": "After thoughts\nHaving gone through these calculations and doing some reading and research, I realize that there are a lot of trade offs to be made when looking for an optimal solution. I think the flume to battery path needs to be looked at as a system and not as individual parts to be connected together. Reverse engineering Michael’s hydroelectric system was a fun learning experience. The cost of the major components, batteries, generator and charge controller were about \\$7000 dollars. I have no idea how much the dam costs. I certainly would not invest thousands of dollars in a project without a feasibility study and an analysis of alternatives. The extent of Michael’s planning seemed to consist of nothing more than wishful thinking.\nAll the successful micro hydroelectric installations I’ve read about while researching this topic have common characteristics: - a conservative estimated power production of at least 1kW - a return on investment (ROI) not exceeding a few years\nPlanning for micro hydroelectric projects should consider both engineering and economics. If Michael was going to build the dam anyway, and he really wanted a waterwheel, regardless of the ROI, he should have looked into building the largest diameter waterwheel that could have been accommodated at the site, either by excavating a tail race or by moving the supports down stream a bit. A 15 foot diameter wheel could have provided about 1kW from February to June and maybe 250 Watts during the dry season. But a wheel with a 15 foot diameter is big and would have to be assembled on site. Now you’re talking about something requiring planning and forethought. The toy wheel that Jerry built was hopeless from the beginning."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#recommendations-provided-by-joe-malovich",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#recommendations-provided-by-joe-malovich",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Recommendations provided by Joe Malovich",
    "text": "Recommendations provided by Joe Malovich\nA YouTuber named Joe Malovich provided comments and recommendations in [9]. I’ve summarized some of his suggestions:\n- reduce bucket dept - extend mill race past existing stand and get a larger dia wheel - use the hydro function on the charge controller - size of generator - 200 GPM at 7 ft = 250 watts - get solar - conduct due diligence on Missouri Wind and Solar - change to a Pelton system or in-line turbine\nJoe has an interesting set of videos documenting his experiments with micro hydro power here and here."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#system-redesign",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#system-redesign",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "System redesign",
    "text": "System redesign\nI’ll venture down this rabbit hole at some point in the future. I’ll look at two options: 1. Redesign the waterwheel to have larger diameter and width, keep the RPM to about 10 2. Look at a penstock with a Pelton or in-line turbine\nA company called Waterwheel Factory makes waterwheels at their manufacturing facility in Franklin, North Carolina. Prices for a 7 to 10 foot diameter wheel are \\$6K to \\$12K and a 11 to 15 foot diameter wheel are \\$12K to \\$20K.\nOne of the disadvantages of a waterwheel is the low rotational speed which requires gears or pulleys to step up the rotations to a speed required by the generator. Low RPM alternators are not really that low, being 200 to 500 RPM versus the 1500 to 2000 RPM of a normal alternator.\nRunning the calculations with the following efficiencies: - \\(\\eta\\) for waterwheel = 70% - \\(\\eta\\) for generator = 95%\n- \\(\\eta\\) for charge controller = 99%"
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#estimated-power-production-vs-flow-and-height-1",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#estimated-power-production-vs-flow-and-height-1",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "Estimated power production vs flow and height",
    "text": "Estimated power production vs flow and height\n\neta_waterwheel = 0.7 # for a small waterwheel or turbine\neta_generator = 0.95\neta_charge_controller = 0.98\neta_total = eta_waterwheel*eta_generator*eta_charge_controller\n\nflow_rate = np.linspace(50,500,10)*ureg.gal/ureg.min # flow rate range in gallons per minute\nhead_height = np.array([10,15,20,25,30])*ureg.feet # head height, range in feet\n# convert to metric\nQ = flow_rate.to_base_units()\nH = head_height.to_base_units()\n\nfor i in range(len(head_height)):\n    plt.plot(flow_rate.to('gal/min'),(rho*g*Q*H[i]*eta_total).to('watt'),\n        label = 'water head: {:s}'.format(f\"{H[i].to('feet'):.0f~P}\"))\n\nplt.legend(bbox_to_anchor=(1.4, 1))\nplt.grid()\n\nplt.ylim((0,1000))\nplt.xlim((100,500))                                          \nplt.ylabel('electrical power, W')\nplt.xlabel('water flow rate, GPM')\n\nplt.title('Estimated production of hydro power')\nplt.show()\n\n\n\n\nThe green line is for 20 feet of water head, which appears sufficient to produce the minimum power needed at minimum water flow."
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#references",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#references",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "References",
    "text": "References\n\nLoftis Party of Six: YouTube, Webpage, Facebook, AirBnB\nBuilding A Dam for Hydroelectric Power - Part 1 - Off Grid Cabin - EP #22, Jul 12, 2020\nFinishing the Dam @ the Off Grid Cabin - Part 2 / EP #23, Jul 16, 2020\nThe Dam is Holding Water!!! & The Off Grid Cabin gets WeBoost / EP #24, Jul 19, 2020\nInstalling the Water Wheel for HYDRO POWER @ the Off Grid Cabin!! EP #25, Jul 26, 2020\nDid we MAKE HYDRO POWER Using Our Waterwheel for Self Reliant Off Grid Living?, Jan 17, 2021\nPossible Hydro Power Solutions for our Off Grid Cabin? A BIG THANKS TO YOU!!, Jan 24, 2021\nIs Our DAM SAFE??? What Happened After a BIG RAIN 🌧!!!, Apr 11, 2021\nFixed Waterwheel Suggestions for Loftis Party of Six - weROAM, Jan 26, 2021,\nMaking old Wagon Wheels into a Water Wheel - Slideshow\nEmanuele Quaranta, Roberto Revelli, Gravity water wheels as a micro hydropower energy source: A review based on historic data, design methods, efficiencies and modern optimizations, Renewable and Sustainable Energy Reviews, Volume 97, 2018"
  },
  {
    "objectID": "Overshot waterwheel hydroelectric power system analysis.html#youtube-comments",
    "href": "Overshot waterwheel hydroelectric power system analysis.html#youtube-comments",
    "title": "Overshot waterwheel hydroelectric power system analysis",
    "section": "YouTube comments",
    "text": "YouTube comments\nI read through quite a few of the comments. Most were supportive and wished Michael success. A handful said only a small amount of power could be harvested from his hydro system. Someone who claimed to be an engineer said that the dam didn’t need any rebar. Others said he needed to switch to AC because everybody knows that DC can’t go as far. One commenter said that beavers build bigger dams and they don’t use rebar, mud and sticks are sufficient.\nI would say that about 90% of the comments which addressed technical topics were absolutely ridiculous and made by people with no science or technical training. Some who had hands on experience offered advice to do what they did because that was what worked for them. But I don’t think any of them had experience with a waterwheel based hydroelectric systems.\nMy favorite comments where these:\nFrom [9], lefou lefoou wrote:\nWow, kudos my friend. You’re very brave with your build and pray strategy. I love it. As you have discovered, what you started is a journey. Now for a build like this one, your initial results are absolutely normal, anything else would of been a miracle. But you did make a huge mistake, and I do believe you realized it. Your biggest mistake was that you did seem to accept defeat in your last video. If you ask me you might have stumbled across the best marketing idea I have seen on YouTube in a long time. When you said in this video that it was strange that most people listen to videos to learn things and in this case your viewers are teaching you. I garentie that if you make this the theme of your YouTube channel and you do it strategically and well you are going to annihilate the YouTube algorithm. The viewers that follow you could become so invested in your success they would become what we call superfans and you would have engagement tru the roof. I would suggest the book ( superfans by pat flynn). Now I’m the complete opposite of you, I dabble in everything, real estate, homesteading, new energies, marketing, etc.. etc.. I learn everything and do nothing. But I really do think you should talk to a really good marketer about this reverse learning YouTube channel idea, I think this could be a gold mine, if done right. I will definitely be following you, I’m really curious where you go from here. Don’t hesitate to reach out. You’re a very interesting fellow.\nFrom [6], Kris Harbour Natural Building wrote:\nHi, One of your subscribers sent me over here to maybe offer some advice. The first thing i notice is you are trying to get to 24v. really you need to be shooting for twice that unloaded as once you load it it will slow down. I think this is the problem you are having as far as voltage. so you need to be looking to get the Alternator up to closer to 50v unloaded or you will just loose all the voltage as soon as its loaded.\nAs far as expectations go. I wouldn’t be counting on more than a few watts from that at the flow. in fact you might even struggle to overcome the losses to get the the voltage at that flow. A water wheel is only about 50% efficient at best and that amount of water wont do much. So in summery. you need more water or more head and to try and get a unloaded free spin voltage of at least twice as much as your system needs so when you load the wheel it will be slowed down to 24v. I see from the clip of it running at 50w the controler had loaded the waterwheel to 40v. That controler isnt really designed for that kind of system. That is the reason it was jumping around when you had 30-50w coming in. It was trying to find the maximum power point. The controler itself uses quite a bit of power just to run and is quite a big loss that you dont need. If you get the wheel to free spin at 50v and then direct connect to your batteries you will take away that loss and as long as your cable is sized properly then you wont loose to much in the cable. That would mean it would not be controlled and could overcharge the batteries but a simple disconnect solenoid would work for that to just cut the incoming power once a set voltage is reached as it would not cause your wheel any problems to free spin. Something like this would work just fine. https://www.ebay.co.uk/itm/10-000-WATT-Battery-Dump-Controller-SOLAR-Wind-Regulator-G4-440-24-VOLT-N/232747971845?hash=item3630dc3505:g:3MsAAOSwZula4ovW The midnite controler is way overkill for a system like that. put some solar panels on that controler and put it to a better use.\nFrom what i can see you only have about 1m of head so here are some numbers for you. After losses in the wheel and cable you would need 25 liters per second (appox 400 gpm) to make 100w at your batteries. And that would be at a system efficiency of 50% witch would be on the high side for a wheel like that.\nAll is not lost with it. small amounts of power over a long period of time are not at all worthless. But you have to keep efficiency in mind so you dont loose it all. So big cables, as few bearings as possible as few belts as possible as few pulleys as possible. And as few controllers as possible. every time you move a part you loose watts of power. But the main issue is lack of flow and losses in the wheel. I would need to know your average flows but it looks to me like with some modifications and changes, like wheel design, cable sizes, and direct drive. you could expect to get a some power from it But unless you have very large flows it really wont ever power the cabin. But even 20w 24h a day is almost 0.5kwh so it could be worth it but i would need some numbers from you.\nThe dam you have and the setup looks like it could run a low head turbine as apposed to a water wheel and i would be happy to advise on that but i would need you to monitor the flows for at least a year first to make sure it was viable and to size it properly. It dose look like that water source goes up higher on your land? how high up dose it go while still on your property? you might have more luck taking the water from there in a pipe down to a small turbine. But again i would need to know the head anf flow number to help with that.\nIf you want any help with it feel free to email me about it at Diyharbor@gmail.com"
  },
  {
    "objectID": "FRT analysis.html",
    "href": "FRT analysis.html",
    "title": "Two amplifier RIAA phono preamp",
    "section": "",
    "text": "Last update: 7 Apr 2023\nannual report: 2022\nshare price data: 4/7/2023 \\$97.74, beta 1.22\n\n\nThis notebook was developed to use as a template to analyze dividend paying companies as potential investments. This company was selected for analysis because it is on the Dividend Aristocrat list and passes the quick look tests.\n\n\n\nFederal Realty is a recognized leader in the ownership, operation and redevelopment of high-quality retail-based properties located primarily in major coastal markets from Washington, D.C. to Boston as well as San Francisco and Los Angeles.\nThe stated primary business objective is to own, manage, acquire and redevelop a portfolio of high quality retail focused properties that will: - provide increasing cash flow for distribution to shareholders - generate higher internal growth than the shopping center industry over the long term - provide potential for capital appreciation - protect investor capital\nThe company specializes in the ownership, management, and redevelopment of high quality retail and mixed-use properties located primarily in densely populated and affluent communities in strategically selected metropolitan markets in the Northeast and Mid-Atlantic regions of the United States, as well as in California and South Florida. As of December 31, 2020, the company owned or had a majority interest in community and neighborhood shopping centers and mixed-use properties which are operated as 101 predominantly retail real estate projects comprising approximately 23.4 million square feet.\nSector(s): Real Estate\nIndustry: REIT—Retail\n\n\n\nRecommendation: buy\nFollow the link to the Conclusion.\n\n\n\nFederal Realty Investment Trust is a real estate investment trust that invests in shopping centers in the Northeastern United States, the Mid-Atlantic states, California, and South Florida.\nRevision history:\n- 1/10/2022: Copied from VZ notebook and reorganized - Feb 2022: updated quick look, reorganized flow of calculations, corrected usage of financial rates, organized end sections - 23 Mar 2022: Cleaning up financial data spreadsheet. Removed NAIC tab. Removed duplicate reveneu data. - 27 Mar 2022: MFG template copied from BMY - 24 Apr 2022: MFG template copied and modified for REIT, FRT is analyized in the REIT-template - 3 May 2022: replaced np.linalg.lstsq with np.polyfit in NAIC forecast, added Future forecast based on historical data notes, Dilution notes, decision model - 4 May 2022: saved as BANK-template analysis\n\n\n\nThe following sections of this notebook contain the financial analysis for the company.\nContents \n\nStock screener results\nLoad financial spreadsheet\nDiscounted cash flow analysis, baseline\nDCF Scenarios\nNACI stock selection guide analysis\nFuture stock price\nDividend payout\nManagement performance\nDecision model\nConclusion\nNotes\nReferences"
  },
  {
    "objectID": "FRT analysis.html#abstract",
    "href": "FRT analysis.html#abstract",
    "title": "Two amplifier RIAA phono preamp",
    "section": "",
    "text": "This notebook was developed to use as a template to analyze dividend paying companies as potential investments. This company was selected for analysis because it is on the Dividend Aristocrat list and passes the quick look tests."
  },
  {
    "objectID": "FRT analysis.html#introduction",
    "href": "FRT analysis.html#introduction",
    "title": "Two amplifier RIAA phono preamp",
    "section": "",
    "text": "Federal Realty is a recognized leader in the ownership, operation and redevelopment of high-quality retail-based properties located primarily in major coastal markets from Washington, D.C. to Boston as well as San Francisco and Los Angeles.\nThe stated primary business objective is to own, manage, acquire and redevelop a portfolio of high quality retail focused properties that will: - provide increasing cash flow for distribution to shareholders - generate higher internal growth than the shopping center industry over the long term - provide potential for capital appreciation - protect investor capital\nThe company specializes in the ownership, management, and redevelopment of high quality retail and mixed-use properties located primarily in densely populated and affluent communities in strategically selected metropolitan markets in the Northeast and Mid-Atlantic regions of the United States, as well as in California and South Florida. As of December 31, 2020, the company owned or had a majority interest in community and neighborhood shopping centers and mixed-use properties which are operated as 101 predominantly retail real estate projects comprising approximately 23.4 million square feet.\nSector(s): Real Estate\nIndustry: REIT—Retail"
  },
  {
    "objectID": "FRT analysis.html#bottom-line-up-front",
    "href": "FRT analysis.html#bottom-line-up-front",
    "title": "Two amplifier RIAA phono preamp",
    "section": "",
    "text": "Recommendation: buy\nFollow the link to the Conclusion."
  },
  {
    "objectID": "FRT analysis.html#company-description",
    "href": "FRT analysis.html#company-description",
    "title": "Two amplifier RIAA phono preamp",
    "section": "",
    "text": "Federal Realty Investment Trust is a real estate investment trust that invests in shopping centers in the Northeastern United States, the Mid-Atlantic states, California, and South Florida.\nRevision history:\n- 1/10/2022: Copied from VZ notebook and reorganized - Feb 2022: updated quick look, reorganized flow of calculations, corrected usage of financial rates, organized end sections - 23 Mar 2022: Cleaning up financial data spreadsheet. Removed NAIC tab. Removed duplicate reveneu data. - 27 Mar 2022: MFG template copied from BMY - 24 Apr 2022: MFG template copied and modified for REIT, FRT is analyized in the REIT-template - 3 May 2022: replaced np.linalg.lstsq with np.polyfit in NAIC forecast, added Future forecast based on historical data notes, Dilution notes, decision model - 4 May 2022: saved as BANK-template analysis"
  },
  {
    "objectID": "FRT analysis.html#analysis",
    "href": "FRT analysis.html#analysis",
    "title": "Two amplifier RIAA phono preamp",
    "section": "",
    "text": "The following sections of this notebook contain the financial analysis for the company.\nContents \n\nStock screener results\nLoad financial spreadsheet\nDiscounted cash flow analysis, baseline\nDCF Scenarios\nNACI stock selection guide analysis\nFuture stock price\nDividend payout\nManagement performance\nDecision model\nConclusion\nNotes\nReferences"
  },
  {
    "objectID": "FRT analysis.html#stock-screener-results",
    "href": "FRT analysis.html#stock-screener-results",
    "title": "Two amplifier RIAA phono preamp",
    "section": "1) Stock screener results ",
    "text": "1) Stock screener results \nThis company was selected to analyze because it is a Dividend Aristocrat.\n~~This company was selected from the Fidelity stock screener results. The search results are based on Dividend yield (high and very high, 2.83% and higher), Volume 90 average (high and very high. 535k and higher) and Revenue Growth 5 years (0 or higher). ~~\nCurrent news\nA review of the financial news sites from yahoo and google showed the following:\n- Federal Realty Investment Trust (NYSE: FRT) is acquiring Kingstowne Towne Center in Kingstowne, Virginia for a total purchase price of \\$200 million. The acquisition will close in two phases. Federal Realty has closed on phase one of the acquisition for \\$100 million and expects to close on phase two for \\$100 million in July, subject to customary closing conditions. Combined, the property comprises 410,000 square feet of retail space on 45 acres of land. Located in Virginia’s Fairfax County near TSA’s new headquarters, Kingstowne Towne Center is surrounded by 5,200 homes, four commercial office buildings, and a planned multifamily development, and is part of a one million-square-foot regional retail node that attracts approximately 8.3 million visits annually—amongst the most visited retail destinations in Virginia. - Through the fiscal year ended December 31, 2021, the business of the registrant was conducted by an entity known as Federal Realty Investment Trust, a Maryland real estate investment trust (the “Predecessor”). On December 2, 2021, the Predecessor’s Board of Trustees approved the reorganization of the Predecessor’s business into an umbrella partnership real estate investment trust, or “UPREIT.”\nReview quarterly results\nSince this analysis mainly looks at the annual reports, a review of the quarterly reports and the most recent 12 months is needed to see if the recent quarterly trends match the yearly trends. - yahoo finance shows TTM Total Revenue is about equal to the most current 10K revenue. - The Compustat Company Research from Fidelity (from Sep. 29, 2021) shows: not reviewed\nAverage daily volume\nAverage daily volume: 499,387\nDividend yield\nForward dividend yield: 3.51%"
  },
  {
    "objectID": "FRT analysis.html#load-financial-spreadsheet",
    "href": "FRT analysis.html#load-financial-spreadsheet",
    "title": "Two amplifier RIAA phono preamp",
    "section": "2) Load financial spreadsheet ",
    "text": "2) Load financial spreadsheet \nData from consolidated financial statements and annual reports was collected and entered into a spreadsheet. All numerical data is converted from thousands or millions of dollars to dollars. The stock share price history was obtained from yahoo and is included as a tab in the spreadsheet. Other tabs in the spreadsheet are various worksheets.\n\nticker = 'FRT' # company ticker symbol\nos.chdir('/home/jim/Documents/Dividend Investing/DCF data/')\n\nfile_name = ticker+'_Financials.xlsx'\ndf_dcf_sheet = pd.read_excel(file_name,sheet_name='DCF data')\n#df_NAIC_financials = pd.read_excel(file_name,sheet_name='NAIC data')\ndf_metrics_sheet = pd.read_excel(file_name,sheet_name='metrics')\ndf_price_history = pd.read_excel(file_name,sheet_name='Historical Prices')\n\n# change the working director back to the Jupyter folder\nos.chdir('/home/jim/Documents/JupyterLab/Discount Cash Flow Analysis/')\n\n\n# convert dates from string to datetime format in stock price history\nprice_date_list = []\nfor i in range(len(df_price_history)):\n    price_date_list.append(datetime.strptime(str(df_price_history['Date'][i]), '%Y-%m-%d'))\n\ndf_price_history.insert(0, 'datetime', price_date_list)  # insert a new column with datetime data\ndf_price_history.sort_values(by=['datetime'], inplace=True) # sort data frame by datetime\n\ndf_price_history.set_index('datetime',inplace=True)\n\n#df_price_history.head()\n\n\n2.1) Format data frame \nGenerate a new data frame that holds the financial data needed for the DCF model. Data from financial statements is copied into a spreadsheet which contains the data used in the analysis. The data in the DCF_data tab is in a consistent format for ease of use by this notebook. Standard names are used for the rows and columns.\n\n#column names: fiscal years \nfy_data = df_dcf_sheet.columns[1:].values.astype('datetime64[Y]')-1970\n#line 0: Total revenue  \nrevenue_data = df_dcf_sheet.iloc[0].to_numpy()[1:].astype('float')\n#line 1: Cost of goods sold\nCost_of_goods_sold_data = df_dcf_sheet.iloc[1].to_numpy()[1:].astype('float')\n#line 2: General and administrative\nGeneral_and_administrative_data = df_dcf_sheet.iloc[2].to_numpy()[1:].astype('float')\n#line 3: Research and development\nResearch_and_development_data = df_dcf_sheet.iloc[3].to_numpy()[1:].astype('float')\n#line 4: Depreciation and amortization\nDepreciation_and_amortization_data = df_dcf_sheet.iloc[4].to_numpy()[1:].astype('float')\n#line 5: Investment\nInvestment_data = df_dcf_sheet.iloc[5].to_numpy()[1:].astype('float')\n# line 6: Income before income taxes\nIncome_before_income_taxes_data = df_dcf_sheet.iloc[6].to_numpy()[1:].astype('float')\n# line 7: Income tax\nIncome_tax_data = df_dcf_sheet.iloc[7].to_numpy()[1:].astype('float')\n# line 8: Accounts receivable\nAccounts_receivable_data = df_dcf_sheet.iloc[8].to_numpy()[1:].astype('float')\n# line 9: Inventories\nInventories_data = df_dcf_sheet.iloc[9].to_numpy()[1:].astype('float')\n# line 10: Accounts payable\nAccounts_payable_data = df_dcf_sheet.iloc[10].to_numpy()[1:].astype('float')\n# line 11: Current assets\nCurrent_assets_data = df_dcf_sheet.iloc[11].to_numpy()[1:].astype('float')\n# line 12: Current liabilities\nCurrent_liabilities_data = df_dcf_sheet.iloc[12].to_numpy()[1:].astype('float')\n# line 13: Long term debt\nLong_term_debt_data = df_dcf_sheet.iloc[13].to_numpy()[1:].astype('float')\n# line 14: Shares outstanding\nShares_outstanding_data = df_dcf_sheet.iloc[14].to_numpy()[1:].astype('float')\n\n\n# make a new data frame to store selected financial data\ndf_dcf_data = pd.DataFrame(data={\n    'FY':fy_data[::-1],\n    'revenue':revenue_data[::-1],\n    'cost_of_goods_sold':Cost_of_goods_sold_data[::-1],\n    'general_and_administrative':General_and_administrative_data[::-1],\n    'research_and_development':Research_and_development_data[::-1],\n    'depreciation':Depreciation_and_amortization_data[::-1],\n    'investment':Investment_data[::-1],\n    'income_before_income_taxes':Income_before_income_taxes_data[::-1],\n    'income_tax':Income_tax_data[::-1],\n    'accounts_receivable':Accounts_receivable_data[::-1],\n    'inventories':Inventories_data[::-1],\n    'accounts_payable':Accounts_payable_data[::-1], \n    'current_assets':Current_assets_data[::-1],\n    'current_liabilities':Current_liabilities_data[::-1],\n    'long_term_debt':Long_term_debt_data[::-1],\n    'shares_outstanding':Shares_outstanding_data[::-1]\n    })\n\n#df_dcf_data"
  },
  {
    "objectID": "FRT analysis.html#discounted-cash-flow-analysis-baseline",
    "href": "FRT analysis.html#discounted-cash-flow-analysis-baseline",
    "title": "Two amplifier RIAA phono preamp",
    "section": "3) Discounted cash flow analysis, baseline ",
    "text": "3) Discounted cash flow analysis, baseline \nDiscounted cash flow (DCF) is a valuation method used to estimate the value of an investment based on its expected future cash flows. DCF analysis attempts to figure out the value of an investment today, based on projections of how much money it will generate in the future. In finance, discounted cash flow (DCF) analysis is a method of valuing a security, project, company, or asset using the concepts of the time value of money. The DCF method used in this notebook follows [1].\nThe value of any financial investment equals the present value of the expected future cash flows, discounted for risk and timing of these cash flows. The DCF method to value stocks is a four step process.\n1. Develop a set of future free cash flows for the corporation based on revenue growth, net operating profit margin, income tax rate and fix and working capital requirements. 2. Estimate the discount rate for the cash flows based on expected timing and risk. 3. Discount the cash flows and total them to calculate the value for the corporation as a whole. 4. Subtract the debt, preferred stock value and other claims and divide by the number of shares outstanding to get the intrinsic value.\nSections - Revenue growth rate - Net operating profit margin - Tax rate - Depreciation Rate - Investment Rate - Working Capital Rate - Current Assets - Current Liabilities - Value of Debt Outstanding - Current stock price - Shares outstanding - 10 year treasury bond yield - Bond yield spread to treasury - Preferred stock yield - Equity risk premium - Company specific beta - DCF model inputs - Future cash flows\n\nFuture forecast based on historical data\nThe DCF model uses historical financial data to estimate future cash flows. However, future changes are largely unpredictable, so we assume that the past record can be used as a rough guide to the future. The more questionable this assumption is, the less valuable is the analysis. So the DCF model is more useful when applied to stable well established companies, since companies with stable earnings are easier to forecast.\n\n\nRevenue growth rate \nThe revenue growth rate (also sometimes called net sales) of the corporation plus any other revenues associated with the main operations of the business. It does not include dividends, interest income or non-operating income. Historic revenue data is obtained from consolidated income statements. The year over year change in revenue is calculated and converted to a percent, then an average revenue growth rate is calculated.\nAdjustments for a REIT\nThe revenue is from Total revenue on the income statement and includes: - Rental income - Mortgage interest income - other - Management and other fees from affiliates\nExclude other income, gains on sale of real estate and interest expenses.\n\n# calculate the percent change in revenue\npcr = np.zeros(len(df_dcf_data['revenue'].to_numpy())) # percent change in revenue\nfor i in range(len(df_dcf_data['revenue'].to_numpy()[0:-1])):\n    pcr[i+1] = ((df_dcf_data['revenue'].to_numpy()[i+1] - df_dcf_data['revenue'].to_numpy()[i])/\n                df_dcf_data['revenue'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Revenue, $B')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['revenue']/1e9, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcr,'+-g')\n    \nax2.set_ylabel('% Change in revenue',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,50))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Revenue')\nplt.show()\n\n\n\n\nObservation:\nExcept for 2020, revenues have grown every year. If 2020 and 2021 is removed from the revenue data series:\n\n#exclude 2021 and 2020 from average calculation\nprint('average revenue growth rate 2015 to 2019: {:.2f}%'.format(pcr[-7:-2].mean()))\n\naverage revenue growth rate 2015 to 2019: 2.04%\n\n\n\nrgr_avg = pcr[-5:].mean()/100 # last five years\nprint('average revenue growth rate: {:.2f}%'.format(rgr_avg*100))\n\naverage revenue growth rate: 4.03%\n\n\n\n\nNet operating profit margin \nNet Operating Profit should reflect the future revenue generating ability and expense requirements of the operating business that comprise the ongoing operations of the company.\n\\(\\text{NOPM} = \\frac{\\text{Revenue} - \\text{Expenses}}{\\text{Revenue}}\\)\n\\(\\text{Expenses} = \\text{Cost of Goods Sold (CGS)} + \\text{General and Administrative (G&A)} + \\text{Research and Development (R&D)}\\)\nGeneral and Administrative (G&A) is also called Sales, General and Administrative (SG&A)\nAdjustments for a REIT\nG&A and R&D costs are zero. Total operating expenses include Depreciation and amortization, so this item is removed from CGS.\nTotal expenses is made up of: - Rental expenses - Real estate taxes - General and administrative - Depreciation and amortization\nThese are all lumped into CGS and Depreciation and amortization is removed. Depreciation and amortization is a non-cash charge, therefore, we add back the charge to total expenses. The idea is that Depreciation and amortization as an expense unfairly reduces our net income because realestate its value over the period.\n\n# NOP = (Revenue - Expenses)\nnop = df_dcf_data['revenue'].to_numpy() - df_dcf_data['cost_of_goods_sold'].to_numpy()\n\n# net operating profit margin as percent of revenue\nnopm = nop/df_dcf_data['revenue'].to_numpy()\n\n# plot as grouped bar chart with labels on right and working capital rate on left\n# calculate position of bars\nx1_bar_position = []\nx2_bar_position = []\nfor i in df_dcf_data['FY']:\n    x1_bar_position.append(i-relativedelta(months=1))\n    x2_bar_position.append(i+relativedelta(months=1))\n    \nwidth = 40  # the width of the bars\n    \n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Net operating profit, \\\\$B')\n\nax1.bar(x1_bar_position,df_dcf_data['cost_of_goods_sold'].to_numpy()/1e9, width,label='CGS')\nax1.bar(x2_bar_position,nop/1e9, width,label='NOP')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,0.7))\nax1.legend()\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:cyan'\n\nax2.plot(df_dcf_data['FY'],nopm*100,'+-c')\n    \nax2.set_ylabel('% NOPM',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,100))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Net Operating Profit')\nplt.show()\n\n\n\n\nObservation\nThe NOP has been between 0.4 and 0.5 billion dollars during the past few years. The average net operating profit margin for the last 5 years is calculated below.\n\n#Average net operating profit margin\nnopm_avg = nopm[-5:].mean()\nprint('average net operating profit margin: {:.2f}%'.format(nopm_avg*100))\n\naverage net operating profit margin: 62.41%\n\n\n\n\nTax rate \nTax payments are taken from the consolidated income statement, provision for income taxes. The effect of taxes on profits is accounted for.\n\\(\\text{Tax rate} = \\frac{\\text{Income taxes}}{\\text{Income before income taxes}}\\)\nAdjustments for a REIT\nREITs have very little income tax. Income average tax rate is set to zero.\n\n# Average tax rate\ntax_rate_avg = 0 #tax_rate[-5:].mean()\nprint('average tax rate: {:.2f}%'.format(tax_rate_avg*100))\n\naverage tax rate: 0.00%\n\n\n\n\nDepreciation Rate \nThe depreciation rate is used to project the future net investment cash flows. The effect is to reduce the amount of FCFF. Depreciation amounts are from the Consolidated Statement of Cash Flows, Depreciation and Amortization.\n\\(\\text{Depreciation Rate}=\\frac{\\text{Depreciation and Amortization}}{\\text{Revenues}}\\)\nDepreciation is the write off or expensing of a percentage of the historical cost of an asset over the asset’s useful life. Property, plant and equipment (PP&E) are long term or non current assets owned or controlled by the company and used to manufacture and or sell the company’s products. The balance sheet typically shows all categories of PP&E grouped together, net of accumulated depreciation. Depreciation represents wear and tear on an asset or the fact that an asset gets used up over time. Companies record depreciation expense in the income statement every year for all depreciable assets in service or used by the company during the year. The difference between GAAP and Tax Accounting methods is handled through deferred taxes.\nAmortization is the write off or expensing of the cost of a financial instrument or an intangible asset over the shorter of its useful life or legal life. Amortization is similar to depreciation and reflects the declining useful life and value of the intangible asset over time. Companies in research and development intensive fields typically have many patents. Such industries include high technology, pharmaceuticals and chemicals.\n\n# depreciation rate\ndepreciation_rate = df_dcf_data['depreciation'] / df_dcf_data['revenue'].to_numpy()\n\n# plot depreciation on left and rate on right\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $B')\n\nax1.bar(df_dcf_data['FY'],df_dcf_data['depreciation']/1e9, width=100,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:Blue'\n\nax2.plot(df_dcf_data['FY'],depreciation_rate*100,'+-')\n    \nax2.set_ylabel('% Depreciation rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,50))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Depreciation')\nplt.show()\n\n\n\n\nObservation:\nDepreciation has been running at about a consistant 30% of revenues.\n\n# average depreciation rate\ndepreciation_rate_avg = depreciation_rate[-5:].mean()\nprint('average depreciation rate: {:.2f}%'.format(depreciation_rate_avg*100))\n\naverage depreciation rate: 28.08%\n\n\n\n\nInvestment Rate \nTaken from Consolidated Statement of Cash Flows, Cash used for investing activities. Net investment in the dollar amount needed to support the growth of the firm. Included investments in properties, plant equipment in excess of the depreciation expenses associated with past investments. Net investment decreases the amount of money available to the stockholders. Investment in property, plant and equipment is necessary to both maintain service and sales and also to grow revenues and profits. Investment amounts should include capital expenditures and research and development.\n\\(Ir=\\frac {\\text {Capital Expenditures}}{\\text{Revenues}}\\)\nFor this company, the yearly investment amounts are taken from the Consolidated Statements of Cash Flows, Net Cash Used in Investing Activities.\nAdjustments for a REIT\nThe yearly investment amounts are taken from the Consolidated Statements of Cash Flows, Net Cash Used in Investing Activities\n\n# investment rate\ninvestment_rate = df_dcf_data['investment'] / df_dcf_data['revenue'].to_numpy()\n\n# plot investment on left and rate on right\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, \\\\$B')\n\nax1.bar(df_dcf_data['FY'],df_dcf_data['investment']/1e9, width=100,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:Blue'\n\nax2.plot(df_dcf_data['FY'],investment_rate*100,'+-')\n    \nax2.set_ylabel('% New Investment Rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,100))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('New Investment')\nplt.show()\n\n\n\n\nObservation:\n\n# average investment rate\ninvestment_rate_avg = investment_rate[-5:].mean()\nprint('average investment rate: {:.2f}%'.format(investment_rate_avg*100))\n\naverage investment rate: 48.29%\n\n\n\n\nWorking Capital Rate \nWorking capital is needed to support the corporate sales effort of any company. Often a company’s incremental change in net working capital either positive or negative is approximately proportional to its change in revenue.\n\\(\\text{Working capital} = \\text{Accounts Receivable} + \\text{Inventories} - \\text{Accounts Payable}\\)\nWorking capital is a company’s net investment in its accounts receivable and its inventories (cash outflows), minus its accounts payable (a cash inflow). Working capital and taxes are cash outflows from the corporation that are not available to pay debts and stockholders.\nAdjustments for a REIT\nFor a REIT, the working capital rate is set to zero. REITs generally have no data for inventories and accounts receivable.\n\n# average working capital rate\nworking_capital_rate_avg = 0 #working_capital_rate[-5:].mean()\nprint('average working capital rate: {:.2f}%'.format(working_capital_rate_avg*100))\n\naverage working capital rate: 0.00%\n\n\n\n\nCurrent assets \nTotal Current Assets from the most recent balance sheet statement of the company. Current assets include inventory, cash and accounts receivables.\nAdjustments for a REIT\nCurrent assets are in cluded in total assets for a REIT. Current assets are calculted as follows:\n\\(\\text{Current assets} = \\text{Cash and cash equivalents} + \\text{Accounts and notes receivable}\\)\n\n# plot Short Term Assets\nwidth = 100  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nplt.bar(df_dcf_data['FY'],df_dcf_data['current_assets']/1e9, width)\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Current assets')\nplt.ylabel('dollars, \\\\$B')\n\nplt.grid()\nplt.show()\n\n\n\n\nObservation:\nAn increase in cash and cash equivalents for 2019 on account of the pandemic.\n\nsta = df_dcf_data['current_assets'].iloc[-1]\nprint('Current assets: ${:.2f}B'.format(sta/1e9))\n\nCurrent assets: $0.28B\n\n\n\n\nCurrent liabilities \nTotal Current Liabilities from the most recent balance sheet consolidated statement.\nAdjustments for a REIT\nCurrent Liabilities are calculted as follows:\n\\(\\text{Current liabilities} = \\text{Notes payable, net} + \\text{Accounts payable and accrued expenses} + \\text{Security deposits payable}\\)\n\n# plot Short Term Liabilities\n\nwidth = 100  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nplt.bar(df_dcf_data['FY'],df_dcf_data['current_liabilities']/1e9, width)\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Current liabilities')\nplt.ylabel('dollars, \\\\$B')\n\nplt.grid()\nplt.show()\n\n\n\n\n\nprint('Average of current liabilities: ${:.2f}B'.format(df_dcf_data['current_liabilities'].mean()/1e9))\n\nAverage of current liabilities: $0.48B\n\n\nObservation:\nOther current liabilities increased in 2019 and 2020.\n\nstl = df_dcf_data['current_liabilities'].iloc[-1]\nprint('Current liabilities: ${:.2f}B'.format(stl/1e9))\n\nCurrent liabilities: $0.82B\n\n\n\n\nValue of Debt Outstanding \nAmount of debt outstanding from the most recent balance sheet of the company.\nAdjustments for a REIT\nValue of Debt Outstanding (long term debt) is calculted as follows:\n\\(\\text{Value of Debt Outstanding} = \\text{Total liabilities} - \\text{Current liabilities}\\)\n\n# calculate the percent change in debt, pcd\npcd = np.zeros(len(df_dcf_data['long_term_debt'].to_numpy())) # percent change in debt\nfor i in range(len(df_dcf_data['long_term_debt'].to_numpy()[0:-1])):\n    pcd[i+1] = ((df_dcf_data['long_term_debt'].to_numpy()[i+1] - df_dcf_data['long_term_debt'].to_numpy()[i])/\n                df_dcf_data['long_term_debt'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars, $B')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['long_term_debt']/1e9, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcd,'+-g')\n    \nax2.set_ylabel('% Change in debt',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-40,100))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('debt')\nplt.show()\n\n\n\n\n\ndgr_avg = pcd[1:].mean()/100\nprint('average debt growth rate: {:.2f}%'.format(dgr_avg*100))\n\naverage debt growth rate: 6.82%\n\n\nObservation:\nAs of December 31, 2021, FRT had approximately \\$4.1 billion of debt outstanding. Of that outstanding debt, approximately \\$341.6 million was secured by all or a portion of 7 of their real estate projects. As of December 31, 2021, approximately 92.6% of FRT debt was fixed rate or fixed via interest rate swap agreements, which includes all of their property secured debt and our unsecured senior notes. FRT organizational documents do not limit the level or amount of debt that they may incur. The amount of our debt outstanding from time to time could have important consequences to our shareholders.\nRising interest rates could adversely affect our cash flow and the market price of our outstanding debt and preferred shares. Of FRT’s \\$4.1 billion of debt outstanding as of December 31, 2021, approximately \\$356.5 million bears interest at a variable rate, of which, \\$300.0 million is unsecured term loan that bears interest at a variable rate of LIBOR plus 80 basis points and \\$56.5 million in mortgages payable that bear interest at a variable rate of LIBOR plus 195 basis points and are effectively fixed through two interest rate swap agreements.\nFRT also has a \\$1.0 billion revolving credit facility, on which no balance was outstanding at December 31, 2021, that bears interest at LIBOR plus 77.5 basis points.\nAs of December 31, 2021, there is no balance outstanding on FRT’s \\$1.0 billion unsecured revolving credit facility and they had cash and cash equivalents of \\$162.1 million. FRT also had outstanding forward sales agreements for net proceeds of \\$264.0 million as of December 31, 2021. FRT has no debt maturing until June 2023.\nIn addition, an increase in market interest rates may lead purchasers of FRT debt securities and preferred shares to demand a higher annual yield, which could adversely affect the market price of FRT’s outstanding debt securities and preferred shares and the cost and/or timing of refinancing or issuing additional debt securities or preferred shares.\nFor the year ended 2021, the weighted average amount of borrowings outstanding on our revolving credit facility was \\$19.6 million, and the weighted average interest rate, before amortization of debt fees, was 0.9%.\nThe interest rates on these mortgages range from 3.91% to 5.00%.\n\nvod = df_dcf_data['long_term_debt'].iloc[-1]\nprint('Total long term debt and other: ${:.2f}B'.format(vod/1e9))\n\nTotal long term debt and other: $4.20B\n\n\n\n\nCurrent stock price \nMost recent stock price for the company. The current stock price is used to calculate the market value of the firm. Use the market value when looking at market capitalization for common stock.\n\ncsp = 97.74 #95.93 # current stock price\nprint('current stock price: ${:,.2f}'.format(csp))\n\ncurrent stock price: $97.74\n\n\n\n\nShares outstanding \nThe number of shares outstanding is used to calculate the intrinsic stock value.\n\nso = df_dcf_data['shares_outstanding'].iloc[-1] # shares outstanding\nprint('shares outstanding, basic: {:,.0f}'.format(so))\n\nshares outstanding, basic: 81,353,180\n\n\n\n# calculate the percent change in shares outstanding, pcso\npcso = np.zeros(len(df_dcf_data['shares_outstanding'].to_numpy())) # percent change in debt\nfor i in range(len(df_dcf_data['shares_outstanding'].to_numpy()[0:-1])):\n    pcso[i+1] = ((df_dcf_data['shares_outstanding'].to_numpy()[i+1] - df_dcf_data['shares_outstanding'].to_numpy()[i])/\n                df_dcf_data['shares_outstanding'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('shares outstanding, M')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['shares_outstanding']/1e6, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcso,'+-g')\n    \nax2.set_ylabel('% Change in shares outstanding',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-5,25))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Shares outstanding')\nplt.show()\n\n\n\n\n\nprint('average shares outstanding growth rate: {:.2f}%'.format(pcso[1:].mean()))\n\naverage shares outstanding growth rate: 2.34%\n\n\nObservation:\nFor the year ended December 31, 2021, FRT issued 847,471 common shares at a weighted average price per share of \\$104.19 for net cash proceeds of \\$87.0 million including paying \\$0.9 million in commissions and \\$0.4 million in additional offering expenses related to the sales of these common shares.\nFRT has the capacity to issue up to \\$175.0 million in common shares under the ATM program.\n\n\nDilution\nDilution occurs when a company issues new shares that result in a decrease in existing stockholders’ ownership percentage of that company. Stock dilution can also occur when holders of stock options, such as company employees, or holders of other optionable securities exercise their options. When the number of shares outstanding increases, each existing stockholder owns a smaller, or diluted, percentage of the company, making each share less valuable.\nInvestigate why there is a historic growth trend in number of shares outstanding. Search annual report for dilutive actions: - share sales - convertable debt - employee options\nSearch results:\n\n\n10 year treasury bond yield \nThe 10 year treasury yield is used as a measure of the risk free rate.\nYield: 3.45%\niShares 7-10 Year Treasury Bond ETF (IEF)\nAverage Yield to Maturity: 3.5%\n\ntby = (3.45+3.5)/2/100  # 10 year treasury bond yield, average of data from sources listed above\nprint('10 year treasury bond yield: {:,.2f}%'.format(tby*100))\n\n10 year treasury bond yield: 3.48%\n\n\n\n\nBond yield spread to treasury \nThe spread to treasury implies that all corporate debt will have a higher yield than yields associated with comparable maturity US Treasury Bonds. The best way to determine default risk is to see how a particular company’s debt is trading in the market and compare it on a spread basis with comparable maturity yields.\nLook at the following or use a default rating systems that are published by the three major rating agencies, Standards and Poors Corp, Moody’s Investor Services and Fitch & Company.\nPIMCO Active Bond Exchange-Traded Fund (BOND)\nYield: 3.44%\niShares 5-10 Year Investment Grade Corporate Bond ETF (IGIB)\nAverage Yield to Maturity: 5.15%\niShares 10+ Year Investment Grade Corporate Bond ETF (IGLB)\nAverage Yield to Maturity: 5.35%\nWeb resources: - http://www.standardpoor.com/\n- http://bond.yahoo.com/rates.html\n- http://www.moodys.com/cust/default.asp\n- http://www.fitchibca.com/corporate/index.cfm\n\nbystt = ((3.44+5.15+5.35)/3-tby)/100           # bond yield spread (average) to treasury spread\nprint('Bond yield spread to treasury: {:,.2f}%'.format(bystt*100))\n\nBond yield spread to treasury: 4.61%\n\n\n\n\nPreferred stock yield \nAmount of preferred stock outstanding from the most recent balance sheet of the company.\nFrom the balance sheet:\n- Preferred shares, authorized 15,000,000 shares, \\$.01 par: 5.0% Series C Cumulative Redeemable Preferred Shares, (stated at liquidation preference $25,000 per share), 6,000 shares issued and outstanding - 5.417% Series 1 Cumulative Convertible Preferred Shares, (stated at liquidation preference \\$25 per share), 399,896 shares issued and outstanding\nSee below for inputs to model.\n\npsy = (5+5.417)/2/100  # preferred stock yield\nprint('preferred stock yield: {:,.2f}%'.format(psy*100))\n\nvps = 6000*25000 + 399896*25 # value of preferred stock\nprint('value of preferred stock: {:,.2f}'.format(vps))\n\npreferred stock yield: 5.21%\nvalue of preferred stock: 159,997,400.00\n\n\n\n\nEquity risk premium \nThe expected excess return a hypothetical average investor would require of a diversified portfolio of stock (assumed beta = 1.0) over the yield on the 10-year Treasury Bond. The equity risk premium has been going down over the years.\n- 1926 to 1990: 5.5%\n- 1962 to 1990: 3.25%\n- 1981 to 1990: 0.19%\nIn times of sustained economic growth the risk premium demanded by investors generally declines.\nI’m going to use 3% as the equity risk premium.\n\neq_rp = 3.0/100             # equity risk premium\nprint('Equity risk premium: {:,.2f}%'.format(eq_rp*100))\n\nEquity risk premium: 3.00%\n\n\n\n\nCompany specific beta \nThe Beta used is Beta of Equity. Beta is the monthly price change of a particular company relative to the monthly price change of the S&P 500. The time period for Beta is 5 years when available. This value can be obtained at yahoo finance.\nA measure of risk of an individual stock. It measures volatility of return - a higher beta means a higher risk. A financial model that uses Beta as its sole measure of risk (signal factor model) is called a Capital Asset Pricing Model (CAPM).\n\nbeta = 1.22 # company specific beta\nprint('Company specific beta: {:,.2f}'.format(beta))\n\nCompany specific beta: 1.22\n\n\n\n\nDCF model inputs \nBelow are the DCF model inputs. These values were calculated above.\n\n# various rates\nrgr = rgr_avg              # revenue growth rate\nprint('revenue growth rate: {:,.2f}%'.format(rgr*100))\nnopm = nopm_avg             # net operating profit margin\nprint('net operating profit margin: {:,.2f}%'.format(nopm*100))\ntr = tax_rate_avg               # tax rate\nprint('tax rate: {:,.2f}%'.format(tr*100))\ndr = depreciation_rate_avg              # depreciation rate (% of revenue)\nprint('depreciation rate: {:,.2f}%'.format(dr*100))\nir = investment_rate_avg              # investment rate (% of revenue)\nprint('investment rate: {:,.2f}%'.format(ir*100))\nwcr = working_capital_rate_avg            # working capital rate (% of revenue)\nprint('working capital rate: {:,.2f}%'.format(wcr*100))\n\nrevenue growth rate: 4.03%\nnet operating profit margin: 62.41%\ntax rate: 0.00%\ndepreciation rate: 28.08%\ninvestment rate: 48.29%\nworking capital rate: 0.00%\n\n\nExcess return period\nThe excess return period is based on a judgment call. The authors of [1] use the 1-5-7-10 rule. They group companies into one of four general categories and excess return periods. They use a 10 year excess return period to calculate what they would consider the maximum value. They use a more conservative 1 year, 5 year or 7 year return period to calculate a more reasonable or minimum value.\n- 1 year: Boring companies that operate in a highly competitive, low margin industry in which they have nothing particular going for them. - 5 year: Decent companies that have a recognizable name and decent reputation and perhaps a regulatory benefit (utility company), but can’t control pricing or growth. - 7 year: Good companies with good brand names, large companies of scale, good marketing channels and consumer identification (e.g. McDonald’s) - 10 year: Great companies with great growth potential, tremendous marketing power, band names and in-place benefits (e.g. Intel, Microsoft, Coca Cola, Disney)\nThe excess return period used for the base case is ten years, which should lead to a higher calculated intrinsic value.\n\n# General Inputs\nfy_start = df_dcf_data['FY'].iloc[-1].year # fiscal year to start excess return period\nerp = 10 # excess return period, years\nrev_start = df_dcf_data['revenue'].to_numpy()[-1] # starting revenues for excess return period\nprint('starting year: {:.0f}'.format(fy_start))\nprint('excess return period: {:.0f} years'.format(erp))\nprint('starting revenues: ${:,.2f}B'.format(rev_start/1e9))\nprint('shares outstanding: {:,.0f}'.format(so))\n\nstarting year: 2022\nexcess return period: 10 years\nstarting revenues: $1.07B\nshares outstanding: 81,353,180\n\n\n\nps_mv = vps               # preferred stock, market value \nprint('preferred stock, market value : ${:,.2f}B'.format(ps_mv/1e9))\ncs_mv = csp*so            # common stock, market value \nprint('common stock, market value: ${:,.2f}B'.format(cs_mv/1e9))\n\npreferred stock, market value : $0.16B\ncommon stock, market value: $7.95B\n\n\nLong Term Debt, Market Value, ltd_mv\nUse the book value for long term debt. Various online resources can be used to research this item. These include, Bondsonline and Bloomberg. The book value of debt and preferred stock is an accounting measure that relates to how much money was raised by the company when each security was issued. The market value of debt and the preferred and common stock is the price that specific obligations would trade at in today’s market.\nLong term debt for firms can take one of two forms. It can be a long-term loan from a bank or other financial institution or it can be a long-term bond issued to financial markets, in which case the creditors are the investors in the bond. Firms often have long term obligations that are not captured in the long term debt item. These include obligations to lessors on assets that firms have leased, to employees in the form of pension fund and health care benefits yet to be paid, and to the government in the form of taxes deferred. In the last two decades, accountants have increasingly moved towards quantifying these liabilities and showing them as long term liabilities.\n\nltd_mv = vod              # market value of long term debt\ntmv = ltd_mv+ps_mv+cs_mv  # total market value \nprint('total market value: ${:,.2f}B'.format(tmv/1e9))\n\ntotal market value: $12.31B\n\n\nCost of Common Equity, cce\nThe expected excess return a hypothetical average investor would require of a diversified portfolio of stock (assumed beta = 1.0) over the yield on the 10-year Treasury Bond. The annual rate of return that an investor expects to earn when investing in shares of a company is known as the cost of common equity. It includes dividends and increases in the market value.\n\ncce = tby+beta*eq_rp      # cost of common equity or the expected return for the stock\nprint('cost of common equity: {:,.2f}%'.format(cce*100))\n\ncost of common equity: 7.13%\n\n\nLong Term Debt, Average Yield, ltd_ay\nThe total cost of long term debt.\n\nltd_ay = tby+bystt        # long term debt average yield\nprint('long term debt average yield: {:,.2f}%'.format(ltd_ay*100))\n\nlong term debt average yield: 8.09%\n\n\nLong Term Debt, After Tax Yield, ltd_aty\nThe tax benefits of long term debt. Interest payments are tax deductible for the company.\n\nltd_aty = ltd_ay*(1-tr)   # long term debt after tax yield\nprint('long term debt after tax yield: {:,.2f}%'.format(ltd_aty*100))\n\nltd_pc = vod/tmv          # weight for long term debt \nltd_ate = ltd_aty*ltd_pc  # after tax effect of long term debt \nps_ay = psy               # preferred stock, average yield \nps_aty = ps_ay            # preferred stock, average yield \nprint('preferred stock, average yield: {:,.2f}%'.format(ps_aty*100))\n\nps_pc = ps_mv/tmv         # preferred stock, % capital \nps_ate = ps_aty*ps_pc     # preferred stock, after tax effect \ncs_ay = cce               # common stock, average yield \ncs_aty = cce              # common stock, after tax yield \nprint('common stock, after tax yield: {:,.2f}%'.format(cs_aty*100))\n\ncs_pc = cs_mv/tmv         # common stock, % capital \ncs_ate = cs_aty*cs_pc     # common stock, after tax effect \nprint('common stock, after tax effet: {:,.2f}%'.format(cs_ate*100))\n\ntate = ltd_ate+ps_ate+cs_ate # total after tax effect \nprint('total after tax effect: {:,.2f}%'.format(tate*100))\ntpc = ltd_pc+ps_pc+cs_pc     # total % Capital\nprint('total % Capital: {:,.2f}%'.format(tpc*100))\n\nlong term debt after tax yield: 8.09%\npreferred stock, average yield: 5.21%\ncommon stock, after tax yield: 7.13%\ncommon stock, after tax effet: 4.61%\ntotal after tax effect: 7.43%\ntotal % Capital: 100.00%\n\n\nWeighted average cost of capital\nA company’s weighted average cost of capital (WACC) is the weighted average of the company’s current cost of debt and equity calculated by using current debt, preferred stock and common stock market values. The WACC of the company, calculated after tax, is the discount rate used in the DCF valuation procedures. The WACC, which is the cost of the different components of financing used by the firm, weighted by their market value proportions. These include debt, preferred stock, and common stock.\nWACC: Weighted Average Cost of Capital, the rate used to discount cash flows, based on the following three factors. 1. Base rate of return. 2. Expected return based on debt and preferred stock. 3. Expected return on common stock and Beta.\nAll adjusted for the tax advantage of interest payments and the percentage of debt, preferred stock and common stock.\n\nwacc = tate\nprint('weighted average cost of capital: {:.1f}%'.format(wacc*100))\n\nweighted average cost of capital: 7.4%\n\n\n\n\nFuture cash flows \nThe future cash flows to the firm are projected based on revenue growth. The cash flows are then discounted using the WACC and the ISV is calculated.\n\n# make a list of the fiscal years in excess return period \nfy = np.zeros(erp+1)\nfy[0] = fy_start\nfor i in range(1,erp+1): \n    fy[i]=fy_start+i\n\nrev = np.zeros(len(fy))\nciwc = np.zeros(len(fy))\nrev[0] = rev_start  #*rgr+rev_start   # find the future revenue using constant revenue growth rate \n\nfor i in range(1,len(fy)): \n    rev[i] = rev[i-1]*rgr+rev[i-1]  # find the future revenue \n    ciwc[i] = (rev[i]-rev[i-1])*wcr  # find the change in working capital \n\nnet_op = np.zeros(len(fy)) # net operating profit\nadj_taxes = np.zeros(len(fy))\nnopat = np.zeros(len(fy))\ninvest = np.zeros(len(fy))\ndepre = np.zeros(len(fy))\nnet_invest = np.zeros(len(fy))\nfcff = np.zeros(len(fy))\ndisc_fact = np.zeros(len(fy))\ndisc_fcff = np.zeros(len(fy))                \n\n# calculate values in table \nfor i in range(1,len(fy)):\n    net_op[i] = rev[i]*nopm # net operating profit margin\n    adj_taxes[i] = net_op[i]*tr # net operating profit adjusted for taxes\n    nopat[i] = net_op[i]-adj_taxes[i] # after tax net operating profit\n    invest[i] = rev[i]*ir # future investments\n    depre[i] = rev[i]*dr # future depreciations\n    net_invest[i] = invest[i]-depre[i] # net investments\n    fcff[i] = nopat[i]-net_invest[i]-ciwc[i] # free cash flow to the firm\n    disc_fact[i] = 1/((1+wacc)**i) # discount factor\n    disc_fcff[i] = disc_fact[i]*fcff[i] # discounted free cash flow to the firm\n    \ndcrv = nopat[-1]/wacc*disc_fact[-1] # discounted corporate residual value\nderp_fcff = disc_fcff.sum() # discounted excess return period FCFF\n\ntcv = derp_fcff+dcrv+sta # total corporate value\ntvce = tcv-vod-vps-stl # total value of common equity\nisv = tvce/so # intrinsic stock value\n\n# print cash flows in a table\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format('Year','Rev','NOP','AdjTaxes',\n    'NOPAT','Invest.','Deprec.','dInvest.','dWC','FCFF','DF','DF*FCFF'))\nfor i in range(len(fy)):\n    print('{:4.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.4f}{:10,.0f}'.format(fy[i],\n        rev[i]/1e6,net_op[i]/1e6,adj_taxes[i]/1e6,nopat[i]/1e6,invest[i]/1e6,depre[i]/1e6,(invest[i]-depre[i])/1e6,ciwc[i]/1e6,\n        fcff[i]/1e6,disc_fact[i],disc_fcff[i]/1e6))\n\nYear       Rev       NOP  AdjTaxes     NOPAT   Invest.   Deprec.  dInvest.       dWC      FCFF        DF   DF*FCFF\n2022     1,074         0         0         0         0         0         0         0         0    0.0000         0\n2023     1,118       698         0       698       540       314       226         0       472    0.9308       439\n2024     1,163       726         0       726       562       327       235         0       491    0.8664       425\n2025     1,210       755         0       755       584       340       244         0       510    0.8064       412\n2026     1,258       785         0       785       608       353       254         0       531    0.7506       399\n2027     1,309       817         0       817       632       368       265         0       552    0.6987       386\n2028     1,362       850         0       850       658       382       275         0       575    0.6503       374\n2029     1,417       884         0       884       684       398       286         0       598    0.6053       362\n2030     1,474       920         0       920       712       414       298         0       622    0.5634       350\n2031     1,533       957         0       957       740       430       310         0       647    0.5244       339\n2032     1,595       995         0       995       770       448       322         0       673    0.4881       328\n\n\n\n# Intrinsic Value\nprint('discounted excess return period FCFF: ${:,.2f}B'.format(derp_fcff/1e9))\nprint('discounted corporate residual value: ${:,.2f}B'.format(dcrv/1e9))\nprint('total corporate value: ${:,.2f}B'.format(tcv/1e9))\nprint('total value of common equity: ${:,.2f}B'.format(tvce/1e9))\ntvce_baseline = tvce # save value as baseline case\nisv_baseline = isv # save the isv for the baseline case\nprint('intrinsic stock value, baseline case: ${:,.2f}'.format(isv_baseline))\nprint('current stock price: ${:,.2f}'.format(csp))\n\ndiscounted excess return period FCFF: $3.81B\ndiscounted corporate residual value: $6.53B\ntotal corporate value: $10.63B\ntotal value of common equity: $5.45B\nintrinsic stock value, baseline case: $66.99\ncurrent stock price: $97.74\n\n\nObservation:\nThe base line DCF analysis produces an intrinsic stock value of \\$105. Some adjustments will be made in the scenario 1 case.\nThe calculations used here can be verified by using the Valuepro web site, which calculates ISV based on the same method (not working as of 2/5/2022).\n\n\nList of all inputs to the DCF model\nThe following print statements format the inputs to the model similar to how they are presented on the Valuepro page.\n\nprint('{:&gt;35s} {:&lt;10.0f} {:&gt;35s} {:,.3f}'.format('Excess return period, years:',erp,'Depreciation rate, %:',dr*100))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.3f}'.format('Starting revenues, $B:',\n    rev_start/1e9,'Investment rate, %:',ir*100))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:,.3f}'.format('Revenue growth rate, %:',\n    rgr*100,'Working capital rate, %:',wcr*100))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:,.3f}'.format('Net operating profit margin, %:',\n    nopm*100,'Current assets, $B:',sta/1e9))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:.3f}'.format('Tax rate, %:',\n    tr*100,'Current liabilities, $B:',stl/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.2f}'.format('Current stock price, $:',\n    csp,'Equity risk premium, %:',eq_rp*100))\nprint('{:&gt;35s} {:&lt;10,.0f} {:&gt;35s} {:,.2f}'.format('Shares outstanding, basic, M:',\n    so/1e6,'Company specific beta:',beta))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:.3f}'.format('10 year treasury bond yield, %:',\n    tby*100,'Total long term debt and other, $B:',vod/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.3f}'.format('Bond yield spread to treasury, %:',\n    bystt*100,'Value of preferred stock, $B:',vps/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f}'.format('Preferred stock yield, %:',psy*100))\n\n       Excess return period, years: 10                       Depreciation rate, %: 28.081\n             Starting revenues, $B: 1.07                       Investment rate, %: 48.295\n            Revenue growth rate, %: 4.029                 Working capital rate, %: 0.000\n    Net operating profit margin, %: 62.407                     Current assets, $B: 0.283\n                       Tax rate, %: 0.000                 Current liabilities, $B: 0.820\n            Current stock price, $: 97.74                  Equity risk premium, %: 3.00\n      Shares outstanding, basic, M: 81                      Company specific beta: 1.22\n    10 year treasury bond yield, %: 3.48       Total long term debt and other, $B: 4.202\n  Bond yield spread to treasury, %: 4.61             Value of preferred stock, $B: 0.160\n          Preferred stock yield, %: 5.21      \n\n\n\n# weighted average cost of capital inputs\nprint('Weighted Average Cost of Capital')\nprint('Cost of common equity')\nprint('{:s}'.format('-'*37))\nprint('{:&gt;32s} {:,.2f}'.format('10 year treasury bond yield, %:',tby*100))\nprint('{:&gt;32s} {:,.2f}'.format('Company specific beta:',beta))\nprint('{:&gt;32s} {:,.2f}'.format('Equity risk premium, %:',eq_rp*100))\nprint('{:s}'.format('-'*37))\nprint('{:&gt;32s} {:,.2f}'.format('Cost of common equity, %:',cce*100))\nprint()\n\nprint('Market Capitalization and After-Tax Weighted Average Cost of Capital')\nprint()\nprint('{:s}{:^10s}{:^10s}{:^10s}{:^15s}{:^15s}'.format(' '*20,'Current','After-Tax','Market','%','Weighted After-'))\nprint('{:s}{:^10s}{:^10s}{:^10s}{:^15s}{:^15s}'.format(' '*20,'Yield','Yield','Value','Capitalization','Tax Yield'))\n\nprint('{:s}'.format('-'*80))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Long term debt',\n    ltd_ay*100,(tby+eq_rp)*(1-tr)*100,vod/1e9,ltd_pc*100,ltd_ate*100))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Preferred stock',\n     psy*100,ps_ate*100,vps/1e9,ps_pc*100,ps_ate*100))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Common stock',\n     cs_ay*100,cs_aty*100,cs_mv/1e9,cs_pc*100,cs_aty*100))\nprint('{:s}'.format('-'*80))\nprint('{:&lt;37s}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('',tmv/1e9,tpc*100,wacc*100))\n\nWeighted Average Cost of Capital\nCost of common equity\n-------------------------------------\n 10 year treasury bond yield, %: 3.48\n          Company specific beta: 1.22\n         Equity risk premium, %: 3.00\n-------------------------------------\n       Cost of common equity, %: 7.13\n\nMarket Capitalization and After-Tax Weighted Average Cost of Capital\n\n                     Current  After-Tax   Market         %       Weighted After-\n                      Yield     Yield     Value   Capitalization    Tax Yield   \n--------------------------------------------------------------------------------\nLong term debt         8.09      6.48         4       34.12           2.76\nPreferred stock        5.21      0.07         0        1.30           0.07\nCommon stock           7.13      7.13         8       64.58           7.13\n--------------------------------------------------------------------------------\n                                             12      100.00           7.43"
  },
  {
    "objectID": "FRT analysis.html#dcf-scenarios",
    "href": "FRT analysis.html#dcf-scenarios",
    "title": "Two amplifier RIAA phono preamp",
    "section": "4) DCF Scenarios ",
    "text": "4) DCF Scenarios \nThe following adjustments were made to various model parameters. - excess return period was adjusted to a more conservative 5 years - revenue growth rate was adjusted to 5% to reflect pre Covid growth. (base case = 3.037 %) - net operating profit margin was adjusted to 60% (base case = 62.820%) - tax rate was adjusted to 0% (base case = 0%) - depreciation rate was adjusted to 27% (base case = 27.492%) - investment rate was adjust to 53% (base case = 53.184%) - working capital rate was set to an even 0% (base case = 0%) - weighted average cost of capital was adjusted up by 2% to reflect higher interest rates and provide a margin of safety (base case = 3.8%)\n\nprint('adjusted DCF input values and rates')\nerp = 5\nprint('excess return period: {:,.0f} years'.format(erp))\nrgr = 5/100 # setting growth rate to 5%, since this is more in line with Covid grown \nprint('revenue growth rate: {:,.1f}%'.format(rgr*100))\nnopm = isv_s1_nopm = 60/100\nprint('net operating profit margin: {:.2f}%'.format(nopm*100))\ntr = isv_s1_tr =0/100\nprint('tax rate: {:.2f}%'.format(tr*100))\ndr = 27/100\nprint('depreciation rate: {:,.2f}%'.format(dr*100))\nir = 53/100              # investment rate (% of revenue)\nprint('investment rate: {:,.2f}%'.format(ir*100))\nwcr = 0/100\nprint('working capital rate: {:,.1f}%'.format(wcr*100))\nwacc_adj = (wacc+0.02) # weighted average cost of capital, increased by 2%\n#wacc_adj = 1/100\nprint('weighted average cost of capital: {:.1f}%'.format(wacc_adj*100))\n\nadjusted DCF input values and rates\nexcess return period: 5 years\nrevenue growth rate: 5.0%\nnet operating profit margin: 60.00%\ntax rate: 0.00%\ndepreciation rate: 27.00%\ninvestment rate: 53.00%\nworking capital rate: 0.0%\nweighted average cost of capital: 9.4%\n\n\n\n# make a list of the fiscal years in excess return period \nfy = np.zeros(erp+1)\nfy[0] = fy_start\nfor i in range(1,erp+1): \n    fy[i]=fy_start+i\n\nrev = np.zeros(len(fy))\nciwc = np.zeros(len(fy))\nrev[0] = rev_start  #*rgr+rev_start   # find the future revenue using constant revenue growth rate \n\nfor i in range(1,len(fy)): \n    rev[i] = rev[i-1]*rgr+rev[i-1]  # find the future revenue \n    ciwc[i] = (rev[i]-rev[i-1])*wcr  # find the change in working capital \n\nnet_op = np.zeros(len(fy))\nadj_taxes = np.zeros(len(fy))\nnopat = np.zeros(len(fy))\ninvest = np.zeros(len(fy))\ndepre = np.zeros(len(fy))\nnet_invest = np.zeros(len(fy))\nfcff = np.zeros(len(fy))\ndisc_fact = np.zeros(len(fy))\ndisc_fcff = np.zeros(len(fy))                \n\n# calculate values in table \nfor i in range(1,len(fy)): \n    net_op[i] = rev[i]*nopm # net operating profit\n    adj_taxes[i] = net_op[i]*tr # net operating profit adjusted for taxes\n    nopat[i] = net_op[i]-adj_taxes[i] # after tax net operating profit\n    invest[i] = rev[i]*ir # future investments\n    depre[i] = rev[i]*dr # future depreciations\n    net_invest[i] = invest[i]-depre[i] # net investments\n    fcff[i] = nopat[i]-net_invest[i]-ciwc[i] # free cash flow to the firm\n    disc_fact[i] = 1/((1+wacc_adj)**i) # discount factor\n    disc_fcff[i] = disc_fact[i]*fcff[i] # discounted free cash flow to the firm\n    \ndcrv = nopat[-1]/wacc*disc_fact[-1] # discounted corporate residual value\nderp_fcff = disc_fcff.sum() # discounted excess return period FCFF\n\ntcv = derp_fcff+dcrv+sta # total corporate value\ntvce = tcv-vod-vps-stl # total value of common equity\nisv = tvce/so # intrinsic stock value\n\n# print cash flows in a table\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format(\n    'Year','Rev','NOP','AdjTaxes',\n    'NOPAT','Invest.','Deprec.','dInvest.','dWC','FCFF','DF','DF*FCFF'))\nfor i in range(len(fy)):\n    print('{:4.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.4f}{:10,.0f}'.\n        format(fy[i],rev[i]/1e6,net_op[i]/1e6,adj_taxes[i]/1e6,nopat[i]/1e6,invest[i]/1e6,depre[i]/1e6,\n        (invest[i]-depre[i])/1e6,ciwc[i]/1e6,fcff[i]/1e6,disc_fact[i],disc_fcff[i]/1e6))\n\nYear       Rev       NOP  AdjTaxes     NOPAT   Invest.   Deprec.  dInvest.       dWC      FCFF        DF   DF*FCFF\n2022     1,074         0         0         0         0         0         0         0         0    0.0000         0\n2023     1,128       677         0       677       598       305       293         0       384    0.9138       350\n2024     1,185       711         0       711       628       320       308         0       403    0.8350       336\n2025     1,244       746         0       746       659       336       323         0       423    0.7630       323\n2026     1,306       784         0       784       692       353       340         0       444    0.6972       310\n2027     1,371       823         0       823       727       370       357         0       466    0.6371       297\n\n\n\n# Intrinsic Value\nprint('discounted excess return period FCFF: ${:,.2f}B'.format(derp_fcff/1e9))\nprint('discounted corporate residual value: ${:,.2f}B'.format(dcrv/1e9))\nprint('total corporate value: ${:,.2f}B'.format(tcv/1e9))\nprint('total value of common equity: ${:,.2f}B'.format(tvce/1e9))\ntvce_S1 = tvce # save value as scenario 1\nisv_S1 = isv # save the isv for scenario 1 case\nprint('intrinsic stock value, scenario 1 case: ${:,.2f}'.format(isv_S1))\nprint('current stock price: ${:,.2f}'.format(csp))\n\ndiscounted excess return period FCFF: $1.62B\ndiscounted corporate residual value: $7.05B\ntotal corporate value: $8.95B\ntotal value of common equity: $3.77B\nintrinsic stock value, scenario 1 case: $46.32\ncurrent stock price: $97.74\n\n\n\nprint('ISV serario 1 to current stock price ratio: {:,.2f}%'.format(isv_S1/csp*100))\n\nISV serario 1 to current stock price ratio: 47.39%\n\n\nThe DCF model calculates with adjustments an intrinsic stock value of \\$90, which is less than the current stock price. Based on ISV serario 1, current price is overvalued by 28%. Adjusting the WACC to 1% would make the ISV about equal to the current stock price."
  },
  {
    "objectID": "FRT analysis.html#naci-stock-selection-guide-analysis",
    "href": "FRT analysis.html#naci-stock-selection-guide-analysis",
    "title": "Two amplifier RIAA phono preamp",
    "section": "5) NACI stock selection guide analysis ",
    "text": "5) NACI stock selection guide analysis \nThis analysis follows the NAIC stock selection guide (SSG) [2]. The SSG relates revenue growth, EPS and share price history and makes a prediction about the future share price.\nThe National Association of Investors Clubs (NAIC) is a nonprofit organization dedicated to educating individual investors and investment clubs to become successful lifelong investors. NAIC’s Stock Selection Guide (SSG) is used in the following cells to analyze the company’s growth and whether the stock is selling at a reasonable price.\nThe SSG was originally developed in the 1950s as a paper worksheet by the not-for-profit National Association of Investors Corporation (NAIC). The SSG aims to aid individual investors in the fundamental analysis and selection of common stocks by reviewing components of a company’s growth, quality, and value.\n\nLoad data from metrics sheet\n\n# column names: fiscal years \nfy_data = df_metrics_sheet.columns[1:].values.astype('datetime64[Y]')-1970\n# line 0: Net income\nnet_income_data = df_metrics_sheet.iloc[0].to_numpy()[1:].astype('float')\n# line 1: Shareholder equity\nshareholder_equity_data =  df_metrics_sheet.iloc[1].to_numpy()[1:].astype('float')\n# line 2: Total liabilities\ntotal_liabilities_data = df_metrics_sheet.iloc[2].to_numpy()[1:].astype('float')\n# line 3: Free cash flow, Net cash provided by operating activities \nfree_cash_flow_data =  df_metrics_sheet.iloc[3].to_numpy()[1:].astype('float')\n# line 4: Dividends\ndividends_data =  df_metrics_sheet.iloc[4].to_numpy()[1:].astype('float')\n# line 5: Total assets\ntotal_assets_data = df_metrics_sheet.iloc[5].to_numpy()[1:].astype('float')\n# line 6: Earnings per share\neps_data = df_metrics_sheet.iloc[6].to_numpy()[1:].astype('float')\n# line 7: Dividends per share  \ndps_data = df_metrics_sheet.iloc[7].to_numpy()[1:].astype('float')\n# line 8: Total tangible assets\ntotal_tangible_assets_data = df_metrics_sheet.iloc[8].to_numpy()[1:].astype('float')\n# line 9: Liabilities w/o deposits\nliabilities_wo_deposits_data = df_metrics_sheet.iloc[9].to_numpy()[1:].astype('float')\n# line 10: Provision for credit losses\nprovision_for_credit_losses_data = df_metrics_sheet.iloc[10].to_numpy()[1:].astype('float')\n# line 11: Short-term borrowings\nshort_term_borrowings_data = df_metrics_sheet.iloc[11].to_numpy()[1:].astype('float')\n# line 12: Preferred stock\npreferred_stock_data = df_metrics_sheet.iloc[12].to_numpy()[1:].astype('float')\n# line 13: Net cash used in investing activities \nnet_cash_used_in_investing_activities_data = df_metrics_sheet.iloc[13].to_numpy()[1:].astype('float')\n\n\n# make a new data frame to store data from metrics sheet\ndf_metrics_data = pd.DataFrame(data={\n    'FY':fy_data[::-1],\n    'net_income':net_income_data[::-1],\n    'shareholder_equity':shareholder_equity_data[::-1],\n    'total_liabilities':total_liabilities_data[::-1],\n    'free_cash_flow':free_cash_flow_data[::-1],\n    'dividends':dividends_data[::-1],\n    'total_assets':total_assets_data[::-1],\n    'eps':eps_data[::-1],    \n    'dps':dps_data[::-1],\n    'total_tangible_assets':total_tangible_assets_data[::-1],\n    'liabilities_wo_deposits':liabilities_wo_deposits_data[::-1],    \n    'provision_for_credit_losses':provision_for_credit_losses_data[::-1],\n    'short_term_borrowings':short_term_borrowings_data[::-1], \n    'preferred_stock':preferred_stock_data[::-1],\n    'net_cash_used_in_investing_activities':net_cash_used_in_investing_activities_data[::-1]\n    })\n\n#df_metrics_data\n\ncheck for matching years in both data frames\n\nif all(df_dcf_data['FY'] == df_metrics_data['FY']) != True:\n    print('error, years in data frame don\\'t match')\n    stop # this is not python code, so jupyterlab will throw an error\nelse:\n    print('OK, years in data frame match')\n\nOK, years in data frame match\n\n\n\n\nNAIC section 1: Visual analysis\nHigh and low price history for each year\nFrom the daily price history obtained from yahoo finance, the high and low closing price for each is obtained and the data saved to the financial data frame as new columns.\n\n#column names: fiscal years \nyears_list = df_metrics_sheet.columns[1:].values.astype('str')[::-1]\n\n# convert years to datetime format\nyear_ended_list = []\nfor i in years_list:\n    year_ended_list.append(datetime.strptime(i, '%Y'))\n\n# make emnpy lists to store open, close, high and low price data for each fiscal year\nfy_open = []\nfy_close = []\nfy_high = []\nfy_low = []\n\nfor i in year_ended_list:\n    start = i\n    end = i + relativedelta(years=1)\n    p1 = df_price_history.truncate(before=start, after=end)\n    if len(p1) == 0:\n        fy_open.append(np.nan)\n        fy_close.append(np.nan)        \n        fy_high.append(np.nan)\n        fy_low.append(np.nan)\n    else:\n        fy_open.append(p1['Open'].iloc[0])\n        fy_close.append(p1['Close'].iloc[-1])        \n        fy_high.append(p1['Close'].max())\n        fy_low.append(p1['Close'].min())\n\n# convert from list to numpy array\nfy_open = np.asarray(fy_open)\nfy_close = np.asarray(fy_close)\nfy_high = np.asarray(fy_high)\nfy_low = np.asarray(fy_low)\n\nPlotting the data\nThe annual sales, EPS and the high and low share price is plotted on a semilog plot. A consistent percentage change in the data will plot on the semi-log chart as a straight line.\nThe stock price is plotted separately from the sales and earnings for clarity.\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\n# figsize() function to adjust the size\nplt.subplots(figsize=(15, 5))\n\n# using subplot function and creating\n# plot one\nplt.subplot(1, 2, 1)\nwidth = 3  # the width of the bars\n#plt.bar(year_ended_list,fy_high-fy_low, width,bottom=fy_low,label='price')\nj = 0\nfor i in year_ended_list:\n    color = 'green'\n    if fy_open[j] &gt; fy_close[j]: color= 'red'\n    # high/low lines\n    plt.plot([i,i],[fy_low[j],fy_high[j]],color=color, linewidth=width)\n    # open marker\n    plt.plot([i,i-relativedelta(months=1)], [fy_open[j],fy_open[j]], color=color, linewidth=width)\n    # close marker\n    plt.plot([i,i+relativedelta(months=1)], [fy_close[j],fy_close[j]], color=color, linewidth=width)\n    j += 1\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\n#plt.ylim((20,80))\nplt.title('Yearly stock high and low price range')\nplt.ylabel('stock price, $')\n#plt.legend()\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n\nplt.plot(df_metrics_data['FY'],df_dcf_data['revenue']/1e9,'+-',label='revenue, $B')\nplt.plot(df_metrics_data['FY'],df_metrics_data['eps'],'+-',label='EPS, $')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\n#plt.yscale('log')\n#plt.yticks([0.1,1,10,100,1000,10000],['0.1','1','10','100','1000','10000'])\n#plt.ylim((0.1,1000))\nplt.title('Revenue and EPS')\nplt.ylabel('Revenue and EPS')\nplt.legend()\n\nplt.grid()\n\n# space between the plots\nplt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\nObservation:\nShare price has been usually trading in the 80 to 140 dollar range the last 5 years, indicating that the market does not see FRT as a growth company. EPS have been eratic over the last 5 years.\n\n\nNAIC section 3, Price earnings history\nSection 3 of the SSG is the Price-Earnings history. The following table is built from the high and low prices each year and the earnings per share. The high and low Price/Earnings ratios are calculated for each year and are listed in the columns labeled h-per and l-per.\n\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format('year','high','low','eps',\n    'h-per','l-per'))\nfor i in range(len(year_ended_list)):\n    print('{:s}{:10,.2f}{:10,.2f}{:10,.2f}{:10,.2f}{:10,.2f}'.format(year_ended_list[i].strftime(\"%Y\"),\n        fy_high[i], fy_low[i],df_metrics_data['eps'][i],\n        fy_high[i]/df_metrics_data['eps'][i],\n        fy_low[i]/df_metrics_data['eps'][i]))\n\nyear      high       low       eps     h-per     l-per\n2010     84.32     63.07      1.99     42.37     31.69\n2011     92.45     75.31      2.29     40.37     32.89\n2012    110.03     89.23      2.36     46.62     37.81\n2013    117.96     96.21      2.47     47.76     38.95\n2014    137.18    100.90      2.42     56.69     41.69\n2015    150.27    124.96      3.04     49.43     41.11\n2016    170.35    136.98      3.51     48.53     39.03\n2017    145.29    120.52      3.97     36.60     30.36\n2018    135.55    108.11      3.18     42.63     34.00\n2019    141.16    115.81      4.61     30.62     25.12\n2020    131.07     65.81      1.62     80.91     40.62\n2021    137.12     82.27      3.26     42.06     25.24\n2022    139.37     87.91      4.71     29.59     18.66\n\n\nAverage high and P/E for select years\nThe average price to earning ratio based on high and low stock prices is calculated.\n\n#Average high P/E for years \npe_avg_high = (fy_high/df_metrics_data['eps']).mean()\nprint('average high P/E {:.2f}'.format(pe_avg_high))\n#Average low P/E for years \npe_avg_low = (fy_low/df_metrics_data['eps']).mean()\nprint('average low P/E {:.2f}'.format(pe_avg_low))\n\naverage high P/E 45.71\naverage low P/E 33.63\n\n\n\nEstimate future EPS\nUse polyfit to get EPS slope and intercept of a least square fit.\n\ny = df_metrics_data['eps']\nx = np.arange(len(y))\nm, c = np.polyfit(x, y, 1)\nprint('EPS slope: {:.2f}'.format(m))\nprint('EPS intercept: {:.2f}'.format(c))\nlstsq_fit = m*x + c  # data points for each year\n\nEPS slope: 0.15\nEPS intercept: 2.14\n\n\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('EPS')\n\nax1.plot(df_metrics_data['FY'],df_metrics_data['eps'], 'o',label='EPS')\nax1.plot(df_metrics_data['FY'],lstsq_fit, '-',label='least squares fit')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,4))\nax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('EPS and least squares fit')\nplt.show()\n\n\n\n\nUsing the equation for the best fit line, find the y value for the eps point at five years in the future.\n\n# estimated eps in 5 years\neps_5yr_est = m*(x[-1]+5) + c\nprint('estimated eps in 5 years: {:.1f}'.format(eps_5yr_est))\n\nestimated eps in 5 years: 4.7\n\n\nUsing the high and low price to earning ratio from above and the projected eps, calculate the range of stock price in five years.\n\nnaic_price_eps_low = eps_5yr_est*pe_avg_low\nnaic_price_eps_high = eps_5yr_est*pe_avg_high\nprint('estimated price range in 5 years: ${:.2f} to ${:.2f}'.format(naic_price_eps_low,naic_price_eps_high))\n\nestimated price range in 5 years: $157.04 to $213.44\n\n\nThis is the estimated price range of the stock based on projected EPS and is a guide for what the stock price might be if conditions remain the same. Since the slope of the EPS history is negative, the projected stock price is negative.\n\n\nNAIC section 3: 5 year estimated EPS, preferred method\nSee page 87 and figure 10-1, Need the following data:\n- estimate sales in 5 years based on sales growth - NOPM - Tax rate - shares outstanding\nNet Operating Profit should reflect the future revenue generating ability and expense requirements of the operating business that comprise the ongoing operations of the company.\n\\(\\text{NOPM} = \\frac{\\text{Revenue} - \\text{Expenses}}{\\text{Revenue}}\\)\nTax payments are taken from the consolidated income statement, provision for income taxes. The effect of taxes on profits is accounted for.\n\\(\\text{Tax rate} = \\frac{\\text{Income taxes}}{\\text{Income before income taxes}}\\)\nTo get future EPS\n\\(\\text{future EPS} = \\frac {\\text{future revenue} \\times \\text{NOPM} \\times \\text{(1-tax rate)}}{\\text{number of shares}}\\)\nUse polyfit to get revenue least square fit\n\ny = df_dcf_data['revenue']/1e6\nx = np.arange(len(y))\nm, c = np.polyfit(x, y, 1)\nprint('revenue slope: {:.2f}'.format(m))\nprint('revenue intercept: {:.2f}'.format(c))\nlstsq_fit = m*x + c  # data points for each year\n\nrevenue slope: 41.61\nrevenue intercept: 530.32\n\n\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $M')\n\nax1.plot(df_metrics_data['FY'],df_dcf_data['revenue']/1e6, 'o',label='revenue')\nax1.plot(df_metrics_data['FY'],lstsq_fit, '-',label='least squares fit')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,4))\nax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Revenue and least squares fit')\nplt.show()\n\n\n\n\nUsing the equation for the best fit line, find the y value for the EPS point at five years in the future.\n\n# estimated revenue in 5 years\nrev_5yr_est = m*(x[-1]+5) + c\nprint('estimated rev in 5 years: ${:,.1f}M'.format(rev_5yr_est))\n\nestimated rev in 5 years: $1,237.6M\n\n\nNote: might need to include estimate of number of shares outstanding in 5 years.\n\nprint('starting revenues: ${:,.2f}'.format(rev_start/1e9))\n\nstarting revenues: $1.07\n\n\nUsing the adjusted NOPM and tax rate from scenario 1.\nadjusted DCF input values and rates\n\npm_nopm = isv_s1_nopm # use nopm from scenario 1\npm_tax_rate = isv_s1_tr # use tr from scenario 1\npm_eps_5yr_est = rev_5yr_est*pm_nopm*(1-pm_tax_rate)*1e6/df_dcf_data['shares_outstanding'].iloc[-1] \n#pm_eps_5yr_est = rev_5yr_est*nopm_avg*1e6/df_dcf_data['shares_outstanding'].iloc[-1] \nprint('using preferred method: estimated eps in 5 years: ${:.2f}'.format(pm_eps_5yr_est))\n\nusing preferred method: estimated eps in 5 years: $9.13\n\n\nUsing the high and low price to earning ratio from above and the projected EPS, calculate the range of stock price in five years.\n\nnaic_price_pm_low = pm_eps_5yr_est*pe_avg_low\nnaic_price_pm_high = pm_eps_5yr_est*pe_avg_high\nprint('estimated price range in 5 years from preferred method: {:.2f} to {:.2f}'.format(\n    naic_price_pm_low,naic_price_pm_high))\n\nestimated price range in 5 years from preferred method: 306.95 to 417.19\n\n\nObservation:\nBased on revenue growth, the projected stock price is higher than the current price. However, based on price history, the stock is not expected to appreciate."
  },
  {
    "objectID": "FRT analysis.html#future-stock-price",
    "href": "FRT analysis.html#future-stock-price",
    "title": "Two amplifier RIAA phono preamp",
    "section": "6) Future stock price ",
    "text": "6) Future stock price \nThe projected future stock price is estimated from the results shown in this notebook based on DCF intrinsic stock value, the NAIC method or a combination of both. The DCF method does not consider market sentiment or popularity of the stock, whereas the NAIC method looks at the PE and EPS to develop the historical consensus that the market has put on the price of the stock. Both the NAIC and the DCF valuation should be considered. The DCF valuation is of the current ISV which is used as an indication of the future value, since it is assumed that the market price will converge eventually to the intrinsic value.\nThe estimated future stock price considers the following:\n- base case ISV - Senario ISV - NAIC EPS growth - NAIC preferred method\nUsing 5 year NAIC as a conservative estimate for the 10 year value and the analysis results, a judgment call is made concerning the price to put on the future value of the stock.\n\nprint('estimated price range in 5 years from EPS: ${:.2f} to ${:.2f}'.format(naic_price_eps_low,naic_price_eps_high))\nprint('estimated price range in 5 years from preferred method: ${:.2f} to ${:.2f}'.format(\n    naic_price_pm_low,naic_price_pm_high))\n\nprint('intrinsic stock value, baseline case: ${:,.2f}'.format(isv_baseline))\nprint('intrinsic stock value, scenario 1 case: ${:,.2f}'.format(isv_S1))\n\nprint('current stock price: ${:,.2f}'.format(csp))\n\nestimated price range in 5 years from EPS: $157.04 to $213.44\nestimated price range in 5 years from preferred method: $306.95 to $417.19\nintrinsic stock value, baseline case: $66.99\nintrinsic stock value, scenario 1 case: $46.32\ncurrent stock price: $97.74\n\n\nThe estimated price range in 5 years from the preferred method is \\$86.14 to \\$116.85. Taking the average and using that value on the IRR calculations.\nUsing the average of:\n- low estimated price from EPS and the low - estimated price from the preferred method - intrinsic stock value, scenario 1 case\nuse average of NAIC low price\n\n#fsp = (naic_price_eps_low + naic_price_pm_low + csp)/3 # estimated future stock price\nfsp = (naic_price_eps_low) # estimated future stock price\nprint('estimated future stock price: ${:,.2f}'.format(fsp))\n\nestimated future stock price: $157.04"
  },
  {
    "objectID": "FRT analysis.html#dividend-payout",
    "href": "FRT analysis.html#dividend-payout",
    "title": "Two amplifier RIAA phono preamp",
    "section": "7) Dividend payout ",
    "text": "7) Dividend payout \nThe dividend payout examines the amount shareholders are getting from the company relative to earnings or revenue. It is an important metric to determine how the business is operating and whether it has enough growth potential.\n\nDividend history\n\n# calculate the percent change in dividends\npcd = np.zeros(len(df_metrics_data['dps'])) # percent change in dividend\nfor i in range(len(df_metrics_data['dps'][0:-1])):\n    pcd[i+1] = ((df_metrics_data['dps'][i+1] - df_metrics_data['dps'][i])/\n                df_metrics_data['dps'][i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dividend per share, $')\n\n# plot revenue as single bar\nplt.bar(df_metrics_data['FY'],df_metrics_data['dps'], width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(year_ended_list,pcd,'+-g')\n    \nax2.set_ylabel('% Change in dividend',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,20))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Dividend history per share')\nplt.show()\n\n\n\n\n\nadgr = pcd[-6:].mean() #last 6 years\nprint('average dividend growth rate: {:.2f}%'.format(adgr))\n\naverage dividend growth rate: 2.00%\n\n\n\n\nDividend yield\nDividend yield equals the annual dividend per share divided by the stock’s price per share. The plot below shows the history of dividend yield over the evaluation period.\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nwidth = 50  # the width of the bars\nplt.bar(df_metrics_data['FY'],(df_metrics_data['dps']/fy_high-df_metrics_data['dps']/fy_low)*100, \n        width,bottom=df_metrics_data['dps']/fy_low*100,label='yield')\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.ylim((1,10))\nplt.title('Range of dividend yield each year')\nplt.ylabel('dividend yield, %')\n#plt.legend()\nplt.grid()\n\n# show plot\nplt.show()\n\n\n\n\nThe dividend yield for the past five years has been in the 2.5 to 3.5 percent range.\n\n\nDividend payout ratio\nThe dividend payout ratio is a relative measure of how much the company is paying to shareholders in dividends compared to other metrics such as revenue, earnings or cash flow. The dividend payout ratio is plotted as a ratio of dividends to net income, free cash flow (Net cash provided by operating activities) and NOP. The payout ratio is useful for assessing a dividend’s sustainability. Companies are extremely reluctant to cut dividends since it can drive the stock price down and reflect poorly on management’s abilities.\nPayout ratio using net income\nPayout ratio using net income plots the ratio of dividend payout divided by net income:\n\\(\\frac {\\text{Dividends}}{\\text{Net income}}\\)\nDepending on how net income is listed in the financial statements, it may include large other charges.\nPayout ratio using cash flow\nPayout ratio using net cash flow plots the ratio of dividend payout divided by cash flow:\n\\(\\frac {\\text{Dividends}}{\\text{cash flow}}\\)\nCash flow from operating activities usually includes a long list of items. Some insight might be obtained from this ratio. The trend should be consistent.\nPayout ratio using NOP\nPayout ratio using NOP plots the ratio of dividend payout divided by NOP:\n\\(\\frac {\\text{Dividends}}{\\text{NOP}}\\)\nNOP is calculated above and might be different from net income listed in the financial statements. This ratio should be the lowest numerically of the three plots.\n\nAdjustments for a REIT\nDividend payout ratio does not apply to a REIT because of the 90% payout requirement.\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Payout ratio')\n\nax1.plot(df_metrics_data['FY'],df_metrics_data['dividends']/df_metrics_data['net_income'], '-+',\n    label='Payout ratio using net income')\nax1.plot(df_metrics_data['FY'],df_metrics_data['dividends']/df_metrics_data['free_cash_flow'], '-*',\n    label='Payout ratio using cash flow')\nax1.plot(df_metrics_data['FY'],df_metrics_data['dividends']/nop, '-+',\n    label='dividends/NOP')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,5))\nax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Payout ratio')\nplt.show()\n\n\n\n\n\n# average the last three years\nprint('Dividends are paid at {:.1f}% of net income'.format(\n    (df_metrics_data['dividends']/df_metrics_data['net_income'])[-3:].mean()*100))\nprint('Dividends are paid at {:.1f}% of cash flow'.format(\n    (df_metrics_data['dividends']/df_metrics_data['free_cash_flow'])[-3:].mean()*100))\nprint('Dividends are paid at {:.1f}% of NOP'.format((df_metrics_data['dividends']/nop)[-3:].mean()*100))\n\nDividends are paid at 150.5% of net income\nDividends are paid at 75.4% of cash flow\nDividends are paid at 58.0% of NOP\n\n\nPayout ratio using net income: Payout ratio using cash flow:\nPayout ratio using NOP:\n\n\n\nInternal Rate of Return (IRR) calculations\nThe internal rate of return (IRR) is the discount rate that makes the net present value (NPV) of all cash flows equal to zero in a discounted cash flow analysis. Generally speaking, the higher an internal rate of return, the more desirable an investment is to undertake.\nAs explained above, the stock price has not changed by much over the years, even though the revenue and dividends have been increasing. The final stock price is set equal to the current price.\nUsing the average dividend growth rate calculated above, a series of estimated future dividend payments are generated.\n\nfdp = np.zeros(len(df_metrics_data['dps'])) # future dividend payments\nfdp[0] = df_metrics_data['dps'].iat[-1]\nfor i in range(len(df_metrics_data['dps'][0:-1])):\n    fdp[i+1] = fdp[i]+fdp[i]*adgr/100\n\n\nprint('current stock price: ${:,.2f}'.format(csp))\n\n#fsp = 100 #csp #500 #(csp + 102.05 + 138.82)/3 # final stock price, $\nprint('final stock price: ${:,.2f}'.format(fsp))\n\ncurrent stock price: $97.74\nfinal stock price: $157.04\n\n\n\nest_cf = np.copy(fdp) # make a copy of the estimated cash flow\n\n# cash flows, initial purchase, dividend payments and final sale\nest_cf[0] = est_cf[0] - csp # subtract purchase price from the first dividend payment\nest_cf[-1] = est_cf[-1] + fsp # include the sale price with the final dividend payment\n\n\ndividend_irr = np.irr(est_cf)\nprint('Dividend IRR: {:.2f}%'.format(dividend_irr*100))\n\nDividend IRR: 8.57%\n\n\nAccording to global investment bank Goldman Sachs, 10-year stock market returns have averaged 9.2% over the past 140 years. and according to 10-Year Annualized Rolling Returns, the long term average is about 10%. However there are many years where the rolling 10 year average return is below 4%.\nThe calculated IRR is 8%, which is a decent return and significantly higher than current interest rates."
  },
  {
    "objectID": "FRT analysis.html#management-performance",
    "href": "FRT analysis.html#management-performance",
    "title": "Two amplifier RIAA phono preamp",
    "section": "8) Management performance ",
    "text": "8) Management performance \nThe following analysis somewhat follows the Warren Buffett strategy as outlined in [3]. This strategy is essentially value investing where companies are chosen that meet a set of criteria and who’s stock price is below the intrinsic value plus a margin of safety. These investments are usually held for the long term.\n\nFinancial metrics\nThe following analysis looks at financial ratios over the evaluation period. Financial ratios can be used to judge management performance. Consistent favorable trends are an indication that management is taking care of the company.\nTotal liabilities to total assets ratio\nDebt to equity and debt to NOP ratios\nFinancial ratios: RoE, RoA and PM\nNAIC section 2: Evaluating management\nNormalized data from consolidated statements\nMarket metrics\nOne dollar premise\nShare price vs EPS\nMarket capitalization\nQualitative metrics\nSimple and understandable business model\nFavorable long term prospects\nCommodity reliance\nConsistent operating history\nrationality:\n\nfocus on core aspects\nonly invest in high ROE businesses\nfocus on shareholder equity\n\n\n\nFinancial metrics \nThe following financial metrics are examined over the evaluation period. We are looking for favorable trends and evidence of consistent operations. Some red flags will also be evident in the plots.\nRed flags:\n- Shrinking gross profit margin - Receivables growing faster than sales - Rising debt-to-equity ratio - Several years of revenue trending down - Unsteady cash flow - Rising accounts receivable or inventory in relation to sales - Rising outstanding share count - Consistently higher liabilities than assets - Decreasing gross profit margin - Increasing revenue while cash flow remains the same - Unusual changes in key financial ratios\n\nTotal liabilities to total assets ratio\nThe ratio of liabilities to assets is plotted over the evaluation period. For most companies examined the liabilities are the total liabilities and the ratio is calculated using total assets and total tangible assets. Total tangible assets have goodwill and intangibles removed from the total. The ratio gives an indication of how much the company is worth versus how much the company owes. Ideally the ratio of liabilities to assets should be less than one.\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('ratio')\n\n# plot liabilities\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['total_assets'], '-+',\n    label='total liabilities to total assets')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['total_tangible_assets'], '-*',\n    label='total liabilities to total tangible assets')\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,1))\nax1.legend(bbox_to_anchor=(1.8, 1))\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\n#ax2.plot(year_ended_list,pcd,'+-g')\nax2.plot(df_metrics_data['FY'],\n    (df_metrics_data['total_assets']-df_metrics_data['total_tangible_assets'])/df_metrics_data['total_assets']*100,\n    ':',color=color,label='intangible assets to total assets')\n    \nax2.set_ylabel('% intangible assets',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,100))\nax2.legend(bbox_to_anchor=(1.7, 0))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Total liabilities to total assets ratio')\nplt.show()\n\n\n\n\nFRT has no intangible assets, total assets are the same as total tangible assets. Total liabilities to total assets ratio is about 60%.\n\n\nDebt to equity and debt to NOP ratios\nThe debt-to-equity ratio (D/E) is another key characteristic Buffett considers carefully. Buffett prefers to see a small amount of debt so that earnings growth is being generated from shareholders’ equity as opposed to borrowed money. The D/E ratio is calculated as follows:\n\\(\\text{Debt-to-Equity Ratio} = \\frac {\\text{Total Liabilities}} {\\text{Shareholders' Equity}} \\text{  OR  } \\frac {\\text{Long term debt}} {\\text{Shareholders' Equity}}\\)\nThis ratio shows the proportion of equity and debt the company uses to finance its assets, and the higher the ratio, the more debt—rather than equity—is financing the company. A high debt level compared to equity can result in volatile earnings and large interest expenses. For a more stringent test, investors sometimes use only long-term debt instead of total liabilities in the calculation above.\nD/E is the traditional way to look at a company’s debt. Some rules of thumb say that the D/E should not be above 2 or 3. However the D/E company’s typically vary by industry. The ratio of LT debt to NOP gives the number of years it would take the company to pay back debt from NOP, the lower the number the shorter amount of time.\n\\(\\text{Debt-to-NOP Ratio} = \\frac {\\text{Total Liabilities}} {\\text{NOP}}\\)\n\ntangible_equity = df_metrics_data['total_tangible_assets'] - df_metrics_data['total_liabilities']\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('ratio')\n\nax1.plot(df_metrics_data['FY'],df_dcf_data['long_term_debt']/df_metrics_data['shareholder_equity'],\n    '-^',label='(LT debt)/Equity')\n#ax1.plot(year_ended_list,df_dcf_data['long_term_debt']/tangible_equity, '-',label='(LT debt)/(Tangible Equity)')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['shareholder_equity'],\n    '-*',label='(total liabilities)/Equity')\n#ax1.plot(year_ended_list,total_liabilities/BV, '-^',label='(total liabilities)/BV')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/nop, '-+',label='(total liabilities)/NOP')\n#ax1.plot(year_ended_list,total_liabilities/net_income, '-+',label='(total liabilities)/(net income)')\n#ax1.plot(year_ended_list,df_dcf_data['current_liabilities']/nop, '-*',label='(current liabilities)/NOP')\n#ax1.plot(year_ended_list,Liabilities_wo_deposits/nop, '-+',label='(Liabilities w/o deposits)/NOP')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,10))\n#ax1.legend()\nax1.legend(bbox_to_anchor=(1.6, 1))\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Various debt ratios')\nplt.show()\n\n\n\n\n(LT debt)/Equity is plotted and is below 2 for each year in the evaluation period. A threshold of 2 is traditionally the upper limit for a reasonable amount of debt that a company should carry.\n(total liabilities)/Equity is plotted and except for 2020 has been below the threshold of 2.\n(total liabilities)/NOP to is plotted for each year in the evaluation period is below 10. A value of 10 has been chosen as the threshold for this ratio and indicates how many years it would take the company to pay off total liabilities from the NOP generated each year. A threshold of ten seems like a reasonable level of debt measured against NOP.\n\n\nFinancial ratios\nVarious ratios can be used to judge management performance. Consistent favorable trends are an indication that management is taking care of the company.\nReturn on equity\nSometimes return on equity (RoE) is referred to as stockholder’s return on investment. It reveals the rate at which shareholders earn income on their shares. Buffett always looks at RoE to see whether a company has consistently performed well compared to other companies in the same industry. RoE is calculated as follows:\n\\(\\text{Return on Equity} = \\frac {\\text{Net Income}} {\\text{Shareholder's Equity}}\\)\nLooking at the RoE in just the last year isn’t enough. The investor should view the RoE from the past five to 10 years to analyze historical performance.\n\\(\\text{Shareholders’ Equity} = \\text{Total Assets} − \\text{Total Liabilities}\\)\nFor this company, this method of getting Shareholders’ Equity gives negative values. On the Consolidated Balance Sheets, there is a line for Total stockholders’ equity, which is used.\nReturn on Assets\nReturn on assets is a profitability ratio that provides how much profit a company is able to generate from its assets. In other words, return on assets (RoA) measures how efficient a company’s management is in generating earnings from their economic resources or assets on their balance sheet.\n\\(\\text{Return on assets} = \\frac {\\text{Net Income}} {\\text{Total Assets}}\\)\nCalculating the RoA of a company can be helpful in comparing a company’s profitability over multiple quarters and years as well as comparing to similar companies. However, it’s important to compare companies of similar size and industry.\nFor example, banks tend to have a large number of total assets on their books in the form of loans, cash, and investments. A large bank could easily have over \\$2 trillion in assets while putting up a net income that’s similar to companies in other industries. Although the bank’s net income or profit might be similar to an unrelated company and the bank might have high-quality assets, the bank’s RoA will be lower. The larger number of total assets must be divided into the net income, creating a lower RoA for the bank.\nSimilarly, auto manufacturing requires huge facilities and specialized equipment. A lucrative software company that sells downloadable programs online may generate the same net profits, but it could have a significantly higher RoA than its more asset-heavy counterparts. When utilizing this metric to compare productivity across businesses, it’s important to take into account what types of assets are required to function in a given industry, rather than simply comparing the figures.\nProfit Margin\nA company’s profitability depends not only on having a good profit margin, but also on consistently increasing it. This margin is calculated by dividing net income by net sales. For a good indication of historical profit margins, investors should look back at least five years. A high-profit margin indicates the company is executing its business well, but increasing margins mean management has been extremely efficient and successful at controlling expenses.\n\\(\\text{Profit Margin} = \\frac {\\text{Net Income}} {\\text{Revenue}}\\)\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('percent')\n\nax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_metrics_data['shareholder_equity']*100,\n    '-+',label='RoE')\nax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_metrics_data['total_assets']*100,\n    '-*',label='RoA')\n#ax1.plot(df_metrics_data['FY'],total_liabilities/shareholder_equity, '-^',label='D/E')\nax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_dcf_data['revenue']*100,\n    '-^',label='Profit margin')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,14))\n#ax1.legend()\nax1.legend(bbox_to_anchor=(1.05, 1))\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Various ratios')\nplt.show()\n\n\n\n\nObservation:\nThe trends for RoE, RoA and profit margin are shown above. The effect of the acquisition of Celgene has caused these ratios to go negative in 2020. From 2016 to 2019, these ratios had wide variation year to year, with 2017 showing a decline followed by a large increase the following year. Normally steady performance is better than the erratic variation shown here.\n\n\n\nNAIC section 2: Evaluating management\nSee page 86, figure 9-1.\n- % pretax profit on sales, (net before taxes)/rev - % earned on equity (another way of saying RoE, using calculated equity)\nPercent earned on equity is a measure of financial performance calculated by dividing net income by equity. Because equity is equal to a company’s assets minus its debt, percent earned on equity is considered the return on net assets. Percent earned on equity is considered a gauge of a corporation’s profitability and how efficient it is in generating profits.\nThis section is not applicable to a REIT, since income before taxes and income tax are not considered since the values are low."
  },
  {
    "objectID": "FRT analysis.html#decision-model",
    "href": "FRT analysis.html#decision-model",
    "title": "Two amplifier RIAA phono preamp",
    "section": "9) Decision model ",
    "text": "9) Decision model \nThe decision model establishes thresholds that are to used in the purchase decision. There are three hard decision thresholds in this model which are:\n1. Intrinic value 2. Debt 3. Dividend payout ratio 4. Dividend IIR\nThe first threshold is based on the intrinisic value of the company as calculated by the DCF model semario 1. Reconizing that absolute intrinsic value is an elusive concept, judgement, justified by facts (assets, earnings, dividends, debt and cash flow), establishes the value by adjusting various rates, based on judgement and using a five year forward projection period. This should give a intrinsic value that is based on the historical data, modified by judgement.\nI’m using a threshold of the intrinsic value calculated in senario 1 (isv_S1) that is greater than 70% of the current stock price, provided that the NAIC valuation is above the current stock price. This accounts for the inadequacy or incorrectness of the data, the uncertainties of the future, and considers the behavior of the market.\nThe second threshold is the level of debt. The ratios of (LT debt)/Equity, (total liabilities)/Equity and (total liabilities)/NOP are ploted for the evaluation period. Over the evaluation period the (LT debt)/Equity and (total liabilities)/Equity should be less than 2 and stable. A threshold of 2 has been discussed in the litureture as a level of debt that a company can reasonably take on.\nThe thereshold for (total liabilities)/NOP is set at 10. This means that the company can pay off all the liabilities with tens years worth of NOP, which seems like a reasonable timeframe for an established and stable company.\nThe third threshold is the dividend payout ratio and is a relative measure of how much the company is paying to shareholders in dividends compared to the metrics of NOP and free cash flow (Net cash provided by operating activities). The payout ratio is useful for assessing a dividend’s sustainability. Payout ratio for a REIT is established by tax law and not used as an evaluation criteria. For other industries a threshold of 50% has been set as the limit.\nThe dividend IRR threshold is the internal rate of return for investor dividend cash flow (divident_irr) should be greater than 10 year treasury bond yield (tby) plus the equity risk premium (eq_rp). Otherwise, other investment operatunities should be looked at.\nIn the decision model there are soft thresholds based on judgement. Soft thresholds are a collection of ratios and analysis that taken together tell a story of the performance of the conmpany and manatgments ability to run the company and support dividends over the long term. Use judgement and make an evalaution.\nThe third critiera is a collection of ratios and analysis that taken together tell a story of the performance of the conmpany and manatgments ability to run the company and support dividends over the long term. Use judgement and make an evalaution. These are the following:\n1. Financial metrics 2. Market metrics 3. Qualitative metrics\nThe soft thresholds are discused in section 10.\n\nCheck DCF and NAIC value thresholds\n\n# check DCF senario 1\ndcf_score = isv_S1/csp #ratio of isv to csp\ndcf_threshold = 0.7\nif dcf_score &lt; 0.7:\n    print('FAIL, DCF score is less than {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\nelse:\n    print('PASS, DCF score is above {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\n\n# check NAIC\nnaic_score = np.array([naic_price_eps_low,naic_price_pm_low]).min()/csp\nnaic_threshold = 1\nif naic_score &lt; 1:\n    print('FAIL, NAIC score is less than {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\nelse:\n    print('PASS, NAIC score is above {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\n\n# check both scores\nif naic_score &lt; 1 or dcf_score &lt; 0.7:\n    print('One or both DCF and NAIC scores failed')\nelse:\n    print('Both DCF and NAIC scores pass')\n\nFAIL, DCF score is less than 0.7 at 0.5\nPASS, NAIC score is above 1.0 at 1.6\nOne or both DCF and NAIC scores failed\n\n\n\n\nCheck debt thresholds\n\ndebt_lookback = 4\navg_LT_debt2EQ = df_dcf_data['long_term_debt'][-debt_lookback:].mean()/df_metrics_data['shareholder_equity'][-debt_lookback:].mean()\navg_TLiability2EQ = df_metrics_data['total_liabilities'][-debt_lookback:].mean()/df_metrics_data['shareholder_equity'][-debt_lookback:].mean()\navg_TLiability2NOP = df_metrics_data['total_liabilities'][-debt_lookback:].mean()/nop[-debt_lookback:].mean()\n\nprint('long term debt to shareholder equity ratio = {:.2f}'.format(avg_LT_debt2EQ))\nprint('total liabilities to shareholder equity ratio = {:.2f}'.format(avg_TLiability2EQ))\nprint('total liabilities to NOP ratio = {:.2f}'.format(avg_TLiability2NOP))\n\nif (avg_LT_debt2EQ &gt; 2) or (avg_TLiability2EQ &gt; 2) or (avg_TLiability2NOP &gt; 10):\n    print('FAILED one of the debt threshold limits')\n\nlong term debt to shareholder equity ratio = 1.56\ntotal liabilities to shareholder equity ratio = 1.78\ntotal liabilities to NOP ratio = 7.97\n\n\n\n\nCheck dividend payout and IIR thresholds\n\n# check dividend payout ratio average the last three years\nprint('Dividends are paid at {:.1f}% of cash flow'.format(\n    (df_metrics_data['dividends']/df_metrics_data['free_cash_flow'])[-3:].mean()*100))\nprint('Dividends are paid at {:.1f}% of NOP'.format((df_metrics_data['dividends']/nop)[-3:].mean()*100))\n\nif ((df_metrics_data['dividends']/nop)[-3:].mean() or (df_metrics_data['dividends']/df_metrics_data['free_cash_flow'])[-3:].mean()) &gt; 0.5:\n    print('FAIL, dividend payout ration too high')\n\nDividends are paid at 75.4% of cash flow\nDividends are paid at 58.0% of NOP\nFAIL, dividend payout ration too high\n\n\n\n# Check dividend IRR limit\nif dividend_irr &lt; (tby+eq_rp):\n    print('FAIL, dividend IRR is less than {:.2f} at {:.2f}'.format((tby+eq_rp)*100,dividend_irr*100))\nelse:\n    print('PASS, dividend IRR is above {:.2f} at {:.2f}'.format((tby+eq_rp)*100,dividend_irr*100))\n\nPASS, dividend IRR is above 6.48 at 8.57\n\n\n\n# check DCF senario 1\ndcf_score = isv_S1/csp #ratio of isv to csp\ndcf_threshold = 0.7\nif dcf_score &lt; 0.7:\n    print('FAIL, DCF score is less than {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\nelse:\n    print('PASS, DCF score is above {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\n\n# check NAIC\nnaic_score = np.array([naic_price_eps_low,naic_price_pm_low]).min()/csp\nnaic_threshold = 1\nif naic_score &lt; 1:\n    print('FAIL, NAIC score is less than {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\nelse:\n    print('PASS, NAIC score is above {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\n\n# check both scores\nif naic_score &lt; 1 and dcf_score &lt; 0.7:\n    print('One or both DCF and NAIC scores failed')\nelse:\n    print('Both DCF and NAIC scores pass')\n\nFAIL, DCF score is less than 0.7 at 0.5\nPASS, NAIC score is above 1.0 at 1.6\nBoth DCF and NAIC scores pass"
  },
  {
    "objectID": "FRT analysis.html#conclusion",
    "href": "FRT analysis.html#conclusion",
    "title": "Two amplifier RIAA phono preamp",
    "section": "10) Conclusion ",
    "text": "10) Conclusion \nThe following is a summary of the results described above:\n- Stock screener results:. - Current news: - Review quarterly results: - Average daily volume: - Dividend yield: - Discounted cash flow analysis, baseline: - DCF Scenarios: Adjustments were made and the ISV\n- NACI stock selection guide analysis: - Dividend payout: - Management performance:\n- Financial metrics: - Market metrics: - Qualitative metrics: see above\nConcerns:\nSummary:\nRecommendation: Don’t buy above $50 per share."
  },
  {
    "objectID": "FRT analysis.html#notes",
    "href": "FRT analysis.html#notes",
    "title": "Two amplifier RIAA phono preamp",
    "section": "10) Notes ",
    "text": "10) Notes \nThe following notes outline the changes to the DCF model for financial and REIT companies.\nValuing a REIT\nNotes from Valuepro Book, page 237\n\nNOPM: To calculate operating income take rental revenue and subtracted total real estate expenses and G&A expenses. To arrive at the NOPM divide the adjusted income from real estate by real estate rental revenue. For the REIT, take income from real estate, which includes depreciation and amortization, and subtract GSA. Exclude other income, gains on sale of real estate and interest expenses.\nREIT has no traditional R&D costs\n\nREIT is not taxed at the corporate level, tax rate: should be near zero.\nDepreciation and capital expenditures are significantly higher for REITs than in other companies.\nNew property acquisitions are not directly accounted for in the DCF model for a REIT.\n\nWorking capitol: accounts payable, rents and security deposits\nShort term assets: cash, rents and other receivables and prepaid expenses\nShort term liabilities: accounts payable, advance rents security deposits\n\nWorking capital is almost zero, which is similar to other financial companies.\nThe consolidated balance sheet lists the assets as: - Real estate held for investment, at cost: - Land - Buildings and improvements - Total real estate held for investment, at cost - Less accumulated depreciation and amortization - Real estate held for investment, net - Real estate and lease intangibles held for sale, net - Cash and cash equivalents &lt;- current asset - Accounts receivable, net &lt;- current asset - Lease intangible assets, net - Other assets, net\nThe line items indicated above have been taken to be the current assets. Intangibles and long term items have been excluded.\nThe consolidated balance sheet lists the liabilities as: - Distributions payable &lt;- current liabilities - Accounts payable and accrued expenses &lt;- current liabilities - Lease intangible liabilities, net - Other liabilities - Line of credit payable and commercial paper &lt;- current liabilities - Term loans, net - Mortgages payable, net &lt;- current liabilities - Notes payable, net\nThe line items indicated above have been taken to be the current liabilities.\nValuing a financial company\nNotes from Valuepro Book, page 206\n\nTotal revenue comes from the total interest and dividend income line on the income statement. The calculation of operating income is more inclusive for a financial company than for an industrial or high tech company. For financial companies, operating revenue includes all normal revenue items plus interest income, dividends received and other investment income.\nCost of Goods Sold (CGS) comes from the Total interest expense line on the statement of income.\nGeneral and Administrative (G&A) are set to zero since they are included in the interest expense line\nA financial company has no traditional R&D costs\n\\(\\text{Cost of Goods Sold (CGS)} = \\text{Total interest expense} + \\text{Total non-interest expense}\\)\n\nGeneral and Administrative (G&A) are set to zero since they are included in the interest expense line\nA financial company has no traditional R&D costs\nDepreciation and amortization of premises and equipment from Consolidated Statements of Cash Flows.\n\nAmortization of other acquisition-related intangible assets is not included.\nNew investment and Depreciation: Property, plant and equipment expenditures and depreciation charges are significantly lower for a financial company. A typical manufacturing company, in order to grow its business, invests a significant portion of its revenues in plant, property and equipment (PPE). Financial companies invest very little in the way of PPE. However, software, risk management systems and acquisitions of other businesses, need to be included.\n\nFrom the Consolidated Statements of Cash Flows, under Cash Flows from Investing Activities - Purchases of premises and equipment - Purchases of leased equipment, net\n\nWorking capital supports manufacturing and service activities of nonfinancial companies. For financial companies, their principal liabilities and assets are financial claims that take the place of working capital. Because there is no differentiation between current and long term assets and liabilities for a financial company, we adjust working capital charges to zero. A financial company generally invests all of its funds in other financial assets, which have characteristics of current assets rather than PP&E.\n\\(\\text{Accounts Receivable} = 0\\)\n\\(\\text{Inventories} = 0\\)\n\\(\\text{Accounts Payable} = 0\\)\n\\(\\text{working capital} = 0\\)\nShort term assets: The balance sheets of most financial companies do not separate assets and liabilities into current and long term categories. When calculating the short term assets take the total assets and subtract goodwill and intangible assets also subtract other assets of questionable value. Subtract long term assets such as PP&E from total assets.\n\n\\(\\text{Short term assets} = \\text{Total assets} - \\text{good will and others of questionable value} - \\text{Premises and equipment}\\)\n\nA financial company’s principal liabilities are deposits, Federal funds purchased, trading account liabilities, insurance policy and claims reserves, contract holder funds and short term borrowing. To be consistent with the treatment of interest and an operating expense for financial companies, include long term debt in the short term liability category.\n\nShort term liabilities: Include long term debt.\n\n\\(\\text{Long term debt} = 0\\)\nExcess return period\nThe excess return period is based on a judgment call. The authors of [2] use the 1-5-7-10 rule. They group companies into one of four general categories and excess return periods. They use a 10 year excess return period to calculate what they would consider the maximum value. They use a more conservative 1 year, 5 year or 7 year return period to calculate a more reasonable or minimum value.\n- 1 year: Boring companies that operate in a highly competitive, low margin industry in which they have nothing particular going for them.\n- 5 year: Decent companies that have a recognizable name and decent reputation and perhaps a regulatory benefit (utility company), but can’t control pricing or growth.\n- 7 year: Good companies with good brand names, large companies of scale, good marketing channels and consumer identification (e.g. McDonald’s)\n- 10 year: Great companies with great growth potential, tremendous marketing power, band names and in-place benefits (e.g. Intel, Microsoft, Coca Cola, Disney)\nNotes about negative working capital\nThe company has a negative working capital rate. Negative working capital describes a situation where a company’s current liabilities exceed its current assets as stated on the firm’s balance sheet. In other words, there is more short-term debt than there are short-term assets.\nNegative working capital most often arises when a business generates cash very quickly because it can sell products to its customers before it has to pay the bills to its vendors for the original goods or raw materials. In this way, the company is effectively using the vendor’s money to grow.\nDividend Aristocrat, Achiever & Champion\nThis company was selected for analysis because it is on the Dividend Aristocrat list and passes the quick look tests. This notebook will be used as a template when analyzing other companies.\n\nAristocrat: S&P 500 Dividend Aristocrats is designed to measure the performance of S&P 500 index constituents that have followed a policy of consistently increasing dividends every year for at least 25 consecutive years.\nAchiever: The Broad Dividend Achievers Index. Eligible companies must be incorporated in the U.S. or its territories, trade on the NYSE, NASDAQ or AMEX, and have increased its annual regular dividend payments for the last 10 or more consecutive years.\nhttps://dividendvaluebuilder.com/dividend-achievers-list/\nhttps://www.marketbeat.com/dividends/achievers/\nChampion: This list includes companies that had increased their dividend for at least 25 consecutive years, and includes additional companies that had paid higher dividends without having increased the payout in every calendar year.\nhttps://dividendvaluebuilder.com/dividend-champions-list/\nhttps://www.dividendgrowthinvestor.com/p/dividend-champions-list.html"
  },
  {
    "objectID": "FRT analysis.html#references",
    "href": "FRT analysis.html#references",
    "title": "Two amplifier RIAA phono preamp",
    "section": "12) References ",
    "text": "12) References \n\nGray, Gary, et al. Streetsmart Guide to Valuing a Stock: the Savvy Investors Key to Beating the Market. McGraw-Hill, 2004.\nO’Hara, Thomas E., and Ken Janke. Starting and Running a Profitable Investment Club: the Official Guide from the National Association of Investors Corporation. Times Business, 1998.\nRobert G. Hagstrom, The Warren Buffett Way, Wiley, 2013"
  },
  {
    "objectID": "SMNA_example.html",
    "href": "SMNA_example.html",
    "title": "Symbolic modified nodal analysis example",
    "section": "",
    "text": "This JupyterLab notebook uses the SymPy, NumPy,SciPy and the Python programming language libraries to analyize an electrical circuit. A circuit analysis method called the Modified Nodal Analysis was used to derive the symbolic circuit equations and Python libraries were used to solve the equations. The purpose of this analysis is to demonstrate the capability of using the Python libraries in electrical engineering circuit analysis. A link to my Jupyter Notebook rendered as a web page is here."
  },
  {
    "objectID": "SMNA_example.html#abstract",
    "href": "SMNA_example.html#abstract",
    "title": "Symbolic modified nodal analysis example",
    "section": "",
    "text": "This JupyterLab notebook uses the SymPy, NumPy,SciPy and the Python programming language libraries to analyize an electrical circuit. A circuit analysis method called the Modified Nodal Analysis was used to derive the symbolic circuit equations and Python libraries were used to solve the equations. The purpose of this analysis is to demonstrate the capability of using the Python libraries in electrical engineering circuit analysis. A link to my Jupyter Notebook rendered as a web page is here."
  },
  {
    "objectID": "SMNA_example.html#introduction",
    "href": "SMNA_example.html#introduction",
    "title": "Symbolic modified nodal analysis example",
    "section": "Introduction",
    "text": "Introduction\nThis notebook walks through the Python code used to generate and solve the circuit network equations. The example starts with a net list for the circuit shown below."
  },
  {
    "objectID": "SMNA_example.html#circuit-description",
    "href": "SMNA_example.html#circuit-description",
    "title": "Symbolic modified nodal analysis example",
    "section": "Circuit description",
    "text": "Circuit description\nThe schematic of the circuit is shown below with each node explicity annotated. The circuit has 9 lines in netlist, 9 branches and 5 unknown currents.\n\nThe net list for this circuit is:\nR2 2 5 2\nV1 1 0 1\nI1 4 0 0\nV2 0 5 0\nE1 3 0 1 4 2\nF1 2 3 V2 2\nR1 1 4 2\nC1 1 2 1\nL1 4 3 1\nI1 has been set to zero for the AC analysis.\n\nimport os\nfrom sympy import *\nimport numpy as np\nfrom scipy import signal\nimport matplotlib.pyplot as plt\nimport pandas as pd\ninit_printing()"
  },
  {
    "objectID": "SMNA_example.html#symbolic-mna-code",
    "href": "SMNA_example.html#symbolic-mna-code",
    "title": "Symbolic modified nodal analysis example",
    "section": "Symbolic MNA code",
    "text": "Symbolic MNA code\n\n# initialize variables\nnum_rlc = 0 # number of passive elements\nnum_ind = 0 # number of inductors\nnum_v = 0    # number of independent voltage sources\nnum_i = 0    # number of independent current sources\ni_unk = 0  # number of current unknowns\nnum_opamps = 0   # number of op amps\nnum_vcvs = 0     # number of controlled sources of various types\nnum_vccs = 0\nnum_cccs = 0\nnum_ccvs = 0\nnum_cpld_ind = 0 # number of coupled inductors"
  },
  {
    "objectID": "SMNA_example.html#read-the-net-list-and-preprocess-it",
    "href": "SMNA_example.html#read-the-net-list-and-preprocess-it",
    "title": "Symbolic modified nodal analysis example",
    "section": "Read the net list and preprocess it",
    "text": "Read the net list and preprocess it\nThe following steps are performed:\n\nremove blank lines and comments\n\nconvert first letter of element name to upper case\n\nremoves extra spaces between entries\n\ncount number of entries on each line, make sure the count is correct, count each element type\n\n\nexample_net_list = '''R2 2 5 2\nV1 1 0 1\nI1 4 0 0\nV2 0 5 0\nE1 3 0 1 4 2\nF1 2 3 V2 2\nR1 1 4 2\nC1 1 2 1\nL1 4 3 1'''\n\n\ncontent = example_net_list.splitlines()\n\ncontent = [x.strip() for x in content]  #remove leading and trailing white space\n# remove empty lines\nwhile '' in content:\n    content.pop(content.index(''))\n\n# remove comment lines, these start with a asterisk *\ncontent = [n for n in content if not n.startswith('*')]\n# remove other comment lines, these start with a semicolon ;\ncontent = [n for n in content if not n.startswith(';')]\n# remove spice directives, these start with a period, .\ncontent = [n for n in content if not n.startswith('.')]\n# converts 1st letter to upper case\n#content = [x.upper() for x in content] &lt;- this converts all to upper case\ncontent = [x.capitalize() for x in content]\n# removes extra spaces between entries\ncontent = [' '.join(x.split()) for x in content]\n\n\nfor i in content:\n    print(i)\n\nR2 2 5 2\nV1 1 0 1\nI1 4 0 0\nV2 0 5 0\nE1 3 0 1 4 2\nF1 2 3 v2 2\nR1 1 4 2\nC1 1 2 1\nL1 4 3 1\n\n\n\nline_cnt = len(content) # number of lines in the netlist\nbranch_cnt = 0  # number of branches in the netlist\n# check number of entries on each line, count each element type\nfor i in range(line_cnt):\n    x = content[i][0]\n    tk_cnt = len(content[i].split()) # split the line into a list of words\n\n    if (x == 'R') or (x == 'L') or (x == 'C'):\n        if tk_cnt != 4:\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 4\".format(tk_cnt))\n        num_rlc += 1\n        branch_cnt += 1\n        if x == 'L':\n            num_ind += 1\n    elif x == 'V':\n        if tk_cnt != 4:\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 4\".format(tk_cnt))\n        num_v += 1\n        branch_cnt += 1\n    elif x == 'I':\n        if tk_cnt != 4:\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 4\".format(tk_cnt))\n        num_i += 1\n        branch_cnt += 1\n    elif x == 'O':\n        if tk_cnt != 4:\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 4\".format(tk_cnt))\n        num_opamps += 1\n    elif x == 'E':\n        if (tk_cnt != 6):\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 6\".format(tk_cnt))\n        num_vcvs += 1\n        branch_cnt += 1\n    elif x == 'G':\n        if (tk_cnt != 6):\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 6\".format(tk_cnt))\n        num_vccs += 1\n        branch_cnt += 1\n    elif x == 'F':\n        if (tk_cnt != 5):\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 5\".format(tk_cnt))\n        num_cccs += 1\n        branch_cnt += 1\n    elif x == 'H':\n        if (tk_cnt != 5):\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 5\".format(tk_cnt))\n        num_ccvs += 1\n        branch_cnt += 1\n    elif x == 'K':\n        if (tk_cnt != 4):\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 4\".format(tk_cnt))\n        num_cpld_ind += 1\n    else:\n        print(\"unknown element type in branch {:d}, {:s}\".format(i,content[i]))"
  },
  {
    "objectID": "SMNA_example.html#parser",
    "href": "SMNA_example.html#parser",
    "title": "Symbolic modified nodal analysis example",
    "section": "Parser",
    "text": "Parser\nThe parser performs the following operations.\n\nputs branch elements into data frame\n\ncounts number of nodes\n\ndata frame labels:\n\nelement: type of element\n\np node: positive node\n\nn node: negative node, for a current source, the arrow point terminal, LTspice puts the inductor phasing dot on this terminal\n\ncp node: controlling positive node of branch\n\ncn node: controlling negative node of branch\n\nVout: opamp output node\n\nvalue: value of element or voltage\n\nVname: voltage source through which the controlling current flows. Need to add a zero volt voltage source to the controlling branch.\n\nLname1: name of coupled inductor 1\n\nLname2: name of coupled inductor 2\n\n\n# build the pandas data frame\ndf = pd.DataFrame(columns=['element','p node','n node','cp node','cn node',\n    'Vout','value','Vname','Lname1','Lname2'])\n\n# this data frame is for branches with unknown currents\ndf2 = pd.DataFrame(columns=['element','p node','n node'])\n\n\nFunctions to load branch elements into data frame and check for gaps in node numbering\n\n# loads voltage or current sources into branch structure\ndef indep_source(line_nu):\n    tk = content[line_nu].split()\n    df.loc[line_nu,'element'] = tk[0]\n    df.loc[line_nu,'p node'] = int(tk[1])\n    df.loc[line_nu,'n node'] = int(tk[2])\n    df.loc[line_nu,'value'] = float(tk[3])\n\n# loads passive elements into branch structure\ndef rlc_element(line_nu):\n    tk = content[line_nu].split()\n    df.loc[line_nu,'element'] = tk[0]\n    df.loc[line_nu,'p node'] = int(tk[1])\n    df.loc[line_nu,'n node'] = int(tk[2])\n    df.loc[line_nu,'value'] = float(tk[3])\n\n# loads multi-terminal elements into branch structure\n# O - Op Amps\ndef opamp_sub_network(line_nu):\n    tk = content[line_nu].split()\n    df.loc[line_nu,'element'] = tk[0]\n    df.loc[line_nu,'p node'] = int(tk[1])\n    df.loc[line_nu,'n node'] = int(tk[2])\n    df.loc[line_nu,'Vout'] = int(tk[3])\n\n# G - VCCS\ndef vccs_sub_network(line_nu):\n    tk = content[line_nu].split()\n    df.loc[line_nu,'element'] = tk[0]\n    df.loc[line_nu,'p node'] = int(tk[1])\n    df.loc[line_nu,'n node'] = int(tk[2])\n    df.loc[line_nu,'cp node'] = int(tk[3])\n    df.loc[line_nu,'cn node'] = int(tk[4])\n    df.loc[line_nu,'value'] = float(tk[5])\n\n# E - VCVS\n# in sympy E is the number 2.718, replacing E with Ea otherwise, sympify() errors out\ndef vcvs_sub_network(line_nu):\n    tk = content[line_nu].split()\n    df.loc[line_nu,'element'] = tk[0].replace('E', 'Ea')\n    df.loc[line_nu,'p node'] = int(tk[1])\n    df.loc[line_nu,'n node'] = int(tk[2])\n    df.loc[line_nu,'cp node'] = int(tk[3])\n    df.loc[line_nu,'cn node'] = int(tk[4])\n    df.loc[line_nu,'value'] = float(tk[5])\n\n# F - CCCS\ndef cccs_sub_network(line_nu):\n    tk = content[line_nu].split()\n    df.loc[line_nu,'element'] = tk[0]\n    df.loc[line_nu,'p node'] = int(tk[1])\n    df.loc[line_nu,'n node'] = int(tk[2])\n    df.loc[line_nu,'Vname'] = tk[3].capitalize()\n    df.loc[line_nu,'value'] = float(tk[4])\n\n# H - CCVS\ndef ccvs_sub_network(line_nu):\n    tk = content[line_nu].split()\n    df.loc[line_nu,'element'] = tk[0]\n    df.loc[line_nu,'p node'] = int(tk[1])\n    df.loc[line_nu,'n node'] = int(tk[2])\n    df.loc[line_nu,'Vname'] = tk[3].capitalize()\n    df.loc[line_nu,'value'] = float(tk[4])\n\n# K - Coupled inductors\ndef cpld_ind_sub_network(line_nu):\n    tk = content[line_nu].split()\n    df.loc[line_nu,'element'] = tk[0]\n    df.loc[line_nu,'Lname1'] = tk[1].capitalize()\n    df.loc[line_nu,'Lname2'] = tk[2].capitalize()\n    df.loc[line_nu,'value'] = float(tk[3])\n\n# function to scan df and get largest node number\ndef count_nodes():\n    # need to check that nodes are consecutive\n    # fill array with node numbers\n    p = np.zeros(line_cnt+1)\n    for i in range(line_cnt):\n        # need to skip coupled inductor 'K' statements\n        if df.loc[i,'element'][0] != 'K': #get 1st letter of element name\n            p[df['p node'][i]] = df['p node'][i]\n            p[df['n node'][i]] = df['n node'][i]\n\n    # find the largest node number\n    if df['n node'].max() &gt; df['p node'].max():\n        largest = df['n node'].max()\n    else:\n        largest =  df['p node'].max()\n\n    largest = int(largest)\n    # check for unfilled elements, skip node 0\n    for i in range(1,largest):\n        if p[i] == 0:\n            print('nodes not in continuous order, node {:.0f} is missing'.format(p[i-1]+1))\n\n    return largest\n\n\n\nLoad circuit netlist into the data frames\n\n# load branch info into data frame\nfor i in range(line_cnt):\n    x = content[i][0]\n\n    if (x == 'R') or (x == 'L') or (x == 'C'):\n        rlc_element(i)\n    elif (x == 'V') or (x == 'I'):\n        indep_source(i)\n    elif x == 'O':\n        opamp_sub_network(i)\n    elif x == 'E':\n        vcvs_sub_network(i)\n    elif x == 'G':\n        vccs_sub_network(i)\n    elif x == 'F':\n        cccs_sub_network(i)\n    elif x == 'H':\n        ccvs_sub_network(i)\n    elif x == 'K':\n        cpld_ind_sub_network(i)\n    else:\n        print(\"unknown element type in branch {:d}, {:s}\".format(i,content[i]))\n\n29 Nov 2023: When the D matrix is built, independent voltage sources are processed in the data frame order when building the D matrix. If the voltage source followed element L, H, F, K types in the netlist, a row was inserted that put the voltage source in a different row in relation to its position in the Ev matrix. This would cause the node attached to the terminal of the voltage source to be zero volts.\nSolution - The following block of code was added to move voltage source types to the beginning of the net list dataframe before any calculations are performed.\n\n# Check for position of voltages sources in the dataframe.\nsource_index = [] # keep track of voltage source row number\nother_index = [] # make a list of all other types\nfor i in range(len(df)):\n    # process all the elements creating unknown currents\n    x = df.loc[i,'element'][0]   #get 1st letter of element name\n    if (x == 'V'):\n        source_index.append(i)\n    else:\n        other_index.append(i)\n\ndf = df.reindex(source_index+other_index,copy=True) # re-order the data frame\ndf.reset_index(drop=True, inplace=True) # renumber the index\n\n\n# count number of nodes\nnum_nodes = count_nodes()\n\n# Build df2: consists of branches with current unknowns, used for C & D matrices\n# walk through data frame and find these parameters\ncount = 0\nfor i in range(len(df)):\n    # process all the elements creating unknown currents\n    x = df.loc[i,'element'][0]   #get 1st letter of element name\n    if (x == 'L') or (x == 'V') or (x == 'O') or (x == 'E') or (x == 'H') or (x == 'F'):\n        df2.loc[count,'element'] = df.loc[i,'element']\n        df2.loc[count,'p node'] = df.loc[i,'p node']\n        df2.loc[count,'n node'] = df.loc[i,'n node']\n        count += 1"
  },
  {
    "objectID": "SMNA_example.html#print-net-list-report",
    "href": "SMNA_example.html#print-net-list-report",
    "title": "Symbolic modified nodal analysis example",
    "section": "Print net list report",
    "text": "Print net list report\n\n# print a report\nprint('Net list report')\nprint('number of lines in netlist: {:d}'.format(line_cnt))\nprint('number of branches: {:d}'.format(branch_cnt))\nprint('number of nodes: {:d}'.format(num_nodes))\n# count the number of element types that affect the size of the B, C, D, E and J arrays\n# these are current unknows\ni_unk = num_v+num_opamps+num_vcvs+num_ccvs+num_cccs+num_ind\nprint('number of unknown currents: {:d}'.format(i_unk))\nprint('number of RLC (passive components): {:d}'.format(num_rlc))\nprint('number of inductors: {:d}'.format(num_ind))\nprint('number of independent voltage sources: {:d}'.format(num_v))\nprint('number of independent current sources: {:d}'.format(num_i))\nprint('number of op amps: {:d}'.format(num_opamps))\nprint('number of E - VCVS: {:d}'.format(num_vcvs))\nprint('number of G - VCCS: {:d}'.format(num_vccs))\nprint('number of F - CCCS: {:d}'.format(num_cccs))\nprint('number of H - CCVS: {:d}'.format(num_ccvs))\nprint('number of K - Coupled inductors: {:d}'.format(num_cpld_ind))\n\nNet list report\nnumber of lines in netlist: 9\nnumber of branches: 9\nnumber of nodes: 5\nnumber of unknown currents: 5\nnumber of RLC (passive components): 4\nnumber of inductors: 1\nnumber of independent voltage sources: 2\nnumber of independent current sources: 1\nnumber of op amps: 0\nnumber of E - VCVS: 1\nnumber of G - VCCS: 0\nnumber of F - CCCS: 1\nnumber of H - CCVS: 0\nnumber of K - Coupled inductors: 0\n\n\n\ndf\n\n\n\n\n\n\n\n\nelement\np node\nn node\ncp node\ncn node\nVout\nvalue\nVname\nLname1\nLname2\n\n\n\n\n0\nV1\n1\n0\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\n\n\n1\nV2\n0\n5\nNaN\nNaN\nNaN\n0.0\nNaN\nNaN\nNaN\n\n\n2\nR2\n2\n5\nNaN\nNaN\nNaN\n2.0\nNaN\nNaN\nNaN\n\n\n3\nI1\n4\n0\nNaN\nNaN\nNaN\n0.0\nNaN\nNaN\nNaN\n\n\n4\nEa1\n3\n0\n1\n4\nNaN\n2.0\nNaN\nNaN\nNaN\n\n\n5\nF1\n2\n3\nNaN\nNaN\nNaN\n2.0\nV2\nNaN\nNaN\n\n\n6\nR1\n1\n4\nNaN\nNaN\nNaN\n2.0\nNaN\nNaN\nNaN\n\n\n7\nC1\n1\n2\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\n\n\n8\nL1\n4\n3\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ndf2\n\n\n\n\n\n\n\n\nelement\np node\nn node\n\n\n\n\n0\nV1\n1\n0\n\n\n1\nV2\n0\n5\n\n\n2\nEa1\n3\n0\n\n\n3\nF1\n2\n3\n\n\n4\nL1\n4\n3\n\n\n\n\n\n\n\n\n# store the data frame as a pickle file\n# df.to_pickle(fn+'.pkl')  # &lt;- uncomment if needed\n\n\n# initialize some symbolic matrix with zeros\n# A is formed by [[G, C] [B, D]]\n# Z = [I,E]\n# X = [V, J]\nV = zeros(num_nodes,1)\nI = zeros(num_nodes,1)\nG = zeros(num_nodes,num_nodes)  # also called Yr, the reduced nodal matrix\ns = Symbol('s')  # the Laplace variable\n\n# count the number of element types that affect the size of the B, C, D, E and J arrays\n# these are element types that have unknown currents\ni_unk = num_v+num_opamps+num_vcvs+num_ccvs+num_ind+num_cccs\n# if i_unk == 0, just generate empty arrays\nB = zeros(num_nodes,i_unk)\nC = zeros(i_unk,num_nodes)\nD = zeros(i_unk,i_unk)\nEv = zeros(i_unk,1)\nJ = zeros(i_unk,1)\n\n\nsome debugging notes:\nIs it possible to have i_unk == 0 ?, what about a network with only current sources? This would make B = 0 for example. Did one test, need to run others\nIs there a valid op amp case where B is n by 1?"
  },
  {
    "objectID": "SMNA_example.html#g-matrix",
    "href": "SMNA_example.html#g-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "G matrix",
    "text": "G matrix\nThe G matrix is n by n, where n is the number of nodes. The matrix is formed by the interconnections between the resistors, capacitors and VCCS type elements. In the original paper G is called Yr, where Yr is a reduced form of the nodal matrix excluding the contributions due to voltage sources, current controlling elements, etc. In python row and columns are: G[row, column]\n\n# G matrix\nfor i in range(len(df)):  # process each row in the data frame\n    n1 = df.loc[i,'p node']\n    n2 = df.loc[i,'n node']\n    cn1 = df.loc[i,'cp node']\n    cn2 = df.loc[i,'cn node']\n    # process all the passive elements, save conductance to temp value\n    x = df.loc[i,'element'][0]   #get 1st letter of element name\n    if x == 'R':\n        g = 1/sympify(df.loc[i,'element'])\n    if x == 'C':\n        g = s*sympify(df.loc[i,'element'])\n    if x == 'G':   #vccs type element\n        g = sympify(df.loc[i,'element'].lower())  # use a symbol for gain value\n\n    if (x == 'R') or (x == 'C'):\n        # If neither side of the element is connected to ground\n        # then subtract it from the appropriate location in the matrix.\n        if (n1 != 0) and (n2 != 0):\n            G[n1-1,n2-1] += -g\n            G[n2-1,n1-1] += -g\n\n        # If node 1 is connected to ground, add element to diagonal of matrix\n        if n1 != 0:\n            G[n1-1,n1-1] += g\n\n        # same for for node 2\n        if n2 != 0:\n            G[n2-1,n2-1] += g\n\n    if x == 'G':    #vccs type element\n        # check to see if any terminal is grounded\n        # then stamp the matrix\n        if n1 != 0 and cn1 != 0:\n            G[n1-1,cn1-1] += g\n\n        if n2 != 0 and cn2 != 0:\n            G[n2-1,cn2-1] += g\n\n        if n1 != 0 and cn2 != 0:\n            G[n1-1,cn2-1] -= g\n\n        if n2 != 0 and cn1 != 0:\n            G[n2-1,cn1-1] -= g\n\nG  # display the G matrix\n\n\\(\\displaystyle \\left[\\begin{matrix}C_{1} s + \\frac{1}{R_{1}} & - C_{1} s & 0 & - \\frac{1}{R_{1}} & 0\\\\- C_{1} s & C_{1} s + \\frac{1}{R_{2}} & 0 & 0 & - \\frac{1}{R_{2}}\\\\0 & 0 & 0 & 0 & 0\\\\- \\frac{1}{R_{1}} & 0 & 0 & \\frac{1}{R_{1}} & 0\\\\0 & - \\frac{1}{R_{2}} & 0 & 0 & \\frac{1}{R_{2}}\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "SMNA_example.html#b-matrix",
    "href": "SMNA_example.html#b-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "B Matrix",
    "text": "B Matrix\nThe B matrix is an n by m matrix with only 0, 1 and -1 elements, where n = number of nodes and m is the number of current unknowns, i_unk. There is one column for each unknown current. The code loop through all the branches and process elements that have stamps for the B matrix:\n\nVoltage sources (V)\n\nOpamps (O)\n\nCCVS (H)\n\nCCCS (F)\n\nVCVS (E)\n\nInductors (L)\n\nThe order of the columns is as they appear in the netlist. CCCS (F) does not get its own column because the controlling current is through a zero volt voltage source, called Vname and is already in the net list.\n\n# generate the B Matrix\nsn = 0   # count source number as code walks through the data frame\nfor i in range(len(df)):\n    n1 = df.loc[i,'p node']\n    n2 = df.loc[i,'n node']\n    n_vout = df.loc[i,'Vout'] # node connected to op amp output\n\n    # process elements with input to B matrix\n    x = df.loc[i,'element'][0]   #get 1st letter of element name\n    if x == 'V':\n        if i_unk &gt; 1:  #is B greater than 1 by n?, V\n            if n1 != 0:\n                B[n1-1,sn] = 1\n            if n2 != 0:\n                B[n2-1,sn] = -1\n        else:\n            if n1 != 0:\n                B[n1-1] = 1\n            if n2 != 0:\n                B[n2-1] = -1\n        sn += 1   #increment source count\n    if x == 'O':  # op amp type, output connection of the opamp goes in the B matrix\n        B[n_vout-1,sn] = 1\n        sn += 1   # increment source count\n    if (x == 'H') or (x == 'F'):  # H: ccvs, F: cccs,\n        if i_unk &gt; 1:  #is B greater than 1 by n?, H, F\n            # check to see if any terminal is grounded\n            # then stamp the matrix\n            if n1 != 0:\n                B[n1-1,sn] = 1\n            if n2 != 0:\n                B[n2-1,sn] = -1\n        else:\n            if n1 != 0:\n                B[n1-1] = 1\n            if n2 != 0:\n                B[n2-1] = -1\n        sn += 1   #increment source count\n    if x == 'E':   # vcvs type, only ik column is altered at n1 and n2\n        if i_unk &gt; 1:  #is B greater than 1 by n?, E\n            if n1 != 0:\n                B[n1-1,sn] = 1\n            if n2 != 0:\n                B[n2-1,sn] = -1\n        else:\n            if n1 != 0:\n                B[n1-1] = 1\n            if n2 != 0:\n                B[n2-1] = -1\n        sn += 1   #increment source count\n    if x == 'L':\n        if i_unk &gt; 1:  #is B greater than 1 by n?, L\n            if n1 != 0:\n                B[n1-1,sn] = 1\n            if n2 != 0:\n                B[n2-1,sn] = -1\n        else:\n            if n1 != 0:\n                B[n1-1] = 1\n            if n2 != 0:\n                B[n2-1] = -1\n        sn += 1   #increment source count\n\n# check source count\nif sn != i_unk:\n    print('source number, sn={:d} not equal to i_unk={:d} in matrix B'.format(sn,i_unk))\n\nB   # display the B matrix\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\\\0 & 0 & 1 & -1 & -1\\\\0 & 0 & 0 & 0 & 1\\\\0 & -1 & 0 & 0 & 0\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "SMNA_example.html#c-matrix",
    "href": "SMNA_example.html#c-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "C matrix",
    "text": "C matrix\nThe C matrix is an m by n matrix with only 0, 1 and -1 elements (except for controlled sources). The code is similar to the B matrix code, except the indices are swapped. The code loops through all the branches and process elements that have stamps for the C matrix:\n\nVoltage sources (V)\n\nOpamps (O)\n\nCCVS (H)\n\nCCCS (F)\n\nVCVS (E)\n\nInductors (L)\n\n\nOp Amp elements\nThe op amp element is assumed to be an ideal op amp and use of this component is valid only when used in circuits with a DC path (a short or a resistor) from the output terminal to the negative input terminal of the op amp. No error checking is provided and if the condition is violated, the results likely will be erroneous.\nReferences use in the debugging of the opamp stamp:\n\nDesign of Analog Circuits Through Symbolic Analysis, edited by Mourad Fakhfakh, Esteban Tlelo-Cuautle, Francisco V. Fernández\n\nComputer Aided Design and Design Automation, edited by Wai-Kai Chen\n\n\n# find the the column position in the C and D matrix for controlled sources\n# needs to return the node numbers and branch number of controlling branch\ndef find_vname(name):\n    # need to walk through data frame and find these parameters\n    for i in range(len(df2)):\n        # process all the elements creating unknown currents\n        if name == df2.loc[i,'element']:\n            n1 = df2.loc[i,'p node']\n            n2 = df2.loc[i,'n node']\n            return n1, n2, i  # n1, n2 & col_num are from the branch of the controlling element\n\n    print('failed to find matching branch element in find_vname')\n\n\n# generate the C Matrix\nsn = 0   # count source number as code walks through the data frame\nfor i in range(len(df)):\n    n1 = df.loc[i,'p node']\n    n2 = df.loc[i,'n node']\n    cn1 = df.loc[i,'cp node'] # nodes for controlled sources\n    cn2 = df.loc[i,'cn node']\n    n_vout = df.loc[i,'Vout'] # node connected to op amp output\n\n    # process elements with input to B matrix\n    x = df.loc[i,'element'][0]   #get 1st letter of element name\n    if x == 'V':\n        if i_unk &gt; 1:  #is B greater than 1 by n?, V\n            if n1 != 0:\n                C[sn,n1-1] = 1\n            if n2 != 0:\n                C[sn,n2-1] = -1\n        else:\n            if n1 != 0:\n                C[n1-1] = 1\n            if n2 != 0:\n                C[n2-1] = -1\n        sn += 1   #increment source count\n\n    if x == 'O':  # op amp type, input connections of the opamp go into the C matrix\n        # C[sn,n_vout-1] = 1\n        if i_unk &gt; 1:  #is B greater than 1 by n?, O\n            # check to see if any terminal is grounded\n            # then stamp the matrix\n            if n1 != 0:\n                C[sn,n1-1] = 1\n            if n2 != 0:\n                C[sn,n2-1] = -1\n        else:\n            if n1 != 0:\n                C[n1-1] = 1\n            if n2 != 0:\n                C[n2-1] = -1\n        sn += 1   # increment source count\n\n    if x == 'F':  # need to count F (cccs) types\n        sn += 1   #increment source count\n    if x == 'H':  # H: ccvs\n        if i_unk &gt; 1:  #is B greater than 1 by n?, H\n            # check to see if any terminal is grounded\n            # then stamp the matrix\n            if n1 != 0:\n                C[sn,n1-1] = 1\n            if n2 != 0:\n                C[sn,n2-1] = -1\n        else:\n            if n1 != 0:\n                C[n1-1] = 1\n            if n2 != 0:\n                C[n2-1] = -1\n        sn += 1   #increment source count\n    if x == 'E':   # vcvs type, ik column is altered at n1 and n2, cn1 & cn2 get value\n        if i_unk &gt; 1:  #is B greater than 1 by n?, E\n            if n1 != 0:\n                C[sn,n1-1] = 1\n            if n2 != 0:\n                C[sn,n2-1] = -1\n            # add entry for cp and cn of the controlling voltage\n            if cn1 != 0:\n                C[sn,cn1-1] = -sympify(df.loc[i,'element'].lower())\n            if cn2 != 0:\n                C[sn,cn2-1] = sympify(df.loc[i,'element'].lower())\n        else:\n            if n1 != 0:\n                C[n1-1] = 1\n            if n2 != 0:\n                C[n2-1] = -1\n            vn1, vn2, df2_index = find_vname(df.loc[i,'Vname'])\n            if vn1 != 0:\n                C[vn1-1] = -sympify(df.loc[i,'element'].lower())\n            if vn2 != 0:\n                C[vn2-1] = sympify(df.loc[i,'element'].lower())\n        sn += 1   #increment source count\n\n    if x == 'L':\n        if i_unk &gt; 1:  #is B greater than 1 by n?, L\n            if n1 != 0:\n                C[sn,n1-1] = 1\n            if n2 != 0:\n                C[sn,n2-1] = -1\n        else:\n            if n1 != 0:\n                C[n1-1] = 1\n            if n2 != 0:\n                C[n2-1] = -1\n        sn += 1   #increment source count\n\n# check source count\nif sn != i_unk:\n    print('source number, sn={:d} not equal to i_unk={:d} in matrix C'.format(sn,i_unk))\n\nC   # display the C matrix\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & -1\\\\- ea_{1} & 0 & 1 & ea_{1} & 0\\\\0 & 0 & 0 & 0 & 0\\\\0 & 0 & -1 & 1 & 0\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "SMNA_example.html#d-matrix",
    "href": "SMNA_example.html#d-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "D matrix",
    "text": "D matrix\nThe D matrix is an m by m matrix, where m is the number of unknown currents.\n&gt; m = i_unk = num_v+num_opamps+num_vcvs+num_ccvs+num_ind+num_cccs\nStamps that affect the D matrix are: inductor, ccvs and cccs\ninductors: minus sign added to keep current flow convention consistent\nCoupled inductors notes:\nCan the K statement be anywhere in the net list, even before Lx and Ly?\n12/6/2017 doing some debugging on with coupled inductors\nLTspice seems to put the phasing dot on the neg node when it generates the netlist\nThis code uses M for mutual inductance, LTspice uses k for the coupling coefficient.\n\n# generate the D Matrix\nsn = 0   # count source number as code walks through the data frame\nfor i in range(len(df)):\n    n1 = df.loc[i,'p node']\n    n2 = df.loc[i,'n node']\n    #cn1 = df.loc[i,'cp node'] # nodes for controlled sources\n    #cn2 = df.loc[i,'cn node']\n    #n_vout = df.loc[i,'Vout'] # node connected to op amp output\n\n    # process elements with input to D matrix\n    x = df.loc[i,'element'][0]   #get 1st letter of element name\n    if (x == 'V') or (x == 'O') or (x == 'E'):  # need to count V, E & O types\n        sn += 1   #increment source count\n\n    if x == 'L':\n        if i_unk &gt; 1:  #is D greater than 1 by 1?\n            D[sn,sn] += -s*sympify(df.loc[i,'element'])\n        else:\n            D[sn] += -s*sympify(df.loc[i,'element'])\n        sn += 1   #increment source count\n\n    if x == 'H':  # H: ccvs\n        # if there is a H type, D is m by m\n        # need to find the vn for Vname\n        # then stamp the matrix\n        vn1, vn2, df2_index = find_vname(df.loc[i,'Vname'])\n        D[sn,df2_index] += -sympify(df.loc[i,'element'].lower())\n        sn += 1   #increment source count\n\n    if x == 'F':  # F: cccs\n        # if there is a F type, D is m by m\n        # need to find the vn for Vname\n        # then stamp the matrix\n        vn1, vn2, df2_index = find_vname(df.loc[i,'Vname'])\n        D[sn,df2_index] += -sympify(df.loc[i,'element'].lower())\n        D[sn,sn] = 1\n        sn += 1   #increment source count\n\n    if x == 'K':  # K: coupled inductors, KXX LYY LZZ value\n        # if there is a K type, D is m by m\n        vn1, vn2, ind1_index = find_vname(df.loc[i,'Lname1'])  # get i_unk position for Lx\n        vn1, vn2, ind2_index = find_vname(df.loc[i,'Lname2'])  # get i_unk position for Ly\n        # enter sM on diagonals = value*sqrt(LXX*LZZ)\n\n        D[ind1_index,ind2_index] += -s*sympify('M{:s}'.format(df.loc[i,'element'].lower()[1:]))  # s*Mxx\n        D[ind2_index,ind1_index] += -s*sympify('M{:s}'.format(df.loc[i,'element'].lower()[1:]))  # -s*Mxx\n\n# display the The D matrix\nD\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0\\\\0 & - f_{1} & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & - L_{1} s\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "SMNA_example.html#v-matrix",
    "href": "SMNA_example.html#v-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "V matrix",
    "text": "V matrix\nThe V matrix is an n by 1 matrix formed of the node voltages, where n is the number of nodes. Each element in V corresponds to the voltage at the node.\nMaybe make small v’s v_1 so as not to confuse v1 with V1.\n\n# generate the V matrix\nfor i in range(num_nodes):\n    V[i] = sympify('v{:d}'.format(i+1))\n\nV  # display the V matrix\n\n\\(\\displaystyle \\left[\\begin{matrix}v_{1}\\\\v_{2}\\\\v_{3}\\\\v_{4}\\\\v_{5}\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "SMNA_example.html#j-matrix",
    "href": "SMNA_example.html#j-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "J matrix",
    "text": "J matrix\nThe J matrix is an m by 1 matrix, where m is the number of unknown currents. &gt;i_unk = num_v+num_opamps+num_vcvs+num_ccvs+num_ind+num_cccs\n\n# The J matrix is an mx1 matrix, with one entry for each i_unk from a source\n#sn = 0   # count i_unk source number\n#oan = 0   #count op amp number\nfor i in range(len(df2)):\n    # process all the unknown currents\n    J[i] = sympify('I_{:s}'.format(df2.loc[i,'element']))\n\nJ  # diplay the J matrix\n\n\\(\\displaystyle \\left[\\begin{matrix}I_{V1}\\\\I_{V2}\\\\I_{Ea1}\\\\I_{F1}\\\\I_{L1}\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "SMNA_example.html#i-matrix",
    "href": "SMNA_example.html#i-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "I matrix",
    "text": "I matrix\nThe I matrix is an n by 1 matrix, where n is the number of nodes. The value of each element of I is determined by the sum of current sources into the corresponding node. If there are no current sources connected to the node, the value is zero.\n\n# generate the I matrix, current sources have n2 = arrow end of the element\nfor i in range(len(df)):\n    n1 = df.loc[i,'p node']\n    n2 = df.loc[i,'n node']\n    # process all the passive elements, save conductance to temp value\n    x = df.loc[i,'element'][0]   #get 1st letter of element name\n    if x == 'I':\n        g = sympify(df.loc[i,'element'])\n        # sum the current into each node\n        if n1 != 0:\n            I[n1-1] -= g\n        if n2 != 0:\n            I[n2-1] += g\n\nI  # display the I matrix\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\0\\\\0\\\\- I_{1}\\\\0\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "SMNA_example.html#ev-matrix",
    "href": "SMNA_example.html#ev-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "Ev matrix",
    "text": "Ev matrix\nThe Ev matrix is mx1 and holds the values of the independent voltage sources.\n\n# generate the E matrix\nsn = 0   # count source number\nfor i in range(len(df)):\n    # process all the passive elements\n    x = df.loc[i,'element'][0]   #get 1st letter of element name\n    if x == 'V':\n        Ev[sn] = sympify(df.loc[i,'element'])\n        sn += 1\n\nEv   # display the E matrix\n\n\\(\\displaystyle \\left[\\begin{matrix}V_{1}\\\\V_{2}\\\\0\\\\0\\\\0\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "SMNA_example.html#z-matrix",
    "href": "SMNA_example.html#z-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "Z matrix",
    "text": "Z matrix\nThe Z matrix holds the independent voltage and current sources and is the combination of 2 smaller matrices I and Ev. The Z matrix is (m+n) by 1, n is the number of nodes, and m is the number of independent voltage sources. The I matrix is n by 1 and contains the sum of the currents through the passive elements into the corresponding node (either zero, or the sum of independent current sources). The Ev matrix is m by 1 and holds the values of the independent voltage sources.\n\nZ = I[:] + Ev[:]  # the + operator in python concatenates the lists\nZ  # display the Z matrix\n\n\\(\\displaystyle \\left[ 0, \\  0, \\  0, \\  - I_{1}, \\  0, \\  V_{1}, \\  V_{2}, \\  0, \\  0, \\  0\\right]\\)"
  },
  {
    "objectID": "SMNA_example.html#x-matrix",
    "href": "SMNA_example.html#x-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "X matrix",
    "text": "X matrix\nThe X matrix is an (n+m) by 1 vector that holds the unknown quantities (node voltages and the currents through the independent voltage sources). The top n elements are the n node voltages. The bottom m elements represent the currents through the m independent voltage sources in the circuit. The V matrix is n by 1 and holds the unknown voltages. The J matrix is m by 1 and holds the unknown currents through the voltage sources\n\nX = V[:] + J[:]  # the + operator in python concatenates the lists\nX  # display the X matrix\n\n\\(\\displaystyle \\left[ v_{1}, \\  v_{2}, \\  v_{3}, \\  v_{4}, \\  v_{5}, \\  I_{V1}, \\  I_{V2}, \\  I_{Ea1}, \\  I_{F1}, \\  I_{L1}\\right]\\)"
  },
  {
    "objectID": "SMNA_example.html#a-matrix",
    "href": "SMNA_example.html#a-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "A matrix",
    "text": "A matrix\nThe A matrix is (m+n) by (m+n) and will be developed as the combination of 4 smaller matrices, G, B, C, and D.\n\nn = num_nodes\nm = i_unk\nA = zeros(m+n,m+n)\nfor i in range(n):\n    for j in range(n):\n        A[i,j] = G[i,j]\n\nif i_unk &gt; 1:\n    for i in range(n):\n        for j in range(m):\n            A[i,n+j] = B[i,j]\n            A[n+j,i] = C[j,i]\n\n    for i in range(m):\n        for j in range(m):\n            A[n+i,n+j] = D[i,j]\n\nif i_unk == 1:\n    for i in range(n):\n        A[i,n] = B[i]\n        A[n,i] = C[i]\n\nA  # display the A matrix\n\n\\(\\displaystyle \\left[\\begin{matrix}C_{1} s + \\frac{1}{R_{1}} & - C_{1} s & 0 & - \\frac{1}{R_{1}} & 0 & 1 & 0 & 0 & 0 & 0\\\\- C_{1} s & C_{1} s + \\frac{1}{R_{2}} & 0 & 0 & - \\frac{1}{R_{2}} & 0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & -1 & -1\\\\- \\frac{1}{R_{1}} & 0 & 0 & \\frac{1}{R_{1}} & 0 & 0 & 0 & 0 & 0 & 1\\\\0 & - \\frac{1}{R_{2}} & 0 & 0 & \\frac{1}{R_{2}} & 0 & -1 & 0 & 0 & 0\\\\1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & -1 & 0 & 0 & 0 & 0 & 0\\\\- ea_{1} & 0 & 1 & ea_{1} & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & - f_{1} & 0 & 1 & 0\\\\0 & 0 & -1 & 1 & 0 & 0 & 0 & 0 & 0 & - L_{1} s\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "SMNA_example.html#generate-the-circuit-equations",
    "href": "SMNA_example.html#generate-the-circuit-equations",
    "title": "Symbolic modified nodal analysis example",
    "section": "generate the circuit equations",
    "text": "generate the circuit equations\n\nequ = Eq(A*Matrix(X),Matrix(Z))\nequ\n\n\\(\\displaystyle \\left[\\begin{matrix}- C_{1} s v_{2} + I_{V1} + v_{1} \\left(C_{1} s + \\frac{1}{R_{1}}\\right) - \\frac{v_{4}}{R_{1}}\\\\- C_{1} s v_{1} + I_{F1} + v_{2} \\left(C_{1} s + \\frac{1}{R_{2}}\\right) - \\frac{v_{5}}{R_{2}}\\\\I_{Ea1} - I_{F1} - I_{L1}\\\\I_{L1} - \\frac{v_{1}}{R_{1}} + \\frac{v_{4}}{R_{1}}\\\\- I_{V2} - \\frac{v_{2}}{R_{2}} + \\frac{v_{5}}{R_{2}}\\\\v_{1}\\\\- v_{5}\\\\- ea_{1} v_{1} + ea_{1} v_{4} + v_{3}\\\\I_{F1} - I_{V2} f_{1}\\\\- I_{L1} L_{1} s - v_{3} + v_{4}\\end{matrix}\\right] = \\left[\\begin{matrix}0\\\\0\\\\0\\\\- I_{1}\\\\0\\\\V_{1}\\\\V_{2}\\\\0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\nBuilt a python dictionary of element values\n\nelement_value_keys = []\nelement_value_values = []\n\nfor i in range(len(df)):\n    if df.iloc[i]['element'][0] == 'F' or df.iloc[i]['element'][0] == 'E' or df.iloc[i]['element'][0] == 'G' or df.iloc[i]['element'][0] == 'H':\n        element_value_keys.append(var(df.iloc[i]['element'].lower()))\n        element_value_values.append(df.iloc[i]['value'])\n        #print('{:s}:{:f},'.format(df.iloc[i]['element'].lower(),df.iloc[i]['value']))\n    else:\n        element_value_keys.append(var(df.iloc[i]['element']))\n        element_value_values.append(df.iloc[i]['value'])\n        #print('{:s}:{:.4e},'.format(df.iloc[i]['element'],df.iloc[i]['value']))\n\nelement_values = dict(zip(element_value_keys, element_value_values))"
  },
  {
    "objectID": "SMNA_example.html#symbolic-solution",
    "href": "SMNA_example.html#symbolic-solution",
    "title": "Symbolic modified nodal analysis example",
    "section": "Symbolic solution",
    "text": "Symbolic solution\n\nsymbolic_solution = solve(equ,X)\nsymbolic_solution\n\n\\(\\displaystyle \\left\\{ I_{Ea1} : \\frac{- C_{1} I_{1} R_{1} R_{2} ea_{1} s - C_{1} I_{1} R_{1} R_{2} s - C_{1} L_{1} V_{1} f_{1} s^{2} - C_{1} L_{1} V_{2} f_{1} s^{2} - C_{1} R_{1} V_{1} ea_{1} f_{1} s - C_{1} R_{1} V_{1} f_{1} s - C_{1} R_{1} V_{2} ea_{1} f_{1} s - C_{1} R_{1} V_{2} f_{1} s + C_{1} R_{2} V_{1} s + I_{1} R_{1} ea_{1} f_{1} - I_{1} R_{1} ea_{1} + I_{1} R_{1} f_{1} - I_{1} R_{1} - V_{1} f_{1} + V_{1}}{C_{1} L_{1} R_{2} s^{2} + C_{1} R_{1} R_{2} ea_{1} s + C_{1} R_{1} R_{2} s - L_{1} f_{1} s + L_{1} s - R_{1} ea_{1} f_{1} + R_{1} ea_{1} - R_{1} f_{1} + R_{1}}, \\  I_{F1} : \\frac{- C_{1} V_{1} f_{1} s - C_{1} V_{2} f_{1} s}{C_{1} R_{2} s - f_{1} + 1}, \\  I_{L1} : \\frac{- I_{1} R_{1} ea_{1} - I_{1} R_{1} + V_{1}}{L_{1} s + R_{1} ea_{1} + R_{1}}, \\  I_{V1} : \\frac{- C_{1} I_{1} L_{1} R_{2} s^{2} + C_{1} L_{1} V_{1} f_{1} s^{2} - C_{1} L_{1} V_{1} s^{2} + C_{1} L_{1} V_{2} f_{1} s^{2} - C_{1} L_{1} V_{2} s^{2} + C_{1} R_{1} V_{1} ea_{1} f_{1} s - C_{1} R_{1} V_{1} ea_{1} s + C_{1} R_{1} V_{1} f_{1} s - C_{1} R_{1} V_{1} s + C_{1} R_{1} V_{2} ea_{1} f_{1} s - C_{1} R_{1} V_{2} ea_{1} s + C_{1} R_{1} V_{2} f_{1} s - C_{1} R_{1} V_{2} s - C_{1} R_{2} V_{1} s + I_{1} L_{1} f_{1} s - I_{1} L_{1} s + V_{1} f_{1} - V_{1}}{C_{1} L_{1} R_{2} s^{2} + C_{1} R_{1} R_{2} ea_{1} s + C_{1} R_{1} R_{2} s - L_{1} f_{1} s + L_{1} s - R_{1} ea_{1} f_{1} + R_{1} ea_{1} - R_{1} f_{1} + R_{1}}, \\  I_{V2} : \\frac{- C_{1} V_{1} s - C_{1} V_{2} s}{C_{1} R_{2} s - f_{1} + 1}, \\  v_{1} : V_{1}, \\  v_{2} : \\frac{C_{1} R_{2} V_{1} s + V_{2} f_{1} - V_{2}}{C_{1} R_{2} s - f_{1} + 1}, \\  v_{3} : \\frac{I_{1} L_{1} R_{1} ea_{1} s + R_{1} V_{1} ea_{1}}{L_{1} s + R_{1} ea_{1} + R_{1}}, \\  v_{4} : \\frac{- I_{1} L_{1} R_{1} s + L_{1} V_{1} s + R_{1} V_{1} ea_{1}}{L_{1} s + R_{1} ea_{1} + R_{1}}, \\  v_{5} : - V_{2}\\right\\}\\)"
  },
  {
    "objectID": "SMNA_example.html#numeric-solution",
    "href": "SMNA_example.html#numeric-solution",
    "title": "Symbolic modified nodal analysis example",
    "section": "Numeric solution",
    "text": "Numeric solution\nSubstitue the element values into the equations and solve for unknown node voltages and currents. Need to set the current source, I1, to zero.\n\nequ1a = equ.subs(element_values)\nequ1a\n\n\\(\\displaystyle \\left[\\begin{matrix}I_{V1} - 1.0 s v_{2} + v_{1} \\cdot \\left(1.0 s + 0.5\\right) - 0.5 v_{4}\\\\I_{F1} - 1.0 s v_{1} + v_{2} \\cdot \\left(1.0 s + 0.5\\right) - 0.5 v_{5}\\\\I_{Ea1} - I_{F1} - I_{L1}\\\\I_{L1} - 0.5 v_{1} + 0.5 v_{4}\\\\- I_{V2} - 0.5 v_{2} + 0.5 v_{5}\\\\v_{1}\\\\- v_{5}\\\\- 2.0 v_{1} + v_{3} + 2.0 v_{4}\\\\I_{F1} - 2.0 I_{V2}\\\\- 1.0 I_{L1} s - v_{3} + v_{4}\\end{matrix}\\right] = \\left[\\begin{matrix}0\\\\0\\\\0\\\\0\\\\0\\\\1.0\\\\0\\\\0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\nSolve for voltages and currents in terms of Laplace variable s.\n\nu1 = solve(equ1a,X)\nu1\n\n\\(\\displaystyle \\left\\{ I_{Ea1} : \\frac{- 2.0 s^{2} - 10.0 s - 1.0}{2.0 s^{2} + 11.0 s - 6.0}, \\  I_{F1} : - \\frac{2.0 s}{2.0 s - 1.0}, \\  I_{L1} : \\frac{1}{s + 6.0}, \\  I_{V1} : \\frac{s^{2} + 4.0 s + 1.0}{2.0 s^{2} + 11.0 s - 6.0}, \\  I_{V2} : - \\frac{s}{2.0 s - 1.0}, \\  v_{1} : 1.0, \\  v_{2} : \\frac{2.0 s}{2.0 s - 1.0}, \\  v_{3} : \\frac{4.0}{s + 6.0}, \\  v_{4} : \\frac{s + 4.0}{s + 6.0}, \\  v_{5} : 0.0\\right\\}\\)"
  },
  {
    "objectID": "SMNA_example.html#ac-analysis",
    "href": "SMNA_example.html#ac-analysis",
    "title": "Symbolic modified nodal analysis example",
    "section": "AC analysis",
    "text": "AC analysis\nSolve equations for \\(\\omega\\) equal to 1 radian per second, s = 1j.\n\nequ1a_1rad_per_s = equ1a.subs({s:1j})\nequ1a_1rad_per_s  # display the equations\n\n\\(\\displaystyle \\left[\\begin{matrix}I_{V1} + v_{1} \\cdot \\left(0.5 + 1.0 i\\right) - 1.0 i v_{2} - 0.5 v_{4}\\\\I_{F1} - 1.0 i v_{1} + v_{2} \\cdot \\left(0.5 + 1.0 i\\right) - 0.5 v_{5}\\\\I_{Ea1} - I_{F1} - I_{L1}\\\\I_{L1} - 0.5 v_{1} + 0.5 v_{4}\\\\- I_{V2} - 0.5 v_{2} + 0.5 v_{5}\\\\v_{1}\\\\- v_{5}\\\\- 2.0 v_{1} + v_{3} + 2.0 v_{4}\\\\I_{F1} - 2.0 I_{V2}\\\\- 1.0 i I_{L1} - v_{3} + v_{4}\\end{matrix}\\right] = \\left[\\begin{matrix}0\\\\0\\\\0\\\\0\\\\0\\\\1.0\\\\0\\\\0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\nans1 = solve(equ1a_1rad_per_s,X)\nans1\n\n\\(\\displaystyle \\left\\{ I_{Ea1} : -0.637837837837838 + 0.372972972972973 i, \\  I_{F1} : -0.8 + 0.4 i, \\  I_{L1} : 0.162162162162162 - 0.027027027027027 i, \\  I_{V1} : 0.237837837837838 - 0.172972972972973 i, \\  I_{V2} : -0.4 + 0.2 i, \\  v_{1} : 1.0, \\  v_{2} : 0.8 - 0.4 i, \\  v_{3} : 0.648648648648649 - 0.108108108108108 i, \\  v_{4} : 0.675675675675676 + 0.0540540540540541 i, \\  v_{5} : 0.0\\right\\}\\)\n\n\n\nfor name, value in ans1.items():\n    print('{:5s}: mag: {:10.6f} phase: {:11.5f} deg'.format(str(name),float(abs(value)),float(arg(value)*180/np.pi)))\n\nv1   : mag:   1.000000 phase:     0.00000 deg\nv2   : mag:   0.894427 phase:   -26.56505 deg\nv3   : mag:   0.657596 phase:    -9.46232 deg\nv4   : mag:   0.677834 phase:     4.57392 deg\nv5   : mag:   0.000000 phase:         nan deg\nI_V1 : mag:   0.294086 phase:   -36.02737 deg\nI_V2 : mag:   0.447214 phase:   153.43495 deg\nI_Ea1: mag:   0.738882 phase:   149.68322 deg\nI_F1 : mag:   0.894427 phase:   153.43495 deg\nI_L1 : mag:   0.164399 phase:    -9.46232 deg"
  },
  {
    "objectID": "SMNA_example.html#ac-sweep",
    "href": "SMNA_example.html#ac-sweep",
    "title": "Symbolic modified nodal analysis example",
    "section": "AC Sweep",
    "text": "AC Sweep\nLooking at node 4 voltage.\n\nv4, I_F1, I_Ea1, v1, v2, v5, I_L1, v3, I_V1, I_V2 = symbols(' v4 I_F1 I_Ea1 v1 v2 v5 I_L1 v3 I_V1 I_V2')\n\n\nH = u1[v4]\nH\n\n\\(\\displaystyle \\frac{s + 4.0}{s + 6.0}\\)\n\n\n\nnum, denom = fraction(H) #returns numerator and denominator\n\n# convert symbolic to numpy polynomial\na = np.array(Poly(num, s).all_coeffs(), dtype=float)\nb = np.array(Poly(denom, s).all_coeffs(), dtype=float)\nsystem_c1 = (a, b) # system for circuit 1\n\n\nx = np.linspace(0.1*2*np.pi, 100*2*np.pi, 10000, endpoint=True)\nw_c1, mag_c1, phase_c1 = signal.bode(system_c1, w=x) # returns: rad/s, mag in dB, phase in deg\n\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\n#plt.semilogx(frequency, 20*np.log10(np.abs(voltage)),'-k')    # Bode magnitude plot\nplt.semilogx(w_c1/(2*np.pi), mag_c1,'-b')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((-30,20))\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:blue'\n\n#plt.semilogx(frequency, np.angle(voltage)*180/np.pi,':',color=color)  # Bode phase plot\nplt.semilogx(w_c1/(2*np.pi), phase_c1,':',color='tab:red')  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-5,25))\n\nplt.title('Bode plot')\nplt.show()"
  },
  {
    "objectID": "Symbolic Modified Nodal Analysis.html",
    "href": "Symbolic Modified Nodal Analysis.html",
    "title": "Symbolic Modified Nodal Analysis",
    "section": "",
    "text": "Last update: 30 Nov 2023"
  },
  {
    "objectID": "Symbolic Modified Nodal Analysis.html#introduction",
    "href": "Symbolic Modified Nodal Analysis.html#introduction",
    "title": "Symbolic Modified Nodal Analysis",
    "section": "Introduction",
    "text": "Introduction\nThis nodal analysis code started as a translation from some C code to generate a nodal admittance matrix that I had written in 1988. I wrote this code for two reasons. Free versions of Spice for the PC didn’t exist at the time and I wanted to use some of the code from the Numerical Recipes in C [1]. The original C code worked well and calculated numeric solutions. I then started writing some C code to generate the matrices with symbolic values and then intended to use LISP to symbolically solve the equations. I didn’t get too far with this effort. The LISP code would generate huge symbolic strings with no simplification. The output was a big pile of trash that was not in the least bit useful or decipherable.\nIn 2014, I started to use python for my little coding projects and engineering calculations. There are some nice python libraries for numeric and symbolic calculations (such as NumPy and SymPy), so I decided to try writing a python script to generate the node equations based on the old C code I had written many years before. Part way into this project I discovered that there is a new nodal analysis technique being taught today in engineering school called the modified nodal analysis [2][3]. My motivation for reviving this coding project is my continued interest in circuit analysis and synthesis.\nThe modified nodal analysis provides an algorithmic method for generating systems of independent equations for linear circuit analysis. Some of my younger colleagues at work were taught this method, but I never heard of it until a short time ago. These days, I never really analyze a circuit by hand, unless it’s so simple that you can almost do it by inspection. Most problems that an electrical engineer encounters on the job are complex enough that they use computers to analyze the circuits. LTspice is the version of Spice that I use, since it’s free and does a good job converging when analyzing switching circuits."
  },
  {
    "objectID": "Symbolic Modified Nodal Analysis.html#python-code",
    "href": "Symbolic Modified Nodal Analysis.html#python-code",
    "title": "Symbolic Modified Nodal Analysis",
    "section": "Python code",
    "text": "Python code\nMy code started initially by following Erik Cheever’s Analysis of Resistive Circuits MATLAB code [4], to generate modified nodal equations. I somewhat followed his MATLAB file for resistors, capacitors, opamps and independent sources. The naming of the matrices follows his convention. The preprocessor and parser code was converted from my old C code. The use of pandas for a data frame is new and SymPy is used to do the math and the use of element stamps is from [5].\nInductors are being addressed in the D matrix, which is different than the way Erik Cheever’s code works. Erik’s code puts inductors into the G matrix as 1/s/L. My code puts the inductor contribution into the D matrix. Coupled inductors also affect the D matrix, so it makes sense to allow the inductors to be in the D matrix rather than the G matrix.\nNetwork equations: The network equations are a set of independent equations expressed in this code in matrix form. There is an equation for each node based on Kirchhoff’s current law KCL and an equation for each current unknown. The current unknowns are the currents from the voltages sources, op amps, voltage controlled voltage sources, current controlled voltage sources, current controlled current sources and inductors.\nThe network equations are in matrix form:\n\\[A\\cdot X = Z\\]\nThe A matrix describes the connectivity of the resistors, capacitors and G type (VCCS) circuit elements. The column vector X are the unknown node voltages and unknown currents terms from the voltage sources and inductors. The column vector Z is made of the known voltages and currents. The A matrix is formed by four sub matrices, G, B, C and D, which are described below.\n\\[A = \\begin{bmatrix}G B\\\\C D\\end{bmatrix}\\]\nThe matrix G is formed from the coefficients representing the KCL equations for each node. The positive diagonal of \\(G_{k,k}\\) are the conductance terms of the resistor and capacitor elements connected to node k. The off diagonal terms of \\(G_{k,j}\\) are the resistors and capacitor conductances connecting node k to node j. G type elements (VCCS) have input to the G matrix at the connection and controlling node positions.\nThe B matrix describes the connectivity of the unknown branch currents. Independent voltage sources, opamps, H, F and E type elements as well as inductors have inputs to the B matrix.\nThe C matrix describes the connectivity of the unknown branch currents and is mainly the transpose of B matrix, with the exception of the F type elements (CCCS) and includes the E type value.\nThe D matrix describes also connectivity of the unknown currents. The D matrix is composed of zeros unless there are controlled sources and inductors in the network.\nThe X vector is comprised of the V and J vectors as shown below.\n\\[X = \\begin{bmatrix}V\\\\J\\end{bmatrix}\\]\nThe V vector contains the node voltages which are the voltage unknowns to be solved for. The J vector contains the unknown currents from each voltage source.\nThe Z vector is comprised of the I and Ev vectors as shown below.\n\\[Z = \\begin{bmatrix}I\\\\Ev\\end{bmatrix}\\]\nThe I vector contains the known currents and the Ev vector contains the known voltages. The Ev designation is used for the voltage vector (and not simply ‘E’) because SymPy uses e and E sometimes for the mathematical constant 2.71, sometimes called Euler’s number. The use of E or e as a symbol in SymPy was causing errors when the code was run.\nPutting all the parts together:\n\\[\\begin{bmatrix}G B\\\\C D\\end{bmatrix} \\cdot \\begin{bmatrix}V\\\\J\\end{bmatrix} = \\begin{bmatrix}I\\\\Ev\\end{bmatrix}\\]\nStamps: Stamps are templates for modifying the B, C and D matrices and facilitate the construction of the matrices. The stamps used in this implementation of the MNA follow the stamps of [5].\n\nCode description\nThe code is divided in the following sections.\nPreprocessor: The preprocessor reads in the netlist text file and removes comments, extra spaces and blank lines. The first letter of the element type is capitalized to make subsequent parsing of the file easier. The number of lines are counted and the number of entries on each line are checked to make sure the count is consistent with the element type.\nParser: The parser code loads the preprocessed netlist into a data frame. A report is generated which consists of a count of the element types in the netlist.\nMatrix formulation: Each of the matrices and vectors are generated.\nCircuit equation generation: The circuit equations are generated in a for loop. Sympy automatically does some simplification according to its default settings. Two for loops perform the matrix multiplication on the equation.\n\\(A\\cdot X = Z\\)\n\n\nCode validation\nBasic validation of the code consisted of analyzing simple networks and examining the results. A more comprehensive evaluation of the code was performed by solving test circuits and comparing the results to LTSpice. As of October 2023 all the element types have been tested. See the circuits used for validation here. The validation circuits range from simple to large and complex. The largest validation circuit consist of 32 nodes, 59 branches and multiple instances all of the element types. For this large test circuit, there are small numerical differences between the Python modified nodal analysis (MNA) code results and the LTSpice solution, which are describe in the test report. Additionally, various interesting problem circuits have been solved using the MNA code and comparing the results to LTSpice. These problem circuits can also be found in the github repository. Code verification often looks at requirements or specifications versus what was implemented. This project didn’t have a formal set of requirements, only a general goal of implementing symbolic MNA using the Python libraries. No formal software or code verification is included."
  },
  {
    "objectID": "Symbolic Modified Nodal Analysis.html#users-guide",
    "href": "Symbolic Modified Nodal Analysis.html#users-guide",
    "title": "Symbolic Modified Nodal Analysis",
    "section": "User’s guide",
    "text": "User’s guide\nA netlist is text file that contains the connectivity information of the circuit. A netlist in the input to the analysis code and the output are a set of equations that can be solved for the unknown voltages and currents. The circuits are described in terms of the components and the connections to the other components. Netlist can be generated with a text editor or exported from a schematic capture program. LTspice can be used to draw the schematic and a netlist can be exported to the python code to generate the network equations.\nNetlist file format:\nA circuit description in spice, which is called a netlist, consists of a statements defining each circuit element and its connection to circuit nodes. A node is a any point on a circuit where two or more circuit elements meet. The nodes are numbered from 1 to N in any order and node 0 is the ground node or circuit common. A ground node is required. Choose a ground or reference node, which usually is taken to be at a potential of zero volt. All other node voltages constitute n unknowns. The nodes should be numbered in consecutive order. Each line in the netlist are either comments, spice directives or circuit elements.\nSpice directives are commands to spice and the first character on the line is a period. Comment lines start with a * or ;. The default file extension is ‘.net’. The python code does some preprocessing of the netlist to check the basic formatting of the netlist is correct.\nThe preprocessor performs the following steps: - remove blank lines and comments - convert first letter of element name to uppercase - removes extra spaces between entries - counts number of entries on each line to make sure the count is correct and counts each element type\nThe element types that are supported are resistors, capacitors, inductors, independent sources and controlled sources. Each line in the netlist file contains a circuit element.\nThe format for the element description is\nletterXXX n1 n2 value\nWhere:\nletter signifies the element type, i.e. R, L, C, V, I, O, E, F, G, H or K\nXXX is a string of letters or numbers that uniquely identify the element.\nn1, n2 and value are the terminal nodes and the value of the element.\nThe element types are described in the following sections.\nResistors, capacitors and inductors:\nThe resistors, capacitors and inductors are described by the following line:\nR/L/CXXXXXXX N1 N2 value \nWhere:\nXXX = the name of the component, can be any length\nN1 = the first terminal\nN2 = the second terminal\nValue = component value in ohms, farads or henrys.\nFor example, a resistor named R1 connected between nodes 1 and 2 with a value of 3000 ohms.\nR1 2 4 3000  \nSpice supports other parameters, but these are not allowed in this python implementation.\nCoupled inductors:\nTwo coupled inductors are described by the following line.\nKXX LYY LZZ VALUE  \nThe parameters are:\nLYY = the name of the first coupled inductor\nLZZ = the name of the second coupled inductor\nVALUE = the coefficient of coupling, K, where 0 &lt; K\nThe orientation of the inductors is determined by the first node, which is considered to be the dotted node. When LTspice generates a net list the phasing dot gets assocated with the negative node. This does not seem to affect the equations generated by the python code. LTspice uses the coupling coefficient, k. The symbolic equations use the mutual inductance, M.\n\\[M = k\\sqrt{L_1L_2}\\]\nIndependent sources:\nA voltage source is described by the following line.\nVXX N+ N- VALUE  \nThe parameters are:\nN+ = the name of the positive terminal\nN- = the name of the negative terminal\nVALUE = the value of the DC voltage\nA current source is described by the following line.\nIXX N+ N- VALUE  \nThe parameters are:\nN+= the name of the positive terminal, current leaves this terminal (pointy end of the arrow)\nN- = the name of the negative terminal VALUE = the value of the DC current\nControlled sources:\nThe voltage-controlled dependent sources are defined using statements of the form\nGXX or EXX nout+ nout- nc+ nc- gain  \nwhere E is a voltage-controlled voltage source, G is a voltage-controlled current source, the output voltage is connected between nodes nout+ and nout-, and the control voltage is measured at node nc+ with respect to node nc-.\nExamples:\nE1 5 1 4 3 10\ndefines a voltage source that makes node 5 a voltage 10*(v4 − v3) above the voltage at node 1.\nG1 2 1 5 8 50 \ndefines a current source connected between node 2 (the + node) and node 1 and supplying a current 50 *(v5 − v8).\nThe current-controlled dependent sources are defined by statements of the form\nFXX or HXX nout+ nout- vcontrol gain   \nwhere F is a current-controlled current source, H is a current-controlled voltage source, and the output current source is connected between nodes nout+ and nout-, with positive current flowing through the source from node nout+ to nout-. The control current flows from the positive node of the source vcontrol through the source and out the negative node.\nExamples:\nFds 11 9 Vsens 1.25\ndefines a current source connected from node 11 to node 9 that generates a current 1.25 times the current flowing through the source Vsens.\nH1 30 20 V5 100\ndefines a voltage source connected from node 30 to node 20 and supplying a voltage 100 times the current through the source V5. It is frequently necessary to add a voltage source with value 0 volts to the circuit to sense the control current for these sources.\nThe direction of positive controlling current flow is from the positive node, through the source, to the negative node of VNAM. VALUE is the current gain.\nOp Amps:\nAn opamp component is described by the following line.\nOXX N+ N- Vout\nThe output of the opamp is a voltage source. Two input terminals are at the same potential.\nThe op amp element is assumed to be an ideal op amp and use of this component is valid only when used in circuits with a DC path (a short or a resistor) from the output terminal to the negative input terminal of the op amp. No error checking is provided and if the condition is violated, the results will be likely erroneous. Need to work on implementing a better opamp model.\nProcedure:\n1. Draw the circuit to be analyzed in LTSpice or some other schematic capture program. Label the nodes. The Symbolic Modified Network Analysis code will provide warnings for netlist formatting errors and non consecutive node numbering, but will still generate nodal equations which may be erroneous. Users should verify the results. 2. Export the netlist of the circuit and convert component values to units of Ohms, Farads and Henrys. Use scientific notation, for example, replace component values such as 2k with 2e3 and 2u with 2e-6.\n3. Change Op Amp reference designators, for example U1 to O1 (capitol letter O, not zero).\n4. Voltage sources and current sources need to be set to zero in some cases.\n5. Modify the nodal analysis Jupyter notebook code to read the net list. Run all the cells in the notebook.\n6. Copy the symbol list, the A, X and Z matrices, and the element values in dictionary format to a new notebook. See end of the nodal analysis Jupyter notebook where these items are displayed.\n7. Review the test and problem circuits for examples."
  },
  {
    "objectID": "Symbolic Modified Nodal Analysis.html#example",
    "href": "Symbolic Modified Nodal Analysis.html#example",
    "title": "Symbolic Modified Nodal Analysis",
    "section": "Example",
    "text": "Example\nThe example circuit contains a capactior, inductor, resistors along with independent and dependent sources. This circuit is from [6], page 69, figure 4.8, with modifications. The circuit was drawn in LTSpice and the circuit nodes are labeled. For no particular reason, the reference note was chosen to be the center node of the circuit and is connected to the ground symbol. AC analysis was performed at 1 rad/sec and over a range of frequencies. The results are compared to those obtained from LTSpice.\n\nThe netlist from LTSpice is shown below with the V1 voltage source line editied.\n* SMNA_example.asc\nR2 2 5 2\nV1 1 0 1\nI1 4 0 9\nV2 0 5 0\nE1 3 0 1 4 2\nF1 2 3 V2 2\nR1 1 4 2\nC1 1 2 1\nL1 4 3 1 Rser=0\n* ;.ac list   0.159154943091895\n.ac dec 100 0.1 100\n.backanno\n.end\n\nCircuit equations\nThe following network equations were generted by the Sympy code.\n\\(- C_{1} s v_{2} + I_{V1} + v_{1} \\left(C_{1} s + \\frac{1}{R_{1}}\\right) - \\frac{v_{4}}{R_{1}} = 0\\)\n\\(- C_{1} s v_{1} + I_{F1} + v_{2} \\left(C_{1} s + \\frac{1}{R_{2}}\\right) - \\frac{v_{5}}{R_{2}} = 0\\)\n\\(I_{Ea1} - I_{F1} - I_{L1} = 0\\)\n\\(I_{L1} - \\frac{v_{1}}{R_{1}} + \\frac{v_{4}}{R_{1}} = - I_{1}\\)\n\\(- I_{V2} - \\frac{v_{2}}{R_{2}} + \\frac{v_{5}}{R_{2}} = 0\\)\n\\(v_{1} = V_{1}\\)\n\\(- v_{5} = V_{2}\\)\n\\(- ea_{1} v_{1} + ea_{1} v_{4} + v_{3} = 0\\)\n\\(I_{F1} - I_{V2} f_{1} = 0\\)\n\\(- I_{L1} L_{1} s - v_{3} + v_{4} = 0\\)\n\n\nSymbolic solution\nSympy was used to solve the network equations and the node voltage results are shown below.\n\\(v_{1} = V_{1}\\)\n\\(v_{2} = \\frac{C_{1} R_{2} V_{1} s + V_{2} f_{1} - V_{2}}{C_{1} R_{2} s - f_{1} + 1}\\)\n\\(v_{3} = \\frac{I_{1} L_{1} R_{1} ea_{1} s + R_{1} V_{1} ea_{1}}{L_{1} s + R_{1} ea_{1} + R_{1}}\\)\n\\(v_{4} = \\frac{- I_{1} L_{1} R_{1} s + L_{1} V_{1} s + R_{1} V_{1} ea_{1}}{L_{1} s + R_{1} ea_{1} + R_{1}}\\)\n\\(v_{5} = - V_{2}\\)\n\n\nThe Jupyter notebook\nThe notebook for this example is here."
  },
  {
    "objectID": "Symbolic Modified Nodal Analysis.html#links",
    "href": "Symbolic Modified Nodal Analysis.html#links",
    "title": "Symbolic Modified Nodal Analysis",
    "section": "Links",
    "text": "Links\nSymbolic Modified Nodal Analysis github page."
  },
  {
    "objectID": "Symbolic Modified Nodal Analysis.html#references",
    "href": "Symbolic Modified Nodal Analysis.html#references",
    "title": "Symbolic Modified Nodal Analysis",
    "section": "References",
    "text": "References\n\nNumerical Recipes in C: The Art of Scientific Computing, William H. Press, Brian P. Flannery, Saul A. Teukolsky, William T. Vetterling, Cambridge University Press; 1988\n\nThe modified nodal approach to network analysis, Chung-Wen Ho, A. Ruehli, P. Brennan, IEEE Transactions on Circuits and Systems ( Volume: 22, Issue: 6, Jun 1975 )\n\nModified nodal analysis\n\nAnalysis of Resistive Circuits\n\nECE 570 Session 3, Computer Aided Engineering for Integrated Circuits\n\nD. E. Johnson, J. L. Hilburn, and J. R. Johnson, Basic Electric Circuit Analysis, Prentice-Hall, Inc. 1978"
  },
  {
    "objectID": "source.html",
    "href": "source.html",
    "title": "Source",
    "section": "",
    "text": "Future links to my github page."
  },
  {
    "objectID": "index.html#other-engineering-topics",
    "href": "index.html#other-engineering-topics",
    "title": "A collection of Jupyter Notebooks",
    "section": "Other engineering topics",
    "text": "Other engineering topics\n\natmospheric water extraction\ncanaon ball balistics\nthermal wall (what ever that’s called)\n\n\nEngineering Mathmatics"
  },
  {
    "objectID": "OLD Two amplifier RIAA Phono Preamp.html",
    "href": "OLD Two amplifier RIAA Phono Preamp.html",
    "title": "Two amplifier RIAA phono preamp",
    "section": "",
    "text": "Photo of a phono stylus playing a record (photo taken from Holland Koningsdam, hallway, deck 7, midship)"
  },
  {
    "objectID": "OLD Two amplifier RIAA Phono Preamp.html#abstract",
    "href": "OLD Two amplifier RIAA Phono Preamp.html#abstract",
    "title": "Two amplifier RIAA phono preamp",
    "section": "Abstract",
    "text": "Abstract\nThis paper analyizes the circuit for the RIAA preamp given in the Texas Instruments application note AN346, High-Performance Audio Applications of the LM833, shown in the app note as Figure 3. The schematic for the phono preamplifier was entered into LTSpice and the circuit net list was generated. A circuit analysis method called the Modified Nodal Analysis was used to derive the symbolic circuit equations and Python libraries were used to solve the equations. The preamplifier transfer function was used to calculate the Bode, impuse and step response plots. The Python results were compared to those from LTSpice. Deviation from the RIAA response curve was also examined. The sensitivity, Monte Carlo and worst case analysis for the preamplifier circuit was performed. The JupyterLab notebook show cases the use of Python in electrical engineering and circuit analysis.\nContents\n1. Introduction\n2. RIAA pre-emphasis curve\n3. AN346 RIAA Phono Preamplifier Design Procedure\n4. Analysis of the phono preamplifier circuit\n5. Summary"
  },
  {
    "objectID": "OLD Two amplifier RIAA Phono Preamp.html#introduction",
    "href": "OLD Two amplifier RIAA Phono Preamp.html#introduction",
    "title": "Two amplifier RIAA phono preamp",
    "section": "Introduction",
    "text": "Introduction\nThis JupyterLab notebook uses the SymPy, NumPy,SciPy and the Python programming language libraries to analyze a phono preamplifier circuit from the Texas Instruments application note, AN346, High-Performance Audio Applications of the LM833. The purpose of this analysis is to demonstrate the capability of using the Python libraries in electrical engineering circuit analysis. The circuit chosen for this analysis is a two stage RIAA Phono Preamplifier described in the application note. The preamplifier is designed to accurately reproduce the RIAA equalization curve required for play back of Vinyl LP records. The preamplifier provides about 35 dB of gain at 1kHz along with the proper gain profile and phase response. The schematic of the circuit is shown below with each node explicity annotated.\n\nThere has been a resurgence on the popularity of Vinyl LP records over the last few years. So the use of a RIAA Phono Preamplifier is still relevant in this age where most music is delivered by streaming. Two internet news stories describe the current market for Vinyl LP records:\nThey Said the Album Was Dying. They Were Wrong\n\nVinyl sales have grown steadily for 17 years, but jumped by a stunning 46% in 2020 and 51% in 2021 …\n\nWhy Vinyl Records Are Making a Comeback in 2022\n\nThis year, 2020, marks the first year in more than a generation since record sales — that is to say physical vinyl records — have surpassed CD sales. The reasons for this are twofold: CD sales have dropped dramatically in recent years, while sales of vinyl records are actually up this year. And while you might think it’s nostalgic Boomers or Gen Xers behind the renaissance of records, in fact surveys show it’s millennial consumers driving the rising trend in vinyl sales.\n\n\nSo vinyl is here to stay, it seems, despite all technological advances that would have seemed to threaten it. The same RIAA study that found records surpassing CDs also revealed that streaming music now account for more than 85% of all music enjoyed. Only 6% of music is now downloaded, even less than is physically purchased in the form of records, CDs, or the last tapes.\n\nToday there are hundreds of phono products sold on Amazon. Phono preamps range in price from $10 to $1,000 dollars.\nThe Phono Preamplifier also known as a phono stage, is an audio component that amplifies the signal from your turntable to a level that allows you to connect it to your sound system the same way you would with any other audio source. In addition to boosting the signal from the phono carterage, the preamp applies the RIAA equalization curve to the signal, reverting it back to the shape it was on the original recording. Phono cartridge output varies depending on the type of phono cartridge. Moving Magnet (MM) or Moving Iron (MI) cartridges typically produce a maximum output of 5mV. Moving Coil (MC) cartridges produce a much lower output, typically around 0.5mV maximum. Most phono preamps have switch that allow users to select the type of coil they have installed on their turn table arm.\nTexas Instruments provided the schematic of the preamplifier in their application note to highlight the types of applications their LM833 operational amplifier can support. Application notes are sometimes part of the marketing literature provided along with component data sheets by semiconductor manufactures.\nRIAA equalization is a specification for the recording and playback of phonograph records, established by the Recording Industry Association of America (RIAA). RIAA was formed in 1952. Its original mission was to administer recording copyright fees and problems, work with trade unions, and do research relating to the record industry and government regulations. Early RIAA standards included the RIAA equalization curve, the format of the stereophonic record groove and the dimensions of 33 1/3, 45, and 78 rpm records.\nThe purposes of the equalization are to permit greater recording times (by decreasing the mean width of each groove), to improve sound quality and to reduce the groove damage that would otherwise arise during playback. RIAA equalization is a form of pre-emphasis on recording and de-emphasis on playback. A recording is made with the low frequencies reduced and the high frequencies boosted, and on playback, the opposite occurs. The net result is a flat frequency response, but with attenuation of high-frequency noise such as hiss and clicks that arise from the recording medium. Reducing the low frequencies also limits the excursions the cutter needs to make when cutting a groove. Groove width is thus reduced, allowing more grooves to fit into a given surface area, permitting longer recording times. This also reduces physical stresses on the stylus, which might otherwise cause distortion or groove damage during playback.\n\nScope\nThe analysis presented in this notebook is intended to illustrate the use of Python for circuit analysis. This is not a tutorial on how to design a better phono preamp. The circuit taken from the Texas Instruments application note is examined for what it is, which is a suggested application for the use of their audio grade op amp. However, in this analysis I don’t address the performance of the op amp relative to the implementation of the RIAA equalization curve. I’m more concerned with examining the circuit’s ability to reproduce the proper gain and phase over the audio band. The performance LM833 op amp is assumed to be sufficient for this application and in my analysis of the circuit, I’ve replaced the LM833 with an ideal op amp model. Also, it is assumed that the reader is familiar with electronic components such as resistors, capacitors and operational amplifiers also known as op amps or opamps.\n\n\nMethodology\nThe analysis presented in this notebook will cover a topics that are often presented during a design review. Ususaly during a design review conformance to requirements is presented. For the phono preamp circuit, the main performance requirement is minimum deviation from the RIAA curve. The application note from TI stated that the deviation is less than 0.1 dB over the audio band when using 1% resistors.\nIn this notebook the analysis is divided into sections.\n\nThe analysis will start with an description of the circuit operation and some basic calculations.\nThere are many symbols used in the equations and these are listed in a table for reference. I also tried to be constant with variable names.\nThe RIAA pre-emphasis curve is discussed and the transfer function, pole/zero plot and amplitude and phase response is plotted.\nCalculations for the phono preamplifier design procedure as covered in the application note are presented. The element values obtain with this procedure are the ones used in the analysis.\nThe equations for the transfer function of the preamp are derived by using a tecnhique known as modified nodal analysis.\nThe preamp poles and zeros are plotted and some comments about stability are provided.\nThe amplitude and phase responce of the preamp transfer function is plotted.\nThe impulse, step and group dealy are plotted\nThe amplitude and phase response of the preamp transfer function is plotted against results taken from LTSpice as a check and comparision.\nThe deviation of the amplitude and phase responce from the RIAA curve is plotted.\nSensitivity analsysis, component selection, monte carlo and worst case analysis are presented.\n\n\nimport os\nimport sys\nimport random\nfrom sympy import *\nimport numpy as np\nfrom scipy import signal\nimport matplotlib.pyplot as plt\ninit_printing()\n\n\n\nPython library versions\n\npython: 3.10.9\nnumpy: 1.23.5\nsympy: 1.11.1\nscipy: 1.10.0\nmatplotlib: 3.7.0\n\nDefine a function to return the system gain at a frequency: get_gain()\n\ndef get_gain(freq_Hz, sys):\n    '''\n    freq_Hz: the frequency in Hz for which the system gain is desired\n    sys: a SciPy instance of the LTI class or a tuple describing the system\n    '''\n    f1 = freq_Hz - freq_Hz*0.1 # lower limit of the frequency range\n    f1a = freq_Hz - freq_Hz*0.01 # lower interpolation point\n    f2 = freq_Hz + freq_Hz*0.1 # upper limit of the frequency range\n    f2a = freq_Hz + freq_Hz*0.01  # upper interpolation point\n\n    x_axis_range = np.linspace(f1*2*np.pi, f2*2*np.pi, 1000, endpoint=True) # define the range frequency range\n    w, mag, phase = sys.bode(w=x_axis_range)\n\n    index_for_f1a = np.where(w &gt; f1a*2*np.pi)[0][0]\n    index_for_f2a = np.where(w &gt; f2a*2*np.pi)[0][0]\n\n    return np.interp(freq_Hz, [w[index_for_f1a]/(2*np.pi),w[index_for_f2a]/(2*np.pi)], [mag[index_for_f1a],mag[index_for_f2a]])\n\n\n\nSchematic and circuit description\nThe circuit from Figure 3 of AN346 was entered into LTSpice and the circuit nodes were numbered as shown above. Any schematic capture program could be used to for this as long as a Spice like netlist can be generated. In the schematic, the voltage source V1, is set to 5 mV to represent the output of a Moving Magnet (MM) or Moving Iron (MI) cartridge. The input to the preamp is shunted by a capacitance, which is equal to the sum of the input cable capacitance and the cartridge. This capacitance resonates with the inductance of the moving magnet cartridge to determine the frequency response of the transducer, so when a moving magnet pickup is used, Cp should be carefully chosen so that the total capacitance is equal to the recommended load capitance for that particular cartridge. 100 pF is used in this analysis. Rp is the recommended resistive load for the phono cartridge. In some comercial preamp designs, the value of Rp is user selectable with switches. As shown in the calculations, Cp and Rp have a resonant frequency of 33.86kHz.\n\nCp = 100e-12\nRp = 47e3\nprint('resonant frequency: {:,.1f} Hz'.format(1/(2*np.pi*Rp*Cp)))\n\nresonant frequency: 33,862.8 Hz\n\n\n33,862 Hz is well above the audio range.\nThe first operational amplifier, U1, takes care of the 50 Hz and 500 Hz breakpoints. For the analysis with Python, the op amp is modeled as and ideal opamp. There is expected to be some differences between the LTSpice results and the Python analysis. Using two amplifiers results in accurate conformance to the RIAA curve without reverting to the noisy inverting topology, as well as lower distortion due to the fact that each amplifier is operating at a lower gain than would be the case in a single-amplifier design.\nThe resistor, R1, which has a value of 80.6k\\(\\Omega\\) and the capacitor C1, which has a value of 0.039 \\(\\mu\\)Farads, form a resonant pair with frequency of 50.6 Hz.\n\nR1 = 80.6e3\nC1 = 0.039e-6\nprint('resonant frequency: {:,.1f} Hz'.format(1/(2*np.pi*R1*C1)))\n\nresonant frequency: 50.6 Hz\n\n\n50 Hz is one of the RIAA time constants required by the RIAA specification.\nFrom here on, I’ll refer to resistors that have values in 1000’s of Ohms by using k for thousands of Ohms or just the numerical value if it’s less than 1000. Capacitors will have values indicated in \\(\\mu\\) for micro Farads and p for pico Farads, designated as \\(\\mu\\) or p. \n\nR2 = 8.45e3\nprint('resonant frequency: {:,.1f} Hz'.format(1/(2*np.pi*R2*C1)))\n\nresonant frequency: 482.9 Hz\n\n\nC1 and R2, which has a value of 8.45k, have a resonant frequency of 482.94 Hz. As describe later, these frequencies correspond to the time constants required by the RIAA specification.\nCo provides an AC ground for the non-inverting configuration of U1. Ro along with R1 and R2 set the low frequency gain of U1.\n\nRo = 499\nCo = 200e-6\nprint('resonant frequency: {:,.1f} Hz'.format(1/(2*np.pi*Ro*Co)))\n\nresonant frequency: 1.6 Hz\n\n\nRo=499 and Co=200\\(\\mu\\) have a resonant frequency of 1.6 Hz.\n\nRf=R1+R2\nprint('low frequency voltage gain of U1: {:,.2f} or {:,.1f}dB'.format(1+Rf/Ro, 20*np.log10(1+Rf/Ro)))\n\nlow frequency voltage gain of U1: 179.46 or 45.1dB\n\n\nRo along with R1 and R2 set the low frequency gain of U1 at 45 dB.\n\nR3 = 2.37e3\nC3 = 0.033e-6\nprint('resonant frequency: {:,.1f} Hz'.format(1/(2*np.pi*R3*C3)))\n\nresonant frequency: 2,035.0 Hz\n\n\nR3=2.37k and C3=0.033\\(\\mu\\) have a resonant frequency of 2034.96 Hz and corresponds to the the third time constant specified by RIAA.\nC4=2\\(\\mu\\) and R6=54.9k form a high pass filter with a corner frequency of 1.45Hz.\n\nR6 = 54.9e3\nC4 = 2e-6\nprint('resonant frequency: {:,.1f} Hz'.format(1/(2*np.pi*R6*C4)))\n\nresonant frequency: 1.4 Hz\n\n\nU2, R4 and R5 form a non inverting configuration with a voltage gain of 3.15 or 9.98 dB.\n\nR4=2e3\nR5=4.3e3\nprint('voltage gain of U2: {:,.2f} or {:,.1f}dB'.format(1+R5/R4, 20*np.log10(1+R5/R4)))\n\nvoltage gain of U2: 3.15 or 10.0dB\n\n\n\n\nSymbols\nIn this notebook the following symbols are used:\n\n\n\n\n\n\n\nSymbol\ndefininition\n\n\n\n\ns\nwhen used in a polynominal: the Laplace variable equal to \\(\\alpha + j\\omega\\)\n\n\n\\(\\mu\\)\n\\(1 \\times 10^{-6}\\) multiplier, either: \\(1 \\times 10^{-6}\\) seconds or \\(1 \\times 10^{-6}\\) Farads\n\n\nT\ntime constant: T1, T2, T3\n\n\n\\(\\omega\\)\nangular frequency, radians per second, \\(\\omega = 1/T\\)\n\n\nf\nfrequency in cycles per second, \\(f = \\frac{\\omega}{2\\pi}\\)\n\n\nR\nresistor: R1, R2 etc.\n\n\nC\ncapacitor: C1, C2 etc.\n\n\nv\nnode voltage: v1, v2, v3 etc.\n\n\nV\nvoltage source, e.g. V1\n\n\nA\nmatrix describing the connectivity of the resistors, capacitors and G type (VCCS) circuit elements\n\n\nX\nvector of unknown node voltages and unknown currents\n\n\nZ\nvector of known voltages and currents\n\n\nRIAA_num\nnumerator of the RIAA pre-emphsis transfer function\n\n\nRIAA_den\ndenominator of the RIAA pre-emphsis transfer function\n\n\nw_RIAA\nradian frequncy of the RIAA pre-emphsis transfer function\n\n\nmag_RIAA\nmagnitude of the RIAA pre-emphsis transfer function\n\n\nphase_RIAA\nphase of the RIAA pre-emphsis transfer function\n\n\nRIAA_gain_1kHz\ngain of the RIAA pre-emphsis transfer function at 1kHz\n\n\npreamp_equ_sym\npreamp circuit equations with symbolic values\n\n\nU_sym\nsymbolic solution to network equations, node voltages and unknown currents\n\n\nH_sym\ntransfer function with symbolic coefficients\n\n\npreamp_equ\ncircuit equations with numeric element values\n\n\nH_preamp_num\nnumerator of the transfer function\n\n\nH_preamp_denom\ndenominator of the transfer function\n\n\npreamp_sys\nSciPy representation of the preamp system\n\n\npreamp_gain_1kHz\ngain of the preamp transfer function at 1kHz\n\n\nw_preamp\nradian frequncy of the preamp transfer function\n\n\nmag_preamp\nmagnitude of the preamp transfer function\n\n\nphase_preamp\nphase of the preamp transfer function"
  },
  {
    "objectID": "OLD Two amplifier RIAA Phono Preamp.html#riaa-pre-emphasis-curve",
    "href": "OLD Two amplifier RIAA Phono Preamp.html#riaa-pre-emphasis-curve",
    "title": "Two amplifier RIAA phono preamp",
    "section": "RIAA pre-emphasis curve",
    "text": "RIAA pre-emphasis curve\nThe RIAA equalization curve was established in 1954. The equalization is defined by time constants, T1, T2 and T3. During the Phonograph record manufacturing process, a pre-emphsis is applied to the signal, which allows for longer playback times on phonograph records by decreasing the average width of the groove cut into vinyl phonograph disks. The curve attenuates low frequencies and amplifies high frequencies, relative to 1 kHz. Since low frequencies cause wide undulations in the record groove, they must be attenuated to keep the grove within its bounds. Above 1 kHz, the frequencies are amplified which helps overcome the inherent noise produced by the phonograph needle during play-back.\nThe RIAA disc recording/reproduction standard specifies the time constants of, \\(T1 = 75 \\mu s\\), \\(T2 = 318 \\mu s\\) and \\(T3 = 3180 \\mu s\\) and the pre-emphasis transfer function:\n\\(RIAA(s)=\\frac {(sT_{1}+1)(sT_{3}+1)}{(sT_{2}+1)}\\)\nThe three time constants correspond to the frequencies calculated below.\n\nT1 = 75e-6\nT2 = 318e-6\nT3 = 3180e-6\nprint('{:.0f} \\u03BCs corresponds to {:,.1f} Hz'.format(T1*1e6,1/(T1*2*np.pi)))\nprint('{:.0f} \\u03BCs corresponds to {:,.1f} Hz'.format(T2*1e6,1/(T2*2*np.pi)))\nprint('{:.0f} \\u03BCs corresponds to {:,.1f} Hz'.format(T3*1e6,1/(T3*2*np.pi)))\n\n75 μs corresponds to 2,122.1 Hz\n318 μs corresponds to 500.5 Hz\n3180 μs corresponds to 50.0 Hz\n\n\nThe time constants are put in polynominal form using s as the Laplace variable. The numerator and denominator of the pre-emphasis transfer function is defined below.\n\ns = symbols('s')\nRIAA_num = Eq(((s*T3+1)*(s*T1+1)),0)\nRIAA_denom = Eq(s*T2+1,0)\n\nSolve for the poles and zeros of the pre-emphasis transfer function and plot the locations on the complex s-plane. The zeros of the transfer function are the roots of the numerator polinominal. The poles of the transfer function are the roots of the denominal polinominal.\n\nRIAA_zeros = solve(RIAA_num,s)\nRIAA_poles = solve(RIAA_denom,s)\n\n\nplt.plot(np.real(RIAA_zeros), np.imag(RIAA_zeros), 'ob', markerfacecolor='none')\nplt.plot(np.real(RIAA_poles), np.imag(RIAA_poles), 'xr')\nplt.legend(['Zeros', 'Poles'], loc=2)\nplt.title('Pole / Zero Plot')\nplt.xlabel('real part, \\u03B1')\nplt.ylabel('imaginary part, j\\u03C9')\nplt.grid()\nplt.show()\n\n\n\n\n\nprint('number of zeros: {:d}'.format(len(RIAA_zeros)))\nfor i in RIAA_zeros:\n    print('{:,.2f} Hz'.format(i/(2*np.pi)))\n\nnumber of zeros: 2\n-2,122.07 Hz\n-50.05 Hz\n\n\n\nprint('number of poles: {:d}'.format(len(RIAA_poles)))\nfor i in RIAA_poles:\n    print('{:,.2f} Hz'.format(i/(2*np.pi)))\n\nnumber of poles: 1\n-500.49 Hz\n\n\nAs shown in the plot above, the poles and zeros lay on the negative real axis. The de-emphisis transfer function of the phono pre-amplifier should have poles at the zero locations and a zero in the pole location in the plot above.\nThe code below is used to convert SymPy symbolic equations to a numpy polynomial representation. The SciPy function, TransferFunction, represents the system as the continuous-time transfer function. The Numpy function, logspace, is used to generate data points on a log scale for plotting. The SciPy function, bode, is used to generate the magnitude and phase data of a continuous-time system.\n\na = np.array(Poly(RIAA_num, s).all_coeffs(), dtype=float)\nb = np.array(Poly(RIAA_denom, s).all_coeffs(), dtype=float)\nRIAA_sys = signal.TransferFunction(a,b)\n\nx_axis_range = np.logspace(1, 5.5, 100, endpoint=True)*2*np.pi\n\nw_RIAA, mag_RIAA, phase_RIAA = RIAA_sys.bode(w=x_axis_range) # returns: rad/s, mag in dB, phase in deg\n\nFind the gain at 1kHz so the plots can be normalized for 0 dB at 1 kHz.\n\nRIAA_gain_1kHz = get_gain(1000,RIAA_sys)\n\n\nprint('The RIAA gain at 1kHz: {:.3f} dB'.format(RIAA_gain_1kHz))\n\nThe RIAA gain at 1kHz: 19.911 dB\n\n\nPlot the magnitude and phase of the RIAA curve.\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nplt.semilogx(w_RIAA/(2*np.pi), mag_RIAA-RIAA_gain_1kHz,'-k')    # Bode magnitude plot, normalized to 0 at 1kHz\n\n# mark individual points\np3 = np.where(w_RIAA &gt; 49.9*(2*np.pi))[0][0]\np2 = np.where(w_RIAA &gt; 499.9*(2*np.pi))[0][0]\np1 = np.where(w_RIAA &gt; 2122*(2*np.pi))[0][0]\n\nplt.semilogx(w_RIAA[p1]/(2*np.pi), mag_RIAA[p1]-RIAA_gain_1kHz,'^k')\nplt.semilogx(w_RIAA[p2]/(2*np.pi), mag_RIAA[p2]-RIAA_gain_1kHz,'^k')\nplt.semilogx(w_RIAA[p3]/(2*np.pi), mag_RIAA[p3]-RIAA_gain_1kHz,'^k')\n\nplt.text(w_RIAA[p1]/(2*np.pi), mag_RIAA[p1]-25,'T1')\nplt.text(w_RIAA[p2]/(2*np.pi), mag_RIAA[p2]-25,'T2')\nplt.text(w_RIAA[p3]/(2*np.pi), mag_RIAA[p3]-25,'T3')\n\n# hightlight the audio band, 20 to 20kHz\nplt.axvspan(20, 20e3, color='y', alpha=0.3)\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\nplt.semilogx(w_RIAA/(2*np.pi), phase_RIAA,':',color='b',label='phase')  # Bode phase plot\n\n# mark individual points\nplt.semilogx(w_RIAA[p1]/(2*np.pi), phase_RIAA[p1],'xb')\nplt.semilogx(w_RIAA[p2]/(2*np.pi), phase_RIAA[p2],'xb')\nplt.semilogx(w_RIAA[p3]/(2*np.pi), phase_RIAA[p3],'xb')\n\nax2.set_ylabel('phase, deg',color='b')\nax2.tick_params(axis='y', labelcolor='b')\nax2.set_ylim((0,100))\n\nax2.plot(np.NaN, np.NaN, color='k', label='magnitude')\n\nplt.legend()\nplt.title('RIAA pre-emphasis Bode plot')\nplt.show()\n\n\n\n\nThe plot above shows the frequence response of the RIAA curve. The frequencies coresponsing to the time constants T1, T2 and T3 are plotted on the magnitude and phase curves. The audio band of 20Hz to 20kHz is highlighted. One thing to notice about this curve is that the amplitdue is increasing as the frequency increases. This is not a realistic function, real circuits do not have and inifinite gain at as the frequency goes tio infinity. Also, there is no zero at \\(j\\omega=0\\), so the pre-emphisis transfer function does not block DC."
  },
  {
    "objectID": "OLD Two amplifier RIAA Phono Preamp.html#phono-preamplifier-design-procedure",
    "href": "OLD Two amplifier RIAA Phono Preamp.html#phono-preamplifier-design-procedure",
    "title": "Two amplifier RIAA phono preamp",
    "section": "Phono Preamplifier Design Procedure",
    "text": "Phono Preamplifier Design Procedure\nThe following notebook cells walk through the design procedure given in the application note, starting on page 5.\nA design procedure is shown below with an illustrative example using 1% tolerance E96 components for close conformance to the ideal RIAA curve. Since 1% tolerance capacitors are often difficult to find except in 5% or 10% standard values, the design procedure calls for re-calculation of a few component values so that standard capacitor values can be used.\n\nChoose \\(R_o\\). \\(R_o\\) should be small for minimum noise contribution, but not so small that the feedback network excessively loads the amplifier. Example: Choose \\(R_o = 500\\)\nChoose 1 kHz gain, A1 of first amplifier. This will typically be around 20 dB to 30 dB. Example: Choose A1 = 26 dB = 20\nCalculate \\(R_11 = 8.058 \\times R_o \\times A_1\\)\n\n\nA1 = 20\nRo = 500\nR1 = 8.058 * Ro * A1\nprint('R1={:,.0f}'.format(R1))\n\n\nCalculate C1\n\n\\(C_1 = \\frac {3.18 \\times 10^{-3}}{R_1}\\)\n\nC1 = 3.18e-3/R1\nprint('C1={:.4e}'.format(C1))\n\nC1=3.9464e-08\n\n\nThe calculated value for capacitor C1 is not a standard value, so step 5 takes care of this.\n\nIf C1 is not a convenient value, choose the nearest convenient value and calculate a new R1 from:\n\n\\(R_1 = \\frac {3.18 \\times 10^{-3}}{3.9 \\times 10^{-8}}\\)\nChoose C1 to be 0.039\\(\\mu\\), which is a standard capacitor value.\n\nC1 = 0.039e-6\nR1 = 3.18e-3/C1\nprint('R1={:,.0f}'.format(R1))\n\nR1=81,538\n\n\nNow choose a standard resistor value close to the the calculated value, which is 80.6k.\nE96 resistor values are a set of perfered values for 1% resistors. When doing the calculations to determin the resistor values, the closest standandard value is chosen from the E96 series. The E series of preferred numbers derived for use in electronic components. It consists of the E3, E6, E12, E24, E48, E96 and E192 series, where the number after the ‘E’ designates the quantity of logarithmic value “steps” per decade.\n\nR1 = 80.6e3\n\nCalculate a new value for Ro.\n\\(R_o=\\frac {R_1}{8.058A_1}\\)\n\nRo = R1/(8.058*A1)\nprint('R1={:,.0f}'.format(Ro))\n\nR1=500\n\n\nChoose a standard value close to this value, which is 499.\n\nRo = 499\n\n\nCaluclate R2\n\\(R_2=\\frac {R_1}{9} - R_o\\)\n\n\nR2 = R1/9-Ro\nprint('R1={:,.0f}'.format(R2))\n\nR1=8,457\n\n\nChoose a standard value close to this value, which is 8.45k.\n\nR2 = 8.45e3\n\n\nChoose a convenient value for C3 in the range from 0.01 \\(\\mu\\)F to 0.05 \\(\\mu\\)F.\nExample: C3 = 0.033 \\(\\mu\\)F\n\n\nC3 = 0.033e-6\nprint('C3={:,.3f}\\u03BC'.format(C3*1e6))\n\nC3=0.033μ\n\n\n\nCalculate Rp\n\\(R_p=\\frac {75 \\mu s }{C_3}\\)\n\n\\(75 \\mu s\\) is one of the RIAA time constants.\n\nRp = 75e-6/C3\nprint('Rp={:,.0f}'.format(Rp))\n\nRp=2,273\n\n\n\nChoose a standard value for R3 that is slightly larger than Rp.\nExample: R3 = 2.37k, which is a standard resistor value.\n\n\nR3 = 2.37e3\n\n\nCalculate R6 from \\(1/R_6 = 1/R_P − 1/R_3\\)\n\n\nR6 = 1/(1/Rp-1/R3)\nprint('R6={:,.0f}'.format(R6))\n\nR6=55,374\n\n\n54.9k is the closest standard value.\n\nR6 = 54.9e3\n\n\nCalculate \\(C_4\\) for low-frequency rolloff below 1 Hz from design Equation 5.\n\n\\(C_4=\\frac{1}{2\\pi f_L(R_3+R_6)}\\)\nWhere \\(f_L\\) is the low frequency -3dB corner of the second stage.\nIn the application note, there is a comment on page 4:\n&gt; If the preamplifier is to follow the IEC recommendation (IEC Publication 98, Amendment #4), fL should equal 20.2 Hz.\nThe calculations in the app note use 1 Hz.\n\nf_L = 1.0 # Hz\nC4 = 1/(2*np.pi*f_L*(R3+R6))\nprint('C4={:,.3f}\\u03BC'.format(C4*1e6))\n\nC4=2.779μ\n\n\n2\\(\\mu\\)F is a standard value close to the calculated value.\nExample: C4 = 2 \\(\\mu\\)F.\n\nChoose gain of second amplifier.\nExample: The 1 kHz gain up to the input of the second amplifier is about 26 dB for this example. For an overall 1 kHz gain equal to about 36 dB we choose:\n\\(A_2 = 10 dB = 3.16\\)\nChoose value for R4.\nExample: R4 = 2k\n\n\nR4 = 2e3\n\n\nCalculate \\(R_5 = (A_2 − 1) R_4\\)\n\n\nA2 = 3.16\nR5 = (A2-1)*R4\nprint('R5={:,.0f}'.format(R5))\n\nR5=4,320\n\n\n4.3k is a standard value close to the calculated value.\n\nR5 = 4.3e3\n\n\nCalculate Co for low-frequency rolloff below 1 Hz from design Equation 7.\n\n\\(C_o=\\frac {1}{2\\pi f_o R_o}\\)\nwhere fo is the low-frequency −3 dB corner of the first amplifier. fo is chosen to be 1Hz for the calculations since this frequency is well below the audible frequency range.\n\nfo = 1 # 1 Hz\nCo = 1/(2*np.pi*fo*Ro)\nprint('Co={:,.3f}\\u03BC'.format(Co*1e6))\n\nCo=318.948μ\n\n\nThe value chosen in the app note for this component is 200\\(\\mu\\)F.\n\nCo = 200e-6\n\n\nprint('resonant frequency of Ro and Co: {:.2f}Hz'.format(1/(Co*Ro*2*np.pi)))\n\nresonant frequency of Ro and Co: 1.59Hz"
  },
  {
    "objectID": "OLD Two amplifier RIAA Phono Preamp.html#analysis-of-an-346-phono-preamplifier-circuit",
    "href": "OLD Two amplifier RIAA Phono Preamp.html#analysis-of-an-346-phono-preamplifier-circuit",
    "title": "Two amplifier RIAA phono preamp",
    "section": "Analysis of AN-346 phono preamplifier circuit",
    "text": "Analysis of AN-346 phono preamplifier circuit\nThe schematic of the preamp was entered into LTSpice and the netlist was generated. Starting with a schematic and then using LTSpice to generate the net list eliminates errors that would occure if these circuits were analyised by hand. For small circuits with a handful of components, symbolic solutions of the node equations can be of interest, but for larger circuits, not so much. This is because the number of symbols and equations is too large to offer in insight with out some simplicication.\nIn this section the modified nodal analysis method will be used to generate the circuit equations. The modified nodal analysis provides an algorithmic method for generating systems of independent equations for linear circuit analysis. Most problems that an electrical engineer encounters on the job are complex enough that they use computers to analyze the circuits. The Python code that generates the circuits equations is located here.\n\nModified nodal analysis\nThe preamp circuit has 15 branches, 9 nodes, 3 unknown currents, 14 passive components and 2 op amps. The net list generated by LTSpice and some edits were made to put the component values into scientific notation with units of Ohms, Farads and Henerys and the opamp statements were fixed. The edited netlist is:\nV1 1 0 5e-3m\nO1 3 1 6 \nO2 9 8 2 \nC1 3 5 0.039e-6\nCo 4 0 200e-6\nRo 3 4 499\nR3 6 7 2.37e3\nR1 3 5 80.6e3\nR2 5 6 8.45e3\nCp 1 0 100e-12\nRp 1 0 47e3\nC3 7 0 0.033e-6\nC4 8 7 2e-6\nR6 8 0 54.9e3\nR4 9 0 2e3\nR5 2 9 4.3e3\nThis netlist is read into the Symbolic Modified Nodal Analysis Jupyter notebook and the following circuit equations were generated.\n\\(I_{V1} + v_{1} \\left(Cp s + \\frac{1}{R_{20}}\\right) = 0\\)\n\\(v_{2} \\left(C_{1} s + \\frac{1}{Ro} + \\frac{1}{R_{1}}\\right) + v_{4} \\left(- C_{1} s - \\frac{1}{R_{1}}\\right) - \\frac{v_{3}}{Ro} = 0\\)\n\\(v_{3} \\left(Co s + \\frac{1}{Ro}\\right) - \\frac{v_{2}}{Ro} = 0\\)\n\\(v_{2} \\left(- C_{1} s - \\frac{1}{R_{1}}\\right) + v_{4} \\left(C_{1} s + \\frac{1}{R_{2}} + \\frac{1}{R_{1}}\\right) - \\frac{v_{5}}{R_{2}} = 0\\)\n\\(I_{O1} + v_{5} \\cdot \\left(\\frac{1}{R_{3}} + \\frac{1}{R_{2}}\\right) - \\frac{v_{6}}{R_{3}} - \\frac{v_{4}}{R_{2}} = 0\\)\n\\(- C_{4} s v_{7} + v_{6} \\left(C_{3} s + C_{4} s + \\frac{1}{R_{3}}\\right) - \\frac{v_{5}}{R_{3}} = 0\\)\n\\(- C_{4} s v_{6} + v_{7} \\left(C_{4} s + \\frac{1}{R_{6}}\\right) = 0\\)\n\\(v_{8} \\cdot \\left(\\frac{1}{R_{5}} + \\frac{1}{R_{4}}\\right) - \\frac{v_{9}}{R_{5}} = 0\\)\n\\(I_{O2} + v_{9} \\cdot \\left(\\frac{1}{R_{5}} + \\frac{1}{R_{22}}\\right) - \\frac{v_{8}}{R_{5}} = 0\\)\n\\(v_{1} = V_{1}\\)\n\\(- v_{1} + v_{2} = 0\\)\n\\(- v_{7} + v_{8} = 0\\)\nThe symbols and matrices generated by the modified nodal analysis code are copied here so that the circuit equations can be solved symbilically and later numerically. All the symboles that SymPy needs defined are delared. The A matrix describs the connectivity of the resistors, capacitors and G type (VCCS) circuit elements. The X matrix contains the unknown node voltages and unknown currents. The Z matrix contains the known voltages and currents sources, e.g. V1.\n\nRp, v6, Co, C4, v2, C3, s, I_V1, R6, Ro, R4, C1, R3, I_O2, R5, v3, I_O1, v4, v8, v7, V1, Cp, v1, R1, R2, v5, v9 = symbols(' Rp  v6  Co  C4  v2  C3  s  I_V1  R6  Ro  R4  C1  R3  I_O2  R5  v3  I_O1  v4  v8  v7  V1  Cp  v1  R1  R2  v5  v9 ')\nA = Matrix([[Cp*s + 1/Rp, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 1/R5, 0, 0, 0, 0, 0, 0, -1/R5, 0, 0, 1], [0, 0, C1*s + 1/Ro + 1/R1, -1/Ro, -C1*s - 1/R1, 0, 0, 0, 0, 0, 0, 0], [0, 0, -1/Ro, Co*s + 1/Ro, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, -C1*s - 1/R1, 0, C1*s + 1/R2 + 1/R1, -1/R2, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, -1/R2, 1/R3 + 1/R2, -1/R3, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, -1/R3, C3*s + C4*s + 1/R3, -C4*s, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, -C4*s, C4*s + 1/R6, 0, 0, 0, 0], [0, -1/R5, 0, 0, 0, 0, 0, 0, 1/R5 + 1/R4, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [-1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0]])\nX = Matrix( [v1, v2, v3, v4, v5, v6, v7, v8, v9, I_V1, I_O1, I_O2] )\nZ = Matrix( [0, 0, 0, 0, 0, 0, 0, 0, 0, V1, 0, 0] )\n\nThe equations are displayed in maxtrix form below.\n\npreamp_equ_sym = Eq(A*X,Z)\npreamp_equ_sym\n\n\\(\\displaystyle \\left[\\begin{matrix}I_{V1} + v_{1} \\left(Cp s + \\frac{1}{Rp}\\right)\\\\I_{O2} + \\frac{v_{2}}{R_{5}} - \\frac{v_{9}}{R_{5}}\\\\v_{3} \\left(C_{1} s + \\frac{1}{Ro} + \\frac{1}{R_{1}}\\right) + v_{5} \\left(- C_{1} s - \\frac{1}{R_{1}}\\right) - \\frac{v_{4}}{Ro}\\\\v_{4} \\left(Co s + \\frac{1}{Ro}\\right) - \\frac{v_{3}}{Ro}\\\\v_{3} \\left(- C_{1} s - \\frac{1}{R_{1}}\\right) + v_{5} \\left(C_{1} s + \\frac{1}{R_{2}} + \\frac{1}{R_{1}}\\right) - \\frac{v_{6}}{R_{2}}\\\\I_{O1} + v_{6} \\cdot \\left(\\frac{1}{R_{3}} + \\frac{1}{R_{2}}\\right) - \\frac{v_{7}}{R_{3}} - \\frac{v_{5}}{R_{2}}\\\\- C_{4} s v_{8} + v_{7} \\left(C_{3} s + C_{4} s + \\frac{1}{R_{3}}\\right) - \\frac{v_{6}}{R_{3}}\\\\- C_{4} s v_{7} + v_{8} \\left(C_{4} s + \\frac{1}{R_{6}}\\right)\\\\v_{9} \\cdot \\left(\\frac{1}{R_{5}} + \\frac{1}{R_{4}}\\right) - \\frac{v_{2}}{R_{5}}\\\\v_{1}\\\\- v_{1} + v_{3}\\\\- v_{8} + v_{9}\\end{matrix}\\right] = \\left[\\begin{matrix}0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\V_{1}\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\nThese equations can be solved in thier symbolic form using the solve function from SymPy. The solution time takes about 3 seconds on my i3 laptop.\n\nU_preamp_sym = solve(preamp_equ_sym,X)\n\nThe transfer function for the preamp is the equation for the output node, 2, divided by the equation for the input node 1.\n\nH_preamp_sym = U_preamp_sym[v2]/U_preamp_sym[v1]\n\nThe symbolic solution obtained by SymPy, while not being very interesting since they are unweildly, they illustrate the power of SymPy to easily obtain symbolic solutions that would be very difficult to obtain by hand.\n\nH_preamp_sym\n\n\\(\\displaystyle \\frac{C_{1} C_{4} Co R_{1} R_{2} R_{4} R_{6} V_{1} s^{3} + C_{1} C_{4} Co R_{1} R_{2} R_{5} R_{6} V_{1} s^{3} + C_{1} C_{4} Co R_{1} R_{4} R_{6} Ro V_{1} s^{3} + C_{1} C_{4} Co R_{1} R_{5} R_{6} Ro V_{1} s^{3} + C_{1} C_{4} R_{1} R_{4} R_{6} V_{1} s^{2} + C_{1} C_{4} R_{1} R_{5} R_{6} V_{1} s^{2} + C_{4} Co R_{1} R_{4} R_{6} V_{1} s^{2} + C_{4} Co R_{1} R_{5} R_{6} V_{1} s^{2} + C_{4} Co R_{2} R_{4} R_{6} V_{1} s^{2} + C_{4} Co R_{2} R_{5} R_{6} V_{1} s^{2} + C_{4} Co R_{4} R_{6} Ro V_{1} s^{2} + C_{4} Co R_{5} R_{6} Ro V_{1} s^{2} + C_{4} R_{4} R_{6} V_{1} s + C_{4} R_{5} R_{6} V_{1} s}{V_{1} \\left(C_{1} C_{3} C_{4} Co R_{1} R_{3} R_{4} R_{6} Ro s^{4} + C_{1} C_{3} C_{4} R_{1} R_{3} R_{4} R_{6} s^{3} + C_{1} C_{3} Co R_{1} R_{3} R_{4} Ro s^{3} + C_{1} C_{3} R_{1} R_{3} R_{4} s^{2} + C_{1} C_{4} Co R_{1} R_{3} R_{4} Ro s^{3} + C_{1} C_{4} Co R_{1} R_{4} R_{6} Ro s^{3} + C_{1} C_{4} R_{1} R_{3} R_{4} s^{2} + C_{1} C_{4} R_{1} R_{4} R_{6} s^{2} + C_{1} Co R_{1} R_{4} Ro s^{2} + C_{1} R_{1} R_{4} s + C_{3} C_{4} Co R_{3} R_{4} R_{6} Ro s^{3} + C_{3} C_{4} R_{3} R_{4} R_{6} s^{2} + C_{3} Co R_{3} R_{4} Ro s^{2} + C_{3} R_{3} R_{4} s + C_{4} Co R_{3} R_{4} Ro s^{2} + C_{4} Co R_{4} R_{6} Ro s^{2} + C_{4} R_{3} R_{4} s + C_{4} R_{4} R_{6} s + Co R_{4} Ro s + R_{4}\\right)}\\)\n\n\nThe SymPy function, cancel(), can be used to put the preamp transfunction in to standard canonical form, where the polynomials are expanded with no common factors and the leading coefficients do not have denominators (i.e., are integers).\n\ncancel(H_preamp_sym,s)\n\n\\(\\displaystyle \\frac{s^{3} \\left(C_{1} C_{4} Co R_{1} R_{2} R_{4} R_{6} + C_{1} C_{4} Co R_{1} R_{2} R_{5} R_{6} + C_{1} C_{4} Co R_{1} R_{4} R_{6} Ro + C_{1} C_{4} Co R_{1} R_{5} R_{6} Ro\\right) + s^{2} \\left(C_{1} C_{4} R_{1} R_{4} R_{6} + C_{1} C_{4} R_{1} R_{5} R_{6} + C_{4} Co R_{1} R_{4} R_{6} + C_{4} Co R_{1} R_{5} R_{6} + C_{4} Co R_{2} R_{4} R_{6} + C_{4} Co R_{2} R_{5} R_{6} + C_{4} Co R_{4} R_{6} Ro + C_{4} Co R_{5} R_{6} Ro\\right) + s \\left(C_{4} R_{4} R_{6} + C_{4} R_{5} R_{6}\\right)}{C_{1} C_{3} C_{4} Co R_{1} R_{3} R_{4} R_{6} Ro s^{4} + R_{4} + s^{3} \\left(C_{1} C_{3} C_{4} R_{1} R_{3} R_{4} R_{6} + C_{1} C_{3} Co R_{1} R_{3} R_{4} Ro + C_{1} C_{4} Co R_{1} R_{3} R_{4} Ro + C_{1} C_{4} Co R_{1} R_{4} R_{6} Ro + C_{3} C_{4} Co R_{3} R_{4} R_{6} Ro\\right) + s^{2} \\left(C_{1} C_{3} R_{1} R_{3} R_{4} + C_{1} C_{4} R_{1} R_{3} R_{4} + C_{1} C_{4} R_{1} R_{4} R_{6} + C_{1} Co R_{1} R_{4} Ro + C_{3} C_{4} R_{3} R_{4} R_{6} + C_{3} Co R_{3} R_{4} Ro + C_{4} Co R_{3} R_{4} Ro + C_{4} Co R_{4} R_{6} Ro\\right) + s \\left(C_{1} R_{1} R_{4} + C_{3} R_{3} R_{4} + C_{4} R_{3} R_{4} + C_{4} R_{4} R_{6} + Co R_{4} Ro\\right)}\\)\n\n\nThe Sympy function, factor(), can be used to factor the polynominals it into irreducible factors over the rational numbers.\n\nH_preamp_sym.factor()\n\n\\(\\displaystyle \\frac{C_{4} R_{6} s \\left(R_{4} + R_{5}\\right) \\left(C_{1} Co R_{1} R_{2} s^{2} + C_{1} Co R_{1} Ro s^{2} + C_{1} R_{1} s + Co R_{1} s + Co R_{2} s + Co Ro s + 1\\right)}{R_{4} \\left(C_{1} R_{1} s + 1\\right) \\left(Co Ro s + 1\\right) \\left(C_{3} C_{4} R_{3} R_{6} s^{2} + C_{3} R_{3} s + C_{4} R_{3} s + C_{4} R_{6} s + 1\\right)}\\)\n\n\nThe symbolic solutions obtained above will be used later when the sinsitivity analysis of the preamp is performed. Otherwise the roots in symbolic form don’t seem to be particulary insightful, but are easily obtained by SymPy.\n\nNumerical solution\nThe element values are put into the Python dictionary format so that numerical values can be substituted into the equations.\n\nnominal_component_value = {V1:5.0000e-03, C1:3.9000e-08, Co:2.0000e-04, Ro:4.9900e+02, R3:2.3700e+03, R1:8.0600e+04, \n    R2:8.4500e+03, Cp:1.0000e-10, Rp:4.7000e+04, C3:3.3000e-08, C4:2.0000e-06, R6:5.4900e+04, R4:2.0000e+03, R5:4.3000e+03}\n\n# put the element values into the equations\npreamp_equ = preamp_equ_sym.subs(nominal_component_value)\n\nNow we can diplay the network equations with values for the components instear of symbols.\n\npreamp_equ\n\n\\(\\displaystyle \\left[\\begin{matrix}I_{V1} + v_{1} \\cdot \\left(1.0 \\cdot 10^{-10} s + 2.12765957446809 \\cdot 10^{-5}\\right)\\\\I_{O2} + 0.000232558139534884 v_{2} - 0.000232558139534884 v_{9}\\\\v_{3} \\cdot \\left(3.9 \\cdot 10^{-8} s + 0.00201641496392288\\right) - 0.00200400801603206 v_{4} + v_{5} \\left(- 3.9 \\cdot 10^{-8} s - 1.24069478908189 \\cdot 10^{-5}\\right)\\\\- 0.00200400801603206 v_{3} + v_{4} \\cdot \\left(0.0002 s + 0.00200400801603206\\right)\\\\v_{3} \\left(- 3.9 \\cdot 10^{-8} s - 1.24069478908189 \\cdot 10^{-5}\\right) + v_{5} \\cdot \\left(3.9 \\cdot 10^{-8} s + 0.000130750143157091\\right) - 0.000118343195266272 v_{6}\\\\I_{O1} - 0.000118343195266272 v_{5} + 0.000540284123536314 v_{6} - 0.000421940928270042 v_{7}\\\\- 2.0 \\cdot 10^{-6} s v_{8} - 0.000421940928270042 v_{6} + v_{7} \\cdot \\left(2.033 \\cdot 10^{-6} s + 0.000421940928270042\\right)\\\\- 2.0 \\cdot 10^{-6} s v_{7} + v_{8} \\cdot \\left(2.0 \\cdot 10^{-6} s + 1.82149362477231 \\cdot 10^{-5}\\right)\\\\- 0.000232558139534884 v_{2} + 0.000732558139534884 v_{9}\\\\v_{1}\\\\- v_{1} + v_{3}\\\\- v_{8} + v_{9}\\end{matrix}\\right] = \\left[\\begin{matrix}0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0.005\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\nUsing the SymPy solve function we can solve the system of equations.\n\nU_preamp = solve(preamp_equ,X)\n\nThe values of the exponents are very large in the solution. The numerator and denominator for v2 could be normalized. Another option for avoiding large exponents is to 1st normalize the component values by frequency scaling. I suppose that large exponents don’t become a problem as long as they remain under two digits.\nAlmost all platforms map Python floats to the IEEE754 double precision - 64 total bits. The float information using the sys package can be as shown as follows:\n\nsys.float_info\n\nsys.float_info(max=1.7976931348623157e+308, max_exp=1024, max_10_exp=308, min=2.2250738585072014e-308, min_exp=-1021, min_10_exp=-307, dig=15, mant_dig=53, epsilon=2.220446049250313e-16, radix=2, rounds=1)\n\n\n\nprint(sys.float_info)\n\nsys.float_info(max=1.7976931348623157e+308, max_exp=1024, max_10_exp=308, min=2.2250738585072014e-308, min_exp=-1021, min_10_exp=-307, dig=15, mant_dig=53, epsilon=2.220446049250313e-16, radix=2, rounds=1)\n\n\nThe maximum exponent that Python can use is 308.\nLetting SciPy do the math and not worrying about the size of the exponents. The transfer function can be obtained by divideing the equation for node 2 by the equation for node 1. The system transfer function, is \\(H(s) = \\frac {v2}{V1}\\)\n\nH_preamp = U_preamp[v2]/U_preamp[v1]\nH_preamp\n\n\\(\\displaystyle \\frac{200.0 \\cdot \\left(2.76499422921242 \\cdot 10^{69} s^{3} + 8.80353368479522 \\cdot 10^{72} s^{2} + 4.91462150480132 \\cdot 10^{71} s\\right)}{7.65600122987477 \\cdot 10^{65} s^{4} + 1.0469819689888 \\cdot 10^{70} s^{3} + 3.44479259637884 \\cdot 10^{72} s^{2} + 6.18286042227129 \\cdot 10^{73} s + 2.84188944100537 \\cdot 10^{74}}\\)\n\n\nfactor() takes a polynomial and factors it into irreducible factors over the rational numbers. For example:\n\nH_preamp.factor()\n\n\\(\\displaystyle \\frac{6.19554973375798 s \\left(0.000314077770155853 s^{2} + 1.0 s + 0.0558255546098434\\right)}{2.69398278462456 \\cdot 10^{-9} s^{4} + 3.68410520790143 \\cdot 10^{-5} s^{3} + 0.012121487017314 s^{2} + 0.21756161 s + 1.0}\\)\n\n\n\n\nConvert transfer function to SciPy system\nIn this section we convert the SymPy equations into Numpy format.\nExtract the numerator and denominator polynomials so that the system can be defined in SciPy.\n\nH_preamp_num, H_preamp_denom = fraction(H_preamp) #returns numerator and denominator\n\nThe SciPy function, TransferFunction, represents the system as the continuous-time transfer function and takes as inputs the coeeficients of the numerator and denominator polynominals.\n\n# convert symbolic to numpy polynomial\na2 = np.array(Poly(H_preamp_num, s).all_coeffs(), dtype=float)\nb2 = np.array(Poly(H_preamp_denom, s).all_coeffs(), dtype=float)\npreamp_sys = signal.TransferFunction(a2,b2)\n\nThe poles and zeros of the transfer function can easly be obtained with the following code:\n\npreamp_sys_zeros = np.roots(preamp_sys.num)\npreamp_sys_poles = np.roots(preamp_sys.den)\n\n\n\n\nPole zero plot\nThe poles and zeros of the preamp transfer function are plotted.\n\nplt.plot(np.real(preamp_sys_zeros), np.imag(preamp_sys_zeros), 'ob', markerfacecolor='none')\nplt.plot(np.real(preamp_sys_poles), np.imag(preamp_sys_poles), 'xr')\nplt.legend(['Zeros', 'Poles'], loc=2)\nplt.title('Pole / Zero Plot')\nplt.xlabel('real part, \\u03B1')\nplt.ylabel('imaginary part, j\\u03C9')\nplt.grid()\nplt.show()\n\n\n\n\nPoles and zeros of the transfer function plotted on the complex plane. The units are in radian frequency.\nPrinting these values in Hz.\n\nprint('number of zeros: {:d}'.format(len(preamp_sys_zeros)))\nfor i in preamp_sys_zeros:\n    print('{:,.2f} Hz'.format(i/(2*np.pi)))\n\nnumber of zeros: 3\n-506.73 Hz\n-0.01 Hz\n0.00 Hz\n\n\n\nprint('number of poles: {:d}'.format(len(preamp_sys_poles)))\nfor i in preamp_sys_poles:\n    print('{:,.2f} Hz'.format(i/(2*np.pi)))\n\nnumber of poles: 4\n-2,122.88 Hz\n-50.63 Hz\n-1.59 Hz\n-1.39 Hz\n\n\nWe can see that the RIAA time constants, displayed in terms of frequency are present, althought the values diffeer by a few Hz. There are two zeros and two poles at nearly zero hz and these cancel each other.\n\n\nStability\nBy inspecting the plot above, we can tell the preamplifier is stable since the phase shift at 0 dB of gain is less than 180 degrees. Additionally, all the poles of the transfer function are in the left hand plane.\nNow we can find the preamp gain at 1 kHz, so that the bode plots can be normailized.\n\npreamp_gain_1kHz = get_gain(1000, preamp_sys)\nprint('preamp gain at 1kHz: {:f} dB'.format(preamp_gain_1kHz))\n\npreamp gain at 1kHz: 34.783614 dB\n\n\n\n\nBode plot\nUse the SciPy function bode to plot the magnitude and phase of the filter. In electrical engineering, a Bode plot is a graph of the frequency response of a system. It is usually a combination of the magnitude (usually in decibels) of the frequency response and the phase shift. As originally conceived by Hendrik Wade Bode in the 1930s, the plot is an asymptotic approximation of the frequency response, using straight line segments. Bode plots are used to assess the stability of systems by finding the gain and phase margins.\n\nextended_x_axis_range = np.logspace(-2, 8, 5000, endpoint=True)*2*np.pi\n\nw_preamp, mag_preamp, phase_preamp = preamp_sys.bode(w=extended_x_axis_range)\n\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nplt.semilogx(w_preamp/(2*np.pi), mag_preamp,'-k')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\nplt.grid()\nplt.axvspan(20, 20e3, color='y', alpha=0.3)\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'b' #'tab:blue'\n\nplt.semilogx(w_preamp/(2*np.pi), phase_preamp,':',color=color,label='phase')  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.plot(np.NaN, np.NaN, '-', color='k', label='magnitude')\n\nplt.legend(loc=0)\nplt.title('preamplifier Bode plot')\nplt.show()\n\n\n\n\nThe Bode plot for the preamplifier is plotted from 0.01 Hz to 100 MHz. The preamplifier circuit blocks DC because C4 in in series with the audio path.\n\n\nImpulse and step response\nUse the SciPy functions impulse2 and step2 to plot the impulse and step response of the system.\nThe impulse and step response of the filter are plotted below. Any linear, time-invariant is completely characterized by its impulse response. The transfer function is the Laplace transform of the impulse response. The impulse response defines the response of a linear time-invariant system for all frequencies.\nIn electronic engineering and control theory, step response is the time behavior of the outputs of a general system when its inputs change from zero to one in a very short time.\n\nplt.subplots(1,2,figsize=(15, 5))\n\n# using subplot function and creating\n# plot one\nplt.subplot(1, 2, 1)\n\n# impulse response\nt, y = signal.impulse2(preamp_sys,N=500)\nplt.plot(t/1e-3, y)\nplt.title('AN-346 phono preamplifier Impulse response')\nplt.ylabel('volts')\nplt.xlabel('time, msec')\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n\nt, y = signal.step2(preamp_sys,N=500)\nplt.plot(t/1e-3, y)\nplt.title('AN-346 phono preamplifier Step response')\nplt.ylabel('volts')\nplt.xlabel('time, msec')\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\n\nGroup delay\nThe following python code calculates and plots group delay. Frequency components of a signal are delayed when passed through a circuit and the signal delay will be different for the various frequencies unless the circuit has the property of being linear phase. The delay variation means that signals consisting of multiple frequency components will suffer distortion because these components are not delayed by the same amount of time at the output of the device.\nGroup delay: \\(\\tau _{g}(\\omega )=-\\frac {d\\phi (\\omega )}{d\\omega }\\)\n\nw_preamp, mag_preamp, phase_preamp = preamp_sys.bode(w=x_axis_range)\n\nplt.title('AN-346 phono preamplifier group delay')\nplt.semilogx(w_preamp/(2*np.pi), -np.gradient(phase_preamp*np.pi/180)/np.gradient(w_preamp),'-',label='group delay')\n\n#plt.semilogx(w_c1/(2*np.pi), -np.gradient(phase_c1)/w_c1/1e-3,'-',label='phase delay')\nplt.axvspan(20, 20e3, color='y', alpha=0.3)\nplt.ylabel('Group delay, sec')\nplt.xlabel('Frequency, Hz')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nThe plot above shows that for frequencies below 100 Hz, the group delay is as much as 4 ms in the audio band. According to paper, Audibility of Group-Delay Equalization, the threshold is 2 ms. The abstract for the paper states:\n\nThe audibility thresholds for group-delay variation from several previous related studies are shown in Fig. 1. If not otherwise stated, these studies have been conducted using headphones. Green applied Huffman sequences, or truncated impulse responses of second-order allpass filters, to study the audibility of phase distortion. He found a threshold value for the peak group delay of about 2 ms for center frequencies of 625 Hz, 1875 Hz, and 4062 Hz.\n\nThe preamp group delay in the low end of the audio band is of concern and some re-design should be implemented if this preamp was to be implemented. The group delay plotted above agrees with the group delay results obtained from LTSpice simulation of the preamp circuit.\n\n\nComparing results to LTSpice\nThe LM833 TINA-TI Spice Model was entered into LTSpice, but it has some errors when run with LTSpice. So rather than debug the errors, an Analog Devices’s LT1115, was substiuded to obtain simulation results with an opamp model in order to comapre with the Python results, which uses an ideal opamp.\nThe table blow shows that the LM833 and LT1115 have simular performance characteristics.\n\n\n\n\n\n\n\n\n\n\n\nOp Amp\nDistortion %, THD + N at 1 kHz\nNoise, at 1 kHz (nV√Hz)\nSlew Rate, (V/µs)\nGBW (MHz)\nPower Bandwidth\n\n\n\n\nLM833N\n0.002\n4.5\n7\n15\n120 kHz @ 27 Vpp, RL = 2 kΩ, THD ≤ 1%\n\n\nLT1115\n0.002\n0.9\n10\n40\n180 kHz @ 30 VP-P, RL=2k\n\n\n\n\nos.chdir('/home/jeff32/Documents/Solving Electrical Engineering Problems with Python Blog/MNA Problem Circuits/Two amplifier RIAA Phone Preamp/') # change directory to csv file location\n\nfn = 'Two amplifier RIAA Phone Preamp.csv' # data from LTSpice\nLTSpice_data = np.genfromtxt(fn, delimiter=',')\n\n# change the working director back to the Jupyter folder\nos.chdir('/home/jeff32/Documents/JupyterLab/Node Analysis/')  \n\n\nfrequency = np.zeros(len(LTSpice_data))\nvoltage = np.zeros(len(LTSpice_data)).astype(complex)\n\nfor i in range(len(LTSpice_data)):\n    frequency[i] = LTSpice_data[i][0]\n    voltage[i] = LTSpice_data[i][1] + LTSpice_data[i][2]*1j\n\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nplt.semilogx(frequency, 20*np.log10(np.abs(voltage))+11.257,'-k')    # Bode magnitude plot, adding 11.257 dBV offset to normalized LTSpice data at 1KHz\nplt.semilogx(w_preamp/(2*np.pi), mag_preamp-preamp_gain_1kHz,'-b')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\nax1.set_ylim((-40,20))\nplt.grid()\nplt.axvspan(20, 20e3, color='y', alpha=0.3)\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'blue'\n\nplt.semilogx(frequency, np.angle(voltage)*180/np.pi,':',color=color,label='LT1115 phase')  # Bode phase plot\nplt.semilogx(w_preamp/(2*np.pi), phase_preamp,':',color='black',label='MNA phase')  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-5,25))\n\nax2.plot(np.NaN, np.NaN, '-', color='b', label='LT1115 magnitude')\nax2.plot(np.NaN, np.NaN, '-.', color='k', label='MNA magnitude')\n\nplt.legend(loc=0)\n\nplt.title('Bode plot')\nplt.show()\n\n\n\n\nAs is evident in the plot above, the results from LTSpice and Python agree.\n\n\nPreamplifier deviation from RIAA response\nThe plot below shows the deviation of the preamplifier from the RIAA response. The TI app note says conformance to the RIAA curve is within 0.1 dB from 20 Hz to 20 kHz.\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nplt.semilogx(w_preamp/(2*np.pi), (mag_RIAA-RIAA_gain_1kHz) + mag_preamp-preamp_gain_1kHz,'-k')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\nax1.set_ylim((-0.2,0.1))\nplt.grid()\nplt.axvspan(20, 20e3, color='y', alpha=0.3)\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:blue'\n\nplt.semilogx(w_preamp/(2*np.pi), phase_RIAA+phase_preamp,':',color=color,label='phase of S2')  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n\nax2.plot(np.NaN, np.NaN, '-', color='k', label='magnitude of S1')\nplt.legend(loc=0)\n\nplt.title('Bode plot')\nplt.show()\n\n\n\n\nThe calculations below will find the minimum and maximunation deviation of the amplitude response from the RIAA curve.\n\nidx_low = np.where(w_preamp/(2*np.pi) &gt; 20)[0][0]\nidx_high = np.where(w_preamp/(2*np.pi) &gt; 20e3)[0][0]\nprint('preamp deviation from RIAA curve: {:.3f} to {:.3f} dB'.format(((mag_RIAA-RIAA_gain_1kHz) + mag_preamp-preamp_gain_1kHz)[idx_low:idx_high].min(),((mag_RIAA-RIAA_gain_1kHz) + mag_preamp-preamp_gain_1kHz)[idx_low:idx_high].max()))\n\npreamp deviation from RIAA curve: -0.045 to 0.066 dB\n\n\nThe calculations above show that within the audio range the deviation of the preamp amplitude response from the RIAA curve varies from -0.044 to 0.066 dB when the nominal component values are used.\n\n\nSensitivity analysis\nAll circuits have characteristics that dependent on the values of the component. The sensitivity of a circuit’s performance is a measure of how much a particular circuit characteristic changes as a particular component value varies. In this analysis i’ll look at the changes of each pole or zero relative to the compenents value.\nThe root sensitivity function \\(S_x^y\\) gives the change occuring in filter characteristic per \\(\\delta y/ \\delta x\\).\n\\(S_x^y\\) is read as the sensitivity of the characteristic (i.e. y = \\(\\omega_n,\\) or Q or some other characteristic) with respect to the element x.\n\\(S_x^y = \\frac {x}{y} \\frac{\\delta y}{\\delta x}\\)\nWhere x is the filter component that is varied and y is the filter characteristic (\\(\\omega_n,\\) or Q etc.) that we wish to evaluate as x is varied.\nThe preamp transfer function is symbolic form is, H_preamp_sym, and we can get the numerator and denominator with the SymPy fraction function.\n\nH_sym_num, H_sym_denom = fraction(H_preamp_sym)\n\nThe Sympy solve function is used to find the root of the numerator and denimator polynominals.\n\nH_sym_zeros = solve(H_sym_num,s)\nH_sym_poles = solve(H_sym_denom,s)\n\n\nZeros\nHow many roots are there for the numerator polynominial?\n\nprint('there are {:d} zeros'.format(len(H_sym_zeros)))\n\nthere are 3 zeros\n\n\n\n\nZ0\nThe first zero is at DC.\n\nH_sym_Z0 = H_sym_zeros[0]\nH_sym_Z0\n\n\\(\\displaystyle 0\\)\n\n\n\n\nZ1\nThe second zero is given symbolically by the expression:\n\nH_sym_Z1 = H_sym_zeros[1]\nH_sym_Z1\n\n\\(\\displaystyle - \\frac{C_{1} R_{1} + Co R_{1} + Co R_{2} + Co Ro}{2 C_{1} Co R_{1} \\left(R_{2} + Ro\\right)} - \\frac{\\sqrt{C_{1}^{2} R_{1}^{2} + 2 C_{1} Co R_{1}^{2} - 2 C_{1} Co R_{1} R_{2} - 2 C_{1} Co R_{1} Ro + Co^{2} R_{1}^{2} + 2 Co^{2} R_{1} R_{2} + 2 Co^{2} R_{1} Ro + Co^{2} R_{2}^{2} + 2 Co^{2} R_{2} Ro + Co^{2} Ro^{2}}}{2 C_{1} Co R_{1} \\left(R_{2} + Ro\\right)}\\)\n\n\nWhat are the compenets that determine Z1?\n\nprint('the compenets that determine Z1 are: {:s} '.format(str(H_sym_Z1.free_symbols)))\n\nthe compenets that determine Z1 are: {R2, Ro, R1, C1, Co} \n\n\nWhat is the locations of Z1?\n\nprint('location of zero: {:.2f} Hz'.format(N(H_sym_Z1.subs(nominal_component_value))/(2*np.pi)))\n\nlocation of zero: -506.73 Hz\n\n\nZ1 is the zero at 500 Hz and is one of the RIAA time constants.\nWe can fine the sensitivity of Z1 to C1 with the following operation.\n\nS_C1_H_sym_Z1 = (C1/H_sym_Z1)*(H_sym_Z1.diff(C1))\nS_C1_H_sym_Z1\n\n\\(\\displaystyle \\frac{C_{1} \\left(- \\frac{1}{2 C_{1} Co \\left(R_{2} + Ro\\right)} - \\frac{C_{1} R_{1}^{2} + Co R_{1}^{2} - Co R_{1} R_{2} - Co R_{1} Ro}{2 C_{1} Co R_{1} \\left(R_{2} + Ro\\right) \\sqrt{C_{1}^{2} R_{1}^{2} + 2 C_{1} Co R_{1}^{2} - 2 C_{1} Co R_{1} R_{2} - 2 C_{1} Co R_{1} Ro + Co^{2} R_{1}^{2} + 2 Co^{2} R_{1} R_{2} + 2 Co^{2} R_{1} Ro + Co^{2} R_{2}^{2} + 2 Co^{2} R_{2} Ro + Co^{2} Ro^{2}}} + \\frac{C_{1} R_{1} + Co R_{1} + Co R_{2} + Co Ro}{2 C_{1}^{2} Co R_{1} \\left(R_{2} + Ro\\right)} + \\frac{\\sqrt{C_{1}^{2} R_{1}^{2} + 2 C_{1} Co R_{1}^{2} - 2 C_{1} Co R_{1} R_{2} - 2 C_{1} Co R_{1} Ro + Co^{2} R_{1}^{2} + 2 Co^{2} R_{1} R_{2} + 2 Co^{2} R_{1} Ro + Co^{2} R_{2}^{2} + 2 Co^{2} R_{2} Ro + Co^{2} Ro^{2}}}{2 C_{1}^{2} Co R_{1} \\left(R_{2} + Ro\\right)}\\right)}{- \\frac{C_{1} R_{1} + Co R_{1} + Co R_{2} + Co Ro}{2 C_{1} Co R_{1} \\left(R_{2} + Ro\\right)} - \\frac{\\sqrt{C_{1}^{2} R_{1}^{2} + 2 C_{1} Co R_{1}^{2} - 2 C_{1} Co R_{1} R_{2} - 2 C_{1} Co R_{1} Ro + Co^{2} R_{1}^{2} + 2 Co^{2} R_{1} R_{2} + 2 Co^{2} R_{1} Ro + Co^{2} R_{2}^{2} + 2 Co^{2} R_{2} Ro + Co^{2} Ro^{2}}}{2 C_{1} Co R_{1} \\left(R_{2} + Ro\\right)}}\\)\n\n\nEvaluating this numerically with the component values, we get get the sensitivity of Z1 to C1.\n\nprint('the sensitivity of Z1 to C1 is: {:.2f}'.format(N(S_C1_H_sym_Z1.subs(nominal_component_value))))\n\nthe sensitivity of Z1 to C1 is: -1.00\n\n\nDoing the math with SymPy, we can get the sensitivity of Z1 to the other components.\n\nS_R1_H_sym_Z1 = (R1/H_sym_Z1)*(H_sym_Z1.diff(R1))\nprint('the sensitivity of Z1 to R1 is: {:.2f}'.format(N(S_R1_H_sym_Z1.subs(nominal_component_value))))\n\nthe sensitivity of Z1 to R1 is: -0.10\n\n\n\nS_R2_H_sym_Z1 = (R2/H_sym_Z1)*(H_sym_Z1.diff(R2))\nprint('the sensitivity of Z1 to R2 is: {:.2f}'.format(N(S_R2_H_sym_Z1.subs(nominal_component_value))))\n\nthe sensitivity of Z1 to R2 is: -0.85\n\n\n\nS_Co_H_sym_Z1 = (Co/H_sym_Z1)*(H_sym_Z1.diff(Co))\nprint('the sensitivity of Z1 to Co is: {:.4f}'.format(N(S_Co_H_sym_Z1.subs(nominal_component_value))))\n\nthe sensitivity of Z1 to Co is: -0.0002\n\n\n\nS_Ro_H_sym_Z1 = (Ro/H_sym_Z1)*(H_sym_Z1.diff(Ro))\nprint('the sensitivity of Z1 to Ro is: {:.2f}'.format(N(S_Ro_H_sym_Z1.subs(nominal_component_value))))\n\nthe sensitivity of Z1 to Ro is: -0.05\n\n\nLater, we we are doing the worst case analsys, we can ignore Co. \n\n\nZ2\nThe third zero of the transfer function is Z2.\n\nH_sym_Z2 = H_sym_zeros[2]\nH_sym_Z2\n\n\\(\\displaystyle - \\frac{C_{1} R_{1} + Co R_{1} + Co R_{2} + Co Ro}{2 C_{1} Co R_{1} \\left(R_{2} + Ro\\right)} + \\frac{\\sqrt{C_{1}^{2} R_{1}^{2} + 2 C_{1} Co R_{1}^{2} - 2 C_{1} Co R_{1} R_{2} - 2 C_{1} Co R_{1} Ro + Co^{2} R_{1}^{2} + 2 Co^{2} R_{1} R_{2} + 2 Co^{2} R_{1} Ro + Co^{2} R_{2}^{2} + 2 Co^{2} R_{2} Ro + Co^{2} Ro^{2}}}{2 C_{1} Co R_{1} \\left(R_{2} + Ro\\right)}\\)\n\n\n\nprint('the compenets that determine Z2 are: {:s} '.format(str(H_sym_Z2.free_symbols)))\n\nthe compenets that determine Z2 are: {R2, Ro, R1, C1, Co} \n\n\n\nprint('Z2: {:.3e} Hz'.format(N(H_sym_Z2.subs(nominal_component_value))/(2*np.pi)))\n\nZ2: -8.885e-3 Hz\n\n\nThe zero Z2, evaluates to a system zero at DC and is not one the of RIAA time constants.\n\nS_C1_H_sym_Z2 = (C1/H_sym_Z2)*(H_sym_Z2.diff(C1))\nprint('the sensitivity of Z2 to C1 is: {:.3e}'.format(N(S_C1_H_sym_Z2.subs(nominal_component_value))))\n\nthe sensitivity of Z2 to C1 is: -1.580e-4\n\n\n\nS_Co_H_sym_Z2 = (Co/H_sym_Z2)*(H_sym_Z2.diff(Co))\nprint('the sensitivity of Z2 to Co is: {:.2f}'.format(N(S_Co_H_sym_Z2.subs(nominal_component_value))))\n\nthe sensitivity of Z2 to Co is: -1.00\n\n\n\nS_R1_H_sym_Z2 = (R1/H_sym_Z2)*(H_sym_Z2.diff(R1))\nprint('the sensitivity of Z2 to R1 is: {:.2f}'.format(N(S_R1_H_sym_Z2.subs(nominal_component_value))))\n\nthe sensitivity of Z2 to R1 is: -0.90\n\n\n\nS_R2_H_sym_Z2 = (R2/H_sym_Z2)*(H_sym_Z2.diff(R2))\nprint('the sensitivity of Z2 to R2 is: {:.2f}'.format(N(S_R2_H_sym_Z2.subs(nominal_component_value))))\n\nthe sensitivity of Z2 to R2 is: -0.09\n\n\n\nS_Ro_H_sym_Z2 = (Ro/H_sym_Z2)*(H_sym_Z2.diff(Ro))\nprint('the sensitivity of Z2 to Ro is: {:.3f}'.format(N(S_Ro_H_sym_Z2.subs(nominal_component_value))))\n\nthe sensitivity of Z2 to Ro is: -0.006\n\n\n\n\nPoles\nHow many poles are there in the preamp transfer function?\n\nprint('there are {:d} poles in the transfer function'.format(len(H_sym_poles)))\n\nthere are 4 poles in the transfer function\n\n\n\n\nP0\nThe first pole is:\n\nH_sym_P0 = H_sym_poles[0]\nH_sym_P0\n\n\\(\\displaystyle - \\frac{1}{C_{1} R_{1}}\\)\n\n\nThe pole P0 evaluates to one of the RIAA time constants.\n\nprint('P0: {:.2f}Hz'.format(N(H_sym_P0.subs(nominal_component_value))/(2*np.pi)))\n\nP0: -50.63Hz\n\n\n\nS_C1_H_sym_P0 = (C1/H_sym_P0)*(H_sym_P0.diff(C1))\nprint('the sensitivity of P0 to C1 is: {:.2f}'.format(N(S_C1_H_sym_P0.subs(nominal_component_value))))\n\nthe sensitivity of P0 to C1 is: -1.00\n\n\n\nS_R1_H_sym_P0 = (R1/H_sym_P0)*(H_sym_P0.diff(R1))\nprint('the sensitivity of P0 to R1 is: {:.2f}'.format(N(S_R1_H_sym_P0.subs(nominal_component_value))))\n\nthe sensitivity of P0 to R1 is: -1.00\n\n\n\n\nP1\nThe second pole is:\n\nH_sym_P1 = H_sym_poles[1]\nH_sym_P1\n\n\\(\\displaystyle - \\frac{1}{Co Ro}\\)\n\n\nThe pole P1 evaluates to a frequency near DC.\n\nprint('P1: {:.2f}Hz'.format(N(H_sym_P1.subs(nominal_component_value))/(2*np.pi)))\n\nP1: -1.59Hz\n\n\n\nS_Co_H_sym_P1 = (Co/H_sym_P1)*(H_sym_P1.diff(Co))\nprint('the sensitivity of P1 to Co is: {:.2f}'.format(N(S_Co_H_sym_P1.subs(nominal_component_value))))\n\nthe sensitivity of P1 to Co is: -1.00\n\n\n\nS_Ro_H_sym_P1 = (Ro/H_sym_P1)*(H_sym_P1.diff(Ro))\nprint('the sensitivity of P1 to Ro is: {:.2f}'.format(N(S_Ro_H_sym_P1.subs(nominal_component_value))))\n\nthe sensitivity of P1 to Ro is: -1.00\n\n\n\n\nP2\nThe 3rd pole is:\n\nH_sym_P2 = H_sym_poles[2]\nH_sym_P2\n\n\\(\\displaystyle \\frac{- C_{3} R_{3} - C_{4} R_{3} - C_{4} R_{6} - \\sqrt{C_{3}^{2} R_{3}^{2} + 2 C_{3} C_{4} R_{3}^{2} - 2 C_{3} C_{4} R_{3} R_{6} + C_{4}^{2} R_{3}^{2} + 2 C_{4}^{2} R_{3} R_{6} + C_{4}^{2} R_{6}^{2}}}{2 C_{3} C_{4} R_{3} R_{6}}\\)\n\n\n\nprint('the compenets that determine P2 are: {:s} '.format(str(H_sym_P2.free_symbols)))\n\nthe compenets that determine P2 are: {R3, R6, C4, C3} \n\n\n\nprint('P2: {:.2f}Hz'.format(N(H_sym_P2.subs(nominal_component_value))/(2*np.pi)))\n\nP2: -2122.88Hz\n\n\n\nS_C3_H_sym_P2 = (C3/H_sym_P2)*(H_sym_P2.diff(C3))\nprint('the sensitivity of P2 to C3 is: {:.2f}'.format(N(S_C3_H_sym_P2.subs(nominal_component_value))))\n\nthe sensitivity of P2 to C3 is: -1.00\n\n\n\nS_C4_H_sym_P2 = (C4/H_sym_P2)*(H_sym_P2.diff(C4))\nprint('the sensitivity of P2 to C4 is: {:.3e}'.format(N(S_C4_H_sym_P2.subs(nominal_component_value))))\n\nthe sensitivity of P2 to C4 is: -2.829e-5\n\n\n\nS_R3_H_sym_P2 = (R3/H_sym_P2)*(H_sym_P2.diff(R3))\nprint('the sensitivity of P2 to R3 is: {:.2f}'.format(N(S_R3_H_sym_P2.subs(nominal_component_value))))\n\nthe sensitivity of P2 to R3 is: -0.96\n\n\n\nS_R6_H_sym_P2 = (R6/H_sym_P2)*(H_sym_P2.diff(R6))\nprint('the sensitivity of P2 to R6 is: {:.3f}'.format(N(S_R6_H_sym_P2.subs(nominal_component_value))))\n\nthe sensitivity of P2 to R6 is: -0.041\n\n\n\n\nP3\nThe 4th pole is:\n\nH_sym_P3 = H_sym_poles[3]\nH_sym_P3\n\n\\(\\displaystyle \\frac{- C_{3} R_{3} - C_{4} R_{3} - C_{4} R_{6} + \\sqrt{C_{3}^{2} R_{3}^{2} + 2 C_{3} C_{4} R_{3}^{2} - 2 C_{3} C_{4} R_{3} R_{6} + C_{4}^{2} R_{3}^{2} + 2 C_{4}^{2} R_{3} R_{6} + C_{4}^{2} R_{6}^{2}}}{2 C_{3} C_{4} R_{3} R_{6}}\\)\n\n\n\nprint('the compenets that determine P3 are: {:s} '.format(str(H_sym_P3.free_symbols)))\n\nthe compenets that determine P3 are: {R3, R6, C4, C3} \n\n\n\nprint('P3: {:.2f}Hz'.format(N(H_sym_P3.subs(nominal_component_value))/(2*np.pi)))\n\nP3: -1.39Hz\n\n\n\nS_C3_H_sym_P3 = (C3/H_sym_P3)*(H_sym_P3.diff(C3))\nprint('the sensitivity of P3 to C3 is: {:.2e}'.format(N(S_C3_H_sym_P3.subs(nominal_component_value))))\n\nthe sensitivity of P3 to C3 is: -2.83e-5\n\n\n\nS_C4_H_sym_P3 = (C4/H_sym_P3)*(H_sym_P3.diff(C4))\nprint('the sensitivity of P3 to C5 is: {:.2f}'.format(N(S_C4_H_sym_P3.subs(nominal_component_value))))\n\nthe sensitivity of P3 to C5 is: -1.00\n\n\n\nS_R3_H_sym_P3 = (R3/H_sym_P3)*(H_sym_P3.diff(R3))\nprint('the sensitivity of P3 to R3 is: {:.2f}'.format(N(S_R3_H_sym_P3.subs(nominal_component_value))))\n\nthe sensitivity of P3 to R3 is: -0.04\n\n\n\nS_R6_H_sym_P3 = (R6/H_sym_P3)*(H_sym_P3.diff(R6))\nprint('the sensitivity of P3 to R6 is: {:.2f}'.format(N(S_R6_H_sym_P3.subs(nominal_component_value))))\n\nthe sensitivity of P3 to R6 is: -0.96\n\n\nIn the worst case analysis below, the componets that have sensitivites greater than 0.05 are the ones that factor into the worst case analysis.\n\n\n\nComponent selection\nThe table below list each of the components used in the preamp along with a link to the Digikey pages for each of the components in the preamp. Digikey is a larger distributor of electronic components in the US. All the components have operating temperature ranges that exceed the normal household envirment.\nThe resistors chosen are all 1% Metal film type resistors. Metal film resistors possess good noise characteristics and low non-linearity due to a low voltage coefficient. They are also beneficial due to long-term stability.\nThe capacitors are all polypropylene Film capacitor types. Polystyrene or polypropylene are considered the best for audio applications.\nThe Op Amp, LM833N, is a dual bipolar low noise (\\(\\frac {4.5nV}{\\sqrt{Hz}}\\)), wide bandwidth (16 MHz) audio operational amplifier from Texas Instrments.\n\n\n\nRef\nValue\nDescription\nDigikey PN\n\n\n\n\nRo\n499\n±1% 1/4W Metal Film\nRNF14FTD499RCT-ND\n\n\nRp\n47k\n±1% 1/4W Metal Film\n13-MFR-25FTE52-47KTB-ND\n\n\nR1\n80.6k\n±1% 1/4W Metal Film\n80.6KXBK-ND\n\n\nR2\n58.45k\n±1% 1/4W Metal Film\nRNF14FTD8K45CT-ND\n\n\nR3\n2.37k\n±1% 1/4W Metal Film\n13-MFR-25FBF52-2K37-ND\n\n\nR4\n2k\n±1% 1/4W Metal Film\n13-MFR-25FRF52-2KCT-ND\n\n\nR5\n4.3k\n±1% 1/4W Metal Film\nS4.3KCACT-ND\n\n\nR6\n54.9k\n±1% 1/4W Metal Film\nRNF14FTD54K9CT-ND\n\n\nCo\n200\\(\\mu\\)\n10% Film Capacitor 450V Polypropylene\n283-EFDKS45K207F064DH-ND\n\n\nCp\n100p\n10% Film Capacitor 250V Polypropylene\n399-RSBEC0100ZA00M-ND\n\n\nC1\n0.039\\(\\mu\\)\n2% Film Capacitor 25V 63V Polypropylene\nBC2066-ND\n\n\nC3\n0.033\\(\\mu\\)\n1% Film Capacitor 63V 100V Polypropylene\n399-PHE426DJ5330FR17T0CT-ND\n\n\nC4\n2\\(\\mu\\)\n10% Film Capacitor 305V 630V Polypropylene\n495-B32923P3205K000-ND\n\n\nU1, U2\nLM833N\nAudio op amp\n296-44419-5-ND\n\n\n\nThe parts in this list are considered good choices for a first pass at the bill of materials. The size of the production run and the piece part cost are also a factors which must be considered if the preamp is going to be built. One thing to notice is that Co, the 200 \\(\\mu\\) F capacitor is expensive. The use of a polypropylene film capacitor for this component is consistent with the advice of keeping all capacitors in the audio path polystyrene or polypropylene.\n\n\nMonte Carlo simulation\nIn this analysis the circuit equations are solved after assigning random element values from within the tolerance band to the components. This simulates building a large number of circuits with components chosen at random from bins or reals of components during the board stuffing process. All the components are required to met their specifications, but are allowed to have some varaition accorting to theier tolerance. For example a 1% 2k resistor can range from 1980 to 2020 \\(\\Omega\\). In addition to the components initial tolerance, the temperature coefficient and aging of paramters can also be included.\nIn this simulation, I’m only including the initial tolerances of parameters and I’m assuming the distritution is uniform. The Numpy function random.uniform is used to generate the random values within the tolerance range, however, for this function, the hight end-point value may or may not be included in the range depending on floating-point rounding, so if this is important, some adjustments to the code are required. The Numpy function random.seed is used to re-seed the random number generator.\n\nnum = 20 # number of simulations to run\nnew_x_axis_range = np.logspace(1, 5.5, 100, endpoint=True)*2*np.pi\n\n# make some arrays to the hold the results of each run\nmag_ans = np.zeros(shape=(num,len(new_x_axis_range)))\nphase_ans = np.zeros(shape=(num,len(new_x_axis_range)))\n\ncomponent_values_tol = nominal_component_value.copy() # makde a copy\n\nrandom.seed(a=None, version=2) # re-seed the random number generator\n\nThe following takes about 15 seconds to run on for num=20 on an i3 machine.\n\nfor i in range(0,num):\n\n    component_values_tol[Ro] = random.uniform(nominal_component_value[Ro]-nominal_component_value[Ro]*0.01,nominal_component_value[Ro]+nominal_component_value[Ro]*0.01)\n    component_values_tol[Rp] = random.uniform(nominal_component_value[Rp]-nominal_component_value[Rp]*0.01,nominal_component_value[Rp]+nominal_component_value[Ro]*0.01)\n\n    component_values_tol[R1] = random.uniform(nominal_component_value[R1]-nominal_component_value[R1]*0.01,nominal_component_value[R1]+nominal_component_value[R1]*0.01)\n    component_values_tol[R2] = random.uniform(nominal_component_value[R2]-nominal_component_value[R2]*0.01,nominal_component_value[R2]+nominal_component_value[R2]*0.01)    \n    component_values_tol[R3] = random.uniform(nominal_component_value[R3]-nominal_component_value[R3]*0.01,nominal_component_value[R3]+nominal_component_value[R3]*0.01)\n    component_values_tol[R4] = random.uniform(nominal_component_value[R4]-nominal_component_value[R4]*0.01,nominal_component_value[R4]+nominal_component_value[R4]*0.01)    \n    component_values_tol[R5] = random.uniform(nominal_component_value[R5]-nominal_component_value[R5]*0.01,nominal_component_value[R5]+nominal_component_value[R5]*0.01)\n    component_values_tol[R6] = random.uniform(nominal_component_value[R6]-nominal_component_value[R6]*0.01,nominal_component_value[R6]+nominal_component_value[R6]*0.01)    \n\n    component_values_tol[Co] = random.uniform(nominal_component_value[Co]-nominal_component_value[Co]*0.1,nominal_component_value[Co]+nominal_component_value[Co]*0.1)\n    component_values_tol[Cp] = random.uniform(nominal_component_value[Cp]-nominal_component_value[Cp]*0.1,nominal_component_value[Cp]+nominal_component_value[Cp]*0.1)\n    component_values_tol[C1] = random.uniform(nominal_component_value[C1]-nominal_component_value[C1]*0.02,nominal_component_value[C1]+nominal_component_value[C1]*0.02)\n    component_values_tol[C3] = random.uniform(nominal_component_value[C3]-nominal_component_value[C3]*0.01,nominal_component_value[C3]+nominal_component_value[C3]*0.01)\n    component_values_tol[C4] = random.uniform(nominal_component_value[C4]-nominal_component_value[C4]*0.1,nominal_component_value[C4]+nominal_component_value[C4]*0.1)\n\n    # enter the element values\n    preamp_equ_tol = preamp_equ_sym.subs(component_values_tol)\n\n    U_preamp_tol = solve(preamp_equ_tol,X)\n\n    H_preamp_tol = U_preamp_tol[v2]/U_preamp_tol[v1]\n\n    # Extract the numerator and denominator polynomials so that the system can be defined in SciPy.\n    H_preamp_tol_num, H_preamp_tol_denom = fraction(H_preamp_tol) #returns numerator and denominator\n\n    # convert symbolic to numpy polynomial\n    a2 = np.array(Poly(H_preamp_tol_num, s).all_coeffs(), dtype=float)\n    b2 = np.array(Poly(H_preamp_tol_denom, s).all_coeffs(), dtype=float)\n    preamp_sys_tol = signal.TransferFunction(a2,b2)\n\n    w_preamp_sys_tol, mag_preamp_sys_tol, phase_preamp_sys_tol = preamp_sys_tol.bode(w=new_x_axis_range)\n    \n    # save the results from each run\n    mag_ans[i] = mag_preamp_sys_tol\n    phase_ans[i] = phase_preamp_sys_tol\n\n\n\nPreamplifier deviation from RIAA response\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nfor i in range(0,num):\n    plt.semilogx(w_RIAA/(2*np.pi), (mag_RIAA-RIAA_gain_1kHz) + (mag_ans[i]-preamp_gain_1kHz),'-k')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\nax1.set_ylim((-0.3,0.3))\nplt.grid()\nplt.axvspan(20, 20e3, color='y', alpha=0.3)\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:blue'\n\nfor i in range(0,num):\n    plt.semilogx(w_RIAA/(2*np.pi), phase_RIAA+phase_ans[i],':',color=color)  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n\nplt.title('Monte Carlo runs showing deviation from RIAA curve')\nplt.show()\n\n\n\n\nThe Monte Carlo simulation shows that the preamp amplitude response deviatin from the RIAA curve can very from -0.2 to 0.3 dB over the audio band. If the performance requirement for this preamp was to be within \\(\\pm\\) 0.1 dB of the RIAA curve, then some redesign or tighter component tolerancing is required.\n\n\nWorst case analysis\nIn a worst case analysis, we would look at:\n\nminimum and maximum values of the initial component tolerance\n\nmaximum or minumum temperature coefficients of the parameters\n\nmaximum aging or drift of parameter values\n\nSince we usually can’t tell by inspection which combination of minimum and maximum values will give the worst case, we can run a number of simulations in which all combination of minum and maximium variations are included. From the family of results we can look for the worst case.\nHow resistors and capactors in the preamp circuit?\n\nprint('number of components: {:d}'.format(len(nominal_component_value)))\n\nnumber of components: 14\n\n\nHow many combinations min and max combinations?\n\nprint('number of min and max combinations: {:,d}'.format(2**14))\n\nnumber of min and max combinations: 16,384\n\n\n16 thousand simulation runs to too many. From the sensitivity analysis above, only R1, R2, R3, R6, Ro, C1 and C3 are sensitive. Running all combinations of the min and max tolerance for this set is reasonable and is \\(2^7=128\\) combinations.\nThe tolerances for each of the componts is defined below:\n\nTol = {Ro:0.01,R1:0.01,R2:0.01,R3:0.01,R6:0.01,C1:0.02,C3:0.01}\n\nOnly C1 has a tolerance other than 1%.\nThe array ‘run’ is created that consists of a binary count, with leading zeros from 0 to 127. Then the zero values are replaced with -1.\n\nrun = []\nfor i in range(0,2**(len(Tol))):\n    temp = list('{:07b}'.format(i)) # include leading zeros\n    for j in range(len(temp)):\n        temp[j] = int(temp[j])\n    run.append(temp)\n\nrun = np.asarray(run)\nrun = np.where(run == 0, -1, run)\n\nThe first row of run is:\n\nrun[0]\n\narray([-1, -1, -1, -1, -1, -1, -1])\n\n\nIn the for loop below, at i = 0, run[0] would be all -1’s and this could apply the low tolerance range to the nominal component values.\n\nrun[-1]\n\narray([1, 1, 1, 1, 1, 1, 1])\n\n\nThe last time through the for loop, where i = 127, run[-1] is all 1’s and this would apply the high tolerance range to the nominal component values. Between i = 0 and i = 127, all combinations of minumum and maximum tolerance is appled.\n\nnew_x_axis_range = np.logspace(1, 5.5, 100, endpoint=True)*2*np.pi\n\n# make some arrays to hold the results\nmag_ans = np.zeros(shape=(len(run),len(new_x_axis_range)))\nphase_ans = np.zeros(shape=(len(run),len(new_x_axis_range)))\n\nThe following cell takes about 90 seconds to run on an i3 machine.\n\nfor i in range(len(run)):\n    component_values_tol[Ro] = nominal_component_value[Ro]*(1+run[i][0]*Tol[Ro])\n    component_values_tol[Rp] = nominal_component_value[Rp]\n\n    component_values_tol[R1] = nominal_component_value[R1]*(1+run[i][1]*Tol[R1])\n    component_values_tol[R2] = nominal_component_value[R2]*(1+run[i][2]*Tol[R2]) \n    component_values_tol[R3] = nominal_component_value[R3]*(1+run[i][3]*Tol[R3])\n\n    component_values_tol[R4] = nominal_component_value[R4] \n    component_values_tol[R5] = nominal_component_value[R5]\n    component_values_tol[R6] = nominal_component_value[R6]*(1+run[i][4]*Tol[R6])\n\n    component_values_tol[Co] = nominal_component_value[Co]\n    component_values_tol[Cp] = nominal_component_value[Cp]\n    component_values_tol[C1] = nominal_component_value[C1]*(1+run[i][5]*Tol[C1])\n    component_values_tol[C3] = nominal_component_value[C3]*(1+run[i][6]*Tol[C3])\n    component_values_tol[C4] = nominal_component_value[C4]\n    \n    # enter the element values\n    preamp_equ_tol = preamp_equ_sym.subs(component_values_tol)\n\n    U_preamp_tol = solve(preamp_equ_tol,X)\n\n    H_preamp_tol = U_preamp_tol[v2]/U_preamp_tol[v1]\n\n    # Extract the numerator and denominator polynomials so that the system can be defined in SciPy.\n    H_preamp_tol_num, H_preamp_tol_denom = fraction(H_preamp_tol) #returns numerator and denominator\n\n    # convert symbolic to numpy polynomial\n    a2 = np.array(Poly(H_preamp_tol_num, s).all_coeffs(), dtype=float)\n    b2 = np.array(Poly(H_preamp_tol_denom, s).all_coeffs(), dtype=float)\n    preamp_sys_tol = signal.TransferFunction(a2,b2)\n\n    w_preamp_sys_tol, mag_preamp_sys_tol, phase_preamp_sys_tol = preamp_sys_tol.bode(w=new_x_axis_range)\n    mag_ans[i] = mag_preamp_sys_tol\n    phase_ans[i] = phase_preamp_sys_tol\n\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\nfor i in range(0,num):\n    plt.semilogx(w_RIAA/(2*np.pi), (mag_RIAA-RIAA_gain_1kHz) + (mag_ans[i]-preamp_gain_1kHz),'-k')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\nax1.set_ylim((-0.3,0.4))\nplt.grid()\nplt.axvspan(20, 20e3, color='y', alpha=0.3)\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:blue'\n\nfor i in range(0,num):\n    plt.semilogx(w_RIAA/(2*np.pi), phase_RIAA+phase_ans[i],':',color=color)  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n\nplt.title('Worst Case deviation from RIAA curve')\nplt.show()\n\n\n\n\nThe results above show that the worst case tolerance conditions yeild deviations of -0.19 to 0.3 dB from the RIAA curve."
  },
  {
    "objectID": "OLD Two amplifier RIAA Phono Preamp.html#summary",
    "href": "OLD Two amplifier RIAA Phono Preamp.html#summary",
    "title": "Two amplifier RIAA phono preamp",
    "section": "Summary",
    "text": "Summary\nThe circuit presented in this analysis is just one example of many circuits that can be found on line. For each design to be evaluated, some type of side by side analysis should be used used to down select. This notebook can be used as template for any compartivite analysis.\nThe circuit in the app note appears to have low frequency group delay that might be an issue. Also the deviation from the RIAA curve using normal component tolerances does not meet the 0.1 dB requirement. The worst case analysis also confirms this. The circuit employs an expesive 200\\(\\mu\\) Farad film capacitor. The circuit is missing a subsonic filter."
  },
  {
    "objectID": "OLD SMNA_example.html",
    "href": "OLD SMNA_example.html",
    "title": "Symbolic modified nodal analysis example",
    "section": "",
    "text": "This JupyterLab notebook uses the SymPy, NumPy,SciPy and the Python programming language libraries to analyize an electrical circuit. A circuit analysis method called the Modified Nodal Analysis was used to derive the symbolic circuit equations and Python libraries were used to solve the equations. The purpose of this analysis is to demonstrate the capability of using the Python libraries in electrical engineering circuit analysis. A link to my Jupyter Notebook rendered as a web page is here."
  },
  {
    "objectID": "OLD SMNA_example.html#abstract",
    "href": "OLD SMNA_example.html#abstract",
    "title": "Symbolic modified nodal analysis example",
    "section": "",
    "text": "This JupyterLab notebook uses the SymPy, NumPy,SciPy and the Python programming language libraries to analyize an electrical circuit. A circuit analysis method called the Modified Nodal Analysis was used to derive the symbolic circuit equations and Python libraries were used to solve the equations. The purpose of this analysis is to demonstrate the capability of using the Python libraries in electrical engineering circuit analysis. A link to my Jupyter Notebook rendered as a web page is here."
  },
  {
    "objectID": "OLD SMNA_example.html#introduction",
    "href": "OLD SMNA_example.html#introduction",
    "title": "Symbolic modified nodal analysis example",
    "section": "Introduction",
    "text": "Introduction\nThis notebook walks through the Python code used to generate and solve the circuit network equations. The example starts with a net list for the circuit shown below."
  },
  {
    "objectID": "OLD SMNA_example.html#circuit-description",
    "href": "OLD SMNA_example.html#circuit-description",
    "title": "Symbolic modified nodal analysis example",
    "section": "Circuit description",
    "text": "Circuit description\nThe schematic of the circuit is shown below with each node explicity annotated. The circuit has 9 lines in netlist, 9 branches and 5 unknown currents.\n\nThe net list for this circuit is:\nR2 2 5 2\nV1 1 0 1\nI1 4 0 0\nV2 0 5 0\nE1 3 0 1 4 2\nF1 2 3 V2 2\nR1 1 4 2\nC1 1 2 1\nL1 4 3 1\nI1 has been set to zero for the AC analysis.\n\nimport os\nfrom sympy import *\nimport numpy as np\nfrom scipy import signal\nimport matplotlib.pyplot as plt\nimport pandas as pd\ninit_printing()"
  },
  {
    "objectID": "OLD SMNA_example.html#symbolic-mna-code",
    "href": "OLD SMNA_example.html#symbolic-mna-code",
    "title": "Symbolic modified nodal analysis example",
    "section": "Symbolic MNA code",
    "text": "Symbolic MNA code\n\n# initialize variables\nnum_rlc = 0 # number of passive elements\nnum_ind = 0 # number of inductors\nnum_v = 0    # number of independent voltage sources\nnum_i = 0    # number of independent current sources\ni_unk = 0  # number of current unknowns\nnum_opamps = 0   # number of op amps\nnum_vcvs = 0     # number of controlled sources of various types\nnum_vccs = 0\nnum_cccs = 0\nnum_ccvs = 0\nnum_cpld_ind = 0 # number of coupled inductors"
  },
  {
    "objectID": "OLD SMNA_example.html#read-the-net-list-and-preprocess-it",
    "href": "OLD SMNA_example.html#read-the-net-list-and-preprocess-it",
    "title": "Symbolic modified nodal analysis example",
    "section": "Read the net list and preprocess it",
    "text": "Read the net list and preprocess it\nThe following steps are performed:\n\nremove blank lines and comments\n\nconvert first letter of element name to upper case\n\nremoves extra spaces between entries\n\ncount number of entries on each line, make sure the count is correct, count each element type\n\n\nexample_net_list = '''R2 2 5 2\nV1 1 0 1\nI1 4 0 0\nV2 0 5 0\nE1 3 0 1 4 2\nF1 2 3 V2 2\nR1 1 4 2\nC1 1 2 1\nL1 4 3 1'''\n\n\ncontent = example_net_list.splitlines()\n\ncontent = [x.strip() for x in content]  #remove leading and trailing white space\n# remove empty lines\nwhile '' in content:\n    content.pop(content.index(''))\n\n# remove comment lines, these start with a asterisk *\ncontent = [n for n in content if not n.startswith('*')]\n# remove other comment lines, these start with a semicolon ;\ncontent = [n for n in content if not n.startswith(';')]\n# remove spice directives, these start with a period, .\ncontent = [n for n in content if not n.startswith('.')]\n# converts 1st letter to upper case\n#content = [x.upper() for x in content] &lt;- this converts all to upper case\ncontent = [x.capitalize() for x in content]\n# removes extra spaces between entries\ncontent = [' '.join(x.split()) for x in content]\n\n\nfor i in content:\n    print(i)\n\nR2 2 5 2\nV1 1 0 1\nI1 4 0 0\nV2 0 5 0\nE1 3 0 1 4 2\nF1 2 3 v2 2\nR1 1 4 2\nC1 1 2 1\nL1 4 3 1\n\n\n\nline_cnt = len(content) # number of lines in the netlist\nbranch_cnt = 0  # number of branches in the netlist\n# check number of entries on each line, count each element type\nfor i in range(line_cnt):\n    x = content[i][0]\n    tk_cnt = len(content[i].split()) # split the line into a list of words\n\n    if (x == 'R') or (x == 'L') or (x == 'C'):\n        if tk_cnt != 4:\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 4\".format(tk_cnt))\n        num_rlc += 1\n        branch_cnt += 1\n        if x == 'L':\n            num_ind += 1\n    elif x == 'V':\n        if tk_cnt != 4:\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 4\".format(tk_cnt))\n        num_v += 1\n        branch_cnt += 1\n    elif x == 'I':\n        if tk_cnt != 4:\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 4\".format(tk_cnt))\n        num_i += 1\n        branch_cnt += 1\n    elif x == 'O':\n        if tk_cnt != 4:\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 4\".format(tk_cnt))\n        num_opamps += 1\n    elif x == 'E':\n        if (tk_cnt != 6):\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 6\".format(tk_cnt))\n        num_vcvs += 1\n        branch_cnt += 1\n    elif x == 'G':\n        if (tk_cnt != 6):\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 6\".format(tk_cnt))\n        num_vccs += 1\n        branch_cnt += 1\n    elif x == 'F':\n        if (tk_cnt != 5):\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 5\".format(tk_cnt))\n        num_cccs += 1\n        branch_cnt += 1\n    elif x == 'H':\n        if (tk_cnt != 5):\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 5\".format(tk_cnt))\n        num_ccvs += 1\n        branch_cnt += 1\n    elif x == 'K':\n        if (tk_cnt != 4):\n            print(\"branch {:d} not formatted correctly, {:s}\".format(i,content[i]))\n            print(\"had {:d} items and should only be 4\".format(tk_cnt))\n        num_cpld_ind += 1\n    else:\n        print(\"unknown element type in branch {:d}, {:s}\".format(i,content[i]))"
  },
  {
    "objectID": "OLD SMNA_example.html#parser",
    "href": "OLD SMNA_example.html#parser",
    "title": "Symbolic modified nodal analysis example",
    "section": "Parser",
    "text": "Parser\nThe parser performs the following operations.\n\nputs branch elements into data frame\n\ncounts number of nodes\n\ndata frame labels:\n\nelement: type of element\n\np node: positive node\n\nn node: negative node, for a current source, the arrow point terminal, LTspice puts the inductor phasing dot on this terminal\n\ncp node: controlling positive node of branch\n\ncn node: controlling negative node of branch\n\nVout: opamp output node\n\nvalue: value of element or voltage\n\nVname: voltage source through which the controlling current flows. Need to add a zero volt voltage source to the controlling branch.\n\nLname1: name of coupled inductor 1\n\nLname2: name of coupled inductor 2\n\n\n# build the pandas data frame\ndf = pd.DataFrame(columns=['element','p node','n node','cp node','cn node',\n    'Vout','value','Vname','Lname1','Lname2'])\n\n# this data frame is for branches with unknown currents\ndf2 = pd.DataFrame(columns=['element','p node','n node'])\n\n\nFunctions to load branch elements into data frame and check for gaps in node numbering\n\n# loads voltage or current sources into branch structure\ndef indep_source(line_nu):\n    tk = content[line_nu].split()\n    df.loc[line_nu,'element'] = tk[0]\n    df.loc[line_nu,'p node'] = int(tk[1])\n    df.loc[line_nu,'n node'] = int(tk[2])\n    df.loc[line_nu,'value'] = float(tk[3])\n\n# loads passive elements into branch structure\ndef rlc_element(line_nu):\n    tk = content[line_nu].split()\n    df.loc[line_nu,'element'] = tk[0]\n    df.loc[line_nu,'p node'] = int(tk[1])\n    df.loc[line_nu,'n node'] = int(tk[2])\n    df.loc[line_nu,'value'] = float(tk[3])\n\n# loads multi-terminal elements into branch structure\n# O - Op Amps\ndef opamp_sub_network(line_nu):\n    tk = content[line_nu].split()\n    df.loc[line_nu,'element'] = tk[0]\n    df.loc[line_nu,'p node'] = int(tk[1])\n    df.loc[line_nu,'n node'] = int(tk[2])\n    df.loc[line_nu,'Vout'] = int(tk[3])\n\n# G - VCCS\ndef vccs_sub_network(line_nu):\n    tk = content[line_nu].split()\n    df.loc[line_nu,'element'] = tk[0]\n    df.loc[line_nu,'p node'] = int(tk[1])\n    df.loc[line_nu,'n node'] = int(tk[2])\n    df.loc[line_nu,'cp node'] = int(tk[3])\n    df.loc[line_nu,'cn node'] = int(tk[4])\n    df.loc[line_nu,'value'] = float(tk[5])\n\n# E - VCVS\n# in sympy E is the number 2.718, replacing E with Ea otherwise, sympify() errors out\ndef vcvs_sub_network(line_nu):\n    tk = content[line_nu].split()\n    df.loc[line_nu,'element'] = tk[0].replace('E', 'Ea')\n    df.loc[line_nu,'p node'] = int(tk[1])\n    df.loc[line_nu,'n node'] = int(tk[2])\n    df.loc[line_nu,'cp node'] = int(tk[3])\n    df.loc[line_nu,'cn node'] = int(tk[4])\n    df.loc[line_nu,'value'] = float(tk[5])\n\n# F - CCCS\ndef cccs_sub_network(line_nu):\n    tk = content[line_nu].split()\n    df.loc[line_nu,'element'] = tk[0]\n    df.loc[line_nu,'p node'] = int(tk[1])\n    df.loc[line_nu,'n node'] = int(tk[2])\n    df.loc[line_nu,'Vname'] = tk[3].capitalize()\n    df.loc[line_nu,'value'] = float(tk[4])\n\n# H - CCVS\ndef ccvs_sub_network(line_nu):\n    tk = content[line_nu].split()\n    df.loc[line_nu,'element'] = tk[0]\n    df.loc[line_nu,'p node'] = int(tk[1])\n    df.loc[line_nu,'n node'] = int(tk[2])\n    df.loc[line_nu,'Vname'] = tk[3].capitalize()\n    df.loc[line_nu,'value'] = float(tk[4])\n\n# K - Coupled inductors\ndef cpld_ind_sub_network(line_nu):\n    tk = content[line_nu].split()\n    df.loc[line_nu,'element'] = tk[0]\n    df.loc[line_nu,'Lname1'] = tk[1].capitalize()\n    df.loc[line_nu,'Lname2'] = tk[2].capitalize()\n    df.loc[line_nu,'value'] = float(tk[3])\n\n# function to scan df and get largest node number\ndef count_nodes():\n    # need to check that nodes are consecutive\n    # fill array with node numbers\n    p = np.zeros(line_cnt+1)\n    for i in range(line_cnt):\n        # need to skip coupled inductor 'K' statements\n        if df.loc[i,'element'][0] != 'K': #get 1st letter of element name\n            p[df['p node'][i]] = df['p node'][i]\n            p[df['n node'][i]] = df['n node'][i]\n\n    # find the largest node number\n    if df['n node'].max() &gt; df['p node'].max():\n        largest = df['n node'].max()\n    else:\n        largest =  df['p node'].max()\n\n    largest = int(largest)\n    # check for unfilled elements, skip node 0\n    for i in range(1,largest):\n        if p[i] == 0:\n            print('nodes not in continuous order, node {:.0f} is missing'.format(p[i-1]+1))\n\n    return largest\n\n\n\nLoad circuit netlist into the data frames\n\n# load branch info into data frame\nfor i in range(line_cnt):\n    x = content[i][0]\n\n    if (x == 'R') or (x == 'L') or (x == 'C'):\n        rlc_element(i)\n    elif (x == 'V') or (x == 'I'):\n        indep_source(i)\n    elif x == 'O':\n        opamp_sub_network(i)\n    elif x == 'E':\n        vcvs_sub_network(i)\n    elif x == 'G':\n        vccs_sub_network(i)\n    elif x == 'F':\n        cccs_sub_network(i)\n    elif x == 'H':\n        ccvs_sub_network(i)\n    elif x == 'K':\n        cpld_ind_sub_network(i)\n    else:\n        print(\"unknown element type in branch {:d}, {:s}\".format(i,content[i]))\n\n29 Nov 2023: When the D matrix is built, independent voltage sources are processed in the data frame order when building the D matrix. If the voltage source followed element L, H, F, K types in the netlist, a row was inserted that put the voltage source in a different row in relation to its position in the Ev matrix. This would cause the node attached to the terminal of the voltage source to be zero volts.\nSolution - The following block of code was added to move voltage source types to the beginning of the net list dataframe before any calculations are performed.\n\n# Check for position of voltages sources in the dataframe.\nsource_index = [] # keep track of voltage source row number\nother_index = [] # make a list of all other types\nfor i in range(len(df)):\n    # process all the elements creating unknown currents\n    x = df.loc[i,'element'][0]   #get 1st letter of element name\n    if (x == 'V'):\n        source_index.append(i)\n    else:\n        other_index.append(i)\n\ndf = df.reindex(source_index+other_index,copy=True) # re-order the data frame\ndf.reset_index(drop=True, inplace=True) # renumber the index\n\n\n# count number of nodes\nnum_nodes = count_nodes()\n\n# Build df2: consists of branches with current unknowns, used for C & D matrices\n# walk through data frame and find these parameters\ncount = 0\nfor i in range(len(df)):\n    # process all the elements creating unknown currents\n    x = df.loc[i,'element'][0]   #get 1st letter of element name\n    if (x == 'L') or (x == 'V') or (x == 'O') or (x == 'E') or (x == 'H') or (x == 'F'):\n        df2.loc[count,'element'] = df.loc[i,'element']\n        df2.loc[count,'p node'] = df.loc[i,'p node']\n        df2.loc[count,'n node'] = df.loc[i,'n node']\n        count += 1"
  },
  {
    "objectID": "OLD SMNA_example.html#print-net-list-report",
    "href": "OLD SMNA_example.html#print-net-list-report",
    "title": "Symbolic modified nodal analysis example",
    "section": "Print net list report",
    "text": "Print net list report\n\n# print a report\nprint('Net list report')\nprint('number of lines in netlist: {:d}'.format(line_cnt))\nprint('number of branches: {:d}'.format(branch_cnt))\nprint('number of nodes: {:d}'.format(num_nodes))\n# count the number of element types that affect the size of the B, C, D, E and J arrays\n# these are current unknows\ni_unk = num_v+num_opamps+num_vcvs+num_ccvs+num_cccs+num_ind\nprint('number of unknown currents: {:d}'.format(i_unk))\nprint('number of RLC (passive components): {:d}'.format(num_rlc))\nprint('number of inductors: {:d}'.format(num_ind))\nprint('number of independent voltage sources: {:d}'.format(num_v))\nprint('number of independent current sources: {:d}'.format(num_i))\nprint('number of op amps: {:d}'.format(num_opamps))\nprint('number of E - VCVS: {:d}'.format(num_vcvs))\nprint('number of G - VCCS: {:d}'.format(num_vccs))\nprint('number of F - CCCS: {:d}'.format(num_cccs))\nprint('number of H - CCVS: {:d}'.format(num_ccvs))\nprint('number of K - Coupled inductors: {:d}'.format(num_cpld_ind))\n\nNet list report\nnumber of lines in netlist: 9\nnumber of branches: 9\nnumber of nodes: 5\nnumber of unknown currents: 5\nnumber of RLC (passive components): 4\nnumber of inductors: 1\nnumber of independent voltage sources: 2\nnumber of independent current sources: 1\nnumber of op amps: 0\nnumber of E - VCVS: 1\nnumber of G - VCCS: 0\nnumber of F - CCCS: 1\nnumber of H - CCVS: 0\nnumber of K - Coupled inductors: 0\n\n\n\ndf\n\n\n\n\n\n\n\n\nelement\np node\nn node\ncp node\ncn node\nVout\nvalue\nVname\nLname1\nLname2\n\n\n\n\n0\nV1\n1\n0\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\n\n\n1\nV2\n0\n5\nNaN\nNaN\nNaN\n0.0\nNaN\nNaN\nNaN\n\n\n2\nR2\n2\n5\nNaN\nNaN\nNaN\n2.0\nNaN\nNaN\nNaN\n\n\n3\nI1\n4\n0\nNaN\nNaN\nNaN\n0.0\nNaN\nNaN\nNaN\n\n\n4\nEa1\n3\n0\n1\n4\nNaN\n2.0\nNaN\nNaN\nNaN\n\n\n5\nF1\n2\n3\nNaN\nNaN\nNaN\n2.0\nV2\nNaN\nNaN\n\n\n6\nR1\n1\n4\nNaN\nNaN\nNaN\n2.0\nNaN\nNaN\nNaN\n\n\n7\nC1\n1\n2\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\n\n\n8\nL1\n4\n3\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ndf2\n\n\n\n\n\n\n\n\nelement\np node\nn node\n\n\n\n\n0\nV1\n1\n0\n\n\n1\nV2\n0\n5\n\n\n2\nEa1\n3\n0\n\n\n3\nF1\n2\n3\n\n\n4\nL1\n4\n3\n\n\n\n\n\n\n\n\n# store the data frame as a pickle file\n# df.to_pickle(fn+'.pkl')  # &lt;- uncomment if needed\n\n\n# initialize some symbolic matrix with zeros\n# A is formed by [[G, C] [B, D]]\n# Z = [I,E]\n# X = [V, J]\nV = zeros(num_nodes,1)\nI = zeros(num_nodes,1)\nG = zeros(num_nodes,num_nodes)  # also called Yr, the reduced nodal matrix\ns = Symbol('s')  # the Laplace variable\n\n# count the number of element types that affect the size of the B, C, D, E and J arrays\n# these are element types that have unknown currents\ni_unk = num_v+num_opamps+num_vcvs+num_ccvs+num_ind+num_cccs\n# if i_unk == 0, just generate empty arrays\nB = zeros(num_nodes,i_unk)\nC = zeros(i_unk,num_nodes)\nD = zeros(i_unk,i_unk)\nEv = zeros(i_unk,1)\nJ = zeros(i_unk,1)\n\n\nsome debugging notes:\nIs it possible to have i_unk == 0 ?, what about a network with only current sources? This would make B = 0 for example. Did one test, need to run others\nIs there a valid op amp case where B is n by 1?"
  },
  {
    "objectID": "OLD SMNA_example.html#g-matrix",
    "href": "OLD SMNA_example.html#g-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "G matrix",
    "text": "G matrix\nThe G matrix is n by n, where n is the number of nodes. The matrix is formed by the interconnections between the resistors, capacitors and VCCS type elements. In the original paper G is called Yr, where Yr is a reduced form of the nodal matrix excluding the contributions due to voltage sources, current controlling elements, etc. In python row and columns are: G[row, column]\n\n# G matrix\nfor i in range(len(df)):  # process each row in the data frame\n    n1 = df.loc[i,'p node']\n    n2 = df.loc[i,'n node']\n    cn1 = df.loc[i,'cp node']\n    cn2 = df.loc[i,'cn node']\n    # process all the passive elements, save conductance to temp value\n    x = df.loc[i,'element'][0]   #get 1st letter of element name\n    if x == 'R':\n        g = 1/sympify(df.loc[i,'element'])\n    if x == 'C':\n        g = s*sympify(df.loc[i,'element'])\n    if x == 'G':   #vccs type element\n        g = sympify(df.loc[i,'element'].lower())  # use a symbol for gain value\n\n    if (x == 'R') or (x == 'C'):\n        # If neither side of the element is connected to ground\n        # then subtract it from the appropriate location in the matrix.\n        if (n1 != 0) and (n2 != 0):\n            G[n1-1,n2-1] += -g\n            G[n2-1,n1-1] += -g\n\n        # If node 1 is connected to ground, add element to diagonal of matrix\n        if n1 != 0:\n            G[n1-1,n1-1] += g\n\n        # same for for node 2\n        if n2 != 0:\n            G[n2-1,n2-1] += g\n\n    if x == 'G':    #vccs type element\n        # check to see if any terminal is grounded\n        # then stamp the matrix\n        if n1 != 0 and cn1 != 0:\n            G[n1-1,cn1-1] += g\n\n        if n2 != 0 and cn2 != 0:\n            G[n2-1,cn2-1] += g\n\n        if n1 != 0 and cn2 != 0:\n            G[n1-1,cn2-1] -= g\n\n        if n2 != 0 and cn1 != 0:\n            G[n2-1,cn1-1] -= g\n\nG  # display the G matrix\n\n\\(\\displaystyle \\left[\\begin{matrix}C_{1} s + \\frac{1}{R_{1}} & - C_{1} s & 0 & - \\frac{1}{R_{1}} & 0\\\\- C_{1} s & C_{1} s + \\frac{1}{R_{2}} & 0 & 0 & - \\frac{1}{R_{2}}\\\\0 & 0 & 0 & 0 & 0\\\\- \\frac{1}{R_{1}} & 0 & 0 & \\frac{1}{R_{1}} & 0\\\\0 & - \\frac{1}{R_{2}} & 0 & 0 & \\frac{1}{R_{2}}\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "OLD SMNA_example.html#b-matrix",
    "href": "OLD SMNA_example.html#b-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "B Matrix",
    "text": "B Matrix\nThe B matrix is an n by m matrix with only 0, 1 and -1 elements, where n = number of nodes and m is the number of current unknowns, i_unk. There is one column for each unknown current. The code loop through all the branches and process elements that have stamps for the B matrix:\n\nVoltage sources (V)\n\nOpamps (O)\n\nCCVS (H)\n\nCCCS (F)\n\nVCVS (E)\n\nInductors (L)\n\nThe order of the columns is as they appear in the netlist. CCCS (F) does not get its own column because the controlling current is through a zero volt voltage source, called Vname and is already in the net list.\n\n# generate the B Matrix\nsn = 0   # count source number as code walks through the data frame\nfor i in range(len(df)):\n    n1 = df.loc[i,'p node']\n    n2 = df.loc[i,'n node']\n    n_vout = df.loc[i,'Vout'] # node connected to op amp output\n\n    # process elements with input to B matrix\n    x = df.loc[i,'element'][0]   #get 1st letter of element name\n    if x == 'V':\n        if i_unk &gt; 1:  #is B greater than 1 by n?, V\n            if n1 != 0:\n                B[n1-1,sn] = 1\n            if n2 != 0:\n                B[n2-1,sn] = -1\n        else:\n            if n1 != 0:\n                B[n1-1] = 1\n            if n2 != 0:\n                B[n2-1] = -1\n        sn += 1   #increment source count\n    if x == 'O':  # op amp type, output connection of the opamp goes in the B matrix\n        B[n_vout-1,sn] = 1\n        sn += 1   # increment source count\n    if (x == 'H') or (x == 'F'):  # H: ccvs, F: cccs,\n        if i_unk &gt; 1:  #is B greater than 1 by n?, H, F\n            # check to see if any terminal is grounded\n            # then stamp the matrix\n            if n1 != 0:\n                B[n1-1,sn] = 1\n            if n2 != 0:\n                B[n2-1,sn] = -1\n        else:\n            if n1 != 0:\n                B[n1-1] = 1\n            if n2 != 0:\n                B[n2-1] = -1\n        sn += 1   #increment source count\n    if x == 'E':   # vcvs type, only ik column is altered at n1 and n2\n        if i_unk &gt; 1:  #is B greater than 1 by n?, E\n            if n1 != 0:\n                B[n1-1,sn] = 1\n            if n2 != 0:\n                B[n2-1,sn] = -1\n        else:\n            if n1 != 0:\n                B[n1-1] = 1\n            if n2 != 0:\n                B[n2-1] = -1\n        sn += 1   #increment source count\n    if x == 'L':\n        if i_unk &gt; 1:  #is B greater than 1 by n?, L\n            if n1 != 0:\n                B[n1-1,sn] = 1\n            if n2 != 0:\n                B[n2-1,sn] = -1\n        else:\n            if n1 != 0:\n                B[n1-1] = 1\n            if n2 != 0:\n                B[n2-1] = -1\n        sn += 1   #increment source count\n\n# check source count\nif sn != i_unk:\n    print('source number, sn={:d} not equal to i_unk={:d} in matrix B'.format(sn,i_unk))\n\nB   # display the B matrix\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\\\0 & 0 & 1 & -1 & -1\\\\0 & 0 & 0 & 0 & 1\\\\0 & -1 & 0 & 0 & 0\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "OLD SMNA_example.html#c-matrix",
    "href": "OLD SMNA_example.html#c-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "C matrix",
    "text": "C matrix\nThe C matrix is an m by n matrix with only 0, 1 and -1 elements (except for controlled sources). The code is similar to the B matrix code, except the indices are swapped. The code loops through all the branches and process elements that have stamps for the C matrix:\n\nVoltage sources (V)\n\nOpamps (O)\n\nCCVS (H)\n\nCCCS (F)\n\nVCVS (E)\n\nInductors (L)\n\n\nOp Amp elements\nThe op amp element is assumed to be an ideal op amp and use of this component is valid only when used in circuits with a DC path (a short or a resistor) from the output terminal to the negative input terminal of the op amp. No error checking is provided and if the condition is violated, the results likely will be erroneous.\nReferences use in the debugging of the opamp stamp:\n\nDesign of Analog Circuits Through Symbolic Analysis, edited by Mourad Fakhfakh, Esteban Tlelo-Cuautle, Francisco V. Fernández\n\nComputer Aided Design and Design Automation, edited by Wai-Kai Chen\n\n\n# find the the column position in the C and D matrix for controlled sources\n# needs to return the node numbers and branch number of controlling branch\ndef find_vname(name):\n    # need to walk through data frame and find these parameters\n    for i in range(len(df2)):\n        # process all the elements creating unknown currents\n        if name == df2.loc[i,'element']:\n            n1 = df2.loc[i,'p node']\n            n2 = df2.loc[i,'n node']\n            return n1, n2, i  # n1, n2 & col_num are from the branch of the controlling element\n\n    print('failed to find matching branch element in find_vname')\n\n\n# generate the C Matrix\nsn = 0   # count source number as code walks through the data frame\nfor i in range(len(df)):\n    n1 = df.loc[i,'p node']\n    n2 = df.loc[i,'n node']\n    cn1 = df.loc[i,'cp node'] # nodes for controlled sources\n    cn2 = df.loc[i,'cn node']\n    n_vout = df.loc[i,'Vout'] # node connected to op amp output\n\n    # process elements with input to B matrix\n    x = df.loc[i,'element'][0]   #get 1st letter of element name\n    if x == 'V':\n        if i_unk &gt; 1:  #is B greater than 1 by n?, V\n            if n1 != 0:\n                C[sn,n1-1] = 1\n            if n2 != 0:\n                C[sn,n2-1] = -1\n        else:\n            if n1 != 0:\n                C[n1-1] = 1\n            if n2 != 0:\n                C[n2-1] = -1\n        sn += 1   #increment source count\n\n    if x == 'O':  # op amp type, input connections of the opamp go into the C matrix\n        # C[sn,n_vout-1] = 1\n        if i_unk &gt; 1:  #is B greater than 1 by n?, O\n            # check to see if any terminal is grounded\n            # then stamp the matrix\n            if n1 != 0:\n                C[sn,n1-1] = 1\n            if n2 != 0:\n                C[sn,n2-1] = -1\n        else:\n            if n1 != 0:\n                C[n1-1] = 1\n            if n2 != 0:\n                C[n2-1] = -1\n        sn += 1   # increment source count\n\n    if x == 'F':  # need to count F (cccs) types\n        sn += 1   #increment source count\n    if x == 'H':  # H: ccvs\n        if i_unk &gt; 1:  #is B greater than 1 by n?, H\n            # check to see if any terminal is grounded\n            # then stamp the matrix\n            if n1 != 0:\n                C[sn,n1-1] = 1\n            if n2 != 0:\n                C[sn,n2-1] = -1\n        else:\n            if n1 != 0:\n                C[n1-1] = 1\n            if n2 != 0:\n                C[n2-1] = -1\n        sn += 1   #increment source count\n    if x == 'E':   # vcvs type, ik column is altered at n1 and n2, cn1 & cn2 get value\n        if i_unk &gt; 1:  #is B greater than 1 by n?, E\n            if n1 != 0:\n                C[sn,n1-1] = 1\n            if n2 != 0:\n                C[sn,n2-1] = -1\n            # add entry for cp and cn of the controlling voltage\n            if cn1 != 0:\n                C[sn,cn1-1] = -sympify(df.loc[i,'element'].lower())\n            if cn2 != 0:\n                C[sn,cn2-1] = sympify(df.loc[i,'element'].lower())\n        else:\n            if n1 != 0:\n                C[n1-1] = 1\n            if n2 != 0:\n                C[n2-1] = -1\n            vn1, vn2, df2_index = find_vname(df.loc[i,'Vname'])\n            if vn1 != 0:\n                C[vn1-1] = -sympify(df.loc[i,'element'].lower())\n            if vn2 != 0:\n                C[vn2-1] = sympify(df.loc[i,'element'].lower())\n        sn += 1   #increment source count\n\n    if x == 'L':\n        if i_unk &gt; 1:  #is B greater than 1 by n?, L\n            if n1 != 0:\n                C[sn,n1-1] = 1\n            if n2 != 0:\n                C[sn,n2-1] = -1\n        else:\n            if n1 != 0:\n                C[n1-1] = 1\n            if n2 != 0:\n                C[n2-1] = -1\n        sn += 1   #increment source count\n\n# check source count\nif sn != i_unk:\n    print('source number, sn={:d} not equal to i_unk={:d} in matrix C'.format(sn,i_unk))\n\nC   # display the C matrix\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & -1\\\\- ea_{1} & 0 & 1 & ea_{1} & 0\\\\0 & 0 & 0 & 0 & 0\\\\0 & 0 & -1 & 1 & 0\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "OLD SMNA_example.html#d-matrix",
    "href": "OLD SMNA_example.html#d-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "D matrix",
    "text": "D matrix\nThe D matrix is an m by m matrix, where m is the number of unknown currents.\n&gt; m = i_unk = num_v+num_opamps+num_vcvs+num_ccvs+num_ind+num_cccs\nStamps that affect the D matrix are: inductor, ccvs and cccs\ninductors: minus sign added to keep current flow convention consistent\nCoupled inductors notes:\nCan the K statement be anywhere in the net list, even before Lx and Ly?\n12/6/2017 doing some debugging on with coupled inductors\nLTspice seems to put the phasing dot on the neg node when it generates the netlist\nThis code uses M for mutual inductance, LTspice uses k for the coupling coefficient.\n\n# generate the D Matrix\nsn = 0   # count source number as code walks through the data frame\nfor i in range(len(df)):\n    n1 = df.loc[i,'p node']\n    n2 = df.loc[i,'n node']\n    #cn1 = df.loc[i,'cp node'] # nodes for controlled sources\n    #cn2 = df.loc[i,'cn node']\n    #n_vout = df.loc[i,'Vout'] # node connected to op amp output\n\n    # process elements with input to D matrix\n    x = df.loc[i,'element'][0]   #get 1st letter of element name\n    if (x == 'V') or (x == 'O') or (x == 'E'):  # need to count V, E & O types\n        sn += 1   #increment source count\n\n    if x == 'L':\n        if i_unk &gt; 1:  #is D greater than 1 by 1?\n            D[sn,sn] += -s*sympify(df.loc[i,'element'])\n        else:\n            D[sn] += -s*sympify(df.loc[i,'element'])\n        sn += 1   #increment source count\n\n    if x == 'H':  # H: ccvs\n        # if there is a H type, D is m by m\n        # need to find the vn for Vname\n        # then stamp the matrix\n        vn1, vn2, df2_index = find_vname(df.loc[i,'Vname'])\n        D[sn,df2_index] += -sympify(df.loc[i,'element'].lower())\n        sn += 1   #increment source count\n\n    if x == 'F':  # F: cccs\n        # if there is a F type, D is m by m\n        # need to find the vn for Vname\n        # then stamp the matrix\n        vn1, vn2, df2_index = find_vname(df.loc[i,'Vname'])\n        D[sn,df2_index] += -sympify(df.loc[i,'element'].lower())\n        D[sn,sn] = 1\n        sn += 1   #increment source count\n\n    if x == 'K':  # K: coupled inductors, KXX LYY LZZ value\n        # if there is a K type, D is m by m\n        vn1, vn2, ind1_index = find_vname(df.loc[i,'Lname1'])  # get i_unk position for Lx\n        vn1, vn2, ind2_index = find_vname(df.loc[i,'Lname2'])  # get i_unk position for Ly\n        # enter sM on diagonals = value*sqrt(LXX*LZZ)\n\n        D[ind1_index,ind2_index] += -s*sympify('M{:s}'.format(df.loc[i,'element'].lower()[1:]))  # s*Mxx\n        D[ind2_index,ind1_index] += -s*sympify('M{:s}'.format(df.loc[i,'element'].lower()[1:]))  # -s*Mxx\n\n# display the The D matrix\nD\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0\\\\0 & - f_{1} & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & - L_{1} s\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "OLD SMNA_example.html#v-matrix",
    "href": "OLD SMNA_example.html#v-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "V matrix",
    "text": "V matrix\nThe V matrix is an n by 1 matrix formed of the node voltages, where n is the number of nodes. Each element in V corresponds to the voltage at the node.\nMaybe make small v’s v_1 so as not to confuse v1 with V1.\n\n# generate the V matrix\nfor i in range(num_nodes):\n    V[i] = sympify('v{:d}'.format(i+1))\n\nV  # display the V matrix\n\n\\(\\displaystyle \\left[\\begin{matrix}v_{1}\\\\v_{2}\\\\v_{3}\\\\v_{4}\\\\v_{5}\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "OLD SMNA_example.html#j-matrix",
    "href": "OLD SMNA_example.html#j-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "J matrix",
    "text": "J matrix\nThe J matrix is an m by 1 matrix, where m is the number of unknown currents. &gt;i_unk = num_v+num_opamps+num_vcvs+num_ccvs+num_ind+num_cccs\n\n# The J matrix is an mx1 matrix, with one entry for each i_unk from a source\n#sn = 0   # count i_unk source number\n#oan = 0   #count op amp number\nfor i in range(len(df2)):\n    # process all the unknown currents\n    J[i] = sympify('I_{:s}'.format(df2.loc[i,'element']))\n\nJ  # diplay the J matrix\n\n\\(\\displaystyle \\left[\\begin{matrix}I_{V1}\\\\I_{V2}\\\\I_{Ea1}\\\\I_{F1}\\\\I_{L1}\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "OLD SMNA_example.html#i-matrix",
    "href": "OLD SMNA_example.html#i-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "I matrix",
    "text": "I matrix\nThe I matrix is an n by 1 matrix, where n is the number of nodes. The value of each element of I is determined by the sum of current sources into the corresponding node. If there are no current sources connected to the node, the value is zero.\n\n# generate the I matrix, current sources have n2 = arrow end of the element\nfor i in range(len(df)):\n    n1 = df.loc[i,'p node']\n    n2 = df.loc[i,'n node']\n    # process all the passive elements, save conductance to temp value\n    x = df.loc[i,'element'][0]   #get 1st letter of element name\n    if x == 'I':\n        g = sympify(df.loc[i,'element'])\n        # sum the current into each node\n        if n1 != 0:\n            I[n1-1] -= g\n        if n2 != 0:\n            I[n2-1] += g\n\nI  # display the I matrix\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\0\\\\0\\\\- I_{1}\\\\0\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "OLD SMNA_example.html#ev-matrix",
    "href": "OLD SMNA_example.html#ev-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "Ev matrix",
    "text": "Ev matrix\nThe Ev matrix is mx1 and holds the values of the independent voltage sources.\n\n# generate the E matrix\nsn = 0   # count source number\nfor i in range(len(df)):\n    # process all the passive elements\n    x = df.loc[i,'element'][0]   #get 1st letter of element name\n    if x == 'V':\n        Ev[sn] = sympify(df.loc[i,'element'])\n        sn += 1\n\nEv   # display the E matrix\n\n\\(\\displaystyle \\left[\\begin{matrix}V_{1}\\\\V_{2}\\\\0\\\\0\\\\0\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "OLD SMNA_example.html#z-matrix",
    "href": "OLD SMNA_example.html#z-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "Z matrix",
    "text": "Z matrix\nThe Z matrix holds the independent voltage and current sources and is the combination of 2 smaller matrices I and Ev. The Z matrix is (m+n) by 1, n is the number of nodes, and m is the number of independent voltage sources. The I matrix is n by 1 and contains the sum of the currents through the passive elements into the corresponding node (either zero, or the sum of independent current sources). The Ev matrix is m by 1 and holds the values of the independent voltage sources.\n\nZ = I[:] + Ev[:]  # the + operator in python concatenates the lists\nZ  # display the Z matrix\n\n\\(\\displaystyle \\left[ 0, \\  0, \\  0, \\  - I_{1}, \\  0, \\  V_{1}, \\  V_{2}, \\  0, \\  0, \\  0\\right]\\)"
  },
  {
    "objectID": "OLD SMNA_example.html#x-matrix",
    "href": "OLD SMNA_example.html#x-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "X matrix",
    "text": "X matrix\nThe X matrix is an (n+m) by 1 vector that holds the unknown quantities (node voltages and the currents through the independent voltage sources). The top n elements are the n node voltages. The bottom m elements represent the currents through the m independent voltage sources in the circuit. The V matrix is n by 1 and holds the unknown voltages. The J matrix is m by 1 and holds the unknown currents through the voltage sources\n\nX = V[:] + J[:]  # the + operator in python concatenates the lists\nX  # display the X matrix\n\n\\(\\displaystyle \\left[ v_{1}, \\  v_{2}, \\  v_{3}, \\  v_{4}, \\  v_{5}, \\  I_{V1}, \\  I_{V2}, \\  I_{Ea1}, \\  I_{F1}, \\  I_{L1}\\right]\\)"
  },
  {
    "objectID": "OLD SMNA_example.html#a-matrix",
    "href": "OLD SMNA_example.html#a-matrix",
    "title": "Symbolic modified nodal analysis example",
    "section": "A matrix",
    "text": "A matrix\nThe A matrix is (m+n) by (m+n) and will be developed as the combination of 4 smaller matrices, G, B, C, and D.\n\nn = num_nodes\nm = i_unk\nA = zeros(m+n,m+n)\nfor i in range(n):\n    for j in range(n):\n        A[i,j] = G[i,j]\n\nif i_unk &gt; 1:\n    for i in range(n):\n        for j in range(m):\n            A[i,n+j] = B[i,j]\n            A[n+j,i] = C[j,i]\n\n    for i in range(m):\n        for j in range(m):\n            A[n+i,n+j] = D[i,j]\n\nif i_unk == 1:\n    for i in range(n):\n        A[i,n] = B[i]\n        A[n,i] = C[i]\n\nA  # display the A matrix\n\n\\(\\displaystyle \\left[\\begin{matrix}C_{1} s + \\frac{1}{R_{1}} & - C_{1} s & 0 & - \\frac{1}{R_{1}} & 0 & 1 & 0 & 0 & 0 & 0\\\\- C_{1} s & C_{1} s + \\frac{1}{R_{2}} & 0 & 0 & - \\frac{1}{R_{2}} & 0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & -1 & -1\\\\- \\frac{1}{R_{1}} & 0 & 0 & \\frac{1}{R_{1}} & 0 & 0 & 0 & 0 & 0 & 1\\\\0 & - \\frac{1}{R_{2}} & 0 & 0 & \\frac{1}{R_{2}} & 0 & -1 & 0 & 0 & 0\\\\1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & -1 & 0 & 0 & 0 & 0 & 0\\\\- ea_{1} & 0 & 1 & ea_{1} & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & - f_{1} & 0 & 1 & 0\\\\0 & 0 & -1 & 1 & 0 & 0 & 0 & 0 & 0 & - L_{1} s\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "OLD SMNA_example.html#generate-the-circuit-equations",
    "href": "OLD SMNA_example.html#generate-the-circuit-equations",
    "title": "Symbolic modified nodal analysis example",
    "section": "generate the circuit equations",
    "text": "generate the circuit equations\n\nequ = Eq(A*Matrix(X),Matrix(Z))\nequ\n\n\\(\\displaystyle \\left[\\begin{matrix}- C_{1} s v_{2} + I_{V1} + v_{1} \\left(C_{1} s + \\frac{1}{R_{1}}\\right) - \\frac{v_{4}}{R_{1}}\\\\- C_{1} s v_{1} + I_{F1} + v_{2} \\left(C_{1} s + \\frac{1}{R_{2}}\\right) - \\frac{v_{5}}{R_{2}}\\\\I_{Ea1} - I_{F1} - I_{L1}\\\\I_{L1} - \\frac{v_{1}}{R_{1}} + \\frac{v_{4}}{R_{1}}\\\\- I_{V2} - \\frac{v_{2}}{R_{2}} + \\frac{v_{5}}{R_{2}}\\\\v_{1}\\\\- v_{5}\\\\- ea_{1} v_{1} + ea_{1} v_{4} + v_{3}\\\\I_{F1} - I_{V2} f_{1}\\\\- I_{L1} L_{1} s - v_{3} + v_{4}\\end{matrix}\\right] = \\left[\\begin{matrix}0\\\\0\\\\0\\\\- I_{1}\\\\0\\\\V_{1}\\\\V_{2}\\\\0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\nBuilt a python dictionary of element values\n\nelement_value_keys = []\nelement_value_values = []\n\nfor i in range(len(df)):\n    if df.iloc[i]['element'][0] == 'F' or df.iloc[i]['element'][0] == 'E' or df.iloc[i]['element'][0] == 'G' or df.iloc[i]['element'][0] == 'H':\n        element_value_keys.append(var(df.iloc[i]['element'].lower()))\n        element_value_values.append(df.iloc[i]['value'])\n        #print('{:s}:{:f},'.format(df.iloc[i]['element'].lower(),df.iloc[i]['value']))\n    else:\n        element_value_keys.append(var(df.iloc[i]['element']))\n        element_value_values.append(df.iloc[i]['value'])\n        #print('{:s}:{:.4e},'.format(df.iloc[i]['element'],df.iloc[i]['value']))\n\nelement_values = dict(zip(element_value_keys, element_value_values))"
  },
  {
    "objectID": "OLD SMNA_example.html#symbolic-solution",
    "href": "OLD SMNA_example.html#symbolic-solution",
    "title": "Symbolic modified nodal analysis example",
    "section": "Symbolic solution",
    "text": "Symbolic solution\n\nsymbolic_solution = solve(equ,X)\nsymbolic_solution\n\n\\(\\displaystyle \\left\\{ I_{Ea1} : \\frac{- C_{1} I_{1} R_{1} R_{2} ea_{1} s - C_{1} I_{1} R_{1} R_{2} s - C_{1} L_{1} V_{1} f_{1} s^{2} - C_{1} L_{1} V_{2} f_{1} s^{2} - C_{1} R_{1} V_{1} ea_{1} f_{1} s - C_{1} R_{1} V_{1} f_{1} s - C_{1} R_{1} V_{2} ea_{1} f_{1} s - C_{1} R_{1} V_{2} f_{1} s + C_{1} R_{2} V_{1} s + I_{1} R_{1} ea_{1} f_{1} - I_{1} R_{1} ea_{1} + I_{1} R_{1} f_{1} - I_{1} R_{1} - V_{1} f_{1} + V_{1}}{C_{1} L_{1} R_{2} s^{2} + C_{1} R_{1} R_{2} ea_{1} s + C_{1} R_{1} R_{2} s - L_{1} f_{1} s + L_{1} s - R_{1} ea_{1} f_{1} + R_{1} ea_{1} - R_{1} f_{1} + R_{1}}, \\  I_{F1} : \\frac{- C_{1} V_{1} f_{1} s - C_{1} V_{2} f_{1} s}{C_{1} R_{2} s - f_{1} + 1}, \\  I_{L1} : \\frac{- I_{1} R_{1} ea_{1} - I_{1} R_{1} + V_{1}}{L_{1} s + R_{1} ea_{1} + R_{1}}, \\  I_{V1} : \\frac{- C_{1} I_{1} L_{1} R_{2} s^{2} + C_{1} L_{1} V_{1} f_{1} s^{2} - C_{1} L_{1} V_{1} s^{2} + C_{1} L_{1} V_{2} f_{1} s^{2} - C_{1} L_{1} V_{2} s^{2} + C_{1} R_{1} V_{1} ea_{1} f_{1} s - C_{1} R_{1} V_{1} ea_{1} s + C_{1} R_{1} V_{1} f_{1} s - C_{1} R_{1} V_{1} s + C_{1} R_{1} V_{2} ea_{1} f_{1} s - C_{1} R_{1} V_{2} ea_{1} s + C_{1} R_{1} V_{2} f_{1} s - C_{1} R_{1} V_{2} s - C_{1} R_{2} V_{1} s + I_{1} L_{1} f_{1} s - I_{1} L_{1} s + V_{1} f_{1} - V_{1}}{C_{1} L_{1} R_{2} s^{2} + C_{1} R_{1} R_{2} ea_{1} s + C_{1} R_{1} R_{2} s - L_{1} f_{1} s + L_{1} s - R_{1} ea_{1} f_{1} + R_{1} ea_{1} - R_{1} f_{1} + R_{1}}, \\  I_{V2} : \\frac{- C_{1} V_{1} s - C_{1} V_{2} s}{C_{1} R_{2} s - f_{1} + 1}, \\  v_{1} : V_{1}, \\  v_{2} : \\frac{C_{1} R_{2} V_{1} s + V_{2} f_{1} - V_{2}}{C_{1} R_{2} s - f_{1} + 1}, \\  v_{3} : \\frac{I_{1} L_{1} R_{1} ea_{1} s + R_{1} V_{1} ea_{1}}{L_{1} s + R_{1} ea_{1} + R_{1}}, \\  v_{4} : \\frac{- I_{1} L_{1} R_{1} s + L_{1} V_{1} s + R_{1} V_{1} ea_{1}}{L_{1} s + R_{1} ea_{1} + R_{1}}, \\  v_{5} : - V_{2}\\right\\}\\)"
  },
  {
    "objectID": "OLD SMNA_example.html#numeric-solution",
    "href": "OLD SMNA_example.html#numeric-solution",
    "title": "Symbolic modified nodal analysis example",
    "section": "Numeric solution",
    "text": "Numeric solution\nSubstitue the element values into the equations and solve for unknown node voltages and currents. Need to set the current source, I1, to zero.\n\nequ1a = equ.subs(element_values)\nequ1a\n\n\\(\\displaystyle \\left[\\begin{matrix}I_{V1} - 1.0 s v_{2} + v_{1} \\cdot \\left(1.0 s + 0.5\\right) - 0.5 v_{4}\\\\I_{F1} - 1.0 s v_{1} + v_{2} \\cdot \\left(1.0 s + 0.5\\right) - 0.5 v_{5}\\\\I_{Ea1} - I_{F1} - I_{L1}\\\\I_{L1} - 0.5 v_{1} + 0.5 v_{4}\\\\- I_{V2} - 0.5 v_{2} + 0.5 v_{5}\\\\v_{1}\\\\- v_{5}\\\\- 2.0 v_{1} + v_{3} + 2.0 v_{4}\\\\I_{F1} - 2.0 I_{V2}\\\\- 1.0 I_{L1} s - v_{3} + v_{4}\\end{matrix}\\right] = \\left[\\begin{matrix}0\\\\0\\\\0\\\\0\\\\0\\\\1.0\\\\0\\\\0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\nSolve for voltages and currents in terms of Laplace variable s.\n\nu1 = solve(equ1a,X)\nu1\n\n\\(\\displaystyle \\left\\{ I_{Ea1} : \\frac{- 2.0 s^{2} - 10.0 s - 1.0}{2.0 s^{2} + 11.0 s - 6.0}, \\  I_{F1} : - \\frac{2.0 s}{2.0 s - 1.0}, \\  I_{L1} : \\frac{1}{s + 6.0}, \\  I_{V1} : \\frac{s^{2} + 4.0 s + 1.0}{2.0 s^{2} + 11.0 s - 6.0}, \\  I_{V2} : - \\frac{s}{2.0 s - 1.0}, \\  v_{1} : 1.0, \\  v_{2} : \\frac{2.0 s}{2.0 s - 1.0}, \\  v_{3} : \\frac{4.0}{s + 6.0}, \\  v_{4} : \\frac{s + 4.0}{s + 6.0}, \\  v_{5} : 0.0\\right\\}\\)"
  },
  {
    "objectID": "OLD SMNA_example.html#ac-analysis",
    "href": "OLD SMNA_example.html#ac-analysis",
    "title": "Symbolic modified nodal analysis example",
    "section": "AC analysis",
    "text": "AC analysis\nSolve equations for \\(\\omega\\) equal to 1 radian per second, s = 1j.\n\nequ1a_1rad_per_s = equ1a.subs({s:1j})\nequ1a_1rad_per_s  # display the equations\n\n\\(\\displaystyle \\left[\\begin{matrix}I_{V1} + v_{1} \\cdot \\left(0.5 + 1.0 i\\right) - 1.0 i v_{2} - 0.5 v_{4}\\\\I_{F1} - 1.0 i v_{1} + v_{2} \\cdot \\left(0.5 + 1.0 i\\right) - 0.5 v_{5}\\\\I_{Ea1} - I_{F1} - I_{L1}\\\\I_{L1} - 0.5 v_{1} + 0.5 v_{4}\\\\- I_{V2} - 0.5 v_{2} + 0.5 v_{5}\\\\v_{1}\\\\- v_{5}\\\\- 2.0 v_{1} + v_{3} + 2.0 v_{4}\\\\I_{F1} - 2.0 I_{V2}\\\\- 1.0 i I_{L1} - v_{3} + v_{4}\\end{matrix}\\right] = \\left[\\begin{matrix}0\\\\0\\\\0\\\\0\\\\0\\\\1.0\\\\0\\\\0\\\\0\\\\0\\end{matrix}\\right]\\)\n\n\n\nans1 = solve(equ1a_1rad_per_s,X)\nans1\n\n\\(\\displaystyle \\left\\{ I_{Ea1} : -0.637837837837838 + 0.372972972972973 i, \\  I_{F1} : -0.8 + 0.4 i, \\  I_{L1} : 0.162162162162162 - 0.027027027027027 i, \\  I_{V1} : 0.237837837837838 - 0.172972972972973 i, \\  I_{V2} : -0.4 + 0.2 i, \\  v_{1} : 1.0, \\  v_{2} : 0.8 - 0.4 i, \\  v_{3} : 0.648648648648649 - 0.108108108108108 i, \\  v_{4} : 0.675675675675676 + 0.0540540540540541 i, \\  v_{5} : 0.0\\right\\}\\)\n\n\n\nfor name, value in ans1.items():\n    print('{:5s}: mag: {:10.6f} phase: {:11.5f} deg'.format(str(name),float(abs(value)),float(arg(value)*180/np.pi)))\n\nv1   : mag:   1.000000 phase:     0.00000 deg\nv2   : mag:   0.894427 phase:   -26.56505 deg\nv3   : mag:   0.657596 phase:    -9.46232 deg\nv4   : mag:   0.677834 phase:     4.57392 deg\nv5   : mag:   0.000000 phase:         nan deg\nI_V1 : mag:   0.294086 phase:   -36.02737 deg\nI_V2 : mag:   0.447214 phase:   153.43495 deg\nI_Ea1: mag:   0.738882 phase:   149.68322 deg\nI_F1 : mag:   0.894427 phase:   153.43495 deg\nI_L1 : mag:   0.164399 phase:    -9.46232 deg"
  },
  {
    "objectID": "OLD SMNA_example.html#ac-sweep",
    "href": "OLD SMNA_example.html#ac-sweep",
    "title": "Symbolic modified nodal analysis example",
    "section": "AC Sweep",
    "text": "AC Sweep\nLooking at node 4 voltage.\n\nv4, I_F1, I_Ea1, v1, v2, v5, I_L1, v3, I_V1, I_V2 = symbols(' v4 I_F1 I_Ea1 v1 v2 v5 I_L1 v3 I_V1 I_V2')\n\n\nH = u1[v4]\nH\n\n\\(\\displaystyle \\frac{s + 4.0}{s + 6.0}\\)\n\n\n\nnum, denom = fraction(H) #returns numerator and denominator\n\n# convert symbolic to numpy polynomial\na = np.array(Poly(num, s).all_coeffs(), dtype=float)\nb = np.array(Poly(denom, s).all_coeffs(), dtype=float)\nsystem_c1 = (a, b) # system for circuit 1\n\n\nx = np.linspace(0.1*2*np.pi, 100*2*np.pi, 10000, endpoint=True)\nw_c1, mag_c1, phase_c1 = signal.bode(system_c1, w=x) # returns: rad/s, mag in dB, phase in deg\n\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('magnitude, dB')\nax1.set_xlabel('frequency, Hz')\n\n#plt.semilogx(frequency, 20*np.log10(np.abs(voltage)),'-k')    # Bode magnitude plot\nplt.semilogx(w_c1/(2*np.pi), mag_c1,'-b')    # Bode magnitude plot\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((-30,20))\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:blue'\n\n#plt.semilogx(frequency, np.angle(voltage)*180/np.pi,':',color=color)  # Bode phase plot\nplt.semilogx(w_c1/(2*np.pi), phase_c1,':',color='tab:red')  # Bode phase plot\n\nax2.set_ylabel('phase, deg',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-5,25))\n\nplt.title('Bode plot')\nplt.show()"
  },
  {
    "objectID": "OLD NCLHv1 analysis.html",
    "href": "OLD NCLHv1 analysis.html",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "",
    "text": "Last update: 9 Nov 2022\nannual report: 2021\n10Q: Q3 of 2022\nshare price data from: 8/28/2022, \\$13.72\n\\(\\large{\\color {red} {\\text{Work in progress notes, 11-18-2022: still working on updates in quarterly data section.}}}\\)\n\\(\\large{\\color {red} {\\text{Concerns about declining equity and ability to fill ships back to 107% level.}}}\\)\n\\(\\large{\\color {red} {\\text{Frank's said he would not dicount to fill, but will acheive historic levels by 2 quarter of 2023.}}}\\)\nNeed to look at impact of future bookings (since this amount is a liability on the balance sheet) to debt ratios."
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#abstract",
    "href": "OLD NCLHv1 analysis.html#abstract",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Abstract",
    "text": "Abstract\nThis notebook was developed to analyze the financial performance of NCLH. The analysis presented primarily uses financial data prior to fiscal year 2019. Most of which is irrelevant now, since NCLH’s consolidated financial sheets are dramatically different following the shock of the pandemic. From a financial perspective, it’s not really possible to compare the finances of post pandemic NCL to the pre pandemic NCL. On account of the large discontinuity in operations, the company’s pre and post pandemic financials need to be considered separately. Click the link here to jump to my analysis of recent quarterly performance."
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#bottom-line-up-front",
    "href": "OLD NCLHv1 analysis.html#bottom-line-up-front",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Bottom line up front",
    "text": "Bottom line up front\nThe years 2020 to present are omitted from the DCF and NAIC analysis since NCLH and the other cruise lines suspended operations in March of 2020 due to the pandemic. My analysis of recent quarterly performance shows that NCLH will run out of cash sometime in 2023 if the company is not able to increase occupancy level close to 100%. Follow the link to the Conclusion."
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#introduction",
    "href": "OLD NCLHv1 analysis.html#introduction",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Introduction",
    "text": "Introduction\nNorwegian Cruise Line Holdings Ltd., NCLH together with its subsidiaries, operates as a cruise company in North America, Europe, the Asia-Pacific, and internationally. The company operates the Norwegian Cruise Line, Oceania Cruises, and Regent Seven Seas Cruises brands.\nSector(s): Consumer Cyclical Industry: Travel Services"
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#company-description",
    "href": "OLD NCLHv1 analysis.html#company-description",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Company description",
    "text": "Company description\nNorwegian Cruise Line Holdings operates a global cruise company with a portfolio of three brands: Norwegian Cruise Line, Oceania Cruises and Regent Seven Seas Cruises. Norwegian commenced operations from Miami in 1966, launching the modern cruise industry by offering weekly departures from Miami to the Caribbean. It is the third-largest cruise line in the world by passengers, controlling about 8.7% of the total worldwide share of the cruise market by passengers as of 2018.\nThe cruise line was founded as Norwegian Caribbean Line in 1966 by the Norwegian Knut Kloster and the Israeli Ted Arison, with the 8,666-ton, 140-m cruise ship/car ferry, Sunward, which in 1966 operated as a car ferry between Southampton UK and Gibraltar, for that one short season only. Arison soon left to form Carnival Cruise Lines, while Kloster acquired additional ships for Caribbean service. Norwegian pioneered many firsts in the cruise industry, such as the first Out Island Cruise, the first combined air-sea program (marketed as “Cloud 9 Cruises”), which combined low-cost air fares with the cruise, and first shipline to develop new ports in the Caribbean, such as Ocho Rios in Jamaica.\nNorwegians’s second and third ships, the Starward and Skyward, were the first newly built ships designed for the cruise line. Like the original Sunward of 1966, they had the capability to carry automobiles through a well-concealed stern door. Later, this area was turned into cabins and a two-deck movie theater, later to be used as a casino. Norwegian was responsible for many of the cruise innovations that have now become standard throughout the industry.\nNorwegian acquired Orient Lines in 1998. After talks, Norwegian itself was acquired in 2000 by Star Cruises, a subsidiary of Genting Hong Kong, part of the Malaysia-based Genting Group. In 2007, Star Cruises sold Orient Line’s Marco Polo to Transocean Tours, and Orient Lines ceased operations in early 2008.\nIn August 2007, Star Cruises sold 50% of Norwegian for \\$1 billion to US-based Apollo Management to strengthen Norwegian’s financial position.\nIn January 2008, the Apollo Funds acquired 50% of the outstanding ordinary share capital of NCLC. As part of this investment, the Apollo Funds assumed control of NCLC’s Board of Directors. Also, in January 2008, the TPG Viking Funds acquired, in the aggregate, 12.5% of NCLC’s outstanding share capital from the Apollo Funds.\nIn February 2011, NCLH, a Bermuda limited company, was formed.\nIn January 2013, NCLH completed its IPO and the ordinary shares of NCLC were exchanged for the ordinary shares of NCLH, and NCLH became the owner of 100% of the ordinary shares and parent company of NCLC (the “Corporate Reorganization”). In August 2013 and December 2013, NCLH completed the Secondary Offerings.\nIn November 2014, NCLH completed the acquisition of Prestige (Oceania and Regent brands). Frank J. Del Rio (founded Oceania in October 2002), became President and Chief Executive Officer of NCLH. Prior to that Kevin M. Sheehan served as the President and Chief Executive Officer of the Company since August 2010.\nSegment Reporting\nWe have concluded that our business has a single reportable segment. Each brand, Oceania, Regent and Norwegian, constitutes a business for which discrete financial information is available and management regularly reviews the operating results and, therefore, each brand is considered an operating segment. Our operating segments have similar economic characteristics, including similar margins and similar products and services; therefore, we aggregate all of the operating segments into one reportable segment.\nAs of December 31, 2020, NCLH had 28 ships with approximately 59,150 Berths and had orders for nine additional ships to be delivered through 2027. NCLH has nine ships on order across our portfolio of brands. For the Norwegian brand, Project Leonardo will introduce six additional ships with expected delivery dates from 2022 through 2027. For Regent Seven Seas Cruises, NCLH has one Explorer Class Ship on order for delivery in 2023. For Oceania Cruises, NCLH has two Allura Class Ships on order for delivery in 2023 and 2025.\nCOVID-19\nBeginning on March 13, 2020, NCLH suspended all cruise voyages in response to COVID-19. This suspension has been extended through May 31, 2021.\nThe resumption of operations will be dependent, in part, on NCLH’s ability to comply with various governmental regulations, the severity and duration of the COVID-19 pandemic, the lifting of various travel restrictions and travel bans issued by various countries and communities around the world, as well as port availability. NCLH expects a gradual phased relaunch of our ships after the voyage suspension period, with our ships initially operating at reduced occupancy levels. Our selection of itineraries in the short-term will be predicated by port availability and the safety of the destinations we visit.\nSince March 2020, NCLH has launched a series of capital markets transactions to bolster its financial position during the voyage suspension period, which in aggregate raised approximately $5.6 billion.\nNCLH has also taken several additional measures to improve their liquidity through deferring certain ship milestone payments, deferring certain debt amortization payments and extending certain maturities under our debt agreements, including under our agreements with export credit agencies (“ECAs”) and related governments. NCLH has also undertaken several proactive cost reduction and cash conservation measures to mitigate the financial and operational impacts of COVID-19, through the reduction of capital expenditures as well as reductions in operating expenses, including ship operating expenses and selling, general and administrative expenses.\nOn May 5, 2020, in a filing with the Securities and Exchange Commission, Norwegian Cruise Line Holdings (NCLH) said there is “substantial doubt” about its ability to continue as a “going concern” as it faces a liquidity crisis over the next twelve months.\nBy the next day, NCLH was able to secure over \\$2.2 billion of additional liquidity in oversubscribed capital markets transactions, but at a price:\n\n\\$400 million in common stock at \\$11 per share;\n\\$675 million in senior secured notes due 2024 at a 12.25% interest rate;\n\\$750 million in exchangeable notes due 2024 at 6% interest rate, and exchangeable at any time into common shares at \\$13.75; and\n\\$400 million private investment from a global private equity firm.\n\nOn May 7, 2020, NCLH CEO declared that the company has secured enough liquidity to get through potentially 18 months of zero revenues and may resume cruising later in 2020.\nOn July 25, 2021, Norwegian Jade set sail from Athens sailing the Greek Isles as the first ship in the Company’s fleet to resume sailing since the global suspension of voyages in March 2020. August 29 Oceania’s Marina sets sail from Stockholm, the first ship of the Oceania fleet.\nOn May 7 2022 NCL became the first major cruise operator to return the full fleet to service.\n}— Document revision history\n- 1/10/2022: Copied from VZ notebook and reorganized - Feb 2022: updated quick look, reorganized flow of calculations, corrected usage of financial rates, organized end sections - 23 Mar 2022: Cleaning up financial data spreadsheet. Removed NAIC tab. Removed duplicate revenue data. - 27 Mar 2022: MFG template copied from BMY - 3 May 2022: replaced np.linalg.lstsq with np.polyfit in NAIC forecast, added Future forecast based on historical data notes, Dilution notes, decision model - 6/19/2022: copied from MFG template, old NCLH files given v0 or OLD as filenames. - summer 2022: started looking at quarterly results to determine liquidity and default risk. }—"
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#analysis",
    "href": "OLD NCLHv1 analysis.html#analysis",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Analysis",
    "text": "Analysis\nThe following sections of this notebook contain the financial analysis for the company.\nContents \n\nStock screener results\n\nLoad financial spreadsheet\n\nDiscounted cash flow analysis, baseline\n\nDCF Scenarios\n\nNACI stock selection guide analysis\n\nFuture stock price\n\nDividend payout\n\nManagement performance\n\nDecision model\n\nConclusion\n\nNotes\n\nReferences\n\nNew section: Recent quarterly performance\n\n\nCode\nfrom datetime import datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\nfrom dateutil.parser import parse\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom dateutil import parser\nimport os\nfrom pandas.plotting import register_matplotlib_converters\nimport np_financial\nregister_matplotlib_converters()"
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#stock-screener-results",
    "href": "OLD NCLHv1 analysis.html#stock-screener-results",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "1) Stock screener results ",
    "text": "1) Stock screener results \nThis company was selected for analysis because I own 100 shares purchased solely to take part in the shareholder benefit as described below in the dividend section. Also, on occasion, I have large deposits with NCLH for future cruises and in the event of bankruptcy, those deposits may be at risk.\nCurrent news\nA review of the financial news sites from yahoo and google showed the following:\n- Recent news concerns cruise line debt and return to operations. In June 2022, Carnival provided a 2nd Quarter 2022 business update. The results seemed to be playing in the news as negative for Carnival and by extension the other cruise lines. - Low stock price is in the news. - On March 12, 2020, a class action complaint, Eric Douglas v. Norwegian Cruise Lines, Frank J. Del Rio and Mark A. Kempa, Case No. 1:20-CV-21107, was filed in the United States District Court for the Southern District of Florida, naming the Company, Frank J. Del Rio, the Company’s President and Chief Executive Officer, and Mark A. Kempa, the Company’s Executive Vice President and Chief Financial Officer, as defendants. Subsequently, two similar class action complaints were also filed in the United States District Court for the Southern District of Florida naming the same defendants. - On July 31, 2020, a consolidated amended class action complaint was filed by lead plaintiff’s counsel. The complaint asserts claims, purportedly brought on behalf of a class of shareholders, under Sections 10(b) and 20(a) of the Securities Exchange Act of 1934, and Rule 10b-5 promulgated thereunder, and allege that the Company made false and misleading statements to the market and customers about COVID-19. The complaint seeks unspecified damages and an award of costs and expenses, including reasonable attorneys’ fees, on behalf of a purported class of purchasers of our ordinary shares between February 20, 2020 and March 10, 2020.\nReview quarterly results\nQuarterly reports dating back to March 31, 2019 are analyzed below. D/E, Book value and liquidity levels indicate NCLH is a distressed company as a result of the pandemic.\nAverage daily volume\nAverage daily volume: 21,067,488\nDividend yield\nForward dividend yield: NA\nNotes from Crystal Cruises Bankruptcy\nHas anyone filed a claim with CFA Travel Insured?\nCF Travel Insured has acknowledged our claim and admit that our claim is valid. However, they are taking the position that they won’t pay any claims until they determine how much we will have received from the bankruptcy.\nThank you both. I did file a claim with ABC. I read over the entire policy and it says nothing about waiting for a bankruptcy settlement. They keep saying “we only pay for unreimbursed expenses and won’t know what is unreimbursed until the bankruptcy settlement is complete.”\nJune 2nd final date for claiming refunds via the assignee\nPersonally for those who are able to use it, the credit card dispute process is the best course of action - AMEX having proven their worth over and over again over the last 5 months\nThose that had a cruise departing from a US port need to submit through the ABC claims process. Your FMC surety bond is among the assets being held and payment for those claims will be made through the assignee.\nThe ABC claims process is “Assignment for Benefit of Creditors”, a liquidation process under Florida law, that Crystal has undertaken. All foreign flag cruise operators who have cruises leaving from the US (so only the WC would qualify) must post a surety bond of \\$32 million with the Federal Maritime Commission. That surety bond, for non-performance, is an asset of Crystal cruises that is part of the ABC liquidation process.\nOther than this surety bond, Crystal cruises has very few assets, and other creditors have more priority to those assets, while the surety bond can only be used to pay passengers back.\nCrystal Refund Roll Call\nDoes anyone have info. On Federal Maritime Commission procedure for making a claim…. presumably the FMC will cover up to \\$5,000 of non refunded money\nThe surety bond is an asset being managed by the ABC (Assignment for the Benefit of Creditors) group. You need to file a claim with ABC by 11 June. Make sure you state the departure port (as it has to be US the FMC bond to be valid). FMC stated this in an email to me.\nDoes anyone have info. On Federal Maritime Commission procedure for making a claim…. presumably the FMC will cover up to \\$5,000 of non refunded money"
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#load-financial-spreadsheet",
    "href": "OLD NCLHv1 analysis.html#load-financial-spreadsheet",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "2) Load financial spreadsheet ",
    "text": "2) Load financial spreadsheet \nData from consolidated financial statements and annual reports was collected and entered into a spreadsheet. All numerical data is converted from thousands or millions of dollars to dollars. The stock share price history was obtained from yahoo and is included as a tab in the spreadsheet. Other tabs in the spreadsheet are various worksheets.\n\n\nCode\nticker = 'NCLH' # company ticker symbol\nos.chdir('/home/jeff32/Documents/Dividend Investing/DCF data/')\n\nfile_name = ticker+'_Financials.xlsx'\ndf_dcf_sheet = pd.read_excel(file_name,sheet_name='DCF data')\ndf_metrics_sheet = pd.read_excel(file_name,sheet_name='metrics')\ndf_price_history = pd.read_excel(file_name,sheet_name='Historical Prices')\n\n# load quarterly data into dataframe\ndf_qrt_sheet = pd.read_excel(file_name,sheet_name='QRT data')\n\n# change the working director back to the Jupyter folder\nos.chdir('/home/jeff32/Documents/JupyterLab/Discount Cash Flow Analysis/')\n\n\n\n\nCode\n# convert dates from string to datetime format in stock price history\nprice_date_list = []\nfor i in range(len(df_price_history)):\n    price_date_list.append(datetime.strptime(str(df_price_history['Date'][i]), '%Y-%m-%d'))\n\ndf_price_history.insert(0, 'datetime', price_date_list)  # insert a new column with datetime data\ndf_price_history.sort_values(by=['datetime'], inplace=True) # sort data frame by datetime\n\ndf_price_history.set_index('datetime',inplace=True)\n\n#df_price_history.head()\n\n\n\n2.1) Format data frame \nGenerate a new data frame that holds the financial data needed for the DCF model. Data from financial statements is copied into a spreadsheet which contains the data used in the analysis. The data in the DCF_data tab is in a consistent format for ease of use by this notebook. Standard names are used for the rows and columns.\n\n\nCode\n#column names: fiscal years \nfy_data = df_dcf_sheet.columns[1:].values.astype('datetime64[Y]')-1970\n#line 0: Total revenue  \nrevenue_data = df_dcf_sheet.iloc[0].to_numpy()[1:].astype('float')\n#line 1: Cost of goods sold\nCost_of_goods_sold_data = df_dcf_sheet.iloc[1].to_numpy()[1:].astype('float')\n#line 2: General and administrative\nGeneral_and_administrative_data = df_dcf_sheet.iloc[2].to_numpy()[1:].astype('float')\n#line 3: Research and development\nResearch_and_development_data = df_dcf_sheet.iloc[3].to_numpy()[1:].astype('float')\n#line 4: Depreciation and amortization\nDepreciation_and_amortization_data = df_dcf_sheet.iloc[4].to_numpy()[1:].astype('float')\n#line 5: Investment\nInvestment_data = df_dcf_sheet.iloc[5].to_numpy()[1:].astype('float')\n# line 6: Income before income taxes\nIncome_before_income_taxes_data = df_dcf_sheet.iloc[6].to_numpy()[1:].astype('float')\n# line 7: Income tax\nIncome_tax_data = df_dcf_sheet.iloc[7].to_numpy()[1:].astype('float')\n# line 8: Accounts receivable\nAccounts_receivable_data = df_dcf_sheet.iloc[8].to_numpy()[1:].astype('float')\n# line 9: Inventories\nInventories_data = df_dcf_sheet.iloc[9].to_numpy()[1:].astype('float')\n# line 10: Accounts payable\nAccounts_payable_data = df_dcf_sheet.iloc[10].to_numpy()[1:].astype('float')\n# line 11: Current assets\nCurrent_assets_data = df_dcf_sheet.iloc[11].to_numpy()[1:].astype('float')\n# line 12: Current liabilities\nCurrent_liabilities_data = df_dcf_sheet.iloc[12].to_numpy()[1:].astype('float')\n# line 13: Long term debt\nLong_term_debt_data = df_dcf_sheet.iloc[13].to_numpy()[1:].astype('float')\n# line 14: Shares outstanding\nShares_outstanding_data = df_dcf_sheet.iloc[14].to_numpy()[1:].astype('float')\n\n\n\n\nCode\n# make a new data frame to store selected financial data\ndf_dcf_data = pd.DataFrame(data={\n    'FY':fy_data[::-1],\n    'revenue':revenue_data[::-1],\n    'cost_of_goods_sold':Cost_of_goods_sold_data[::-1],\n    'general_and_administrative':General_and_administrative_data[::-1],\n    'research_and_development':Research_and_development_data[::-1],\n    'depreciation':Depreciation_and_amortization_data[::-1],\n    'investment':Investment_data[::-1],\n    'income_before_income_taxes':Income_before_income_taxes_data[::-1],\n    'income_tax':Income_tax_data[::-1],\n    'accounts_receivable':Accounts_receivable_data[::-1],\n    'inventories':Inventories_data[::-1],\n    'accounts_payable':Accounts_payable_data[::-1], \n    'current_assets':Current_assets_data[::-1],\n    'current_liabilities':Current_liabilities_data[::-1],\n    'long_term_debt':Long_term_debt_data[::-1],\n    'shares_outstanding':Shares_outstanding_data[::-1]\n    })\n\n#df_dcf_data"
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#discounted-cash-flow-analysis-baseline",
    "href": "OLD NCLHv1 analysis.html#discounted-cash-flow-analysis-baseline",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "3) Discounted cash flow analysis, baseline ",
    "text": "3) Discounted cash flow analysis, baseline \nDiscounted cash flow (DCF) is a valuation method used to estimate the value of an investment based on its expected future cash flows. DCF analysis attempts to figure out the value of an investment today, based on projections of how much money it will generate in the future. In finance, discounted cash flow (DCF) analysis is a method of valuing a security, project, company, or asset using the concepts of the time value of money. The DCF method used in this notebook follows [1].\nThe value of any financial investment equals the present value of the expected future cash flows, discounted for risk and timing of these cash flows. The DCF method to value stocks is a four step process.\n1. Develop a set of future free cash flows for the corporation based on revenue growth, net operating profit margin, income tax rate and fix and working capital requirements. 2. Estimate the discount rate for the cash flows based on expected timing and risk. 3. Discount the cash flows and total them to calculate the value for the corporation as a whole. 4. Subtract the debt, preferred stock value and other claims and divide by the number of shares outstanding to get the intrinsic value.\nSections - Revenue growth rate - Net operating profit margin - Tax rate - Depreciation Rate - Investment Rate - Working Capital Rate - Current Assets - Current Liabilities - Value of Debt Outstanding - Current stock price - Shares outstanding - 10 year treasury bond yield - Bond yield spread to treasury - Preferred stock yield - Equity risk premium - Company specific beta - DCF model inputs - Future cash flows\n\nFuture forecast based on historical data\nThe DCF model uses historical financial data to estimate future cash flows. However, future changes are largely unpredictable, so we assume that the past record can be used as a rough guide to the future. The more questionable this assumption is, the less valuable is the analysis. So the DCF model is more useful when applied to stable well established companies, since companies with stable earnings are easier to forecast.\n\n\nComments about DCF analysis of a distressed company\nThe DCF analysis presented below was performed for years 2019 and prior, which are the pre-covid years. Companies with negative earnings are difficult to evaluate with DCF. NCLH is suffering from economic distress from strategic problems from the pandemic. As a result, there is financial distress where income, cash flow and the accumulation of large amounts of debt relative to equity weigh heavily on the company’s future viability. The consequent result of near term low or negative earnings and high debt load may make it difficult to access new debt.\nDCF Scenario 2 is presented below, which assumes that NCLH returns to pre-pandemic earnings and values the company in light of higher interest rates and the large debt accumulated.\n\n\nRevenue growth rate \nThe revenue growth rate (also sometimes called net sales) of the corporation plus any other revenues associated with the main operations of the business. It does not include dividends, interest income or non-operating income. Historic revenue data is obtained from consolidated income statements. The year over year change in revenue is calculated and converted to a percent, then an average revenue growth rate is calculated.\nAdjustments\nNo adjustments for this company.\n\n\nCode\n# calculate the percent change in revenue\npcr = np.zeros(len(df_dcf_data['revenue'].to_numpy())) # percent change in revenue\nfor i in range(len(df_dcf_data['revenue'].to_numpy()[0:-1])):\n    pcr[i+1] = ((df_dcf_data['revenue'].to_numpy()[i+1] - df_dcf_data['revenue'].to_numpy()[i])/\n                df_dcf_data['revenue'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Revenue, $B')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['revenue']/1e9, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcr,'+-g')\n    \nax2.set_ylabel('% Change in revenue',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,50))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Revenue')\nplt.show()\n\n\n\n\n\nObservation:\nIn November 2014, NCLH added Oceania and Regent brands, so 2015 and forward, revenue includes these ships, and Frank J. Del Rio (founded Oceania in October 2002) became President and Chief Executive Officer of NCLH. The acquisition of Oceania and Regent is probably the reason for the spike in % change in revenue in 2015. Afterwards, the revenue growth rate returns to the historic mean.\nClearly seen is the dramatic reduction in revenue in years 2020 and 2021 due to the suspension of operations in March of 2020\nCalculate the average revenue growth rate using the years 2016 to 2019.\n\n\nCode\nrgr_avg = pcr[-6:-2].mean()/100 #  years 2016 to 2019\nprint('average revenue growth rate pre Covid: {:.2f}%'.format(rgr_avg*100))\n\n\naverage revenue growth rate pre Covid: -94.51%\n\n\n\n\nNet operating profit margin \nNet Operating Profit should reflect the future revenue generating ability and expense requirements of the operating business that comprise the ongoing operations of the company.\n\\(\\text{NOPM} = \\frac{\\text{Revenue} - \\text{Expenses}}{\\text{Revenue}}\\)\n\\(\\text{Expenses} = \\text{Cost of Goods Sold (CGS)} + \\text{General and Administrative (G&A)} + \\text{Research and Development (R&D)}\\)\nGeneral and Administrative (G&A) is also called Sales, General and Administrative (SG&A)\nAdjustments\nExpenses are the following:\n1. Cost of Goods Sold (CGS) - Commissions, transportation and other - Onboard and other - Payroll and related - Fuel - Food - Other 2. Selling, General & Administrative (SGA)\nNCLH calls GSA, Marketing, general and administrative. The marketing, general and administrative expense also covers the following items: - Non-cash share-based compensation expenses related to equity awards - Severance payments related to restructuring costs - Expenses related to the redeployment of Norwegian Joy from Asia to the U.S. and the closing of the Shanghai office\nThe cruise line does not report any R&D expenses. Impairment loss for 2020 was 1,607,797. This has not been included in the spreadsheet as a line item.\nOnboard and other revenue primarily consists of revenue from gaming, beverage sales, shore excursions, specialty dining, retail sales, spa services and photo services. Our onboard revenue is derived from onboard activities we perform directly or that are performed by independent concessionaires, from which we receive a share of their revenue.\n\n\nCode\n# NOP = (Revenue - Expenses)\nnop = (df_dcf_data['revenue'].to_numpy() - \\\n    (df_dcf_data['cost_of_goods_sold'].to_numpy() + \\\n    df_dcf_data['general_and_administrative'].to_numpy() + \\\n    df_dcf_data['research_and_development'].to_numpy()) )\n\n# net operating profit margin as percent of revenue\nnopm = nop/df_dcf_data['revenue'].to_numpy()\n\n# plot as four grouped bar chart with labels on right and working capital rate on left\n# calculate position of bars\nx1_bar_position = []\nx2_bar_position = []\nx3_bar_position = []\nx4_bar_position = []\nfor i in df_dcf_data['FY']:\n    x1_bar_position.append(i-relativedelta(months=3))\n    x2_bar_position.append(i-relativedelta(months=1))\n    x3_bar_position.append(i+relativedelta(months=1))\n    x4_bar_position.append(i+relativedelta(months=3))\n    \nwidth = 40  # the width of the bars\n    \n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Net operating profit, \\\\$B')\n\nax1.bar(x1_bar_position,df_dcf_data['cost_of_goods_sold'].to_numpy()/1e9, width,label='CGS')\nax1.bar(x2_bar_position,df_dcf_data['general_and_administrative'].to_numpy()/1e9, width,label='G&A')\nax1.bar(x3_bar_position,df_dcf_data['research_and_development'].to_numpy()/1e9, width,label='R&D')\nax1.bar(x4_bar_position,nop/1e9, width,label='NOP')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,4))\nax1.legend()\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:cyan'\n\nax2.plot(df_dcf_data['FY'],nopm*100,'+-c')\n    \nax2.set_ylabel('% NOPM',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,40))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Net Operating Profit')\nplt.show()\n\n\n\n\n\nObservation\nPrior to the suspension of operations, the NOPM was above 25%. Calculate the average nopm for pre covid years.\n\n\nCode\n#Average net operating profit margin\nnopm_avg = nopm[-6:-2].mean() #  years 2016 to 2019\nprint('average net operating profit margin pre Covid: {:.2f}%'.format(nopm_avg*100))\n\n\naverage net operating profit margin pre Covid: -1.00%\n\n\n\n\nTax rate \nTax payments are taken from the consolidated income statement, provision for income taxes. The effect of taxes on profits is accounted for.\n\\(\\text{Tax rate} = \\frac{\\text{Income taxes}}{\\text{Income before income taxes}}\\)\nAdjustments\nIn February 2011, NCLH, a Bermuda limited company, was formed. Under current Bermuda law, NCLH is not subject to tax on income and capital gains. NCLH has received from the Minister of Finance under The Exempted Undertakings Tax Protection Act 1966, as amended, an assurance that, in the event that Bermuda enacts legislation imposing tax computed on profits, income, any capital asset, gain or appreciation, or any tax in the nature of estate duty or inheritance, then the imposition of any such tax shall not be applicable to NCLH or to any of our operations or shares, debentures or other obligations, until March 31, 2035.\nNCLH and NCLC are tax residents of the U.K. and are subject to normal U.K. corporation tax. During 2013, NCLH implemented a restructuring plan to provide a global tax platform for international expansion. As part of the plan, the Company became a tax resident of the U.K. As such, it qualifies for relief from U.S. Branch Profits taxes under the U.S.-U.K. Tax Treaty. In addition, the restructuring resulted in additional interest and depreciation which reduced the Company’s overall income tax expense.\nNCLH under Section 883 of the Code and the related regulations, as a foreign corporation is exempt from U.S. federal income taxation on its U.S. source income derived from the international operation of ships (“shipping income”)\nNCLH believes and has taken the position that substantially all of NCLH’s income, including the income of its ship-owning subsidiaries, is properly categorized as shipping income, and that we do not have a material amount of non-qualifying income.\nEconomic Substance Requirements\nNCLH and NCLC are exempted companies formed under the laws of Bermuda and some of their subsidiaries have been formed in Bermuda, Guernsey, Isle of Man, British Virgin Islands, Cayman Islands or the Bahamas. In June 2018, the European Union issued a scoping paper which set out economic substance requirements that targeted international financial centers, including the jurisdictions listed above, were required to adopt before 2019 with regard to relevant entities based in those jurisdictions.\nPursuant to the legislation passed in each jurisdiction, entities subject to each jurisdiction’s laws that carry out relevant activities as specified in such laws, are required to demonstrate substantial economic substance in that jurisdiction. In general terms, substantial economic substance means:\n- the entity is actually directed and managed in the jurisdiction;\n- core income-generating activities relating to the applicable relevant activity are performed in the jurisdiction; - - there are adequate employees in the jurisdiction;\n- the entity maintains adequate physical presence in the jurisdiction; and\n- there is adequate operating expenditure in the jurisdiction.\nNCLH has evaluated their activities and their subsidiaries and have concluded that in some cases, those activities are ‘relevant activities’ for the purposes of the applicable economic substance laws and that, consequently, certain entities within our organization will be required to demonstrate compliance with these economic substance requirements. NCLH may be subject to increased costs and our management team may be required to devote significant time to satisfying economic substance requirements in certain of these jurisdictions. If such entities cannot establish compliance with these requirements, we may be liable for penalties and fines in the applicable jurisdictions and/or required to re-domicile such entities to different jurisdictions.\n\n\nCode\n# plot as Grouped bar chart with labels on right and tax rate on left\n# calculate position of bars\nx1_bar_position = []\nx2_bar_position = []\nfor i in df_dcf_data['FY']:\n    x1_bar_position.append(i-relativedelta(months=1))\n    x2_bar_position.append(i+relativedelta(months=1))\n\n# calculate tax rate\ntax_rate = df_dcf_data['income_tax']/df_dcf_data['income_before_income_taxes']\n\nwidth = 50  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, \\\\$M')\n\nrects1 = ax1.bar(x1_bar_position,df_dcf_data['income_before_income_taxes']/1e6, width,\n    label='Income before income taxes')\nrects2 = ax1.bar(x2_bar_position,df_dcf_data['income_tax']/1e6, width,\n    label='Income taxes')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((-20,20))\nax1.legend()\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],tax_rate * 100,'+-g')\n    \nax2.set_ylabel('% Tax rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-40,100))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Tax Rate')\nplt.show()\n\n\n\n\n\nObservation:\nNCLH in all but one year, 2019 received an income tax benefit. There is a lot of information in the annual and quarterly reports on NCLH’s tax situation. The acquisition of Prestige in 2014 included deferred tax assets. Certain foreign corporations are exempt from U. S. federal income or branch profits tax on U.S.-source income derived from or incidental to the international operation of ships.\nCalculate average tax rate for pre covid years.\n\n\nCode\n# Average tax rate\ntax_rate_avg = tax_rate[-6:-2].mean()\nprint('average tax rate: {:.2f}%'.format(tax_rate_avg*100))\n\n\naverage tax rate: -0.13%\n\n\n\n\nDepreciation Rate \nThe depreciation rate is used to project the future net investment cash flows. The effect is to reduce the amount of FCFF. Depreciation amounts are from the Consolidated Statement of Cash Flows, Depreciation and Amortization.\n\\(\\text{Depreciation Rate}=\\frac{\\text{Depreciation and Amortization}}{\\text{Revenues}}\\)\nDepreciation is the write off or expensing of a percentage of the historical cost of an asset over the asset’s useful life. Property, plant and equipment (PP&E) are long term or non current assets owned or controlled by the company and used to manufacture and or sell the company’s products. The balance sheet typically shows all categories of PP&E grouped together, net of accumulated depreciation. Depreciation represents wear and tear on an asset or the fact that an asset gets used up over time. Companies record depreciation expense in the income statement every year for all depreciable assets in service or used by the company during the year. The difference between GAAP and Tax Accounting methods is handled through deferred taxes.\nAmortization is the write off or expensing of the cost of a financial instrument or an intangible asset over the shorter of its useful life or legal life. Amortization is similar to depreciation and reflects the declining useful life and value of the intangible asset over time. Companies in research and development intensive fields typically have many patents. Such industries include high technology, pharmaceuticals and chemicals.\n\n\nCode\n# depreciation rate\ndepreciation_rate = df_dcf_data['depreciation'] / df_dcf_data['revenue'].to_numpy()\n\n# plot depreciation on left and rate on right\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $B')\n\nax1.bar(df_dcf_data['FY'],df_dcf_data['depreciation']/1e9, width=100,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:Blue'\n\nax2.plot(df_dcf_data['FY'],depreciation_rate*100,'+-')\n    \nax2.set_ylabel('% Depreciation rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((0,30))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Depreciation')\nplt.show()\n\n\n\n\n\nObservation:\nDepreciation of ships is computed on a straight-line basis over the weighted average useful lives of primarily 30 years after a 15% reduction for the estimated residual value of the ship.\nShip improvement costs that NCLH believes add value to our ships are capitalized to the ship and depreciated over the shorter of the improvements’ estimated useful lives or the remaining useful life of the ship. When they record the retirement of a ship component included within the ship’s cost basis, they estimate the net book value of the component being retired and remove it from the ship’s cost basis.\nRepairs and maintenance activities are charged to expense as incurred. We account for Dry-dock costs under the direct expense method which requires us to expense all Dry-dock costs as incurred.\nNCLH determines the weighted average useful lives of their ships based primarily on our estimates of the useful lives of the ships’ major component systems on the date of acquisition, such as cabins, main diesels, main electric, superstructure and hull. The useful lives of ship improvements are estimated based on the economic lives of the new components. In addition, to determine the useful lives of the ship or ship components, we consider the impact of the historical useful lives of similar assets, manufacturer recommended lives and anticipated changes in technological conditions.\nCalculate average depreciation rate for pre-covid years.\n\n\nCode\n# average depreciation rate\ndepreciation_rate_avg = depreciation_rate[-6:-2].mean()\nprint('average depreciation rate: {:.2f}%'.format(depreciation_rate_avg*100))\n\n\naverage depreciation rate: 21.20%\n\n\n\n\nInvestment Rate \nTaken from Consolidated Statement of Cash Flows, Cash used for investing activities. Net investment in the dollar amount needed to support the growth of the firm. Included investments in properties, plant equipment in excess of the depreciation expenses associated with past investments. Net investment decreases the amount of money available to the stockholders. Investment in property, plant and equipment is necessary to both maintain service and sales and also to grow revenues and profits. Investment amounts should include capital expenditures and research and development.\n\\(Ir=\\frac {\\text {Capital Expenditures}}{\\text{Revenues}}\\)\nFor this company, the yearly investment amounts are taken from the Consolidated Statements of Cash Flows, Net Cash Used in Investing Activities.\nAdjustments\nSee 2020 annual report pdf page 125 for Ship Construction Contracts\n2018 annual report:\nSophisticated and efficient maintenance and operations systems support the technical superiority and modern look of our fleet. In addition to routine repairs and maintenance performed on an ongoing basis and in accordance with applicable requirements, each of our ships is generally taken out of service, approximately every 24 to 60 months, for a period of one or more weeks for scheduled maintenance work, repairs and improvements performed in Dry-dock.\nDry-dock interval is a statutory requirement controlled under IMO requirements reflected in chapters of the International Convention of the Safety of Life at Seas (“SOLAS”) and to some extent the International Load Lines Convention. Under these regulations, it is required that a passenger ship Dry-dock once in five years (depending on age of vessel), twice in 5 years (depending on flag state and age of vessel) and the maximum interval between each Dry-dock cannot exceed 3 years (depending age of vessel and flag state).\nHowever, most of our international ships qualify under a special exemption provided by the Bahamas and/or Marshall Islands (flag state), as applicable, after meeting certain criteria set forth by the ship’s flag state to Dry-dock once every 5 years.\nFuture capital commitments consist of contracted commitments, including ship construction contracts, and future expected capital expenditures necessary for operations as well as our ship refurbishment projects. As of December 31, 2018, anticipated capital expenditures were \\$1.6 billion, \\$1.2 billion and \\$0.7 billion for the years ending December 31,2019, 2020 and 2021, respectively. We have export credit financing in place for the anticipated expenditures related to ship construction contracts of \\$0.6 billion, \\$0.5 billion and \\$0.2 billion for the years ending December 31, 2019, 2020 and 2021, respectively. These future expected capital expenditures will significantly increase our depreciation and amortization expense as we take delivery of the ships.\nProject Leonardo will introduce an additional six ships, each approximately 140,000 Gross Tons with approximately 3,300 Berths, with expected delivery dates from 2022 through 2027, subject to certain conditions. We have a Breakaway Plus Class Ship, Norwegian Encore, with approximately 168,000 Gross Tons with 4,000 Berths, on order for delivery in the fall of 2019. For the Regent brand, we have orders for two Explorer Class Ships, Seven Seas Splendor and an additional ship, to be delivered in 2020 and 2023, respectively. Each of the Explorer Class Ships will be approximately 55,000 Gross Tons and 750 Berths. For the Oceania Cruises brand, we have orders for two Allura Class Ships to be delivered in 2022 and 2025. Each of the Allura Class Ships will be approximately 67,000 Gross Tons and 1,200 Berths.\nThe combined contract prices of the 11 ships on order for delivery was approximately €7.9 billion, or \\$9.1 billion based on the euro/U.S. dollar exchange rate as of December 31, 2018. We have obtained export credit financing which is expected to fund approximately 80% of the contract price of each ship, subject to certain conditions. We do not anticipate any contractual breaches or cancellations to occur. However, if any such events were to occur, it could result in, among other things, the forfeiture of prior deposits or payments made by us and potential claims and impairment losses which may materially impact our business, financial condition and results of operations.\nCapitalized interest for the years ended December 31, 2018, 2017 and 2016 was \\$30.4 million, \\$29.0 million and \\$33.7 million, respectively, primarily associated with the construction of our newbuild ships.\nSee 2018 annual report pdf page 48 for Contractual Obligations,\nSee 2018 annual report pdf page 80 for Property and Equipment, Net, includes anounts for\n- Ships - Ships improvements - Ships under construction - Land and land improvements - Other\nShips under construction include progress payments to the shipyard, planning and design fees and other associated costs. Capitalized interest costs which were primarily associated with the construction or revitalization of ships amounted to \\$30.4 million, \\$29.0 million and \\$33.7 million for the years ended December 31, 2018, 2017 and 2016, respectively.\nSee 2018 annual report pdf page 93 for Ship Construction Contracts, for minimum annual payments for non-cancelable ship construction contracts.\n\n\nCode\n# investment rate\ninvestment_rate = df_dcf_data['investment'] / df_dcf_data['revenue'].to_numpy()\n\n# plot investment on left and rate on right\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, \\\\$B')\n\nax1.bar(df_dcf_data['FY'],df_dcf_data['investment']/1e9, width=100,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:Blue'\n\nax2.plot(df_dcf_data['FY'],investment_rate*100,'+-')\n    \nax2.set_ylabel('% New Investment Rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-10,40))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('New Investment')\nplt.show()\n\n\n\n\n\nObservation:\nCapital expenditures for new ships is NCLH’s largest investment expense. NCLH pays for this expense by financing activities, where, typically 80% of the construction cost is financed. Looking at the years 2016 to 2019, investments are increasing at a rate of about 25% of revenue.\nCalculate average investment rate for pre-covid years.\n\n\nCode\n# average investment rate\ninvestment_rate_avg = investment_rate[-6:-2].mean()\nprint('average investment rate: {:.2f}%'.format(investment_rate_avg*100))\n\n\naverage investment rate: 38.26%\n\n\n\n\nWorking Capital Rate \nWorking capital is needed to support the corporate sales effort of any company. Often a company’s incremental change in net working capital either positive or negative is approximately proportional to its change in revenue.\n\\(\\text{Working capital} = \\text{Accounts Receivable} + \\text{Inventories} - \\text{Accounts Payable}\\)\nWorking capital is a company’s net investment in its accounts receivable and its inventories (cash outflows), minus its accounts payable (a cash inflow). Working capital and taxes are cash outflows from the corporation that are not available to pay debts and stockholders.\nAdjustments\nNo adjustments for this company.\n\n\nCode\n# plot as four grouped bar chart with labels on right and working capital rate on left\n# calculate position of bars\nx1_bar_position = []\nx2_bar_position = []\nx3_bar_position = []\nx4_bar_position = []\nfor i in df_dcf_data['FY']:\n    x1_bar_position.append(i-relativedelta(months=3))\n    x2_bar_position.append(i-relativedelta(months=1))\n    x3_bar_position.append(i+relativedelta(months=1))\n    x4_bar_position.append(i+relativedelta(months=3))\n\n# calculate working capital rate\nworking_capital = (df_dcf_data['accounts_receivable'] + df_dcf_data['inventories']) - \\\n    df_dcf_data['accounts_payable']\nworking_capital_rate = working_capital / df_dcf_data['revenue']\n\nwidth = 40  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, \\\\$B')\n\nrects1 = ax1.bar(x1_bar_position,df_dcf_data['accounts_receivable']/1e9, width,\\\n    label='Accounts Receivable')\nrects2 = ax1.bar(x2_bar_position,df_dcf_data['inventories']/1e9, width, label='Inventory')\n\nrects2 = ax1.bar(x3_bar_position,df_dcf_data['accounts_payable']/1e9, width, label='Accounts Payable')\nrects2 = ax1.bar(x4_bar_position,working_capital/1e9, width, label='Working Capital')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((-50,200))\nax1.legend()\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:Blue'\n\nax2.plot(df_dcf_data['FY'],working_capital_rate * 100,'+-')\n    \nax2.set_ylabel('% Working Capital Rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,50))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Working Capital')\nplt.show()\n\n\n\n\n\nObservation:\nThe working capital amounts for some years is negative because the accounts payable is much higher. A review of the 2018 didn’t provide a clear indication for the large accounts payable amount.\nThe average working capital rate is calculated for the pre-covid years of 2016 to 2019.\n\n\nCode\n# average working capital rate\nworking_capital_rate_avg = working_capital_rate[-6:-2].mean()\nprint('average working capital rate: {:.2f}%'.format(working_capital_rate_avg*100))\n\n\naverage working capital rate: 0.94%\n\n\n\n\nCurrent assets \nTotal Current Assets from the most recent balance sheet statement of the company. Current assets include inventory, cash and accounts receivables.\nAdjustments\nNone for this company.\n\n\nCode\n# plot Short Term Assets\nwidth = 100  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nplt.bar(df_dcf_data['FY'],df_dcf_data['current_assets']/1e9, width)\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Current assets')\nplt.ylabel('dollars, \\\\$B')\n\nplt.grid()\nplt.show()\n\n\n\n\n\nObservation:\nIn 2020, NCLH increased cash on hand to pay for pandemic pause in activities.\nUse 2019 data for current assets.\n\n\nCode\nsta = df_dcf_data['current_assets'].iloc[-3]\nprint('current assets: ${:.2f}B'.format(sta/1e9))\n\n\ncurrent assets: $3.56B\n\n\n\n\nCurrent liabilities \nTotal Current Liabilities from the most recent balance sheet consolidated statement.\nAdjustments\nNone for this company.\n\n\nCode\n# plot Short Term Liabilities\n\nwidth = 100  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nplt.bar(df_dcf_data['FY'],df_dcf_data['current_liabilities']/1e9, width)\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Current liabilities')\nplt.ylabel('dollars, \\\\$B')\n\nplt.grid()\nplt.show()\n\n\n\n\n\nObservation:\nDuring the pandemic pause, NCLH has decreased their short term liabilities by scaling back operations in 2020. Then with the return to service the current liabilities increased to the 2019 level.\nUse average of 2016 to 2019 data for current liabilities.\n\n\nCode\nstl = df_dcf_data['current_liabilities'].iloc[6:-2].mean()\nprint('Average of current liabilities pre Covid: ${:.2f}B'.format(stl/1e9))\n\n\nAverage of current liabilities pre Covid: $2.88B\n\n\n\n\nValue of Debt Outstanding \nAmount of debt outstanding from the most recent balance sheet of the company.\nAdjustments\nNone for this company.\n\n\nCode\n# calculate the percent change in debt, pcd\npcd = np.zeros(len(df_dcf_data['long_term_debt'].to_numpy())) # percent change in debt\nfor i in range(len(df_dcf_data['long_term_debt'].to_numpy()[0:-1])):\n    pcd[i+1] = ((df_dcf_data['long_term_debt'].to_numpy()[i+1] - df_dcf_data['long_term_debt'].to_numpy()[i])/\n                df_dcf_data['long_term_debt'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars, $B')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['long_term_debt']/1e9, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcd,'+-g')\n    \nax2.set_ylabel('% Change in debt',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-40,100))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('debt')\nplt.show()\n\n\n\n\n\nObservation:\nNCLH has increased long term debt on account of Covid-19 in order to pay for activities during the pause.\nUse average for years 2016 to 2019 data for long term debt.\n\n\nCode\nvod = df_dcf_data['long_term_debt'].iloc[6:-2].mean()\nprint('average total long term debt and other pre Covid: ${:.2f}B'.format(vod/1e9))\n\n\naverage total long term debt and other pre Covid: $8.27B\n\n\n\n\nCurrent stock price \nMost recent stock price for the company. The current stock price is used to calculate the market value of the firm. Use the market value when looking at market capitalization for common stock.\n\n\nCode\ncsp = 13.72 # current stock price\nprint('current stock price: ${:,.2f}'.format(csp))\n\n\ncurrent stock price: $13.72\n\n\n\n\nShares outstanding \nThe number of shares outstanding is used to calculate the intrinsic stock value.\nUse 2019 data for shares outstanding.\n\n\nCode\nso = df_dcf_data['shares_outstanding'].iloc[-3] # shares outstanding\nprint('shares outstanding, basic: {:,.0f}'.format(so))\n\n\nshares outstanding, basic: 254,728,932\n\n\n\n\nCode\n# calculate the percent change in shares outstanding, pcso\npcso = np.zeros(len(df_dcf_data['shares_outstanding'].to_numpy())) # percent change in debt\nfor i in range(len(df_dcf_data['shares_outstanding'].to_numpy()[0:-1])):\n    pcso[i+1] = ((df_dcf_data['shares_outstanding'].to_numpy()[i+1] - df_dcf_data['shares_outstanding'].to_numpy()[i])/\n                df_dcf_data['shares_outstanding'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('shares outstanding, M')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['shares_outstanding']/1e6, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcso,'+-g')\n    \nax2.set_ylabel('% Change in shares outstanding',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-5,25))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Shares outstanding')\nplt.show()\n\n\n\n\n\n\n\nCode\nprint('average shares outstanding growth rate: {:.2f}%'.format(pcso[1:].mean()))\nprint('shares outstanding pre-Covid: {:,.0f}'.format(df_dcf_data['shares_outstanding'].iloc[-3]))\n\n\naverage shares outstanding growth rate: 7.67%\nshares outstanding pre-Covid: 254,728,932\n\n\nObservation:\nThe number of shares outstanding comes from the 2020 annual report, see page 50 and Consolidated Statements, pdf page 87. This number needs to be researched a bit more to and should include any dilution from convertible bonds. As of 2020 the basic shares were 254,728,932 and the diluted shares were 254,728,932.\n\n\nDilution\nDilution occurs when a company issues new shares that result in a decrease in existing stockholders’ ownership percentage of that company. Stock dilution can also occur when holders of stock options, such as company employees, or holders of other optionable securities exercise their options. When the number of shares outstanding increases, each existing stockholder owns a smaller, or diluted, percentage of the company, making each share less valuable.\nInvestigate why there is a historic growth trend in the number of shares outstanding. Search annual report for dilutive actions: - share sales - convertible debt - employee options\nSearch results:\nRecent common share issuance has been the most dilutive, almost doubling the number of shares outstanding. For example, in March 2021, NCLH completed an equity offering that resulted in 52,577,947 ordinary shares being issued for gross proceeds of $1.6 billion.\nA relatively smaller number of shares were granted for employee share based compensation, for example, in March 2022, NCLH granted 4.8 million time-based restricted share unit awards to our employees, which primarily vest in substantially equal installments over three years. Additionally, in March 2022, NCLH granted 1.9 million performance-based restricted share units to certain members of our management team, which vest upon the achievement of certain pre-established performance targets established through 2024 and the satisfaction of an additional time-based vesting requirement that generally requires continued employment through March 1, 2025.\nJune 16, 2022 Form 8K: Among other things, the Restated 2013 Plan reflects amendments to: i. increase the number of the Company’s ordinary shares that may be delivered pursuant to all awards granted under the Restated 2013 Plan by an additional 7,000,000 shares, from 32,375,106 shares to a new maximum aggregate limit of 39,375,106 shares;\n\n\nCode\nprint('value ordinary shares authorized for awards: ${:,.2f}B'.format(39375106*csp/1e9))\n\n\nvalue ordinary shares authorized for awards: $0.54B\n\n\n\n\n10 year treasury bond yield \nThe 10 year treasury yield is used as a measure of the risk free rate.\nYield: 3.0350%\niShares 7-10 Year Treasury Bond ETF (IEF)\nAverage Yield to Maturity: 3.04%\n\n\nCode\ntby = (3.035+3.04)/2/100  # 10 year treasury bond yield, average of data from sources listed above\nprint('10 year treasury bond yield: {:,.2f}%'.format(tby*100))\n\n\n10 year treasury bond yield: 3.04%\n\n\n\n\nBond yield spread to treasury \nThe spread to treasury implies that all corporate debt will have a higher yield than yields associated with comparable maturity US Treasury Bonds. The best way to determine default risk is to see how a particular company’s debt is trading in the market and compare it on a spread basis with comparable maturity yields.\nLook at the following or use a default rating system that is published by the three major rating agencies, Standards and Poors Corp, Moody’s Investor Services and Fitch & Company.\nPIMCO Active Bond Exchange-Traded Fund (BOND)\nYield: 2.95%\niShares 5-10 Year Investment Grade Corporate Bond ETF (IGIB)\nAverage Yield to Maturity: 4.74%\niShares 10+ Year Investment Grade Corporate Bond ETF (IGLB)\nAverage Yield to Maturity: 5.08%\nWeb resources: - http://www.standardpoor.com/\n- http://bond.yahoo.com/rates.html\n- http://www.moodys.com/cust/default.asp\n- http://www.fitchibca.com/corporate/index.cfm\n\n\nCode\nbystt = ((2.95+4.74+5.08)/3-tby)/100           # bond yield spread (average) to treasury spread\nprint('Bond yield spread to treasury: {:,.2f}%'.format(bystt*100))\n\n\nBond yield spread to treasury: 4.23%\n\n\n\n\nPreferred stock yield \nAmount of preferred stock outstanding from the most recent balance sheet of the company.\n\n\nCode\npsy = 0/100  # preferred stock yield\nprint('preferred stock yield: {:,.2f}%'.format(psy*100))\n\nvps = 0 # value of preferred stock\nprint('value of preferred stock: {:,.2f}'.format(vps))\n\n\npreferred stock yield: 0.00%\nvalue of preferred stock: 0.00\n\n\n\n\nEquity risk premium \nThe expected excess return a hypothetical average investor would require of a diversified portfolio of stock (assumed beta = 1.0) over the yield on the 10-year Treasury Bond. The equity risk premium has been going down over the years.\n- 1926 to 1990: 5.5%\n- 1962 to 1990: 3.25%\n- 1981 to 1990: 0.19%\nIn times of sustained economic growth the risk premium demanded by investors generally declines.\nI’m going to use 3% as the equity risk premium.\n\n\nCode\neq_rp = 3.0/100             # equity risk premium\nprint('Equity risk premium: {:,.2f}%'.format(eq_rp*100))\n\n\nEquity risk premium: 3.00%\n\n\n\n\nCompany specific beta \nThe Beta used is Beta of Equity. Beta is the monthly price change of a particular company relative to the monthly price change of the S&P 500. The time period for Beta is 5 years when available. This value can be obtained at yahoo finance.\nA measure of risk of an individual stock. It measures volatility of return - a higher beta means a higher risk. A financial model that uses Beta as its sole measure of risk (signal factor model) is called a Capital Asset Pricing Model (CAPM).\n\n\nCode\nbeta = 2.47 # company specific beta\nprint('Company specific beta: {:,.2f}'.format(beta))\n\n\nCompany specific beta: 2.47\n\n\n\n\nDCF model inputs \nBelow are the DCF model inputs. These values were calculated above.\n\n\nCode\n# various rates\nrgr = rgr_avg              # revenue growth rate\nprint('revenue growth rate: {:,.2f}%'.format(rgr*100))\nnopm = nopm_avg             # net operating profit margin\nprint('net operating profit margin: {:,.2f}%'.format(nopm*100))\ntr = tax_rate_avg               # tax rate\nprint('tax rate: {:,.2f}%'.format(tr*100))\ndr = depreciation_rate_avg              # depreciation rate (% of revenue)\nprint('depreciation rate: {:,.2f}%'.format(dr*100))\nir = investment_rate_avg              # investment rate (% of revenue)\nprint('investment rate: {:,.2f}%'.format(ir*100))\nwcr = working_capital_rate_avg            # working capital rate (% of revenue)\nprint('working capital rate: {:,.2f}%'.format(wcr*100))\n\n\nrevenue growth rate: -94.51%\nnet operating profit margin: -1.00%\ntax rate: -0.13%\ndepreciation rate: 21.20%\ninvestment rate: 38.26%\nworking capital rate: 0.94%\n\n\nExcess return period\nThe excess return period is based on a judgment call. The authors of [1] use the 1-5-7-10 rule. They group companies into one of four general categories and excess return periods. They use a 10 year excess return period to calculate what they would consider the maximum value. They use a more conservative 1 year, 5 year or 7 year return period to calculate a more reasonable or minimum value.\n- 1 year: Boring companies that operate in a highly competitive, low margin industry in which they have nothing particular going for them. - 5 year: Decent companies that have a recognizable name and decent reputation and perhaps a regulatory benefit (utility company), but can’t control pricing or growth. - 7 year: Good companies with good brand names, large companies of scale, good marketing channels and consumer identification (e.g. McDonald’s) - 10 year: Great companies with great growth potential, tremendous marketing power, band names and in-place benefits (e.g. Intel, Microsoft, Coca Cola, Disney)\nThe excess return period used for the base case is ten years, which should lead to a higher calculated intrinsic value.\nUse 2019 for starting revenue amount.\n\n\nCode\n# General Inputs\nfy_start = df_dcf_data['FY'].iloc[-1].year # fiscal year to start excess return period\nerp = 10 # excess return period, years\nrev_start = df_dcf_data['revenue'].to_numpy()[-3] # starting revenues for excess return period\nprint('starting year: {:.0f}'.format(fy_start))\nprint('excess return period: {:.0f} years'.format(erp))\nprint('starting revenues: ${:,.2f}B'.format(rev_start/1e9))\nprint('shares outstanding: {:,.0f}'.format(so))\n\n\nstarting year: 2022\nexcess return period: 10 years\nstarting revenues: $1.28B\nshares outstanding: 254,728,932\n\n\n\n\nCode\nps_mv = vps               # preferred stock, market value \nprint('preferred stock, market value : ${:,.2f}B'.format(ps_mv/1e9))\ncs_mv = csp*so            # common stock, market value \nprint('common stock, market value: ${:,.2f}B'.format(cs_mv/1e9))\n\n\npreferred stock, market value : $0.00B\ncommon stock, market value: $3.49B\n\n\nLong Term Debt, Market Value, ltd_mv\nUse the book value for long term debt. Various online resources can be used to research this item. These include, Bondsonline and Bloomberg. The book value of debt and preferred stock is an accounting measure that relates to how much money was raised by the company when each security was issued. The market value of debt and the preferred and common stock is the price that specific obligations would trade at in today’s market.\nLong term debt for firms can take one of two forms. It can be a long-term loan from a bank or other financial institution or it can be a long-term bond issued to financial markets, in which case the creditors are the investors in the bond. Firms often have long term obligations that are not captured in the long term debt item. These include obligations to lessors on assets that firms have leased, to employees in the form of pension fund and health care benefits yet to be paid, and to the government in the form of taxes deferred. In the last two decades, accountants have increasingly moved towards quantifying these liabilities and showing them as long term liabilities.\n\n\nCode\nltd_mv = vod              # market value of long term debt\ntmv = ltd_mv+ps_mv+cs_mv  # total market value \nprint('total market value: ${:,.2f}B'.format(tmv/1e9))\n\n\ntotal market value: $11.76B\n\n\nCost of Common Equity, cce\nThe expected excess return a hypothetical average investor would require of a diversified portfolio of stock (assumed beta = 1.0) over the yield on the 10-year Treasury Bond. The annual rate of return that an investor expects to earn when investing in shares of a company is known as the cost of common equity. It includes dividends and increases in the market value.\n\n\nCode\ncce = tby+beta*eq_rp      # cost of common equity or the expected return for the stock\nprint('cost of common equity: {:,.2f}%'.format(cce*100))\n\n\ncost of common equity: 10.45%\n\n\nLong Term Debt, Average Yield, ltd_ay\nThe total cost of long term debt.\n\n\nCode\nltd_ay = tby+bystt        # long term debt average yield\nprint('long term debt average yield: {:,.2f}%'.format(ltd_ay*100))\n\n\nlong term debt average yield: 7.26%\n\n\nLong Term Debt, After Tax Yield, ltd_aty\nThe tax benefits of long term debt. Interest payments are tax deductible for the company.\n\n\nCode\nltd_aty = ltd_ay*(1-tr)   # long term debt after tax yield\nprint('long term debt after tax yield: {:,.2f}%'.format(ltd_aty*100))\n\nltd_pc = vod/tmv          # weight for long term debt \nltd_ate = ltd_aty*ltd_pc  # after tax effect of long term debt \nps_ay = psy               # preferred stock, average yield \nps_aty = ps_ay            # preferred stock, average yield \nprint('preferred stock, average yield: {:,.2f}%'.format(ps_aty*100))\n\nps_pc = ps_mv/tmv         # preferred stock, % capital \nps_ate = ps_aty*ps_pc     # preferred stock, after tax effect \ncs_ay = cce               # common stock, average yield \ncs_aty = cce              # common stock, after tax yield \nprint('common stock, after tax yield: {:,.2f}%'.format(cs_aty*100))\n\ncs_pc = cs_mv/tmv         # common stock, % capital \ncs_ate = cs_aty*cs_pc     # common stock, after tax effect \nprint('common stock, after tax effet: {:,.2f}%'.format(cs_ate*100))\n\ntate = ltd_ate+ps_ate+cs_ate # total after tax effect \nprint('total after tax effect: {:,.2f}%'.format(tate*100))\ntpc = ltd_pc+ps_pc+cs_pc     # total % Capital\nprint('total % Capital: {:,.2f}%'.format(tpc*100))\n\n\nlong term debt after tax yield: 7.27%\npreferred stock, average yield: 0.00%\ncommon stock, after tax yield: 10.45%\ncommon stock, after tax effet: 3.10%\ntotal after tax effect: 8.22%\ntotal % Capital: 100.00%\n\n\nWeighted average cost of capital\nA company’s weighted average cost of capital (WACC) is the weighted average of the company’s current cost of debt and equity calculated by using current debt, preferred stock and common stock market values. The WACC of the company, calculated after tax, is the discount rate used in the DCF valuation procedures. The WACC, which is the cost of the different components of financing used by the firm, weighted by their market value proportions. These include debt, preferred stock, and common stock.\nWACC: Weighted Average Cost of Capital, the rate used to discount cash flows, based on the following three factors. 1. Base rate of return. 2. Expected return based on debt and preferred stock. 3. Expected return on common stock and Beta.\nAll adjusted for the tax advantage of interest payments and the percentage of debt, preferred stock and common stock.\n\n\nCode\nwacc = tate\nprint('weighted average cost of capital: {:.1f}%'.format(wacc*100))\n\n\nweighted average cost of capital: 8.2%\n\n\n\n\nFuture cash flows \nThe future cash flows to the firm are projected based on revenue growth. The cash flows are then discounted using the WACC and the ISV is calculated.\n\n\nCode\n# make a list of the fiscal years in excess return period \nfy = np.zeros(erp+1)\nfy[0] = fy_start\nfor i in range(1,erp+1): \n    fy[i]=fy_start+i\n\nrev = np.zeros(len(fy))\nciwc = np.zeros(len(fy))\nrev[0] = rev_start  #*rgr+rev_start   # find the future revenue using constant revenue growth rate \n\nfor i in range(1,len(fy)): \n    rev[i] = rev[i-1]*rgr+rev[i-1]  # find the future revenue \n    ciwc[i] = (rev[i]-rev[i-1])*wcr  # find the change in working capital \n\nnet_op = np.zeros(len(fy)) # net operating profit\nadj_taxes = np.zeros(len(fy))\nnopat = np.zeros(len(fy))\ninvest = np.zeros(len(fy))\ndepre = np.zeros(len(fy))\nnet_invest = np.zeros(len(fy))\nfcff = np.zeros(len(fy))\ndisc_fact = np.zeros(len(fy))\ndisc_fcff = np.zeros(len(fy))                \n\n# calculate values in table \nfor i in range(1,len(fy)):\n    net_op[i] = rev[i]*nopm # net operating profit margin\n    adj_taxes[i] = net_op[i]*tr # net operating profit adjusted for taxes\n    nopat[i] = net_op[i]-adj_taxes[i] # after tax net operating profit\n    invest[i] = rev[i]*ir # future investments\n    depre[i] = rev[i]*dr # future depreciations\n    net_invest[i] = invest[i]-depre[i] # net investments\n    fcff[i] = nopat[i]-net_invest[i]-ciwc[i] # free cash flow to the firm\n    disc_fact[i] = 1/((1+wacc)**i) # discount factor\n    disc_fcff[i] = disc_fact[i]*fcff[i] # discounted free cash flow to the firm\n    \ndcrv = nopat[-1]/wacc*disc_fact[-1] # discounted corporate residual value\nderp_fcff = disc_fcff.sum() # discounted excess return period FCFF\n\ntcv = derp_fcff+dcrv+sta # total corporate value\ntvce = tcv-vod-vps-stl # total value of common equity\nisv = tvce/so # intrinsic stock value\n\n# print cash flows in a table\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format('Year','Rev','NOP','AdjTaxes',\n    'NOPAT','Invest.','Deprec.','dInvest.','dWC','FCFF','DF','DF*FCFF'))\nfor i in range(len(fy)):\n    print('{:4.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.4f}{:10,.0f}'.format(fy[i],\n        rev[i]/1e6,net_op[i]/1e6,adj_taxes[i]/1e6,nopat[i]/1e6,invest[i]/1e6,depre[i]/1e6,(invest[i]-depre[i])/1e6,ciwc[i]/1e6,\n        fcff[i]/1e6,disc_fact[i],disc_fcff[i]/1e6))\n\n\nYear       Rev       NOP  AdjTaxes     NOPAT   Invest.   Deprec.  dInvest.       dWC      FCFF        DF   DF*FCFF\n2022     1,280         0         0         0         0         0         0         0         0    0.0000         0\n2023        70        -1         0        -1        27        15        12       -11        -1    0.9241        -1\n2024         4        -0         0        -0         1         1         1        -1        -0    0.8539        -0\n2025         0        -0         0        -0         0         0         0        -0        -0    0.7891        -0\n2026         0        -0         0        -0         0         0         0        -0        -0    0.7292        -0\n2027         0        -0         0        -0         0         0         0        -0        -0    0.6738        -0\n2028         0        -0         0        -0         0         0         0        -0        -0    0.6227        -0\n2029         0        -0         0        -0         0         0         0        -0        -0    0.5754        -0\n2030         0        -0         0        -0         0         0         0        -0        -0    0.5317        -0\n2031         0        -0         0        -0         0         0         0        -0        -0    0.4913        -0\n2032         0        -0         0        -0         0         0         0        -0        -0    0.4540        -0\n\n\n\n\nCode\n# Intrinsic Value\nprint('discounted excess return period FCFF: ${:,.2f}B'.format(derp_fcff/1e9))\nprint('discounted corporate residual value: ${:,.2f}B'.format(dcrv/1e9))\nprint('total corporate value: ${:,.2f}B'.format(tcv/1e9))\nprint('total value of common equity: ${:,.2f}B'.format(tvce/1e9))\ntvce_baseline = tvce # save value as baseline case\nisv_baseline = isv # save the isv for the baseline case\nprint('intrinsic stock value, baseline case: ${:,.2f}'.format(isv_baseline))\nprint('current stock price: ${:,.2f}'.format(csp))\n\n\ndiscounted excess return period FCFF: $-0.00B\ndiscounted corporate residual value: $-0.00B\ntotal corporate value: $3.56B\ntotal value of common equity: $-7.60B\nintrinsic stock value, baseline case: $-29.82\ncurrent stock price: $13.72\n\n\nObservation:\nThe base line DCF analysis produces an intrinsic stock value of \\$80.\n\n\nList of all inputs to the DCF model\nThe following print statements format the inputs to the model similar to how they are presented on the Valuepro page.\n\n\nCode\nprint('{:&gt;35s} {:&lt;10.0f} {:&gt;35s} {:,.3f}'.format('Excess return period, years:',erp,'Depreciation rate, %:',dr*100))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.3f}'.format('Starting revenues, $B:',\n    rev_start/1e9,'Investment rate, %:',ir*100))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:,.3f}'.format('Revenue growth rate, %:',\n    rgr*100,'Working capital rate, %:',wcr*100))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:,.3f}'.format('Net operating profit margin, %:',\n    nopm*100,'Current assets, $B:',sta/1e9))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:.3f}'.format('Tax rate, %:',\n    tr*100,'Current liabilities, $B:',stl/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.2f}'.format('Current stock price, $:',\n    csp,'Equity risk premium, %:',eq_rp*100))\nprint('{:&gt;35s} {:&lt;10,.0f} {:&gt;35s} {:,.2f}'.format('Shares outstanding, basic, M:',\n    so/1e6,'Company specific beta:',beta))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:.3f}'.format('10 year treasury bond yield, %:',\n    tby*100,'Total long term debt and other, $B:',vod/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.3f}'.format('Bond yield spread to treasury, %:',\n    bystt*100,'Value of preferred stock, $B:',vps/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f}'.format('Preferred stock yield, %:',psy*100))\n\n\n       Excess return period, years: 10                       Depreciation rate, %: 21.200\n             Starting revenues, $B: 1.28                       Investment rate, %: 38.264\n            Revenue growth rate, %: -94.514               Working capital rate, %: 0.935\n    Net operating profit margin, %: -1.000                     Current assets, $B: 3.558\n                       Tax rate, %: -0.126                Current liabilities, $B: 2.883\n            Current stock price, $: 13.72                  Equity risk premium, %: 3.00\n      Shares outstanding, basic, M: 255                     Company specific beta: 2.47\n    10 year treasury bond yield, %: 3.04       Total long term debt and other, $B: 8.269\n  Bond yield spread to treasury, %: 4.23             Value of preferred stock, $B: 0.000\n          Preferred stock yield, %: 0.00      \n\n\n\n\nCode\n# weighted average cost of capital inputs\nprint('Weighted Average Cost of Capital')\nprint('Cost of common equity')\nprint('{:s}'.format('-'*37))\nprint('{:&gt;32s} {:,.2f}'.format('10 year treasury bond yield, %:',tby*100))\nprint('{:&gt;32s} {:,.2f}'.format('Company specific beta:',beta))\nprint('{:&gt;32s} {:,.2f}'.format('Equity risk premium, %:',eq_rp*100))\nprint('{:s}'.format('-'*37))\nprint('{:&gt;32s} {:,.2f}'.format('Cost of common equity, %:',cce*100))\nprint()\n\nprint('Market Capitalization and After-Tax Weighted Average Cost of Capital')\nprint()\nprint('{:s}{:^10s}{:^10s}{:^10s}{:^15s}{:^15s}'.format(' '*20,'Current','After-Tax','Market','%','Weighted After-'))\nprint('{:s}{:^10s}{:^10s}{:^10s}{:^15s}{:^15s}'.format(' '*20,'Yield','Yield','Value','Capitalization','Tax Yield'))\n\nprint('{:s}'.format('-'*80))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Long term debt',\n    ltd_ay*100,(tby+eq_rp)*(1-tr)*100,vod/1e9,ltd_pc*100,ltd_ate*100))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Preferred stock',\n     psy*100,ps_ate*100,vps/1e9,ps_pc*100,ps_ate*100))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Common stock',\n     cs_ay*100,cs_aty*100,cs_mv/1e9,cs_pc*100,cs_aty*100))\nprint('{:s}'.format('-'*80))\nprint('{:&lt;37s}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('',tmv/1e9,tpc*100,wacc*100))\n\n\nWeighted Average Cost of Capital\nCost of common equity\n-------------------------------------\n 10 year treasury bond yield, %: 3.04\n          Company specific beta: 2.47\n         Equity risk premium, %: 3.00\n-------------------------------------\n       Cost of common equity, %: 10.45\n\nMarket Capitalization and After-Tax Weighted Average Cost of Capital\n\n                     Current  After-Tax   Market         %       Weighted After-\n                      Yield     Yield     Value   Capitalization    Tax Yield   \n--------------------------------------------------------------------------------\nLong term debt         7.26      6.05         8       70.29           5.11\nPreferred stock        0.00      0.00         0        0.00           0.00\nCommon stock          10.45     10.45         3       29.71          10.45\n--------------------------------------------------------------------------------\n                                             12      100.00           8.22"
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#dcf-scenarios",
    "href": "OLD NCLHv1 analysis.html#dcf-scenarios",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "4) DCF Scenarios ",
    "text": "4) DCF Scenarios \nThe following adjustments were made to various model parameters. - excess return period was adjusted to a more conservative 5 years - revenue growth rate was adjusted to 9% (base case = 9.428%) - net operating profit margin was adjusted to 25% (base case = 28.593%) - tax rate was adjusted to 1% (base case = -0.486%) - depreciation rate was adjusted to 10% (base case = 9.397%) - investment rate was adjust to 25% (base case = 25.003%) - working capital rate was set to an even 1% (base case = 0.979%) - weighted average cost of capital was adjusted up by 2% to reflect higher interest rates and provide a margin of safety (base case = 3.8%)\n\n\nCode\nprint('adjusted DCF input values and rates')\nerp = 5\nprint('excess return period: {:,.0f} years'.format(erp))\nrgr = 9/100\nprint('revenue growth rate: {:,.1f}%'.format(rgr*100))\nnopm = isv_s1_nopm = 25/100  # save nopm rate for NAIC preferred method\nprint('net operating profit margin: {:.2f}%'.format(nopm*100))\ntr = isv_s1_tr = 1/100 #  # save tax rate for NAIC preferred method\nprint('tax rate: {:.2f}%'.format(tr*100))\ndr = 10/100\nprint('depreciation rate: {:,.2f}%'.format(dr*100))\nir = 25/100              # investment rate (% of revenue)\nprint('investment rate: {:,.2f}%'.format(ir*100))\nwcr = 1/100\nprint('working capital rate: {:,.1f}%'.format(wcr*100))\nwacc_adj = (wacc+0.02) # weighted average cost of capital, increased by 2%\nprint('weighted average cost of capital: {:.1f}%'.format(wacc_adj*100))\n\n\nadjusted DCF input values and rates\nexcess return period: 5 years\nrevenue growth rate: 9.0%\nnet operating profit margin: 25.00%\ntax rate: 1.00%\ndepreciation rate: 10.00%\ninvestment rate: 25.00%\nworking capital rate: 1.0%\nweighted average cost of capital: 10.2%\n\n\n\n\nCode\n# make a list of the fiscal years in excess return period \nfy = np.zeros(erp+1)\nfy[0] = fy_start\nfor i in range(1,erp+1): \n    fy[i]=fy_start+i\n\nrev = np.zeros(len(fy))\nciwc = np.zeros(len(fy))\nrev[0] = rev_start  #*rgr+rev_start   # find the future revenue using constant revenue growth rate \n\nfor i in range(1,len(fy)): \n    rev[i] = rev[i-1]*rgr+rev[i-1]  # find the future revenue \n    ciwc[i] = (rev[i]-rev[i-1])*wcr  # find the change in working capital \n\nnet_op = np.zeros(len(fy))\nadj_taxes = np.zeros(len(fy))\nnopat = np.zeros(len(fy))\ninvest = np.zeros(len(fy))\ndepre = np.zeros(len(fy))\nnet_invest = np.zeros(len(fy))\nfcff = np.zeros(len(fy))\ndisc_fact = np.zeros(len(fy))\ndisc_fcff = np.zeros(len(fy))                \n\n# calculate values in table \nfor i in range(1,len(fy)): \n    net_op[i] = rev[i]*nopm # net operating profit\n    adj_taxes[i] = net_op[i]*tr # net operating profit adjusted for taxes\n    nopat[i] = net_op[i]-adj_taxes[i] # after tax net operating profit\n    invest[i] = rev[i]*ir # future investments\n    depre[i] = rev[i]*dr # future depreciations\n    net_invest[i] = invest[i]-depre[i] # net investments\n    fcff[i] = nopat[i]-net_invest[i]-ciwc[i] # free cash flow to the firm\n    disc_fact[i] = 1/((1+wacc_adj)**i) # discount factor\n    disc_fcff[i] = disc_fact[i]*fcff[i] # discounted free cash flow to the firm\n    \ndcrv = nopat[-1]/wacc*disc_fact[-1] # discounted corporate residual value\nderp_fcff = disc_fcff.sum() # discounted excess return period FCFF\n\ntcv = derp_fcff+dcrv+sta # total corporate value\ntvce = tcv-vod-vps-stl # total value of common equity\nisv = tvce/so # intrinsic stock value\n\n# print cash flows in a table\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format(\n    'Year','Rev','NOP','AdjTaxes',\n    'NOPAT','Invest.','Deprec.','dInvest.','dWC','FCFF','DF','DF*FCFF'))\nfor i in range(len(fy)):\n    print('{:4.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.4f}{:10,.0f}'.\n        format(fy[i],rev[i]/1e6,net_op[i]/1e6,adj_taxes[i]/1e6,nopat[i]/1e6,invest[i]/1e6,depre[i]/1e6,\n        (invest[i]-depre[i])/1e6,ciwc[i]/1e6,fcff[i]/1e6,disc_fact[i],disc_fcff[i]/1e6))\n\n\nYear       Rev       NOP  AdjTaxes     NOPAT   Invest.   Deprec.  dInvest.       dWC      FCFF        DF   DF*FCFF\n2022     1,280         0         0         0         0         0         0         0         0    0.0000         0\n2023     1,395       349         3       345       349       140       209         1       135    0.9073       122\n2024     1,521       380         4       376       380       152       228         1       147    0.8232       121\n2025     1,658       414         4       410       414       166       249         1       160    0.7469       120\n2026     1,807       452         5       447       452       181       271         1       175    0.6777       118\n2027     1,969       492         5       487       492       197       295         2       190    0.6149       117\n\n\n\n\nCode\n# Intrinsic Value\nprint('discounted excess return period FCFF: ${:,.2f}B'.format(derp_fcff/1e9))\nprint('discounted corporate residual value: ${:,.2f}B'.format(dcrv/1e9))\nprint('total corporate value: ${:,.2f}B'.format(tcv/1e9))\nprint('total value of common equity: ${:,.2f}B'.format(tvce/1e9))\ntvce_S1 = tvce # save value as scenario 1\nisv_S1 = isv # save the isv for scenario 1 case\nprint('intrinsic stock value, scenario 1 case: ${:,.2f}'.format(isv_S1))\nprint('current stock price: ${:,.2f}'.format(csp))\n\n\ndiscounted excess return period FCFF: $0.60B\ndiscounted corporate residual value: $3.65B\ntotal corporate value: $7.80B\ntotal value of common equity: $-3.35B\nintrinsic stock value, scenario 1 case: $-13.15\ncurrent stock price: $13.72\n\n\nThe DCF model calculates with adjustments an intrinsic stock value of \\$75\n\nScenario #2\nRun the DCF model again with current debt and current number of shares out standing.\n\n\nCode\nvod = 12563518*1000 #df_dcf_data['long_term_debt'].iloc[-1]\nprint('Total long term debt and other: ${:.2f}B'.format(vod/1e9))\n#print('Total long term debt and other pre-Covid: ${:.2f}B'.format(df_dcf_data['long_term_debt'].iloc[-3]/1e9))\n\n\nTotal long term debt and other: $12.56B\n\n\n\n\nCode\nltd_mv = vod              # market value of long term debt\ntmv = ltd_mv+ps_mv+cs_mv  # total market value \nprint('total market value: ${:,.2f}B'.format(tmv/1e9))\n\n\ntotal market value: $16.06B\n\n\n\n\nCode\ncce = tby+beta*eq_rp      # cost of common equity or the expected return for the stock\nprint('cost of common equity: {:,.2f}%'.format(cce*100))\n\n\ncost of common equity: 10.45%\n\n\n\n\nCode\nltd_ay = tby+bystt        # long term debt average yield\nprint('long term debt average yield: {:,.2f}%'.format(ltd_ay*100))\n\n\nlong term debt average yield: 7.26%\n\n\n\n\nCode\nltd_aty = ltd_ay*(1-tr)   # long term debt after tax yield\nprint('long term debt after tax yield: {:,.2f}%'.format(ltd_aty*100))\n\nltd_pc = vod/tmv          # weight for long term debt \nltd_ate = ltd_aty*ltd_pc  # after tax effect of long term debt \nps_ay = psy               # preferred stock, average yield \nps_aty = ps_ay            # preferred stock, average yield \nprint('preferred stock, average yield: {:,.2f}%'.format(ps_aty*100))\n\nps_pc = ps_mv/tmv         # preferred stock, % capital \nps_ate = ps_aty*ps_pc     # preferred stock, after tax effect \ncs_ay = cce               # common stock, average yield \ncs_aty = cce              # common stock, after tax yield \nprint('common stock, after tax yield: {:,.2f}%'.format(cs_aty*100))\n\ncs_pc = cs_mv/tmv         # common stock, % capital \ncs_ate = cs_aty*cs_pc     # common stock, after tax effect \nprint('common stock, after tax effet: {:,.2f}%'.format(cs_ate*100))\n\ntate = ltd_ate+ps_ate+cs_ate # total after tax effect \nprint('total after tax effect: {:,.2f}%'.format(tate*100))\ntpc = ltd_pc+ps_pc+cs_pc     # total % Capital\nprint('total % Capital: {:,.2f}%'.format(tpc*100))\n\n\nlong term debt after tax yield: 7.19%\npreferred stock, average yield: 0.00%\ncommon stock, after tax yield: 10.45%\ncommon stock, after tax effet: 2.27%\ntotal after tax effect: 7.90%\ntotal % Capital: 100.00%\n\n\n\n\nCode\nwacc = tate\nprint('weighted average cost of capital: {:.1f}%'.format(wacc*100))\n\n\nweighted average cost of capital: 7.9%\n\n\n\n\nCode\nso = 417734591 # df_dcf_data['shares_outstanding'].iloc[-1] # shares outstanding\nprint('shares outstanding, basic: {:,.0f}'.format(so))\n\n\nshares outstanding, basic: 417,734,591\n\n\n\n\nCode\nprint('adjusted DCF input values and rates')\nerp = 5\nprint('excess return period: {:,.0f} years'.format(erp))\nrgr = 9/100\nprint('revenue growth rate: {:,.1f}%'.format(rgr*100))\nnopm = isv_s1_nopm = 25/100  # save nopm rate for NAIC preferred method\nprint('net operating profit margin: {:.2f}%'.format(nopm*100))\ntr = isv_s1_tr = 1/100 #  # save tax rate for NAIC preferred method\nprint('tax rate: {:.2f}%'.format(tr*100))\ndr = 10/100\nprint('depreciation rate: {:,.2f}%'.format(dr*100))\nir = 25/100              # investment rate (% of revenue)\nprint('investment rate: {:,.2f}%'.format(ir*100))\nwcr = 1/100\nprint('working capital rate: {:,.1f}%'.format(wcr*100))\nwacc_adj = (wacc+0.02) # weighted average cost of capital, increased by 2%\nprint('weighted average cost of capital: {:.1f}%'.format(wacc_adj*100))\n\n\nadjusted DCF input values and rates\nexcess return period: 5 years\nrevenue growth rate: 9.0%\nnet operating profit margin: 25.00%\ntax rate: 1.00%\ndepreciation rate: 10.00%\ninvestment rate: 25.00%\nworking capital rate: 1.0%\nweighted average cost of capital: 9.9%\n\n\n\n\nCode\n# make a list of the fiscal years in excess return period \nfy = np.zeros(erp+1)\nfy[0] = fy_start\nfor i in range(1,erp+1): \n    fy[i]=fy_start+i\n\nrev = np.zeros(len(fy))\nciwc = np.zeros(len(fy))\nrev[0] = rev_start  #*rgr+rev_start   # find the future revenue using constant revenue growth rate \n\nfor i in range(1,len(fy)): \n    rev[i] = rev[i-1]*rgr+rev[i-1]  # find the future revenue \n    ciwc[i] = (rev[i]-rev[i-1])*wcr  # find the change in working capital \n\nnet_op = np.zeros(len(fy))\nadj_taxes = np.zeros(len(fy))\nnopat = np.zeros(len(fy))\ninvest = np.zeros(len(fy))\ndepre = np.zeros(len(fy))\nnet_invest = np.zeros(len(fy))\nfcff = np.zeros(len(fy))\ndisc_fact = np.zeros(len(fy))\ndisc_fcff = np.zeros(len(fy))                \n\n# calculate values in table \nfor i in range(1,len(fy)): \n    net_op[i] = rev[i]*nopm # net operating profit\n    adj_taxes[i] = net_op[i]*tr # net operating profit adjusted for taxes\n    nopat[i] = net_op[i]-adj_taxes[i] # after tax net operating profit\n    invest[i] = rev[i]*ir # future investments\n    depre[i] = rev[i]*dr # future depreciations\n    net_invest[i] = invest[i]-depre[i] # net investments\n    fcff[i] = nopat[i]-net_invest[i]-ciwc[i] # free cash flow to the firm\n    disc_fact[i] = 1/((1+wacc_adj)**i) # discount factor\n    disc_fcff[i] = disc_fact[i]*fcff[i] # discounted free cash flow to the firm\n    \ndcrv = nopat[-1]/wacc*disc_fact[-1] # discounted corporate residual value\nderp_fcff = disc_fcff.sum() # discounted excess return period FCFF\n\ntcv = derp_fcff+dcrv+sta # total corporate value\ntvce = tcv-vod-vps-stl # total value of common equity\nisv = tvce/so # intrinsic stock value\n\n# print cash flows in a table\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format(\n    'Year','Rev','NOP','AdjTaxes',\n    'NOPAT','Invest.','Deprec.','dInvest.','dWC','FCFF','DF','DF*FCFF'))\nfor i in range(len(fy)):\n    print('{:4.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.4f}{:10,.0f}'.\n        format(fy[i],rev[i]/1e6,net_op[i]/1e6,adj_taxes[i]/1e6,nopat[i]/1e6,invest[i]/1e6,depre[i]/1e6,\n        (invest[i]-depre[i])/1e6,ciwc[i]/1e6,fcff[i]/1e6,disc_fact[i],disc_fcff[i]/1e6))\n\n\nYear       Rev       NOP  AdjTaxes     NOPAT   Invest.   Deprec.  dInvest.       dWC      FCFF        DF   DF*FCFF\n2022     1,280         0         0         0         0         0         0         0         0    0.0000         0\n2023     1,395       349         3       345       349       140       209         1       135    0.9099       123\n2024     1,521       380         4       376       380       152       228         1       147    0.8280       122\n2025     1,658       414         4       410       414       166       249         1       160    0.7534       121\n2026     1,807       452         5       447       452       181       271         1       175    0.6855       120\n2027     1,969       492         5       487       492       197       295         2       190    0.6238       119\n\n\n\n\nCode\n# Intrinsic Value\nprint('discounted excess return period FCFF: ${:,.2f}B'.format(derp_fcff/1e9))\nprint('discounted corporate residual value: ${:,.2f}B'.format(dcrv/1e9))\nprint('total corporate value: ${:,.2f}B'.format(tcv/1e9))\nprint('total value of common equity: ${:,.2f}B'.format(tvce/1e9))\ntvce_S1 = tvce # save value as scenario 1\nisv_S1 = isv # save the isv for scenario 1 case\nprint('intrinsic stock value, scenario 2 case: ${:,.2f}'.format(isv_S1))\nprint('current stock price: ${:,.2f}'.format(csp))\n\n\ndiscounted excess return period FCFF: $0.60B\ndiscounted corporate residual value: $3.85B\ntotal corporate value: $8.01B\ntotal value of common equity: $-7.44B\nintrinsic stock value, scenario 2 case: $-17.80\ncurrent stock price: $13.72\n\n\nIn scenario 2, the CDF model, values NCLH at 22 per share."
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#naci-stock-selection-guide-analysis",
    "href": "OLD NCLHv1 analysis.html#naci-stock-selection-guide-analysis",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "5) NACI stock selection guide analysis ",
    "text": "5) NACI stock selection guide analysis \nThis analysis follows the NAIC stock selection guide (SSG) [2]. The SSG relates revenue growth, EPS and share price history and makes a prediction about the future share price.\nThe National Association of Investors Clubs (NAIC) is a nonprofit organization dedicated to educating individual investors and investment clubs to become successful lifelong investors. NAIC’s Stock Selection Guide (SSG) is used in the following cells to analyze the company’s growth and whether the stock is selling at a reasonable price.\nThe SSG was originally developed in the 1950s as a paper worksheet by the not-for-profit National Association of Investors Corporation (NAIC). The SSG aims to aid individual investors in the fundamental analysis and selection of common stocks by reviewing components of a company’s growth, quality, and value.\n\nComments about NAIC analysis of a distressed company\nThe NAIC analysis presented below was performed for years 2019 and prior, which are the pre-covid years. Companies with negative earnings are difficult to evaluate with this method.\n\n\nLoad data from metrics sheet\n\n\nCode\n# column names: fiscal years \nfy_data = df_metrics_sheet.columns[1:].values.astype('datetime64[Y]')-1970\n# line 0: Net income\nnet_income_data = df_metrics_sheet.iloc[0].to_numpy()[1:].astype('float')\n# line 1: Shareholder equity\nshareholder_equity_data =  df_metrics_sheet.iloc[1].to_numpy()[1:].astype('float')\n# line 2: Total liabilities\ntotal_liabilities_data = df_metrics_sheet.iloc[2].to_numpy()[1:].astype('float')\n# line 3: Free cash flow, Net cash provided by operating activities \nfree_cash_flow_data =  df_metrics_sheet.iloc[3].to_numpy()[1:].astype('float')\n# line 4: Dividends\ndividends_data =  df_metrics_sheet.iloc[4].to_numpy()[1:].astype('float')\n# line 5: Total assets\ntotal_assets_data = df_metrics_sheet.iloc[5].to_numpy()[1:].astype('float')\n# line 6: Earnings per share\neps_data = df_metrics_sheet.iloc[6].to_numpy()[1:].astype('float')\n# line 7: Dividends per share  \ndps_data = df_metrics_sheet.iloc[7].to_numpy()[1:].astype('float')\n# line 8: Total tangible assets\ntotal_tangible_assets_data = df_metrics_sheet.iloc[8].to_numpy()[1:].astype('float')\n# line 9: Liabilities w/o deposits\nliabilities_wo_deposits_data = df_metrics_sheet.iloc[9].to_numpy()[1:].astype('float')\n# line 10: Provision for credit losses\nprovision_for_credit_losses_data = df_metrics_sheet.iloc[10].to_numpy()[1:].astype('float')\n# line 11: Short-term borrowings\nshort_term_borrowings_data = df_metrics_sheet.iloc[11].to_numpy()[1:].astype('float')\n# line 12: Preferred stock\npreferred_stock_data = df_metrics_sheet.iloc[12].to_numpy()[1:].astype('float')\n# line 13: Net cash used in investing activities \nnet_cash_used_in_investing_activities_data = df_metrics_sheet.iloc[13].to_numpy()[1:].astype('float')\n\n\n\n\nCode\n# make a new data frame to store data from metrics sheet\ndf_metrics_data = pd.DataFrame(data={\n    'FY':fy_data[::-1],\n    'net_income':net_income_data[::-1],\n    'shareholder_equity':shareholder_equity_data[::-1],\n    'total_liabilities':total_liabilities_data[::-1],\n    'free_cash_flow':free_cash_flow_data[::-1],\n    'dividends':dividends_data[::-1],\n    'total_assets':total_assets_data[::-1],\n    'eps':eps_data[::-1],    \n    'dps':dps_data[::-1],\n    'total_tangible_assets':total_tangible_assets_data[::-1],\n    'liabilities_wo_deposits':liabilities_wo_deposits_data[::-1],    \n    'provision_for_credit_losses':provision_for_credit_losses_data[::-1],\n    'short_term_borrowings':short_term_borrowings_data[::-1], \n    'preferred_stock':preferred_stock_data[::-1],\n    'net_cash_used_in_investing_activities':net_cash_used_in_investing_activities_data[::-1]\n    })\n\n#df_metrics_data\n\n\ncheck for matching years in both data frames\n\n\nCode\nif all(df_dcf_data['FY'] == df_metrics_data['FY']) != True:\n    print('error, years in data frame don\\'t match')\n    stop # this is not python code, so jupyterlab will throw an error\nelse:\n    print('OK, years in data frame match')\n\n\nOK, years in data frame match\n\n\n\n\nNAIC section 1: Visual analysis\nHigh and low price history for each year\nFrom the daily price history obtained from yahoo finance, the high and low closing price for each is obtained and the data saved to the financial data frame as new columns.\n\n\nCode\n#column names: fiscal years \nyears_list = df_metrics_sheet.columns[1:].values.astype('str')[::-1]\n\n# convert years to datetime format\nyear_ended_list = []\nfor i in years_list:\n    year_ended_list.append(datetime.strptime(i, '%Y'))\n\n# make emnpy lists to store open, close, high and low price data for each fiscal year\nfy_open = []\nfy_close = []\nfy_high = []\nfy_low = []\n\nfor i in year_ended_list:\n    start = i\n    end = i + relativedelta(years=1)\n    p1 = df_price_history.truncate(before=start, after=end)\n    if len(p1) == 0:\n        fy_open.append(np.nan)\n        fy_close.append(np.nan)        \n        fy_high.append(np.nan)\n        fy_low.append(np.nan)\n    else:\n        fy_open.append(p1['Open'].iloc[0])\n        fy_close.append(p1['Close'].iloc[-1])        \n        fy_high.append(p1['Close'].max())\n        fy_low.append(p1['Close'].min())\n\n# convert from list to numpy array\nfy_open = np.asarray(fy_open)\nfy_close = np.asarray(fy_close)\nfy_high = np.asarray(fy_high)\nfy_low = np.asarray(fy_low)\n\n\nPlotting the data\nThe annual sales, EPS and the high and low share price is plotted on a semilog plot. A consistent percentage change in the data will plot on the semi-log chart as a straight line.\nThe stock price is plotted separately from the sales and earnings for clarity.\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\n# figsize() function to adjust the size\nplt.subplots(1,2,figsize=(15, 5))\n\n# using subplot function and creating\n# plot one\nplt.subplot(1, 2, 1)\nwidth = 3  # the width of the bars\n#plt.bar(year_ended_list,fy_high-fy_low, width,bottom=fy_low,label='price')\nj = 0\nfor i in year_ended_list:\n    color = 'green'\n    if fy_open[j] &gt; fy_close[j]: color= 'red'\n    # high/low lines\n    plt.plot([i,i],[fy_low[j],fy_high[j]],color=color, linewidth=width)\n    # open marker\n    plt.plot([i,i-relativedelta(months=1)], [fy_open[j],fy_open[j]], color=color, linewidth=width)\n    # close marker\n    plt.plot([i,i+relativedelta(months=1)], [fy_close[j],fy_close[j]], color=color, linewidth=width)\n    j += 1\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.ylim((0,70))\nplt.title('Yearly stock high and low price range')\nplt.ylabel('stock price, $')\n#plt.legend()\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n\nplt.plot(df_metrics_data['FY'],df_dcf_data['revenue']/1e9,'+-',label='revenue, $B')\nplt.plot(df_metrics_data['FY'],df_metrics_data['eps'],'+-',label='EPS, $')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\n#plt.yscale('log')\n#plt.yticks([0.1,1,10,100,1000,10000],['0.1','1','10','100','1000','10000'])\n#plt.ylim((0.1,1000))\nplt.title('Revenue and EPS')\nplt.ylabel('Revenue and EPS')\nplt.legend()\n\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\nObservation:\nShare price has been trading in the 400 to 60 dollar range the years 2015 to 2019, indicating that the market is not impressed with the company. Average EPS (ignoring 2020) has been increasing along with revenues for the years 2015 to 2019.\n\n\nNAIC section 3, Price earnings history\nSection 3 of the SSG is the Price-Earnings history. The following table is built from the high and low prices each year and the earnings per share. The high and low Price/Earnings ratios are calculated for each year and are listed in the columns labeled h-per and l-per.\n\n\nCode\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format('year','high','low','eps',\n    'h-per','l-per'))\nfor i in range(len(year_ended_list)):\n    print('{:s}{:10,.2f}{:10,.2f}{:10,.2f}{:10,.2f}{:10,.2f}'.format(year_ended_list[i].strftime(\"%Y\"),\n        fy_high[i], fy_low[i],df_metrics_data['eps'][i],\n        fy_high[i]/df_metrics_data['eps'][i],\n        fy_low[i]/df_metrics_data['eps'][i]))\n\n\nyear      high       low       eps     h-per     l-per\n2012       nan       nan      0.95       nan       nan\n2013     35.47     24.79      0.50     70.94     49.58\n2014     48.03     29.65      1.64     29.29     18.08\n2015     63.76     42.93      1.89     33.74     22.71\n2016     57.99     34.40      2.79     20.78     12.33\n2017     59.46     42.79      3.33     17.86     12.85\n2018     60.93     39.55      4.28     14.24      9.24\n2019     59.56     40.71      4.33     13.76      9.40\n2020     59.65      7.77    -15.75     -3.79     -0.49\n2021     33.71     17.79    -12.33     -2.73     -1.44\n2022     23.72     10.38     -5.41     -4.38     -1.92\n\n\nAverage high and P/E for select years\nThe average price to earning ratio based on high and low stock prices is calculated.\nUse data from 2014 to 2019.\n\n\nCode\n#Average high P/E for years \npe_avg_high = (fy_high/df_metrics_data['eps'])[1:-2].mean()\nprint('average high P/E {:.2f}'.format(pe_avg_high))\n#Average low P/E for years \npe_avg_low = (fy_low/df_metrics_data['eps'])[2:-2].mean()\nprint('average low P/E {:.2f}'.format(pe_avg_low))\n\n\naverage high P/E 24.60\naverage low P/E 12.02\n\n\n\nEstimate future EPS\nUse polyfit to get EPS slope and intercept of a least square fit.\nUse data from 2013 to 2019.\n\n\nCode\ny = df_metrics_data['eps'][:-2]\nx = np.arange(len(y))\nm, c = np.polyfit(x, y, 1)\nprint('EPS slope: {:.2f}'.format(m))\nprint('EPS intercept: {:.2f}'.format(c))\nlstsq_fit = m*x + c  # data points for each year\n\n\nEPS slope: -0.81\nEPS intercept: 3.68\n\n\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('EPS')\n\nax1.plot(df_metrics_data['FY'],df_metrics_data['eps'], 'o',label='EPS')\nax1.plot(df_metrics_data['FY'][:-2],lstsq_fit, '-',label='least squares fit')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,4))\nax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('EPS and least squares fit')\nplt.show()\n\n\n\n\n\nUsing the equation for the best fit line, find the y value for the eps point at five years in the future.\n\n\nCode\n# estimated eps in 5 years\neps_5yr_est = m*(x[-1]+5) + c\nprint('estimated eps in 5 years: {:.1f}'.format(eps_5yr_est))\n\n\nestimated eps in 5 years: -6.8\n\n\nUsing the high and low price to earning ratio from above and the projected eps, calculate the range of stock price in five years.\n\n\nCode\nnaic_price_eps_low = eps_5yr_est*pe_avg_low\nnaic_price_eps_high = eps_5yr_est*pe_avg_high\nprint('estimated price range in 5 years: ${:.2f} to ${:.2f}'.format(naic_price_eps_low,naic_price_eps_high))\n\n\nestimated price range in 5 years: $-82.30 to $-168.48\n\n\nThis is the estimated price range of the stock based on projected EPS and is a guide for what the stock price might be if conditions remain the same. Since the slope of the EPS history is negative, the projected stock price is negative.\n\n\nNAIC section 3: 5 year estimated EPS, preferred method\nSee page 87 and figure 10-1, Need the following data:\n- estimate sales in 5 years based on sales growth - NOPM - Tax rate - shares outstanding\nNet Operating Profit should reflect the future revenue generating ability and expense requirements of the operating business that comprise the ongoing operations of the company.\n\\(\\text{NOPM} = \\frac{\\text{Revenue} - \\text{Expenses}}{\\text{Revenue}}\\)\nTax payments are taken from the consolidated income statement, provision for income taxes. The effect of taxes on profits is accounted for.\n\\(\\text{Tax rate} = \\frac{\\text{Income taxes}}{\\text{Income before income taxes}}\\)\nTo get future EPS\n\\(\\text{future EPS} = \\frac {\\text{future revenue} \\times \\text{NOPM} \\times \\text{(1-tax rate)}}{\\text{number of shares}}\\)\nUse polyfit to get revenue least square fit\nUse data from 2013 to 2019.\n\n\nCode\ny = df_dcf_data['revenue'][:-2]/1e6\nx = np.arange(len(y))\nm, c = np.polyfit(x, y, 1)\nprint('revenue slope: {:.2f}'.format(m))\nprint('revenue intercept: {:.2f}'.format(c))\nlstsq_fit = m*x + c  # data points for each year\n\n\nrevenue slope: 243.34\nrevenue intercept: 3069.45\n\n\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $M')\n\nax1.plot(df_metrics_data['FY'],df_dcf_data['revenue']/1e6, 'o',label='revenue')\nax1.plot(df_metrics_data['FY'][:-2],lstsq_fit, '-',label='least squares fit')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,4))\nax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Revenue and least squares fit')\nplt.show()\n\n\n\n\n\nUsing the equation for the best fit line, find the y value for the EPS point at five years in the future.\n\n\nCode\n# estimated revenue in 5 years\nrev_5yr_est = m*(x[-1]+5) + c\nprint('estimated rev in 5 years: ${:,.1f}M'.format(rev_5yr_est))\n\n\nestimated rev in 5 years: $6,232.9M\n\n\nneed to include estimate of number of shares outstanding in 5 years\n\n\nCode\nprint('starting revenues: ${:,.2f}'.format(rev_start/1e9))\n\n\nstarting revenues: $1.28\n\n\nUsing the adjusted NOPM and tax rate from scenario 1.\n\n\nCode\npm_nopm = isv_s1_nopm # use nopm from scenario 1\npm_tax_rate = isv_s1_tr # use tr from scenario 1\n\npm_eps_5yr_est = rev_5yr_est*pm_nopm*(1-pm_tax_rate)*1e6/df_dcf_data['shares_outstanding'].iloc[-1] \n#pm_eps_5yr_est = rev_5yr_est*nopm_avg*1e6/df_dcf_data['shares_outstanding'].iloc[-1] \nprint('using preferred method: estimated eps in 5 years: ${:.2f}'.format(pm_eps_5yr_est))\n\n\nusing preferred method: estimated eps in 5 years: $3.66\n\n\nUsing the high and low price to earning ratio from above and the projected EPS, calculate the range of stock price in five years.\n\n\nCode\nnaic_price_pm_low = pm_eps_5yr_est*pe_avg_low\nnaic_price_pm_high = pm_eps_5yr_est*pe_avg_high\nprint('estimated price range in 5 years from preferred method: {:.2f} to {:.2f}'.format(\n    naic_price_pm_low,naic_price_pm_high))\n\n\nestimated price range in 5 years from preferred method: 43.94 to 89.94\n\n\nObservation: Based on revenue growth, the projected stock price is a bit higher than the current price. However, based on price history, the stock is not expected to appreciate."
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#future-stock-price",
    "href": "OLD NCLHv1 analysis.html#future-stock-price",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "6) Future stock price ",
    "text": "6) Future stock price \nThe projected future stock price is estimated from the results shown in this notebook based on DCF intrinsic stock value, the NAIC method or a combination of both. The DCF method does not consider market sentiment or popularity of the stock, whereas the NAIC method looks at the PE and EPS to develop the historical consensus that the market has put on the price of the stock. Both the NAIC and the DCF valuation should be considered. The DCF valuation is of the current ISV which is used as an indication of the future value, since it is assumed that the market price will converge eventually to the intrinsic value.\nThe estimated future stock price considers the following:\n- base case ISV - Senario ISV - NAIC EPS growth - NAIC preferred method\nUsing 5 year NAIC as a conservative estimate for the 10 year value and the analysis results, a judgment call is made concerning the price to put on the future value of the stock.\n\n\nCode\nprint('estimated price range in 5 years from EPS: ${:.2f} to ${:.2f}'.format(naic_price_eps_low,naic_price_eps_high))\nprint('estimated price range in 5 years from preferred method: ${:.2f} to ${:.2f}'.format(\n    naic_price_pm_low,naic_price_pm_high))\n\nprint('intrinsic stock value, baseline case: ${:,.2f}'.format(isv_baseline))\nprint('intrinsic stock value, scenario 1 case: ${:,.2f}'.format(isv_S1))\n\nprint('current stock price: ${:,.2f}'.format(csp))\n\n\nestimated price range in 5 years from EPS: $-82.30 to $-168.48\nestimated price range in 5 years from preferred method: $43.94 to $89.94\nintrinsic stock value, baseline case: $-29.82\nintrinsic stock value, scenario 1 case: $-17.80\ncurrent stock price: $13.72\n\n\nThe estimated price range in 5 years from the preferred method is \\$86.14 to \\$116.85. However, this no longer applies as a result of the pandemic.\nThe estimated price range in 5 years from the preferred method is \\$86.14 to \\$116.85. Taking the average and using that value on the IRR calculations.\nUsing the average of:\n- low estimated price from EPS and the low - estimated price from the preferred method - intrinsic stock value, scenario 1 case\nuse average of NAIC low price\n\n\nCode\n#fsp = (naic_price_eps_low + naic_price_pm_low + csp)/3 # estimated future stock price\nfsp = (naic_price_eps_low) # estimated future stock price\nprint('estimated future stock price: ${:,.2f}'.format(fsp))\n\n\nestimated future stock price: $-82.30"
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#dividend-payout",
    "href": "OLD NCLHv1 analysis.html#dividend-payout",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "7) Dividend payout ",
    "text": "7) Dividend payout \nNCLH does not pay a dividend.\n\nShareholder benefit\nNCLH provides a shareholder’s benefit which provides \\$100 of onboard credit for a 7 night cruise and \\$250 onboard credit per stateroom on sailings of 15 days or more.\nI purchased 100 shares on Feb 24, 2020 at \\$42.9050 per share. My total cost basis is \\$4,290.50 and so far I’ve received shareholder benefits of \\$300. The current stock price is \\$13.72. The plan is to use the shareholder benefit many times and then sell the stock for the same or higher price. Well, given the disruption to the cruise industry and the dilution of NCLH, getting the original price in the future may not be possible.\nFor this analysis assume the following: - two cruises per year, totaling \\$200 in shareholder benefit which will be called a dividend.\nCalculate shareholder’s benefit per 100 shares and update the df_metrics_data[‘dps’] data.\n\n\nCode\ndf_metrics_data['dps'] = np.ones(len(df_metrics_data['dps']))*200/100 # shareholder benefit per year per share owned\n\n\n\n\nCode\n# calculate the percent change in dividends\npcd = np.zeros(len(df_metrics_data['dps'])) # percent change in dividend\nfor i in range(len(df_metrics_data['dps'][0:-1])):\n    pcd[i+1] = ((df_metrics_data['dps'][i+1] - df_metrics_data['dps'][i])/\n                df_metrics_data['dps'][i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dividend per share, $')\n\n# plot revenue as single bar\nplt.bar(df_metrics_data['FY'],df_metrics_data['dps'], width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(year_ended_list,pcd,'+-g')\n    \nax2.set_ylabel('% Change in dividend',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,20))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Dividend history per share')\nplt.show()\n\n\n\n\n\n\n\nCode\nadgr = pcd[-6:].mean() #last 6 years\nprint('average dividend growth rate: {:.2f}%'.format(adgr))\n\n\naverage dividend growth rate: 0.00%\n\n\n\n\nShareholder’s benefit as dividend yield\nDividend yield equals the annual dividend per share divided by the stock’s price per share. The plot below shows the history of dividend yield over the evaluation period.\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nwidth = 50  # the width of the bars\nplt.bar(df_metrics_data['FY'],(df_metrics_data['dps']/fy_high-df_metrics_data['dps']/fy_low)*100, \n        width,bottom=df_metrics_data['dps']/fy_low*100,label='yield')\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.ylim((0,30))\nplt.title('Range of dividend yield each year')\nplt.ylabel('dividend yield, %')\n#plt.legend()\nplt.grid()\n\n# show plot\nplt.show()\n\n\n\n\n\n\n\nInternal Rate of Return (IRR) calculations\nThe internal rate of return (IRR) is the discount rate that makes the net present value (NPV) of all cash flows equal to zero in a discounted cash flow analysis. Generally speaking, the higher an internal rate of return, the more desirable an investment is to undertake.\nAs explained above, the stock price has not changed by much over the years, even though the revenue and dividends have been increasing. The final stock price is set equal to the current price.\nUsing the average dividend growth rate calculated above, a series of estimated future dividend payments are generated.\n\n\nCode\nfdp = np.zeros(len(df_metrics_data['dps'])) # future dividend payments\nfdp[0] = df_metrics_data['dps'].iat[-1]\nfor i in range(len(df_metrics_data['dps'][0:-1])):\n    fdp[i+1] = fdp[i]+fdp[i]*adgr/100\n\n\n\n\nCode\nprint('current stock price: ${:,.2f}'.format(csp))\n\nfsp = 20 #100 #csp #500 #(csp + 102.05 + 138.82)/3 # final stock price, $\nprint('final stock price: ${:,.2f}'.format(fsp))\n\n\ncurrent stock price: $13.72\nfinal stock price: $20.00\n\n\n\n\nCode\nest_cf = np.copy(fdp) # make a copy of the estimated cash flow\n\n# cash flows, initial purchase, dividend payments and final sale\nest_cf[0] = est_cf[0] - 42.9 # subtract purchase price from the first dividend payment\nest_cf[-1] = est_cf[-1] + fsp # include the sale price with the final dividend payment\n\n\n\n\nCode\ndividend_irr = np_financial.irr(est_cf)\nprint('IRR: {:.2f}%'.format(dividend_irr*100))\n\n\nIRR: -0.29%\n\n\nA negative IRR indicates that my shareholders benefit will not break even with a final stock price of \\$20."
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#management-performance",
    "href": "OLD NCLHv1 analysis.html#management-performance",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "8) Management performance ",
    "text": "8) Management performance \nThe following analysis somewhat follows the Warren Buffett strategy as outlined in [3]. This strategy is essentially value investing where companies are chosen that meet a set of criteria and who’s stock price is below the intrinsic value plus a margin of safety. These investments are usually held for the long term.\n\nFinancial metrics\nThe following analysis looks at financial ratios over the evaluation period. Financial ratios can be used to judge management performance. Consistent favorable trends are an indication that management is taking care of the company.\nTotal liabilities to total assets ratio\nDebt to equity and debt to NOP ratios\nFinancial ratios: RoE, RoA and PM\nNAIC section 2: Evaluating management\nNormalized data from consolidated statements\nMarket metrics\nOne dollar premise\nShare price vs EPS\nMarket capitalization\nQualitative metrics\nSimple and understandable business model\nFavorable long term prospects\nCommodity reliance\nConsistent operating history\nrationality:\n\nfocus on core aspects\nonly invest in high ROE businesses\nfocus on shareholder equity\n\n\n\nFinancial metrics \nThe following financial metrics are examined over the evaluation period. We are looking for favorable trends and evidence of consistent operations. Some red flags will also be evident in the plots.\nRed flags:\n- Shrinking gross profit margin - Receivables growing faster than sales - Rising debt-to-equity ratio - Several years of revenue trending down - Unsteady cash flow - Rising accounts receivable or inventory in relation to sales - Rising outstanding share count - Consistently higher liabilities than assets - Decreasing gross profit margin - Increasing revenue while cash flow remains the same - Unusual changes in key financial ratios\n\nTotal liabilities to total assets ratio\nThe ratio of liabilities to assets is plotted over the evaluation period. For most companies examined the liabilities are the total liabilities and the ratio is calculated using total assets and total tangible assets. Total tangible assets have goodwill and intangibles removed from the total. The ratio gives an indication of how much the company is worth versus how much the company owes. Ideally the ratio of liabilities to assets should be less than one.\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('ratio')\n\n# plot revenue as single bar\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['total_assets'], '-+',\n    label='total liabilities to total assets')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['total_tangible_assets'], '-*',\n    label='total liabilities to total tangible assets')\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,1))\nax1.legend(bbox_to_anchor=(1.8, 1))\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\n#ax2.plot(year_ended_list,pcd,'+-g')\nax2.plot(df_metrics_data['FY'],\n    (df_metrics_data['total_assets']-df_metrics_data['total_tangible_assets'])/df_metrics_data['total_assets']*100,\n    ':',color=color,label='intangible assets to total assets')\n    \nax2.set_ylabel('% intangible assets',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,100))\nax2.legend(bbox_to_anchor=(1.7, 0))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Total liabilities to total assets ratio')\nplt.show()\n\n\n\n\n\nFor the years 2020 and 2021, the ratio of total liabilities to total tangible assets has risen on account of borrowing during the pandemic.\nThe value assigned to goodwill and intangibles is about \\$?? billion.\nThe percentage of intangible assets to total assets is also plotted. The ratio indicates the company has taken on a lot of debt relative to assets and is something of concern.\nThe level of intangible assets are low.\n\n\nDebt to equity and debt to NOP ratios\nThe debt-to-equity ratio (D/E) is another key characteristic Buffett considers carefully. Buffett prefers to see a small amount of debt so that earnings growth is being generated from shareholders’ equity as opposed to borrowed money. The D/E ratio is calculated as follows:\n\\(\\text{Debt-to-Equity Ratio} = \\frac {\\text{Total Liabilities}} {\\text{Shareholders' Equity}} \\text{  OR  } \\frac {\\text{Long term debt}} {\\text{Shareholders' Equity}}\\)\nThis ratio shows the proportion of equity and debt the company uses to finance its assets, and the higher the ratio, the more debt—rather than equity—is financing the company. A high debt level compared to equity can result in volatile earnings and large interest expenses. For a more stringent test, investors sometimes use only long-term debt instead of total liabilities in the calculation above.\nD/E is the traditional way to look at a company’s debt. Some rules of thumb say that the D/E should not be above 2 or 3. However the D/E company’s typically vary by industry. The ratio of LT debt to NOP gives the number of years it would take the company to pay back debt from NOP, the lower the number the shorter amount of time.\n\\(\\text{Debt-to-NOP Ratio} = \\frac {\\text{Total Liabilities}} {\\text{NOP}}\\)\n\n\nCode\ntangible_equity = df_metrics_data['total_tangible_assets'] - df_metrics_data['total_liabilities']\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('ratio')\n\nax1.plot(df_metrics_data['FY'],df_dcf_data['long_term_debt']/df_metrics_data['shareholder_equity'],\n    '-^',label='(LT debt)/Equity')\n#ax1.plot(year_ended_list,df_dcf_data['long_term_debt']/tangible_equity, '-',label='(LT debt)/(Tangible Equity)')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['shareholder_equity'],\n    '-*',label='(total liabilities)/Equity')\n#ax1.plot(year_ended_list,total_liabilities/BV, '-^',label='(total liabilities)/BV')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/nop, '-+',label='(total liabilities)/NOP')\n#ax1.plot(year_ended_list,total_liabilities/net_income, '-+',label='(total liabilities)/(net income)')\n#ax1.plot(year_ended_list,df_dcf_data['current_liabilities']/nop, '-*',label='(current liabilities)/NOP')\n#ax1.plot(year_ended_list,Liabilities_wo_deposits/nop, '-+',label='(Liabilities w/o deposits)/NOP')\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,10))\n#ax1.legend()\nax1.legend(bbox_to_anchor=(1.6, 1))\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Various debt ratios')\nplt.show()\n\n\n\n\n\n(LT debt)/Equity is plotted and prior to 2019 was less than the threshold of 2.\nA threshold of 2 is traditionally the upper limit for a reasonable amount of debt that a company should carry.\n(total liabilities)/Equity is plotted and prior to 2019 was less than the threshold of 2.\n(total liabilities)/NOP to is plotted for each year in the evaluation period and prior to 2019 was less than the threshold of 10.\nA value of 10 has been chosen as the threshold for this ratio and indicates how many years it would take the company to pay off total liabilities from the NOP generated each year. A threshold of ten seems like a reasonable level of debt measured against NOP.\n\n\nFinancial ratios\nVarious ratios can be used to judge management performance. Consistent favorable trends are an indication that management is taking care of the company.\nReturn on equity\nSometimes return on equity (RoE) is referred to as stockholder’s return on investment. It reveals the rate at which shareholders earn income on their shares. Buffett always looks at RoE to see whether a company has consistently performed well compared to other companies in the same industry. RoE is calculated as follows:\n\\(\\text{Return on Equity} = \\frac {\\text{Net Income}} {\\text{Shareholder's Equity}}\\)\nLooking at the RoE in just the last year isn’t enough. The investor should view the RoE from the past five to 10 years to analyze historical performance.\n\\(\\text{Shareholders’ Equity} = \\text{Total Assets} − \\text{Total Liabilities}\\)\nFor this company, this method of getting Shareholders’ Equity gives negative values. On the Consolidated Balance Sheets, there is a line for Total stockholders’ equity, which is used.\nReturn on Assets\nReturn on assets is a profitability ratio that provides how much profit a company is able to generate from its assets. In other words, return on assets (RoA) measures how efficient a company’s management is in generating earnings from their economic resources or assets on their balance sheet.\n\\(\\text{Return on assets} = \\frac {\\text{Net Income}} {\\text{Total Assets}}\\)\nCalculating the RoA of a company can be helpful in comparing a company’s profitability over multiple quarters and years as well as comparing to similar companies. However, it’s important to compare companies of similar size and industry.\nFor example, banks tend to have a large number of total assets on their books in the form of loans, cash, and investments. A large bank could easily have over \\$2 trillion in assets while putting up a net income that’s similar to companies in other industries. Although the bank’s net income or profit might be similar to an unrelated company and the bank might have high-quality assets, the bank’s RoA will be lower. The larger number of total assets must be divided into the net income, creating a lower RoA for the bank.\nSimilarly, auto manufacturing requires huge facilities and specialized equipment. A lucrative software company that sells downloadable programs online may generate the same net profits, but it could have a significantly higher RoA than its more asset-heavy counterparts. When utilizing this metric to compare productivity across businesses, it’s important to take into account what types of assets are required to function in a given industry, rather than simply comparing the figures.\nProfit Margin\nA company’s profitability depends not only on having a good profit margin, but also on consistently increasing it. This margin is calculated by dividing net income by net sales. For a good indication of historical profit margins, investors should look back at least five years. A high-profit margin indicates the company is executing its business well, but increasing margins mean management has been extremely efficient and successful at controlling expenses.\n\\(\\text{Profit Margin} = \\frac {\\text{Net Income}} {\\text{Revenue}}\\)\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('percent')\n\nax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_metrics_data['shareholder_equity']*100,\n    '-+',label='RoE')\nax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_metrics_data['total_assets']*100,\n    '-*',label='RoA')\n#ax1.plot(df_metrics_data['FY'],total_liabilities/shareholder_equity, '-^',label='D/E')\nax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_dcf_data['revenue']*100,\n    '-^',label='Profit margin')\n\nax1.tick_params(axis='y')\nax1.set_ylim((-10,20))\n#ax1.legend()\nax1.legend(bbox_to_anchor=(1.05, 1))\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Various ratios')\nplt.show()\n\n\n\n\n\nObservation:\nThe trends for RoE, RoA and profit margin are shown above. The effect of the pandemic caused the ratios to turn negative and are no longer a meaningful metric.\n\n\n\nNAIC section 2: Evaluating management\nSee page 86, figure 9-1.\n- % pretax profit on sales, (net before taxes)/rev - % earned on equity (another way of saying RoE, using calculated equity)\nPercent earned on equity is a measure of financial performance calculated by dividing net income by equity. Because equity is equal to a company’s assets minus its debt, percent earned on equity is considered the return on net assets. Percent earned on equity is considered a gauge of a corporation’s profitability and how efficient it is in generating profits.\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('percent')\n#ax1.plot(year_ended_list,net_income, '-+',label='net income')\nax1.plot(df_metrics_data['FY'],df_dcf_data['income_before_income_taxes']/df_dcf_data['revenue']*100, '-+',\n         label='income before taxes/rev')\n#ax1.plot(year_ended_list,df_dcf_data['revenue'], '-+',label='revenue')\n#ax1.plot(year_ended_list,free_cash_flow, '-*',label='free cash flow')\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,20))\n#ax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('% pretax profit on sales')\nplt.show()\n\n\n\n\n\nOver the years 2016 to 2020, pretax profit on sales has a downward trend. Ideally this trend should be increasing or at least flat.\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('percent')\n\n#ax1.plot(year_ended_list,shareholder_equity/df_dcf_data['revenue']*100, '-+k',\n#        label='shareholder equity/rev')\n#ax1.plot(year_ended_list,net_income/shareholder_equity*100, '-+',label='RoE')\nax1.plot(df_metrics_data['FY'],\n    df_metrics_data['net_income']/(df_metrics_data['total_assets']-df_metrics_data['total_liabilities'])*100,\n    '-+',label='RoE')\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,20))\n#ax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('% earned on equity')\nplt.show()\n\n\n\n\n\nPercent earned on equity (another way of saying RoE). Percent earned on equity trend has been flat up to 2015, then became erratic.\nPercent earned on equity is a measure of financial performance calculated by dividing net income by equity. Because equity is equal to a company’s assets minus its debt, percent earned on equity is considered the return on net assets. Percent earned on equity is considered a gauge of a corporation’s profitability and how efficient it is in generating profits.\n\n\nPlot normalized data from consolidated statements\nThe following charts examine data from the consolidated financial statements and compare normalized trends over the evaluation period. The first chart plots normalized revenue along with normalized EPS, NOP and free cash flow. All values are normalized to the starting value in the series. Change relative to the normalized starting value can be seen over the evaluation period. Ideally the normalized parameters plotted should track revenue. Any large departures indicate an area of concern.\n\nNormalized consolidated statement of income\nThe following chart shows normalized revenue plotted with normalized parameters from the consolidated statement of income.\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\n# set look back range, left_yr is the index into the date range\nleft_yr = -10\n\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['revenue'][left_yr:]-df_dcf_data['revenue'].iloc[left_yr])/np.abs(df_dcf_data['revenue'].iloc[left_yr])*100,\n    '^-',label='Revenue')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['eps'][left_yr:]-df_metrics_data['eps'].iloc[left_yr])/np.abs(df_metrics_data['eps'].iloc[left_yr])*100,\n    '-.',label='EPS')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (nop[left_yr:]-nop[left_yr])/np.abs(nop[left_yr])*100,\n    '-.',label='NOP')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['free_cash_flow'][left_yr:]-df_metrics_data['free_cash_flow'].iloc[left_yr])/np.abs(df_metrics_data['free_cash_flow'].iloc[left_yr])*100,\n    '-.',label='Free cash flow')\n# net income\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['net_income'][left_yr:]-df_metrics_data['net_income'].iloc[left_yr])/np.abs(df_metrics_data['net_income'].iloc[left_yr])*100,\n    '-.',label='Net income')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n# Changes x-axis range\nplt.gca().set_xbound(year_ended_list[left_yr], year_ended_list[-1])\n\nplt.ylim((-100,500))\nplt.title('Normalized income statement data')\nplt.ylabel('percent change')\n#plt.legend()\nplt.legend(bbox_to_anchor=(1.6, 1))\n\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\n\n\nNormalized income statement 5 year look back\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\n# set look back range, left_yr is the index into the date range\nleft_yr = -6\n\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['revenue'][left_yr:]-df_dcf_data['revenue'].iloc[left_yr])/np.abs(df_dcf_data['revenue'].iloc[left_yr])*100,\n    '^-',label='Revenue')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['eps'][left_yr:]-df_metrics_data['eps'].iloc[left_yr])/np.abs(df_metrics_data['eps'].iloc[left_yr])*100,\n    '-.',label='EPS')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (nop[left_yr:]-nop[left_yr])/np.abs(nop[left_yr])*100,\n    '-.',label='NOP')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['free_cash_flow'][left_yr:]-df_metrics_data['free_cash_flow'].iloc[left_yr])/np.abs(df_metrics_data['free_cash_flow'].iloc[left_yr])*100,\n    '-.',label='Free cash flow')\n# net income\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['net_income'][left_yr:]-df_metrics_data['net_income'].iloc[left_yr])/np.abs(df_metrics_data['net_income'].iloc[left_yr])*100,\n    '-.',label='Net income')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n# Changes x-axis range\nplt.gca().set_xbound(year_ended_list[left_yr], year_ended_list[-1])\n\nplt.ylim((-100,100))\nplt.title('Normalized income statement data')\nplt.ylabel('percent change')\n#plt.legend()\nplt.legend(bbox_to_anchor=(1.6, 1))\n\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\nThe plot above shows a large departure of revenue in 2020.\n\n\nNormalized consolidated balance sheet\nThe following chart shows normalized revenue plotted with normalized parameters from the consolidated balance sheet.\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\n# set look back range, left_yr is the index into the date range\nleft_yr = -10\n\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['revenue'][left_yr:]-df_dcf_data['revenue'].iloc[left_yr])/np.abs(df_dcf_data['revenue'].iloc[left_yr])*100,\n    '^-',label='Revenue')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['total_liabilities'][left_yr:]-df_metrics_data['total_liabilities'].iloc[left_yr])/np.abs(df_metrics_data['total_liabilities'].iloc[left_yr])*100,\n    '-.',label='Total liabilities')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['total_assets'][left_yr:]-df_metrics_data['total_assets'].iloc[left_yr])/np.abs(df_metrics_data['total_assets'].iloc[left_yr])*100,\n    '-.',label='Total assets')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['total_tangible_assets'][left_yr:]-df_metrics_data['total_tangible_assets'].iloc[left_yr])/np.abs(df_metrics_data['total_tangible_assets'].iloc[left_yr])*100,\n    '-.',label='Total tangible assets')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['long_term_debt'][left_yr:]-df_dcf_data['long_term_debt'].iloc[left_yr])/np.abs(df_dcf_data['long_term_debt'].iloc[left_yr])*100,\n    '-.',label='Long term debt')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['current_liabilities'][left_yr:]-df_dcf_data['current_liabilities'].iloc[left_yr])/np.abs(df_dcf_data['current_liabilities'].iloc[left_yr])*100,\n    '-.',label='Current liabilities')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['depreciation'][left_yr:]-df_dcf_data['depreciation'].iloc[left_yr])/np.abs(df_dcf_data['depreciation'].iloc[left_yr])*100,\n    '-.',label='Depreciation & amortization')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n# Changes x-axis range\nplt.gca().set_xbound(year_ended_list[left_yr], year_ended_list[-1])\n\n#plt.ylim((0,4))\nplt.title('Normalized balance statement data')\nplt.ylabel('percent change')\n#plt.legend()\nplt.legend(bbox_to_anchor=(1.6, 1))\n\nplt.grid()\n\n# show plot\nplt.show()\n\n\n\n\n\nObservation\nBalance sheet items generally followed revenue up to 2020, then as revenue tanked, balance sheet items remained generally on the same trajectory.\n\n\nNormalized balance statement 5 year look back\n\n\nCode\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\n# set look back range, left_yr is the index into the date range\nleft_yr = -6\n\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['revenue'][left_yr:]-df_dcf_data['revenue'].iloc[left_yr])/np.abs(df_dcf_data['revenue'].iloc[left_yr])*100,\n    '^-',label='Revenue')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['total_liabilities'][left_yr:]-df_metrics_data['total_liabilities'].iloc[left_yr])/np.abs(df_metrics_data['total_liabilities'].iloc[left_yr])*100,\n    '-.',label='Total liabilities')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['total_assets'][left_yr:]-df_metrics_data['total_assets'].iloc[left_yr])/np.abs(df_metrics_data['total_assets'].iloc[left_yr])*100,\n    '-.',label='Total assets')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_metrics_data['total_tangible_assets'][left_yr:]-df_metrics_data['total_tangible_assets'].iloc[left_yr])/np.abs(df_metrics_data['total_tangible_assets'].iloc[left_yr])*100,\n    '-.',label='Total tangible assets')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['long_term_debt'][left_yr:]-df_dcf_data['long_term_debt'].iloc[left_yr])/np.abs(df_dcf_data['long_term_debt'].iloc[left_yr])*100,\n    '-.',label='Long term debt')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['current_liabilities'][left_yr:]-df_dcf_data['current_liabilities'].iloc[left_yr])/np.abs(df_dcf_data['current_liabilities'].iloc[left_yr])*100,\n    '-.',label='Current liabilities')\nplt.plot(df_metrics_data['FY'][left_yr:],\n    (df_dcf_data['depreciation'][left_yr:]-df_dcf_data['depreciation'].iloc[left_yr])/np.abs(df_dcf_data['depreciation'].iloc[left_yr])*100,\n    '-.',label='Depreciation & amortization')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n# Changes x-axis range\nplt.gca().set_xbound(year_ended_list[left_yr], year_ended_list[-1])\n\n#plt.ylim((0,4))\nplt.title('Normalized balance statement data')\nplt.ylabel('Percent change')\n#plt.legend()\nplt.legend(bbox_to_anchor=(1.6, 1))\n\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\n\n\n\nMarket metrics \nThe share price is determined by the market. The value is determined by the analyst.\n\nOne dollar premise\nThis is a financial test that shows the strength of the business and how well management has rationality allocated to the company’s business.\nFrom a company’s income, subtract all dividends paid to shareholders. What is left over is the company’s retained earnings. Now add the company’s retained earnings over a 10 year period. Next determine the difference between the company’s current market value and its market value 10 years ago. If the business has employed retained earnings unproductively over this ten year period, the market eventually catches up and will set a lower price on the business.\nOnly use pre covid data.\n\n\nCode\nretained_earnings = df_metrics_data['net_income'][-6:-2].sum() - df_metrics_data['dividends'][-6:-2].sum()\nprint('retained earnings: ${:,.2f}B'.format(retained_earnings/1e9))\n\n\nretained earnings: $-1.37B\n\n\n\n\nCode\n# Current market value, share price multiplied by number of shares\ncmv_high = df_dcf_data['shares_outstanding'].iloc[-3]*fy_high[-3]\ncmv_low = df_dcf_data['shares_outstanding'].iloc[-3]*fy_low[-3]\nprint('Current market value: ${:,.2f}B to ${:,.2f}B'.format(cmv_low/1e9,cmv_high/1e9))\n\n\nCurrent market value: $1.98B to $15.19B\n\n\n\n\nCode\n# Past market value, share price multiplied by number of shares\npmv_high = df_dcf_data['shares_outstanding'].iloc[1]*fy_high[1]\npmv_low = df_dcf_data['shares_outstanding'].iloc[1]*fy_low[1]\nprint('Past market value: ${:,.0f}B to ${:,.0f}B'.format(pmv_low/1e9,pmv_high/1e9))\n\n\nPast market value: $5B to $7B\n\n\n\n\nCode\nprint('Difference in market value: ${:,.0f}B to ${:,.0f}B'.format((cmv_low-pmv_low)/1e9,(cmv_high-pmv_high)/1e9))\n\n\nDifference in market value: $-3B to $8B\n\n\nThis difference in market value is greater than the retained earnings.\n\n\nShare price vs EPS\nLooking at the one dollar premise in terms of share price and EPS.\nThe one dollar premise: one dollar of earning should translate into one dollar of market value - this seems the same as a plot of EPS versus share price.\n\n\nCode\n# plotting the eps data points\nax = plt.bar(df_metrics_data['eps'][1:-2],fy_high[1:-2]-fy_low[1:-2],width = .05,bottom=fy_low[1:-2])\nplt.grid()\nplt.ylim((20,80))\nplt.ylabel('high and low share price range')\nplt.xlabel('EPS')\nplt.title('years from 2010 to 2021')\n\nrects = ax.patches\n\n# Make some labels.\nlabels = [year_ended_list[i].strftime(\"%Y\") for i in range(len(year_ended_list))]\nfor rect, label in zip(rects, labels):\n    y_top =  rect.get_y() + rect.get_height()\n    plt.text(rect.get_x(), y_top+1, label, rotation=90,va='bottom')    \n\nplt.show()\n\n\n\n\n\nObservations:\nThe range in share price is roughly the same across the range of EPS. This means that investors are not valuing the company’s EPS.\n\n\n\nQualitative metrics \nBeyond the numbers in the financial statements, there are metrics that are qualitative in nature that are important to the investor. These are subjective measures of business and management operations that influence value. In this section a few qualitative metrics are discussed below.\n\nSimple and understandable business model\nThe business is to provide a vacation at sea.\nFavorable long term prospects\nPoor, with the threat of inflation, higher fuel prices, higher interest rates and low ticket demand.\nCommodity reliance\nNCL provides a service which is easily reproducible by a competitor.\nConsistent operating history\nSeemed consistent prior to the pandemic, but unknown going forward.\nRationality\n\n\nFocus on core aspects\nYes, cruising is the only business NCL operates.\nOnly invest in high ROE businesses\nNA\nFocus on shareholder equity\nSurvival of the company during and after the pandemic has been the focus.\n\n\nMarketing Strategy\nNCLH significantly reduced their marketing activities in 2020 due to the suspension of cruise voyages as a result of the COVID-19 pandemic. Sales and marketing activities have increased in an attempt to further drive demand. Additionally, they continue a deliberate approach on marketing and sales outreach to guests with future cruise credits, as a result of suspended sailings, to encourage redemption of cruise credits towards future sailings. Building customer loyalty among past guests is an important element of marketing strategy. Past guests create a cost-effective means of attracting business, particularly to new ships and itineraries as they are familiar with the brands, products and services and often return to cruise with NCLH.\nFleet Expansion\nFor the Norwegian brand, Project Leonardo will introduce six additional ships, each ranging from approximately 140,000 to 156,300 Gross Tons with approximately 3,300 to 3,550 Berths, with expected delivery dates from 2022 through 2027. For the Oceania Cruises brand, NCLH has orders for two Allura Class Ships to be delivered in 2023 and 2025. Each of the Allura Class Ships will be approximately 67,000 Gross Tons and 1,200 Berths. For the Regent brand, NCLH has one Explorer Class Ship on order to be delivered in 2023, which will be approximately 55,000 Gross Tons and 750 Berths.\n\n\n\n\n\n\n\n\nNorwegian\nOceania\nRegent\n\n\n\n\nPrima Class: 6 ships 2022 to 2027\nAllura Class: 2 ships 2023 and 2025\nExplorer Class: 1 ship 2023\n\n\nNorwegian Encore\nOceania Riviera\nSeven Seas Splendor\n\n\nNorwegian Bliss\nOceania Marina\nSeven Seas Explorer\n\n\nNorwegian Joy\nOceania Nautica\nSeven Seas Voyager\n\n\nNorwegian Escape\nOceania Sirena\nSeven Seas Mariner\n\n\nNorwegian Getaway\nOceania Regatta\nSeven Seas Navigator\n\n\nNorwegian Breakaway\nOceania Insignia\n\n\n\nNorwegian Epic\n\n\n\n\nNorwegian Gem\n\n\n\n\nNorwegian Jade\n\n\n\n\nNorwegian Pearl\n\n\n\n\nNorwegian Jewel\n\n\n\n\nPride of America\n\n\n\n\nNorwegian Dawn\n\n\n\n\nNorwegian Star\n\n\n\n\nNorwegian Sun\n\n\n\n\nNorwegian Sky\n\n\n\n\nNorwegian Spirit\n\n\n\n\n\nShips represent the most significant assets, and NCLH records them at cost less accumulated depreciation. Depreciation of ships is computed on a straight-line basis over the weighted average useful lives of primarily 30 years after a 15% reduction for the estimated residual value of the ship. Ship improvement costs that NCLH believes add value to our ships are capitalized to the ship and depreciated over the shorter of the improvements’ estimated useful lives or the remaining useful life of the ship. When they record the retirement of a ship component included within the ship’s cost basis, they estimate the net book value of the component being retired and remove it from the ship’s cost basis. Repairs and maintenance activities are charged to expense as incurred. NCLH accounts for Dry-dock costs under the direct expense method which requires us to expense all Dry-dock costs as incurred.\nNCLH determines the weighted average useful lives of our ships based primarily on our estimates of the useful lives of the ships’ major component systems on the date of acquisition, such as cabins, main diesels, main electric, superstructure and hull. The useful lives of ship improvements are estimated based on the economic lives of the new components. In addition, to determine the useful lives of the ship or ship components, thay consider the impact of the historical useful lives of similar assets, manufacturer recommended lives and anticipated changes in technological conditions. Given the large and complex nature of our ships, their accounting estimates related to ships and determinations of ship improvement costs to be capitalized require judgment and are uncertain. Should certain factors or circumstances cause them to revise their estimate of ship service lives or projected residual values, depreciation expense could be materially lower or higher. In 2020, one ship had significant improvements that extended the remaining weighted average useful life of the vessel. Accordingly, They have updated our estimate of both its useful life and residual value based on the new weighted average useful life of its current components. The impact of the change in estimate is accounted for on a prospective basis and is not material.\nIf circumstances cause NCLH to change their assumptions in making determinations as to whether ship improvements should be capitalized, the amounts they expense each year as repairs and maintenance costs could increase, partially offset by a decrease in depreciation expense. If they reduced their estimated weighted average 30-year ship service life by one year, depreciation expense for the year ended December 31, 2020 would have increased by \\$19.8 million. In addition, if their ships were estimated to have no residual value, depreciation expense for the same period would have increased by \\$99.6 million. We believe their estimates for ship accounting are reasonable and their methods are consistently applied. They believe that depreciation expense is based on a rational and systematic method to allocate their ships’ costs to the periods that benefit from the ships’ usage.\nBack to Contents"
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#decision-model",
    "href": "OLD NCLHv1 analysis.html#decision-model",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "9) Decision model ",
    "text": "9) Decision model \nThe decision model establishes thresholds that are to be used in the purchase decision. There are three hard decision thresholds in this model which are:\n1. Intrinsic value 2. Debt 3. Dividend payout ratio 4. Dividend IIR\nThe first threshold is based on the intrinsic value of the company as calculated by the DCF model scenario 1. Recognizing that absolute intrinsic value is an elusive concept, judgment, justified by facts (assets, earnings, dividends, debt and cash flow), establishes the value by adjusting various rates, based on judgment and using a five year forward projection period. This should give an intrinsic value that is based on the historical data, modified by judgment.\nI’m using a threshold of the intrinsic value calculated in scenario 1 (isv_S1) that is greater than 70% of the current stock price, provided that the NAIC valuation is above the current stock price. This accounts for the inadequacy or incorrectness of the data, the uncertainties of the future, and considers the behavior of the market.\nThe second threshold is the level of debt. The ratios of (LT debt)/Equity, (total liabilities)/Equity and (total liabilities)/NOP are plotted for the evaluation period. Over the evaluation period the (LT debt)/Equity and (total liabilities)/Equity should be less than 2 and stable. A threshold of 2 has been discussed in the literature as a level of debt that a company can reasonably take on.\nThe threshold for (total liabilities)/NOP is set at 10. This means that the company can pay off all the liabilities with ten years worth of NOP, which seems like a reasonable time frame for an established and stable company.\nThe third threshold is the dividend payout ratio and is a relative measure of how much the company is paying to shareholders in dividends compared to the metrics of NOP and free cash flow (Net cash provided by operating activities). The payout ratio is useful for assessing a dividend’s sustainability. Payout ratio for a REIT is established by tax law and not used as an evaluation criteria. For other industries a threshold of 50% has been set as the limit.\nThe dividend IRR threshold is the internal rate of return for investor dividend cash flow (divident_irr) should be greater than 10 year treasury bond yield (tby) plus the equity risk premium (eq_rp). Otherwise, other investment opportunities should be looked at.\nIn the decision model there are soft thresholds based on judgment. Soft thresholds are a collection of ratios and analysis that taken together tell a story of the performance of the company and management’s ability to run the company and support dividends over the long term. Use judgment and make an evaluation.\nThe third criteria is a collection of ratios and analysis that taken together tell a story of the performance of the company and management’s ability to run the company and support dividends over the long term. Use judgment and make an evaluation. These are the following:\n1. Financial metrics 2. Market metrics 3. Qualitative metrics\nThe soft thresholds are discussed in section 10. These metrics only look at data prior to 2019.\n\nCheck DCF and NAIC value thresholds\n\n\nCode\n# check DCF scenario 1\ndcf_score = isv_S1/csp #ratio of isv to csp\ndcf_threshold = 0.7\nif dcf_score &lt; 0.7:\n    print('FAIL, DCF score is less than {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\nelse:\n    print('PASS, DCF score is above {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\n\n# check NAIC\nnaic_score = np.array([naic_price_eps_low,naic_price_pm_low]).min()/csp\nnaic_threshold = 1\nif naic_score &lt; 1:\n    print('FAIL, NAIC score is less than {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\nelse:\n    print('PASS, NAIC score is above {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\n\n# check both scores\nif naic_score &lt; 1 or dcf_score &lt; 0.7:\n    print('One or both DCF and NAIC scores failed')\nelse:\n    print('Both DCF and NAIC scores pass')\n\n\nFAIL, DCF score is less than 0.7 at -1.3\nFAIL, NAIC score is less than 1.0 at -6.0\nOne or both DCF and NAIC scores failed\n\n\n\n\nCheck debt thresholds\n\n\nCode\ndebt_lookback = 6\navg_LT_debt2EQ = df_dcf_data['long_term_debt'][-debt_lookback:-2].mean()/df_metrics_data['shareholder_equity'][-debt_lookback:-2].mean()\navg_TLiability2EQ = df_metrics_data['total_liabilities'][-debt_lookback:-2].mean()/df_metrics_data['shareholder_equity'][-debt_lookback:-2].mean()\navg_TLiability2NOP = df_metrics_data['total_liabilities'][-debt_lookback:-2].mean()/nop[-debt_lookback:-2].mean()\n\nprint('long term debt to shareholder equity ratio = {:.2f}'.format(avg_LT_debt2EQ))\nprint('total liabilities to shareholder equity ratio = {:.2f}'.format(avg_TLiability2EQ))\nprint('total liabilities to NOP ratio = {:.2f}'.format(avg_TLiability2NOP))\n\nif (avg_LT_debt2EQ &gt; 2) or (avg_TLiability2EQ &gt; 2) or (avg_TLiability2NOP &gt; 10):\n    print('FAILED one of the debt threshold limits')\nelse:\n    print('passed debt threshold limits')\n\n\nlong term debt to shareholder equity ratio = 1.36\ntotal liabilities to shareholder equity ratio = 1.85\ntotal liabilities to NOP ratio = 10.44\nFAILED one of the debt threshold limits\n\n\n\n\nCheck dividend payout and IIR thresholds\n\n\nCode\n# check dividend payout ratio average the last three years\nprint('Dividends are paid at {:.1f}% of cash flow'.format(\n    (df_metrics_data['dividends']/df_metrics_data['free_cash_flow'])[-3:].mean()*100))\nprint('Dividends are paid at {:.1f}% of NOP'.format((df_metrics_data['dividends']/nop)[-3:].mean()*100))\n\nif ((df_metrics_data['dividends']/nop)[-3:].mean() or (df_metrics_data['dividends']/df_metrics_data['free_cash_flow'])[-3:].mean()) &gt; 0.5:\n    print('FAIL, dividend payout ratio too high')\n\n\nDividends are paid at 0.0% of cash flow\nDividends are paid at 0.0% of NOP\n\n\n\n\nCode\n# Check dividend IRR limit\nif dividend_irr &lt; (tby+eq_rp):\n    print('FAIL, dividend IRR is less than {:.2f} at {:.2f}'.format((tby+eq_rp)*100,dividend_irr*100))\nelse:\n    print('PASS, dividend IRR is above {:.2f} at {:.2f}'.format((tby+eq_rp)*100,dividend_irr*100))\n\n\nFAIL, dividend IRR is less than 6.04 at -0.29"
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#recent-quarterly-performance",
    "href": "OLD NCLHv1 analysis.html#recent-quarterly-performance",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Recent quarterly performance ",
    "text": "Recent quarterly performance \nQuarterly financial reports were obtained from the company’s investor relations web page. These reports are available in pdf, spreadsheet or XBRL format. Data from the NCL’s spreadsheets was copied and organized into a new spreadsheet to be used in this notebook.\nThe python code below reads the new excel spreadsheet and converts the data into a pandas dataframe. Dollar amounts are converted from thousands of dollars to straight dollars. Dates are converted from a character string to datetime format.\nThe code below reads the data from the excel sheet which has already been converted into a pandas dataframe and puts the data into temporary variables.\n\n\nCode\n#column names: dates for ending of quarter \nqrtr_dates = df_qrt_sheet.columns[1:].values.astype('datetime64[D]')\n\n# load spreadsheet row data from sheet into temp variables\nrevenue_qrtr_data = df_qrt_sheet.iloc[0].to_numpy()[1:].astype('float')\ncruise_operating_expense_qrtr_data = df_qrt_sheet.iloc[1].to_numpy()[1:].astype('float')\nmarketing_general_and_administrative_qrtr_data = df_qrt_sheet.iloc[2].to_numpy()[1:].astype('float')\ndepreciation_qrtr_data = df_qrt_sheet.iloc[3].to_numpy()[1:].astype('float')\nother_operating_expense_qrtr_data = df_qrt_sheet.iloc[4].to_numpy()[1:].astype('float')\noperating_income_qrtr_data = df_qrt_sheet.iloc[5].to_numpy()[1:].astype('float')\ncash_and_cash_equivalents_qrtr_data = df_qrt_sheet.iloc[6].to_numpy()[1:].astype('float')\naccounts_receivable_qrtr_data = df_qrt_sheet.iloc[7].to_numpy()[1:].astype('float')\ninventories_qrtr_data = df_qrt_sheet.iloc[8].to_numpy()[1:].astype('float')\nprepaid_expenses_and_other_assets_qrtr_data = df_qrt_sheet.iloc[9].to_numpy()[1:].astype('float')\ntotal_current_assets_qrtr_data = df_qrt_sheet.iloc[10].to_numpy()[1:].astype('float')\nproperty_and_equipment_qrtr_data = df_qrt_sheet.iloc[11].to_numpy()[1:].astype('float')\ngoodwill_qrtr_data = df_qrt_sheet.iloc[12].to_numpy()[1:].astype('float')\ntrade_names_qrtr_data = df_qrt_sheet.iloc[13].to_numpy()[1:].astype('float')\nother_long_term_assets_qrtr_data = df_qrt_sheet.iloc[14].to_numpy()[1:].astype('float')\ntotal_assets_qrtr_data = df_qrt_sheet.iloc[15].to_numpy()[1:].astype('float')\ncurrent_portion_of_long_term_debt_qrtr_data = df_qrt_sheet.iloc[16].to_numpy()[1:].astype('float')\naccounts_payable_qrtr_data = df_qrt_sheet.iloc[17].to_numpy()[1:].astype('float')\naccrued_expenses_and_other_liabilities_qrtr_data = df_qrt_sheet.iloc[18].to_numpy()[1:].astype('float')\nadvance_ticket_sales_qrtr_data = df_qrt_sheet.iloc[19].to_numpy()[1:].astype('float')\ntotal_current_liabilities_qrtr_data = df_qrt_sheet.iloc[20].to_numpy()[1:].astype('float')\nlong_term_debt_qrtr_data = df_qrt_sheet.iloc[21].to_numpy()[1:].astype('float')\nother_long_term_liabilities_qrtr_data = df_qrt_sheet.iloc[22].to_numpy()[1:].astype('float')\ntotal_liabilities_qrtr_data = df_qrt_sheet.iloc[23].to_numpy()[1:].astype('float')\nnet_loss_qrtr_data = df_qrt_sheet.iloc[24].to_numpy()[1:].astype('float')\nadditions_to_property_and_equipment_qrtr_data = df_qrt_sheet.iloc[25].to_numpy()[1:].astype('float')\nnet_cash_provided_by_investing_activities_qrtr_data = df_qrt_sheet.iloc[26].to_numpy()[1:].astype('float')\nrepayments_of_long_term_debt_qrtr_data = df_qrt_sheet.iloc[27].to_numpy()[1:].astype('float')\nproceeds_from_long_term_debt_qrtr_data = df_qrt_sheet.iloc[28].to_numpy()[1:].astype('float')\ncommon_share_issuance_proceeds_qrtr_data = df_qrt_sheet.iloc[29].to_numpy()[1:].astype('float')\nearly_redemption_premium_qrtr_data = df_qrt_sheet.iloc[30].to_numpy()[1:].astype('float')\ndeferred_financing_fees_qrtr_data = df_qrt_sheet.iloc[31].to_numpy()[1:].astype('float')\nnet_cash_provided_by_financing_activities_qrtr_data = df_qrt_sheet.iloc[32].to_numpy()[1:].astype('float')\nnet_increase_in_cash_and_cash_equivalents_qrtr_data = df_qrt_sheet.iloc[33].to_numpy()[1:].astype('float')\ncash_and_cash_equivalents_at_beginning_of_period_qrtr_data = df_qrt_sheet.iloc[34].to_numpy()[1:].astype('float')\ncash_and_cash_equivalents_at_end_of_period_qrtr_data = df_qrt_sheet.iloc[35].to_numpy()[1:].astype('float')\nweighted_average_shares_outstanding_basic_qrtr_data = df_qrt_sheet.iloc[36].to_numpy()[1:].astype('float')\nfuture_cruise_credits_qrtr_data = df_qrt_sheet.iloc[37].to_numpy()[1:].astype('float')\n\n\nThe code below takes the temporary variables, converts thousands of dollars to actual dollars, by multiplying by 1000, reverses the order of the data and stores the data back into a new dataframe.\n\n\nCode\n# make a new data frame to store selected quarterly data\ndf_qrtr_data = pd.DataFrame(data={\n    'QTR':qrtr_dates[::-1],\n    'revenue':revenue_qrtr_data[::-1]*1000,\n    'cruise_operating_expense':cruise_operating_expense_qrtr_data[::-1]*1000,\n    'marketing_general_and_administrative':marketing_general_and_administrative_qrtr_data[::-1]*1000,\n    'depreciation':depreciation_qrtr_data[::-1]*1000,\n    'other_operating_expense':other_operating_expense_qrtr_data[::-1]*1000,\n    'operating_income':operating_income_qrtr_data[::-1]*1000,\n    'cash_and_cash_equivalents':cash_and_cash_equivalents_qrtr_data[::-1]*1000,\n    'accounts_receivable':accounts_receivable_qrtr_data[::-1]*1000,\n    'inventories':inventories_qrtr_data[::-1]*1000,\n    'prepaid_expenses_and_other_assets':prepaid_expenses_and_other_assets_qrtr_data[::-1]*1000,\n    'total_current_assets':total_current_assets_qrtr_data[::-1]*1000,\n    'property_and_equipment':property_and_equipment_qrtr_data[::-1]*1000,\n    'goodwill':goodwill_qrtr_data[::-1]*1000,\n    'trade_names':trade_names_qrtr_data[::-1]*1000,\n    'other_long_term_assets':other_long_term_assets_qrtr_data[::-1]*1000,\n    'total_assets':total_assets_qrtr_data[::-1]*1000,\n    'current_portion_of_long_term_debt':current_portion_of_long_term_debt_qrtr_data[::-1]*1000,\n    'accounts_payable':accounts_payable_qrtr_data[::-1]*1000,\n    'accrued_expenses_and_other_liabilities':accrued_expenses_and_other_liabilities_qrtr_data[::-1]*1000,\n    'advance_ticket_sales':advance_ticket_sales_qrtr_data[::-1]*1000,\n    'total_current_liabilities':total_current_liabilities_qrtr_data[::-1]*1000,\n    'long_term_debt':long_term_debt_qrtr_data[::-1]*1000,\n    'other_long_term_liabilities':other_long_term_liabilities_qrtr_data[::-1]*1000,\n    'total_liabilities':total_liabilities_qrtr_data[::-1]*1000,\n    'net_loss':net_loss_qrtr_data[::-1]*1000,\n    'additions_to_property_and_equipment':additions_to_property_and_equipment_qrtr_data[::-1]*1000,\n    'net_cash_provided_by_investing_activities':net_cash_provided_by_investing_activities_qrtr_data[::-1]*1000,\n    'repayments_of_long_term_debt':repayments_of_long_term_debt_qrtr_data[::-1]*1000,\n    'proceeds_from_long_term_debt':proceeds_from_long_term_debt_qrtr_data[::-1]*1000,\n    'common_share_issuance_proceeds':common_share_issuance_proceeds_qrtr_data[::-1]*1000,\n    'early_redemption_premium':early_redemption_premium_qrtr_data[::-1]*1000,\n    'deferred_financing_fees':deferred_financing_fees_qrtr_data[::-1]*1000,\n    'net_cash_provided_by_financing_activities':net_cash_provided_by_financing_activities_qrtr_data[::-1]*1000,\n    'net_increase_in_cash_and_cash_equivalents':net_increase_in_cash_and_cash_equivalents_qrtr_data[::-1]*1000,\n    'cash_and_cash_equivalents_at_beginning_of_period':cash_and_cash_equivalents_at_beginning_of_period_qrtr_data[::-1]*1000,\n    'cash_and_cash_equivalents_at_end_of_period':cash_and_cash_equivalents_at_end_of_period_qrtr_data[::-1]*1000,\n    'weighted_average_shares_outstanding_basic':weighted_average_shares_outstanding_basic_qrtr_data[::-1],\n    'future_cruise_credits':future_cruise_credits_qrtr_data[::-1]\n    })\n\n\n\nRevisions to Previously Reported Quarterly Financial Statements, page 10 of Sept 2022 10Q\nDuring the fourth quarter of 2021, the Company identified an error in its consolidated balance sheet as of September 30, 2021 and consolidated statement of cash flows for the nine months ended September 30, 2021. Based on their nature, certain amounts shown as cash and cash equivalents should have been classified as short-term investments. We have determined that these errors were not material to the previously issued interim financial statements for the period ended September 30, 2021.\nAs a result of the error, the amounts previously reported as cash and cash equivalents have been reclassified to cash flows used in investing activities in the consolidated statement of cash flows for the nine months ended September 30, 2021 as follows (in thousands):\n\n\nNews for Nov 2022\nNCL to Pay Travel Agent Partners Commission on Non-Commissionable Fares\nNorwegian Cruise Line To Pay Travel Advisors Commission on Non-Commissionable Fares\nNCL To Pay Travel Agents Commission on NCFs"
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#income",
    "href": "OLD NCLHv1 analysis.html#income",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Income",
    "text": "Income\nThe plot below shows the recent quarterly income plotted from the first quarter of 2019 to the second quarter of 2022. Revenue, operating income, advance ticket sales and cost of operations are plotted. Markers are included on the zero dollar line that indicate the dates for operations suspended, first ship to return to service and full fleet in operation. The dates for operations suspended (March 13, 2020), 1st ship return to service (July 25, 2021) and full fleet operating (May 7, 2022) are indicated on the plot with markers.\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $B')\n\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['revenue']/1e9,'-+b',label='revenue')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['operating_income']/1e9,'-+r',label='operating income')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['advance_ticket_sales']/1e9,'-+g',label='advance ticket sales')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['cruise_operating_expense']/1e9,'-+c',label='cruise operating expense')\n\nax1.plot(pd.Timestamp(datetime(2020,3,13)),0,'^k',label='operations suspended, March 13') # March 13, 2020, operations suspended\nax1.plot(pd.Timestamp(datetime(2021,7,25)),0,'ob',label='1st ship return to service, July 25') # July 25, 2021, 1st ship return to service\nax1.plot(pd.Timestamp(datetime(2022,5,7)),0,'&gt;k',label='full fleet operating, May 7') # May 7 2022, full fleet operating\n\nax1.tick_params(axis='y')\nax1.set_ylim((-2,3))\nplt.legend(bbox_to_anchor=(1.7, 1))\n#ax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Recent quarterly income items')\nplt.show()\n\n\n\n\n\nRevenue prior to the cruise industry shutdown was between one and two billion dollars per quarter (depending on the season), with revenue in the third quarter of 2019 at about 2 billion dollars. Advance ticket sales were running just under about two billion per quarter. Since the restart of cruise, advanced ticket sales (which include future cruise credits) have returned to pre-pandemic levels. Cruise line operating expenses were about one billion dollars per quarter and have returned to pre-pandemic levels as the full fleet is back online. During the current quarter revenue has exceeded operating expenses, a positive indicator. Operating income has increased during the current quarter.\nIn the plot below, quarterly data for revenue, cruise operating expense, marketing general and administrative and net operating profit (NOP) are shown. The dates for operations suspended (March 13, 2020), 1st ship return to service (July 25, 2021) and full fleet operating (May 7 2022) are indicated on the plot with markers.\n\n\nCode\nnop_qrtr = (df_qrtr_data['revenue'].to_numpy() - \\\n    (df_qrtr_data['cruise_operating_expense'].to_numpy() + \\\n    df_qrtr_data['marketing_general_and_administrative'].to_numpy()  ))\n\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $B')\n\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['revenue']/1e9,'-+b',label='revenue')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['cruise_operating_expense']/1e9,'-+c',label='cruise operating expense')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['marketing_general_and_administrative']/1e9,'-+m',label='marketing general & administrative')\nax1.plot(df_qrtr_data['QTR'],nop_qrtr/1e9,'-+r',label='NOP')\n#ax1.plot(df_qrtr_data['QTR'],df_qrtr_data['advance_ticket_sales']/1e9,'-+g',label='advance ticket sales')\n\nax1.plot(pd.Timestamp(datetime(2020,3,13)),0,'^k',label='operations suspended, March 13') # March 13, 2020, operations suspended\nax1.plot(pd.Timestamp(datetime(2021,7,25)),0,'ob',label='1st ship return to service, July 25') # July 25, 2021, 1st ship return to service\nax1.plot(pd.Timestamp(datetime(2022,5,7)),0,'&gt;k',label='full fleet operating, May 7') # May 7 2022, full fleet operating\n\nax1.tick_params(axis='y')\nax1.set_ylim((-1,2))\nplt.legend(bbox_to_anchor=(1.1, 1))\n#ax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('NOP and supporting data')\nplt.show()\n\n\n\n\n\nNet operating profit (NOP) reflects revenue levels and expense requirements of the operating business.\n\\(\\text{NOPM} = \\frac{\\text{Revenue} - \\text{Expenses}}{\\text{Revenue}}\\)\n\\(\\text{Expenses} = \\text{Cost of Goods Sold (CGS)} + \\text{General and Administrative (G&A)} + \\text{Research and Development (R&D)}\\)\nGeneral and Administrative (G&A) is also called Sales, General and Administrative (SG&A)\nNOP is used to analyze the performance of a company’s core operations without the costs of the capital structure and tax expenses impacting profit. It is a more concise measure of corporate performance since it is able to show earnings before the influence of accounting and financial deductions, also used above in the DCF model. NOP as defined here is roughly equivalent to EBIT (earnings before interest and taxes). What is concerning is that NOP has remained flat at a level of 0.7 billion dollars per quarter for the last four quarters and has not trended up with increasing revenue."
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#statement-of-income",
    "href": "OLD NCLHv1 analysis.html#statement-of-income",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Statement of income",
    "text": "Statement of income\n\nPercentage change in quarterly statement of income\nThe following chart shows percentage change in quarterly consolidated statement of income, looking back over the last ten quarters or so. This can be used to investigate recient trends.\n\\(\\large{\\color{red}{\\text{update this section as analysis of balance sheet}}}\\)\npercent change along with values\nfix colors, make the same b,g,r,c,m,y,k\n\n\nCode\n# Set the locator\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\n# figsize() function to adjust the size\nplt.subplots(1,2,figsize=(15, 5))\n\n# set look back range, left_qtr is the index into the date range\nleft_qtr = -6\n\n# using subplot function and creating two side by side plots\n# plot one\nplt.subplot(1, 2, 1)\n\nplt.plot(df_qrtr_data['QTR'][left_qtr:],df_qrtr_data['revenue'][left_qtr:]/1e9,'-+b',label='revenue')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],df_qrtr_data['cruise_operating_expense'][left_qtr:]/1e9,'-+g',label='cruise operating expense')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],df_qrtr_data['marketing_general_and_administrative'][left_qtr:]/1e9,'-+r',label='marketing general & administrative')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],df_qrtr_data['other_operating_expense'][left_qtr:]/1e9,'-+c',label='other_operating_expense')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],df_qrtr_data['operating_income'][left_qtr:]/1e9,'-+m',label='operating income')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\n# Changes x-axis range\n#plt.gca().set_xbound(year_ended_list[left_qtr], year_ended_list[-1])\n#plt.gca().set_xbound(df_qrtr_data['QTR'].iloc[left_qtr], df_qrtr_data['QTR'].iloc[-1])\n\n#plt.ylim((-100,500))\nplt.title('Recent quarterly income items')\nax1.set_ylabel('dollars, $B')\nplt.legend()\n\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['revenue'][left_qtr:]-df_qrtr_data['revenue'].iloc[left_qtr])/np.abs(df_qrtr_data['revenue'].iloc[left_qtr])*1,\n    '^-b',label='revenue/100')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['cruise_operating_expense'][left_qtr:]-df_qrtr_data['cruise_operating_expense'].iloc[left_qtr])/np.abs(df_qrtr_data['cruise_operating_expense'].iloc[left_qtr])*10,\n    '^-g',label='cruise operating expense/10')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['marketing_general_and_administrative'][left_qtr:]-df_qrtr_data['marketing_general_and_administrative'].iloc[left_qtr])/np.abs(df_qrtr_data['marketing_general_and_administrative'].iloc[left_qtr])*100,\n    '^-r',label='marketing general & administrative')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['other_operating_expense'][left_qtr:]-df_qrtr_data['other_operating_expense'].iloc[left_qtr])/np.abs(df_qrtr_data['other_operating_expense'].iloc[left_qtr])*100,\n    '^-c',label='other operating expense')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['operating_income'][left_qtr:]-df_qrtr_data['operating_income'].iloc[left_qtr])/np.abs(df_qrtr_data['operating_income'].iloc[left_qtr])*100,\n    '^-m',label='operating income')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Percentage change in quarterly income')\nplt.ylabel('percent change')\nplt.legend()\n\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\nThe plot above scales revenue by 100 and cruise operating expense by 10 inorder to get the plots within a simular range. Revenue, cruise operating expense, marketing general & administrative and other operating expense have been increasing over the period as NCL has resumed operations. In the last two quarters operating income reversed its negative trend."
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#balance-sheet",
    "href": "OLD NCLHv1 analysis.html#balance-sheet",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Balance sheet",
    "text": "Balance sheet\n\nPercentage change in quarterly balance sheet\nThe following chart shows percentage change in quarterly consolidated balance sheet, looking back over the last ten quarters or so. The first graph show the current assets and liabilities.\n\\(\\large{\\color{red}{\\text{plot current along side of total assets and liabilities}}}\\)\n\\(\\large{\\color{red}{\\text{organized balance sheet analysis into assets and liabilities, long and short, percent change and magnitude}}}\\)\nSeems like advance ticket sales could be driving quick ratio.\nThink about removing advance ticket sales from debt calculations.\n\n\nCode\n# Set the locator\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\n# figsize() function to adjust the size\nplt.subplots(1,2,figsize=(15, 5))\n\n# set look back range, left_qtr is the index into the date range\nleft_qtr = -6\n\n# using subplot function and creating two side by side plots\n# plot one\nplt.subplot(1, 2, 1)\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['total_current_assets'][left_qtr:]-df_qrtr_data['total_current_assets'].iloc[left_qtr])/np.abs(df_qrtr_data['total_current_assets'].iloc[left_qtr])*100,\n    '^-g',label='total_current_assets')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['total_current_liabilities'][left_qtr:]-df_qrtr_data['total_current_liabilities'].iloc[left_qtr])/np.abs(df_qrtr_data['total_current_liabilities'].iloc[left_qtr])*100,\n    '^-r',label='total_current_liabilities')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\n# Changes x-axis range\n#plt.gca().set_xbound(year_ended_list[left_qtr], year_ended_list[-1])\n#plt.gca().set_xbound(df_qrtr_data['QTR'].iloc[left_qtr], df_qrtr_data['QTR'].iloc[-1])\n\n#plt.ylim((0,20))\nplt.title('Percentage change in current assets and liabilities')\nplt.ylabel('percent change')\nplt.legend()\n\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n# color sequence b,g,r,c,m,y,k\n\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['total_assets'][left_qtr:]-df_qrtr_data['total_assets'].iloc[left_qtr])/np.abs(df_qrtr_data['total_assets'].iloc[left_qtr])*100,\n    '^-g',label='total assets')\nplt.plot(df_qrtr_data['QTR'][left_qtr:],\n    (df_qrtr_data['total_liabilities'][left_qtr:]-df_qrtr_data['total_liabilities'].iloc[left_qtr])/np.abs(df_qrtr_data['total_liabilities'].iloc[left_qtr])*100,\n    '^-r',label='total_liabilities')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Percentage change in total assets and liabilities')\nplt.ylabel('percent change')\nplt.legend()\n\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\n\nCurrent liabilites has been increasing over the past 9 quarters while the current assets have declined. This is probably due to NCL spening its cash reserves as it resumes operations."
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#debt-analysis",
    "href": "OLD NCLHv1 analysis.html#debt-analysis",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Debt analysis",
    "text": "Debt analysis\n\\(\\large{\\color{red}{\\text{update this section, remove principal payments}}}\\)\nThe current long term amounts were obtained from the quarterly reports. Since March 2020, Moody’s has downgraded NCLH’s long-term issuer rating to B2, the senior secured rating to B1 and the senior unsecured rating to Caa1. Since April 2020, S&P Global has downgraded the issuer credit rating to B, lowered the issue-level rating on the \\$875 million Revolving Loan Facility and \\$1.5 billion Term Loan A Facility to BB-, the issue-level rating on the other senior secured notes to B+ and the senior unsecured rating to B-. If the credit ratings were to be further downgraded, or general market conditions were to ascribe higher risk to NCLH’s rating levels, the cruise industry, NCLH’s access to capital and the cost of any debt or equity financing will be further negatively impacted. NCLH also has significant capacity to incur additional indebtedness under the debt agreements and may issue additional ordinary shares from time to time, subject to the authorized number of ordinary shares.\nNCLH may be required to pledge additional collateral and/or post additional cash reserves or take other actions that may reduce the liquidity.\nThe Principal Payout Schedule was obtained from a pdf file on the NCLH investor web page and a new dataframe was created for this data.\n\nUpdated from 2nd qtr reports\nPrincipal Payout Schedule (in U.S. dollars, thousands) As of September 30, 2022\n\n\n\nDate\nDebt\n\n\n\n\nQ4 2022\n331,440\n\n\n2023\n934,497\n\n\n2024\n3,683,554\n\n\n2025\n1,067,735\n\n\n2026\n1,971,347\n\n\n2027\n3,022,242"
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#debt-to-equity-ratio",
    "href": "OLD NCLHv1 analysis.html#debt-to-equity-ratio",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Debt to equity ratio",
    "text": "Debt to equity ratio\nThe debt to equity (D/E) ratio is used to evaluate a company’s financial leverage and is calculated by dividing a company’s total liabilities by its shareholder equity. Higher ratios tend to indicate a company with higher risk to shareholders. When using the D/E ratio, it is very important to consider the industry in which the company operates. Because different industries have different capital needs and growth rates, a relatively high D/E ratio may be common in one industry, while a relatively low D/E may be common in another. Generally speaking, a D/E ratio below 1.0 would be seen as relatively safe, whereas ratios of 2.0 or higher would be considered risky.\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars, $B, or ratio')\n\n# plot revenue, current assets and liabilities\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['revenue']/1e9,'-+',label='revenue')\n#ax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_assets']/1e9,'-+m',label='total assets')\n#ax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_liabilities']/1e9,'-+c',label='total liabilities')\nax1.plot(df_qrtr_data['QTR'],(df_qrtr_data['total_assets']-df_qrtr_data['total_liabilities'])/1e9,'-+',label='equity')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['long_term_debt']/(df_qrtr_data['total_assets']-df_qrtr_data['total_liabilities']),'-+',label='D/E')\n\nax1.plot(pd.Timestamp(datetime(2020,3,13)),0.2,'^k',label='operations suspended, March 13') # March 13, 2020, operations suspended\nax1.plot(pd.Timestamp(datetime(2021,7,25)),0.2,'ob',label='1st ship return to service, July 25') # July 25, 2021, 1st ship return to service\nax1.plot(pd.Timestamp(datetime(2022,5,7)),0.2,'&gt;k',label='full fleet operating, May 7') # May 7 2022, full fleet operating\n\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,14))\nax1.legend(bbox_to_anchor=(1.1, 1))\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\n#ax2 = ax1.twinx()\n#color = 'tab:green'\n\n#ax2.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_assets']/df_qrtr_data['total_current_liabilities'],'-.g',label='current ratio')\n \n#ax2.set_ylabel('current ratio',color=color)\n#ax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((0,3))\n#ax2.legend(bbox_to_anchor=(1.45, 0))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Debt to equity')\nplt.show()\n\n\n\n\n\nThe D/E ratio for NCL was historically less than two before the pandemic, however, presently the D/E ratio is above 30. The company’s survival took precedence over maintaining a healthy balance sheet.\nNov 13, 2022 yahoo finance reported cruise line D/E as follows: - Royal Caribbean Cruises Ltd. (RCL): D/E = 7.47 - Carnival Corporation & plc (CCL): D/E = 4.21 - Norwegian Cruise Line Holdings Ltd. (NCLH): D/E = 36.63\n\nCurrent Ratio\nThe current ratio is a liquidity ratio that measures a company’s ability to pay short-term obligations or those due within one year. In theory, the higher the current ratio, the more capable a company is of paying its obligations because it has a larger proportion of short-term asset value relative to the value of its short-term liabilities. The current ratio can be a useful measure of a company’s short-term solvency when it is placed in the context of what has been historically normal for the company and its peer group. It also offers more insight when calculated repeatedly over several periods.\nThe plot below shows revenue, current assets, current liabilities and the current ratio. Markers are also shown for the dates when operations were suspended, the first NCL ship returned to service and when the full fleet was back in operation.\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars, $B, or current ratio')\n\n# plot revenue, current assets and liabilities\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['revenue']/1e9,'-+b',label='revenue')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_assets']/1e9,'-+m',label='current assets')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_liabilities']/1e9,'-+c',label='current liabilities')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_assets']/df_qrtr_data['total_current_liabilities'],'-.g',label='current ratio')\n\nax1.plot(pd.Timestamp(datetime(2020,3,13)),0.1,'^k',label='operations suspended, March 13') # March 13, 2020, operations suspended\nax1.plot(pd.Timestamp(datetime(2021,7,25)),0.1,'ob',label='1st ship return to service, July 25') # July 25, 2021, 1st ship return to service\nax1.plot(pd.Timestamp(datetime(2022,5,7)),0.1,'&gt;k',label='full fleet operating, May 7') # May 7 2022, full fleet operating\n\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,6))\nax1.legend(bbox_to_anchor=(1.1, 1))\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\n#ax2 = ax1.twinx()\n#color = 'tab:green'\n\n#ax2.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_assets']/df_qrtr_data['total_current_liabilities'],'-.g',label='current ratio')\n \n#ax2.set_ylabel('current ratio',color=color)\n#ax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((0,3))\n#ax2.legend(bbox_to_anchor=(1.45, 0))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Current ratio')\nplt.show()\n\n\n\n\n\nPrior to the pandemic shut down, NCLH was operating with a current ratio of about 0.25, a historical normal level for the company. During the shut down, NCL increased cash holding to sustain the company while cruise operations were suspended and no revenue was being generated. Now that cruise operations have started to ramp up, we see a reversion of the current ratio back to below a ratio of one. Since the company has negative earnings, a ratio below one is of concern, since it indicates the company might not have sufficient cash to sustain operations.\nThe plot below shows cash & cash equivalents and current liabilities less advance ticket sales. Markers are also shown for the dates when operations were suspended, the first NCLH ship returned to service and when the full fleet was back in operation. An additional marker shows the date of March 31, 2023, when the \\$1B commitment available will end. The expiration of the Commitment Facility was extended through March 31, 2023.\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars, $B, or ratio')\n\n# plot revenue, current assets and liabilities\n#ax1.plot(df_qrtr_data['QTR'],df_qrtr_data['advance_ticket_sales']/1e9,'-+',label='advance ticket sales')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['cash_and_cash_equivalents']/1e9,'-+',label='cash & cash equivalents')\n#ax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_liabilities']/1e9,'-+c',label='current liabilities')\nax1.plot(df_qrtr_data['QTR'],(df_qrtr_data['total_current_liabilities']-df_qrtr_data['advance_ticket_sales'])/1e9,'-+m',label='current liabilities less advance ticket sales')\n#ax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_assets']/1e9,'-+',label='current assets')\n\nax1.plot(pd.Timestamp(datetime(2020,3,13)),0.1,'^k',label='operations suspended, March 13') # March 13, 2020, operations suspended\nax1.plot(pd.Timestamp(datetime(2021,7,25)),0.1,'ob',label='1st ship return to service, July 25') # July 25, 2021, 1st ship return to service\nax1.plot(pd.Timestamp(datetime(2022,5,7)),0.1,'&gt;k',label='full fleet operating, May 7') # May 7 2022, full fleet operating\nax1.plot(pd.Timestamp(datetime(2023,3,31)),1.0,'*r',label='$1B commitment available ending, Mar 31') # March 31, 2023, $1B commitment available ending\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,4))\nax1.legend(bbox_to_anchor=(1.1, 1))\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\n#ax2 = ax1.twinx()\n#color = 'tab:green'\n\n#ax2.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_assets']/df_qrtr_data['total_current_liabilities'],'-.g',label='current ratio')\n \n#ax2.set_ylabel('current ratio',color=color)\n#ax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((0,3))\n#ax2.legend(bbox_to_anchor=(1.45, 0))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Liquidity')\nplt.show()\n\n\n\n\n\nOne of the components of current liabilities is advance ticket sales, which are deposits made by customers on future cruises or conversion of refunds owed from canceled cruises to future cruise credits. If we think of advance ticket sales as future revenue not yet classified as revenue, we can subtract that amount from current liabilities to get an indication of actual current liabilities.\nAs of December 31, 2021, NCLH had advance ticket sales of \\$1.8 billion, including the long-term portion, which included approximately \\$0.7 billion of future cruise credits. NCLH also has agreements with credit card processors that, as of December 31, 2021, governed approximately \\$1.3 billion in advance ticket sales that had been received by the Company relating to future voyages. These agreements allow the credit card processors to require under certain circumstances, including the existence of a material adverse change, excessive chargebacks and other triggering events, that the Company maintain a reserve which would be satisfied by posting collateral.\nAlthough the agreements vary, these requirements may generally be satisfied either through a percentage of customer payments withheld or providing cash funds directly to the card processor. Any cash reserve or collateral requested could be increased or decreased. As of December 31, 2021, NCLH had cash collateral reserves of approximately \\$1.2 billion with credit card processors recognized in accounts receivable, net or other long-term assets\nComparing current liabilities less advance ticket sales to cash and cash equivalents, we see that current liabilities less advance ticket sales has recently exceeded cash and cash equivalents, which is of concern."
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#book-value",
    "href": "OLD NCLHv1 analysis.html#book-value",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Book value",
    "text": "Book value\nThe book value of a company is the net difference between that company’s total assets and total liabilities, where book value reflects the total value of a company’s assets that shareholders of that company would receive if the company were to be liquidated. It serves as the total value of the company’s assets that shareholders would theoretically receive if a company was liquidated. This assumes that the liquidation value of assets is equal to the value claimed by the company.\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars, $B')\n\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_assets']/1e9,'-+',label='total assets')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['total_liabilities']/1e9,'-+',label='total liabilities')\nax1.plot(df_qrtr_data['QTR'],(df_qrtr_data['total_assets']-df_qrtr_data['total_liabilities'])/1e9,'-+',label='book value')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,20))\nplt.legend(bbox_to_anchor=(1.6, 1))\n#ax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Book value')\nplt.show()\n\n\n\n\n\nAs shown in the plot above, the difference between total assets and total liabilities has been narrowing over the past ten quarters as NCL has been raising cash with loans and by selling shares. If NCLH takes the \\$1 billion dollar commitment, which would increase liabilities by the same amount, the book value of the company would be about \\$0.5 billion dollars.\n\\(\\large{\\color{red}{\\text{update comments about book value}}}\\)\n\n\nCode\nprint('book value MRQ = ${:.2f}B'.\\\n    format(df_qrtr_data['total_assets'].iloc[-1]/1e9 - (df_qrtr_data['total_liabilities'].iloc[-1]/1e9)))\n\n\nbook value MRQ = $0.07B\n\n\n\n\nCode\nprint('book value with $1 billion dollar commitment = ${:.2f}B'.\\\n    format(df_qrtr_data['total_assets'].iloc[-1]/1e9 - (df_qrtr_data['total_liabilities'].iloc[-1]/1e9 + 1.0)))\n\n\nbook value with $1 billion dollar commitment = $-0.93B\n\n\n\nWeighted average of outstanding shares\nThe weighted average of outstanding shares is a calculation that incorporates any changes in the number of a company’s outstanding shares over a reporting period. The reporting period usually coincides with a company’s quarterly or annual reports. NCL calculates the weighted average of outstanding shares and reports the number. Shares outstanding refers to the amount of stock held by shareholders, including restrictive shares held by company insiders. The plot below shows the number of weighted average of outstanding shares each quarter.\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('basic shares outstanding, M')\n\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['weighted_average_shares_outstanding_basic']/1e6,'-+',label='weighted_average_shares_outstanding_basic, M')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,20))\n#plt.legend(bbox_to_anchor=(1.6, 1))\n#ax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Weighted average shares outstanding basic')\nplt.show()\n\n\n\n\n\nThe plot above shows that weighted average shares outstanding basic at the end of each quarter. NCLH has been raising cash by selling shares and as can be seen, there are almost double the number of shares outstanding.\n\n\nBook value per share\nBook value per share is a method to calculate the per-share book value of a company based on common shareholders’ equity in the company. Should the company dissolve, the book value per common share indicates the dollar value remaining for common shareholders after all assets are liquidated and all debtors are paid. A company’s share price usually trades at many multiples of the book value, so book value can serve as a reference point. A declining book value would be a red flag for an investor.\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars')\n\nax1.plot(df_qrtr_data['QTR'],(df_qrtr_data['total_assets']-df_qrtr_data['total_liabilities'])/df_qrtr_data['weighted_average_shares_outstanding_basic'],'-+',label='book value per share')\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,35))\n#plt.legend(bbox_to_anchor=(1.6, 1))\n#ax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Book value per weighted average share')\nplt.show()\n\nprint('book value per weighted average share, MRQ = ${:.2f}'.\\\n    format((df_qrtr_data['total_assets'].iloc[-1]-df_qrtr_data['total_liabilities'].iloc[-1])/df_qrtr_data['weighted_average_shares_outstanding_basic'].iloc[-1]))\n\n\n\n\n\nbook value per weighted average share, MRQ = $0.16\n\n\nThe Book value per weighted average share at the end of the second quarter of 2022 was less than \\$5 dollars. This measure of value indicates that investors have had their investments diluted by about a factor of 10 compared to pre pandemic share prices. The current share price reflects some optimism in the ability of the company to regain at least some positive cash flow in the near future.\nNovember 13, 2022 yahoo finance reported cruise line Book Value per Share (BVS) as follows: - Royal Caribbean Cruises Ltd. (RCL): BVS = \\$12.56 - Carnival Corporation & plc (CCL): BVS = \\$6.66 - Norwegian Cruise Line Holdings Ltd. (NCLH): BVS = \\$0.95\n\n\nBookings\nNCLH generates the majority of revenue from ticket sales. NCLH has stated that NCLH will not discount tickets to fill their ships as this would hurt the brand.\nBooking trends for full year 2023 remain positive with cumulative booked position in line with a record 2019 inclusive of the Company’s 20% increase in capacity. Pricing continues to be significantly higher than that of 2019 at a similar point in time and thus at record levels for full year 2023.\nThe future cruise credits are not contracts, and therefore, guests who elected this option are excluded from our contract liability balance; however, the credit for the original amount paid is included in advance ticket sales.\nThe future cruise credits issued under these programs are generally valid for any sailing through December 31, 2022, and we may extend the length of time these future cruise credits may be redeemed. The use of such credits may prevent us from garnering certain future cash collections as staterooms booked by guests with such credits will not be available for sale, resulting in less cash collected from bookings to new guests. We may incur incremental commission expense for the use of these future cruise credits.\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $B')\n\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['advance_ticket_sales']/1e9,'-+g',label='advance ticket sales')\nax1.plot(df_qrtr_data['QTR'],df_qrtr_data['future_cruise_credits']/1e9,'-+b',label='future cruise credits')\nax1.plot(df_qrtr_data['QTR'],(df_qrtr_data['advance_ticket_sales']-df_qrtr_data['future_cruise_credits'])/1e9,'-.',label='ticket sales less future cruise credits')\n\nax1.plot(pd.Timestamp(datetime(2020,3,13)),0,'^k',label='operations suspended, March 13') # March 13, 2020, operations suspended\nax1.plot(pd.Timestamp(datetime(2021,7,25)),0,'ob',label='1st ship return to service, July 25') # July 25, 2021, 1st ship return to service\nax1.plot(pd.Timestamp(datetime(2022,5,7)),0,'&gt;k',label='full fleet operating, May 7') # May 7 2022, full fleet operating\n\nax1.tick_params(axis='y')\nax1.set_ylim((-0.1,2.5))\nplt.legend(bbox_to_anchor=(1.7, 1))\n#ax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Ticket sales')\nplt.show()\n\n\n\n\n\nAny value in looking at Passengers carried, Passenger Cruise Days, berths and Capacity Days?\nWhy the difference between advanced ticket sales and ticket sales less FCC? Seems like during the pause, all ticket sales should have been converted to FCC. Ticket sales less future cruise credits should be new bookings, which are almost \\$2B.\n\nFrom Sep 2022 10Q\nDilution from value-add FCCs issued during the pandemic will not carry over into 2023 as the bonus portion of these FCCs expire at YE2022. (from slide 13 of Third Quarter 2022 Earnings Conference Call, November 8, 2022)"
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#liquidity-model",
    "href": "OLD NCLHv1 analysis.html#liquidity-model",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "Liquidity model",
    "text": "Liquidity model\nA liquidity model was developed to estimate future cash on hand and ability to pay future debt. The liquidity model starts with a projected occupancy profile and number of berths available. Revenue is calculated from estimated passenger cruise days. Operating expense is estimated from capacity days. Future cash flow is estimated from revenue less operating expenses and marketing, general and administrative expenses. Liquidity is the running total of future cash flows (which might be negative) and the present cash and cash equivalents less debt payments.\nThe calculations for future liquidity follow these steps: - estimate future occupancy percentage, currently at 80% and rising to final value of 107% - data for future berths - estimate capacity days - estimate passenger cruise days - estimate rev per quarter - estimate operating expense per quarter - calculate future cash flow - calculate cash on hand and subtract debt payments\nDuring the Second Quarter 2022 Earnings Conference Call, August 9, 2022, NCLH stated that they want to reach historical occupancy levels by Q2 2023. The occupancy percentage is currently at 80%.\nLooking out 11 quarters to the \\$3.68 billion dollar principle payment due in 2024, which is 11 quarters from now, make a list of the dates for each quarter and calculate the values in the model at each date.\n\\(\\large{\\color{red}{\\text{check calculations in spreadsheet}}}\\)\nmistake - future berths, start with date of June 2022, need to start with MRQ\n\nFuture berths\nNCLH has nine new ships on order and scheduled to be delivered through 2027. Future berths are contained in the dataframe declared below. Updates from Third Quarter 2022 Earnings Conference Call, November 8, 2022, estimated placement by quarter.\n\n\n\nDate\nShip\nBerths Added\n\n\n\n\n3rd quarter 2022\nPrima\n3,100\n\n\n2nd quarter 2023\nViva\n3,100\n\n\n2nd quarter 2023\nVista\n1,200\n\n\n4th quarter 2023\nGrandeur\n750\n\n\n2024\ndeliveries delayed\n0\n\n\n1st quarter 2025\nPrima+A\n3,550\n\n\n2nd quarter 2025\nAllura Class\n1,200\n\n\n4th quarter 2025\nPrima+B\n3,550\n\n\n4th quarter 2026\nPrima+C\n3,550\n\n\n4th quarter 2027\nPrima+D\n3,550\n\n\n\n\n\nShip Construction Contracts\nFor the Norwegian brand, the first Prima Class Ship, Norwegian Prima, at approximately 143,500 Gross Tons and with 3,100 Berths, was delivered in July 2022.\n\n\nCode\ndf_ship_construction_contracts = pd.DataFrame(data={\n    'date':np.array(['2022-12-30','2023-12-30','2024-12-30','2025-12-30','2026-12-30','2027-12-30']).astype('datetime64[D]'),\n    'amount':np.array([25047,2178120,249655,1463558,1648375,761282])*1000    \n})\n\n\n\n\nCode\n# Set the locator\nlocator = mdates.AutoDateLocator()\n#locator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\n#fmt = mdates.DateFormatter('%b %Y')\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars, $B')\n\n# plot Ship construction contracts\n#ax1.bar(df_material_cash_requirements['date'],df_material_cash_requirements['long term debt']/1e9, width=50,label='long term debt, principal+interest payments')\nax1.bar(df_ship_construction_contracts['date'],df_ship_construction_contracts['amount']/1e9, width=50,label='ship construction')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,14))\n#ax1.legend(bbox_to_anchor=(1.4, 1))\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\n#ax2 = ax1.twinx()\n#color = 'tab:green'\n\n#ax2.plot(df_qrtr_data['QTR'],df_qrtr_data['total_current_assets']/df_qrtr_data['total_current_liabilities'],'-.g',label='current ratio')\n \n#ax2.set_ylabel('current ratio',color=color)\n#ax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((0,3))\n#ax2.legend(bbox_to_anchor=(1.45, 0))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Ship construction contracts')\nplt.show()\n\n\n\n\n\n\nUpdate from 10Q 2022\nThe impacts of COVID-19 on the shipyards where our ships are under construction (or will be constructed), Russia’s ongoing invasion of Ukraine and/or other macroeconomic events, have already resulted in some delays in expected ship deliveries. These impacts along with other potential modifications the Company may make to its newbuilds, including potential initiatives to improve environmental sustainability, are expected to result in additional delays in ship deliveries in the future, which may be prolonged.\nThe combined contract prices of the eight ships on order for delivery as of September 30, 2022 was approximately €6.7 billion, or \\$6.6 billion based on the euro/U.S. dollar exchange rate as of September 30, 2022. We have obtained export credit financing which is expected to fund approximately 80% of the contract price of each ship, subject to certain conditions. We do not anticipate any contractual breaches or cancellations to occur. However, if any such events were to occur, it could result in, among other things, the forfeiture of prior deposits or payments made by us and potential claims and impairment losses which may materially impact our business, financial condition and results of operations.\n\n\nCode\n# data from 2nd qrt, as of June 30, 2022, data presentation\ndf_future_berths = pd.DataFrame(data={\n    'date':np.array(['2022-06-30','2022-09-30','2022-12-31','2023-03-31','2023-06-30','2023-09-30','2023-12-31',\n    '2024-03-31','2024-06-30','2024-09-30','2024-12-31','2025-03-31','2025-06-30','2025-09-30',\n    '2025-12-31','2026-03-31','2026-06-30','2026-09-30','2026-12-31','2027-03-31','2027-06-30',\n    '2027-09-30','2027-12-31','2028-03-31','2028-06-30','2028-09-30','2028-12-31']).astype('datetime64[D]'),\n    'berths':np.array([59150,63000,63000,63000,64200,67300,68050,68050,68050,68050,\n        68050,71600,72800,72800,76350,76350,79900,79900,83450,83450,83450,83450,\n        87000,87000,87000,87000,87000]),\n    'ships':np.array([28,29,29,29,30,31,32,32,32,32,32,33,34,34,35,35,36,36,37,\n        37,37,37,38,38,38,38,38])    \n})\n\n\n\n\nCode\n# Set the locator\n#locator = mdates.AutoDateLocator()\nlocator = mdates.MonthLocator((1,4,7,10))\n#locator = mdates.MonthLocator((3,6,9,12))\nfmt = mdates.DateFormatter('%b %Y')\n\nfig, ax1 = plt.subplots(figsize=(10, 4))\nax1.set_ylabel('Berths, thousands')\n\nax1.plot(df_future_berths['date'],df_future_berths['berths']/1e3,'-^',label='berths')\n\nax1.tick_params(axis='y')\nax1.set_ylim((55,90))\nplt.legend(bbox_to_anchor=(1.2, 1))\n#ax1.legend()\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:red'\n\n#ax2.plot(year_ended_list,pcd,'+-g')\nax2.plot(df_future_berths['date'],df_future_berths['ships'],'-+',color=color,label='ships')    \nax2.set_ylabel('# of ships',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((25,40))\nax2.legend(bbox_to_anchor=(1.2, 0))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Future berths and ships, all NCLH brands')\nplt.show()\n\n\n\n\n\nThe plot above shows future berths and number of ships. Future berths are used to estimate future revenue by using capacity days, passenger cruise days as shown below."
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#conclusion",
    "href": "OLD NCLHv1 analysis.html#conclusion",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "10) Conclusion ",
    "text": "10) Conclusion \nThe following is a summary of the results described above:\n- Current news: Recent news concerns cruise line debt and return to operations. In June 2022, Carnival provided a 2nd Quarter 2022 business update. The results are playing in the news as negative for Carnival and by extension the other cruise lines. - Low cruise line stock price is in the news. - Review quarterly results: Quarterly reports dating back to March 31, 2019 are analyzed below. D/E, Book value and liquidity levels indicate NCLH is a distressed company as a result of the pandemic. - Average daily volume: 21,067,488 - Dividend yield: NA - Discounted cash flow analysis: The DCF analysis presented was performed for years 2019 and prior, which are the pre-covid years. Companies with negative earnings are difficult to evaluate with DCF. NCLH is suffering from economic distress from strategic problems from the pandemic. As a result, there is financial distress where income, cash flow and the accumulation of large amounts of debt relative to equity weigh heavily on the company’s future viability. The consequent result of near term low or negative earnings and high debt load may make it difficult to access new debt. - DCF Scenarios: DCF scenario 2 is presented, which assumes that NCLH returns to pre-pandemic earnings and values the company in light of higher interest rates and the large debt accumulated. - NACI stock selection guide analysis: The NAIC analysis presented below was performed for years 2019 and prior, which are the pre-covid years. Companies with negative earnings are difficult to evaluate with this method. - Dividend payout: NCLH provides a shareholder benefit which provides \\$100 of onboard credit for a 7 night cruise and \\$250 onboard credit per stateroom on sailings of 15 days or more. Shareholder benefit cash flow IRR is negative. - Management performance: Financial metrics such as liabilities to assets and return on investment were calculated. These don’t have much meaning at this point in the company’s history since the pandemic followed by a recession is looking like it will be a fatal blow to the industry. What we see now is a company on life support, getting massive infusions of debt and a continued negative earnings which is trending in the wrong direction.\nConcerns: D/E above 10, and negative earnings.\nSummary: The pandemic followed by a recession is looking like it will be a fatal blow to the industry. The company is on live support, getting massive infusions of debt and continued negative earnings trend that cannot support near term debt payments.\nRecommendation: Do not buy the stock and do not leave large cruise deposits for cruises more than 180 days out."
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#notes",
    "href": "OLD NCLHv1 analysis.html#notes",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "11) Notes ",
    "text": "11) Notes \nThe following notes outline the changes to the DCF model for financial and REIT companies.\nValuing a REIT\nNotes from Valuepro Book, page 237\n\nNOPM: To calculate operating income take rental revenue and subtracted total real estate expenses and G&A expenses. To arrive at the NOPM divide the adjusted income from real estate by real estate rental revenue. For the REIT, take income from real estate, which includes depreciation and amortization, and subtract GSA. Exclude other income, gains on sale of real estate and interest expenses.\nREIT has no traditional R&D costs\n\nREIT is not taxed at the corporate level, tax rate: should be near zero.\nDepreciation and capital expenditures are significantly higher for REITs than in other companies.\nNew property acquisitions are not directly accounted for in the DCF model for a REIT.\n\nWorking capitol: accounts payable, rents and security deposits\nShort term assets: cash, rents and other receivables and prepaid expenses\nShort term liabilities: accounts payable, advance rents security deposits\n\nWorking capital is almost zero, which is similar to other financial companies.\nThe consolidated balance sheet lists the assets as: - Real estate held for investment, at cost: - Land - Buildings and improvements - Total real estate held for investment, at cost - Less accumulated depreciation and amortization - Real estate held for investment, net - Real estate and lease intangibles held for sale, net - Cash and cash equivalents &lt;- current asset - Accounts receivable, net &lt;- current asset - Lease intangible assets, net - Other assets, net\nThe line items indicated above have been taken to be the current assets. Intangibles and long term items have been excluded.\nThe consolidated balance sheet lists the liabilities as: - Distributions payable &lt;- current liabilities - Accounts payable and accrued expenses &lt;- current liabilities - Lease intangible liabilities, net - Other liabilities - Line of credit payable and commercial paper &lt;- current liabilities - Term loans, net - Mortgages payable, net &lt;- current liabilities - Notes payable, net\nThe line items indicated above have been taken to be the current liabilities.\nValuing a financial company\nNotes from Valuepro Book, page 206\n\nTotal revenue comes from the total interest and dividend income line on the income statement. The calculation of operating income is more inclusive for a financial company than for an industrial or high tech company. For financial companies, operating revenue includes all normal revenue items plus interest income, dividends received and other investment income.\nCost of Goods Sold (CGS) comes from the Total interest expense line on the statement of income.\nGeneral and Administrative (G&A) are set to zero since they are included in the interest expense line\nA financial company has no traditional R&D costs\n\\(\\text{Cost of Goods Sold (CGS)} = \\text{Total interest expense} + \\text{Total non-interest expense}\\)\n\nGeneral and Administrative (G&A) are set to zero since they are included in the interest expense line\nA financial company has no traditional R&D costs\nDepreciation and amortization of premises and equipment from Consolidated Statements of Cash Flows.\n\nAmortization of other acquisition-related intangible assets is not included.\nNew investment and Depreciation: Property, plant and equipment expenditures and depreciation charges are significantly lower for a financial company. A typical manufacturing company, in order to grow its business, invests a significant portion of its revenues in plant, property and equipment (PPE). Financial companies invest very little in the way of PPE. However, software, risk management systems and acquisitions of other businesses, need to be included.\n\nFrom the Consolidated Statements of Cash Flows, under Cash Flows from Investing Activities - Purchases of premises and equipment - Purchases of leased equipment, net\n\nWorking capital supports manufacturing and service activities of nonfinancial companies. For financial companies, their principal liabilities and assets are financial claims that take the place of working capital. Because there is no differentiation between current and long term assets and liabilities for a financial company, we adjust working capital charges to zero. A financial company generally invests all of its funds in other financial assets, which have characteristics of current assets rather than PP&E.\n\\(\\text{Accounts Receivable} = 0\\)\n\\(\\text{Inventories} = 0\\)\n\\(\\text{Accounts Payable} = 0\\)\n\\(\\text{working capital} = 0\\)\nShort term assets: The balance sheets of most financial companies do not separate assets and liabilities into current and long term categories. When calculating the short term assets take the total assets and subtract goodwill and intangible assets also subtract other assets of questionable value. Subtract long term assets such as PP&E from total assets.\n\n\\(\\text{Short term assets} = \\text{Total assets} - \\text{good will and others of questionable value} - \\text{Premises and equipment}\\)\n\nA financial company’s principal liabilities are deposits, Federal funds purchased, trading account liabilities, insurance policy and claims reserves, contract holder funds and short term borrowing. To be consistent with the treatment of interest and an operating expense for financial companies, include long term debt in the short term liability category.\n\nShort term liabilities: Include long term debt.\n\n\\(\\text{Long term debt} = 0\\)\nExcess return period\nThe excess return period is based on a judgment call. The authors of [2] use the 1-5-7-10 rule. They group companies into one of four general categories and excess return periods. They use a 10 year excess return period to calculate what they would consider the maximum value. They use a more conservative 1 year, 5 year or 7 year return period to calculate a more reasonable or minimum value.\n- 1 year: Boring companies that operate in a highly competitive, low margin industry in which they have nothing particular going for them.\n- 5 year: Decent companies that have a recognizable name and decent reputation and perhaps a regulatory benefit (utility company), but can’t control pricing or growth.\n- 7 year: Good companies with good brand names, large companies of scale, good marketing channels and consumer identification (e.g. McDonald’s)\n- 10 year: Great companies with great growth potential, tremendous marketing power, band names and in-place benefits (e.g. Intel, Microsoft, Coca Cola, Disney)\nNotes about negative working capital\nThe company has a negative working capital rate. Negative working capital describes a situation where a company’s current liabilities exceed its current assets as stated on the firm’s balance sheet. In other words, there is more short-term debt than there are short-term assets.\nNegative working capital most often arises when a business generates cash very quickly because it can sell products to its customers before it has to pay the bills to its vendors for the original goods or raw materials. In this way, the company is effectively using the vendor’s money to grow.\nDividend Aristocrat, Achiever & Champion\nThis company was selected for analysis because it is on the Dividend Aristocrat list and passes the quick look tests. This notebook will be used as a template when analyzing other companies.\n\nAristocrat: S&P 500 Dividend Aristocrats is designed to measure the performance of S&P 500 index constituents that have followed a policy of consistently increasing dividends every year for at least 25 consecutive years.\nAchiever: The Broad Dividend Achievers Index. Eligible companies must be incorporated in the U.S. or its territories, trade on the NYSE, NASDAQ or AMEX, and have increased its annual regular dividend payments for the last 10 or more consecutive years.\nhttps://dividendvaluebuilder.com/dividend-achievers-list/\nhttps://www.marketbeat.com/dividends/achievers/\nChampion: This list includes companies that had increased their dividend for at least 25 consecutive years, and includes additional companies that had paid higher dividends without having increased the payout in every calendar year.\nhttps://dividendvaluebuilder.com/dividend-champions-list/\nhttps://www.dividendgrowthinvestor.com/p/dividend-champions-list.html"
  },
  {
    "objectID": "OLD NCLHv1 analysis.html#references",
    "href": "OLD NCLHv1 analysis.html#references",
    "title": "Norwegian Cruise Line Holdings (NCLH)",
    "section": "12) References ",
    "text": "12) References \n\nGray, Gary, et al. Streetsmart Guide to Valuing a Stock: the Savvy Investors Key to Beating the Market. McGraw-Hill, 2004.\nO’Hara, Thomas E., and Ken Janke. Starting and Running a Profitable Investment Club: the Official Guide from the National Association of Investors Corporation. Times Business, 1998.\nRobert G. Hagstrom, The Warren Buffett Way, Wiley, 2013"
  },
  {
    "objectID": "OLD HBI analysis.html",
    "href": "OLD HBI analysis.html",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "",
    "text": "Last update: 12 April 2023\nannual report: 2021\nshare price data from: 4/20/2022\n\\(\\Large {\\color {red} {\\text {update dates and stock history before publishing}}}\\)\nUse this file. Some edits were made in the HPL folder.\nWrite this up as a case study.\nIn Feb 2023 HBI anounced end of dividends and stock fell from \\$8.78 to \\$6.20, now trades at \\$4.91.\nhttps://www.fool.com/investing/2023/02/03/hanesbrands-drops-its-dividend-as-times-get-tough/\nhttps://www.yahoo.com/now/hanesbrands-plunges-gloomy-outlook-dividend-152149039.html\nhttps://www.dividendpower.org/2023/03/10/hanesbrands-hbi-dividend-cut-to-zero/\nIs this now a value stock?\n451 shares\nweighted average cost\n\\((116*8.595+110*9.71+153*13.1199+72*14.6763)/(116+110+153+72) = 11.372856541019958\\)\nTotal cost basis\n\\((116*8.595+110*9.71+153*13.1199+72*14.6763) = 5129.158300000001\\)\nnumber of shares\n\\((116+110+153+72) = 451\\)\ncurrent value 2,214\nHold stock for now since intrinic value is above current price.\nData from May 3, 2023\n\\(\\Large {\\color {red} {\\text {Note}}}\\)\nexporting to MD for offline editing, files to bring for ASUS\n- MD file - HTML and convert to pdf - annual report - compustat report"
  },
  {
    "objectID": "OLD HBI analysis.html#abstract",
    "href": "OLD HBI analysis.html#abstract",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "Abstract",
    "text": "Abstract\nThis notebook presents analysis and commentary for HanesBrands (NYSE: HBI). The analysis presented is based on examination of the business fundamentals. A discount cash flow analysis is used to estimate the intrinsic value of the company. A second evaluation method based on earnings history and historical price to earnings ratio is calculated. Using some judgment calls, as explained in the analysis, an intrinsic stock value is calculated. Some shares of HBI were purchased based on dividend yield and the intrinsic stock value. As described in the analysis, HBI suspended the dividend in order to direct funds to pay down the debt. Since the company is not paying a dividend, does it make sense to hold the company as a value stock? The analysis concludes that there is some merit to think the company might be a value stock, but as a non-dividend paying stock, having HBI does not fit my investment goals of holding quality dividend paying stocks.\nAt the time of writing this report, some stock analysis were suggesting that HBI might be a value play, that is, buying this stock on the cheap and holding until the price recovers or the dividend is re-instated. See the articles here and here. My analysis shown below indicates that the even if NOP can be increased over time by 10%, the ratio of NOP to total liabilities remains above 7. (need to recalculate projected total liabilities and clean up the analysis). Historically the ratio was near 5 when HBI initiated their dividend."
  },
  {
    "objectID": "OLD HBI analysis.html#introduction",
    "href": "OLD HBI analysis.html#introduction",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "Introduction",
    "text": "Introduction\nShares of HBI were purchased based on dividend yield and intrinsic value of the company. The dividend yield at the time of purchase was running about 4%. I currently hold 451 shares and my purchase history is:\n\n\n\nDate\nQuantity\nPrice\n\n\n\n\n04/20/2022\n72\n\\$14.67\n\n\n05/02/2022\n153\n\\$13.11\n\n\n06/16/2022\n110\n\\$9.71\n\n\n09/01/2022\n116\n\\$8.595\n\n\n\nMy weighted average cost is \\$11.372 per share and my cost basis is \\$5,129.16. The current value of my shares are \\$2,214, a loss of about 57%. If I hold the shares until September of 2023, all the shares will be long term capital losses.\nSince HBI has canceled the dividend payments, should this stock be retained for its value or for possible restoration of dividends? The most recent financial results for the quarter ending are shown in the current news section below."
  },
  {
    "objectID": "OLD HBI analysis.html#company-description",
    "href": "OLD HBI analysis.html#company-description",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "Company description",
    "text": "Company description\nHanesbrands Inc. is an American multinational clothing company based in Winston-Salem, North Carolina. It employs 65,300 people internationally. On September 6, 2006, the company and several brands were spun off by the Sara Lee Corporation. Hanesbrands owns several clothing brands, including Hanes, Champion, Playtex, Bali, L’eggs, Just My Size, Barely There, Wonderbra, Maidenform, Berlei, and Bonds.\nHanesbrands Inc., a Maryland corporation, is a consumer goods company with a portfolio of leading apparel brands, including Hanes, Champion, Bonds, Maidenform, Bali, Playtex, Bras N Things, JMS/Just My Size, Gear for Sports, Wonderbra, Berlei, Comfortwash, and Alternative. The Company designs, manufactures, sources and sells a broad range of basic apparel such as T-shirts, bras, panties, shapewear, underwear, socks and activewear. The Company’s fiscal year ends on the Saturday closest to December 31. All references to “2022”, “2021” and “2020” relate to the 52-week fiscal year ended on December 31, 2022 and January 1, 2022, and the 53-week fiscal year ended on January 2, 2021, respectively. Two subsidiaries of the Company close one day after the Company’s consolidated year end. The difference in reporting of financial information for these subsidiaries did not have a material impact on the Company’s financial condition, results of operations or cash flows.\nIn late 2020, the Company undertook a comprehensive global business review focused on building consumer-centric growth. The review resulted in the Company’s Full Potential plan, which is its multi-year growth strategy that focuses on four pillars to drive growth and enhance long-term profitability and identifies the initiatives to unlock growth. The Company’s four pillars of growth are to grow the Champion brand globally, drive growth in Innerwear with brands and products that appeal to younger consumers, drive consumer-centricity by delivering innovative products and improving awareness through investments in brand marketing and digital capabilities, and streamline its global portfolio.\nIn the fourth quarter of 2020, the Company began the implementation of its Full Potential plan and as part of its strategy to streamline its portfolio, the Company determined that its personal protective equipment (“PPE”) business was no longer a growth opportunity and recorded a charge of \\$362,913 to write down its entire PPE inventory balance to its estimated net realizable value and a charge of \\$26,400 to accrue for vendor commitments for PPE materials that were paid in 2021. Additionally, the Company commenced an initiative to reduce 20% of its SKUs in inventory in order to streamline product offerings while also implementing a formal lifecycle management process. As a result, the Company recorded a charge of \\$192,704 to write down inventory to its estimated net realizable value taking into account these initiatives. These initiatives will position the Company for long-term growth by driving higher margin sales, lowering costs and improving service to customers.\nIn the first quarter of 2021, the Company announced that it reached the decision to exit its European Innerwear business as part of its strategy to streamline its portfolio under its Full Potential plan and determined that this business met held-for-sale and discontinued operations accounting criteria. Accordingly, the Company began to separately report the results of its European Innerwear business as discontinued operations in its Consolidated Statements of Income, and to present the related assets and liabilities as held for sale in the Consolidated Balance Sheets. On November 4, 2021, the Company announced that it reached an agreement to sell its European Innerwear business to an affiliate of Regent, L.P. and completed the sale on March 5, 2022. Unless otherwise noted, discussion within these notes to the consolidated financial statements relates to continuing operations. See Note “Assets and Liabilities Held for Sale” for additional information.\nIn addition, in the fourth quarter of 2021, the Company reached the decision to divest its U.S. Sheer Hosiery business, including the L’eggs brand, as part of its strategy to streamline its portfolio under its Full Potential plan and determined that this business met held-for-sale accounting criteria, The related assets and liabilities are presented as held for sale in the Consolidated Balance Sheets at December 31, 2022 and January 1, 2022. The operations of the U.S. Sheer Hosiery business are reported in “Other” for all periods presented in Note “Business Segment Information”. The Company is currently exploring potential purchasers for this business and expects to complete the sale within the next 12 months. See Note “Assets and Liabilities Held for Sale” for additional information.\nHBI operations are managed and reported in three operating segments, each of which is a reportable segment for financial reporting purposes: Innerwear, Activewear and International. These segments are organized principally by product category and geographic location. Each segment has its own management team that is responsible for the operations of the segment’s businesses, but the segments share a common supply chain and media and marketing platforms.\nThe following table summarizes HBI operating segments by product category:\n\n\n\n\n\n\n\n\nSegment\nPrimary Products\nPrimary Brands\n\n\n\n\nInnerwear\nBasics, including men’s underwear, women’s panties, children’s underwear and socks and intimate apparel, such as bras and shapewear\nHanes, Maidenform, Bali, Champion, Playtex, JMS/Just My Size, Bras N Things, Polo Ralph Lauren\n\n\nActivewear\nT-shirts, fleece, sport shirts, performance T-shirts and shorts, sports bras, thermals and teamwear\nChampion, Hanes, Gear for Sports, Comfortwash, Alternative, JMS/Just My Size, Hanes Beefy-T\n\n\nInternational\nActivewear, men’s underwear, women’s panties, children’s underwear, intimate apparel, socks and home goods\nChampion, Bonds, Sheridan, Bras N Things, Hanes, Wonderbra, Berlei, Playtex, Zorba, Sol y Oro, Rinbros, Polo Ralph Lauren\n\n\n\nInnerwear net sales decreased 11% compared to prior year primarily due to softer point-of-sale trends, impacts to replenishment orders from retailers’ decisions to reduce broader inventory positions, business disruption as a result of the ransomware attack in the second quarter of 2022 and the overlap of last year’s sales benefits from retailer restocking and government-stimulus spending partially offset by pricing actions taken and retail space gains in the first quarter of 2022.\nActivewear net sales decreased 7% compared to prior year primarily due to softer point-of-sale trends primarily related to the Champion brand, retailer inventory levels and business disruption as a result of the ransomware attack in the second quarter of 2022. The net sales decrease was partially offset by growth in the collegiate and printwear channels and pricing actions primarily taken in the third quarter of 2022.\nNet sales in the International segment decreased 7% compared to prior year due to unfavorable foreign currency exchange rates. The unfavorable impact of foreign currency exchange rates decreased net sales approximately \\$182 million in 2022. International net sales on a constant currency basis, defined as net sales excluding the impact of foreign currency, increased 1%. The impact of foreign currency exchange rates is calculated by applying prior period exchange rates to the current year financial results.\nOther net sales decreased primarily as a result of lower sales at our retail outlets during 2022 compared to prior year partially offset by increased sales from our supply chain to the European Innerwear business. HBI has continued certain sales from our supply chain to this business on a transitional basis after the sale in the first quarter of 2022. These sales and the related profit are included in Other in all periods presented and have not been eliminated as intercompany transactions in consolidation for the period when this business was owned by us.\nHBI’s multi-year growth strategy (“Full Potential plan”) focuses on four pillars to drive growth and enhance long-term profitability and identifies the initiatives to unlock growth. HBI four pillars of growth are to grow the Champion brand globally, drive growth in Innerwear with brands and products that appeal to younger consumers, build e-commerce excellence across channels and streamline our global portfolio. In order to deliver this growth and create a more efficient and productive business model, HBI has launched a multi-year cost savings program intended to self-fund the investments necessary to achieve the Full Potential plan’s objectives. We remain confident that our strong brand portfolio, world-class supply chain and diverse category and geographic footprint will help us unlock our full potential, deliver long-term growth and create stockholder value.\nIncluded in restructuring and other action-related charges within operating profit in 2022 and 2021 were \\$60 million and\\$132 million, respectively, of charges related to the implementation of our Full Potential plan. Full Potential plan charges in 2022 included charges related to supply chain segmentation of \\$18 million to position our manufacturing network to align with revenue growth opportunities of our Full Potential plan demand trends, \\$10 million related to corporate headcount reductions and a non-cash gain of\\$4 million to adjust the valuation allowance related to the U.S. Sheer Hosiery business resulting from a decrease in carrying value due to changes in working capital. Full Potential plan charges in 2021 included a charge of \\$16 million for an action to resize our U.S. corporate office workforce through a voluntary retirement program and impairment charges of \\$7 million related to the full impairment of an indefinite-lived trademark related to a specific brand within the European Innerwear business that was excluded from the disposal group as it was not marketed for sale.\nThe Board of Directors has recently eliminated its prior dividend policy pursuant to which HBI has historically paid a cash dividend on our common stock on a quarterly basis in order to direct free cash flow toward reducing our debt. The declaration and payment of any dividend in the future will be subject to the approval of the Board of Directors and our dividend may thereafter be discontinued or reduced at any time. The Board of Directors regularly evaluates our capital allocation strategy and dividend policy, and any future determination to continue to pay dividends, and the amount of such dividends, will be at the discretion of the Board of Directors. The ability to pay cash dividends is also limited by restrictions or limitations on our ability to obtain sufficient funds through dividends from subsidiaries, as well as by contractual restrictions, including the requirements of the agreements governing our indebtedness. There can be no assurance that HBI will declare cash dividends in the future in any particular amounts, or at all.\nOLD }——————- HanesBrands (NYSE: HBI) makes everyday apparel that is known and loved by consumers around the world for comfort, quality and value. Among the company’s iconic brands are Hanes, the leading basic apparel brand in the United States; Champion, an innovator at the intersection of lifestyle and athletic apparel; and Bonds, which is setting new standards for design and sustainability.\nHBI employs 51,000 associates in 32 countries and has built a strong reputation for workplace quality and ethical business practices. In May 2021, HBI launched its Full Potential plan – the company’s roadmap to drive improved revenue and profits during the next three years and beyond.\nUnlike most apparel companies, more than 70% of the apparel we sell is manufactured in our own facilities or those of dedicated contractors. Owning the majority of our supply chain not only impacts cost, scale and flexibility, but also the ability to adhere to best-in-class workplace and sustainability practices.\nIn 2021, HBI was one of two apparel manufacturers named one of the World’s Most Ethical Companies by Ethisphere and garnered a spot on Barron’s 100 Most Sustainable Companies for the third consecutive year. This follows the December 2020 announcement that HBI earned “A List” recognition for leadership in corporate sustainability in the CDP 2020 Climate Change Report.\nWe are committed to making the world a more comfortable, livable and inclusive place. We have established new, wide-ranging 2030 global sustainability goals and launched a new sustainability website, www.HBISustains.com, designed to increase our transparency on key metrics. We approach sustainability from a broad, holistic perspective and focus our efforts in areas addressed by the United Nations’ Sustainable Development Goals under three pillars: People, Planet and Product.\nhttps://ir.hanesbrands.com/\nhttps://finance.yahoo.com/quote/HBI\nPrevious Close  4.8800\nOpen    4.8000\nBid 4.4200 x 2900\nAsk 4.4000 x 29200\nDay's Range 4.3200 - 4.8500\n52 Week Range   4.3200 - 13.2800\nVolume  15,869,504\nAvg. Volume 12,093,670\nMarket Cap  1.533B\nBeta (5Y Monthly)   N/A\nPE Ratio (TTM)  N/A\nEPS (TTM)   -0.3500\nEarnings Date   May 03, 2023\nForward Dividend & Yield    N/A (N/A)\nEx-Dividend Date    Nov 21, 2022\n1y Target Est   4.64\n\nSector(s): Consumer Cyclical\nIndustry: Apparel Manufacturing\nFull Time Employees: 50,000\nHanesbrands Inc., a consumer goods company, designs, manufactures, sources, and sells a range of basic apparel for men, women, and children. The company operates through three segments: Innerwear, Activewear, and International.\nwww.hanes.com/corporate"
  },
  {
    "objectID": "OLD HBI analysis.html#bottom-line-up-front",
    "href": "OLD HBI analysis.html#bottom-line-up-front",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "Bottom line up front",
    "text": "Bottom line up front\nHBI fails some of the decision model checks. Revisit to see if buy or sell in June. Follow the link to the Conclusion."
  },
  {
    "objectID": "OLD HBI analysis.html#revision-history",
    "href": "OLD HBI analysis.html#revision-history",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "Revision history",
    "text": "Revision history\n\n1/10/2022: Copied from VZ notebook and reorganized\nFeb 2022: updated quick look, reorganized flow of calculations, corrected usage of financial rates, organized end sections\n23 Mar 2022: Cleaning up financial data spreadsheet. Removed NAIC tab. Removed duplicate reveneu data.\n27 Mar 2022: MFG template copied from BMY\n27 Jun 2023:\n\nUpdates to narrative\nupdated Market Cap and total value of common equity plot to look at last full year and current price history\nremoved dividend payout analysis\nIn the Financial ratios section, added return on capital to plot\nIn the NAIC section, added profit margin to plot\nAdded new section called Earnings yield\nRemoved Percent earned on equity since it was another way of saying RoE\nAvg closing price calculated"
  },
  {
    "objectID": "OLD HBI analysis.html#analysis",
    "href": "OLD HBI analysis.html#analysis",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "Analysis",
    "text": "Analysis\nThe following sections of this notebook contain the financial analysis for the company.\nContents \n\nStock screener results\n\nLoad financial spreadsheet\n\nDiscounted cash flow analysis, baseline\n\nDCF Scenarios\n\nNACI stock selection guide analysis\n\nFuture stock price\n\nDividend payout\n\nManagement performance\n\nDecision model\n\nConclusion\n\nNotes\n\nReferences"
  },
  {
    "objectID": "OLD HBI analysis.html#stock-screener-results",
    "href": "OLD HBI analysis.html#stock-screener-results",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "1) Stock screener results ",
    "text": "1) Stock screener results \nThis company was selected from the Fidelity stock screener results. The search results are based on Dividend yield (high and very high, 2.83% and higher), Volume 90 average (high and very high. 535k and higher) and Revenue Growth 5 years (0 or higher).\nCurrent news\nA review of receint SEC filings and financial news sites yahoo and google showed the following:\nOn May 24, 2022, the Company identified that it had become subject to a ransomware attack and activated its incident response and business continuity plans designed to contain the incident. As part of the Company’s forensic investigation and assessment of the impact, the Company determined that certain of its information technology systems were affected by the ransomware attack.\nUpon discovering the incident, the Company took a series of measures to further safeguard the integrity of its information technology systems, including working with cybersecurity experts to contain the incident and implementing business continuity plans to restore and support continued operations. These measures also included resecuring data, remediation of the malware across infected machines, rebuilding critical systems, global password reset and enhanced security monitoring. The Company notified appropriate law enforcement authorities as well as certain data protection regulators, and in addition to the Company’s public announcements of the incident, the Company provided breach notifications and regulatory filings as required by applicable law starting in August 2022. At this time, the Company believes the incident has been contained, the Company has restored its critical information technology systems, and manufacturing, retail and other internal operations continue. There is no ongoing operational impact on the Company’s ability to provide its products and services. The Company maintains insurance, including coverage for cyber-attacks, subject to certain deductibles and policy limitations, in an amount that the Company believes appropriate.\nThe Company is named in two pending lawsuits in connection with its previously disclosed ransomware incident. On October 7, 2022, a putative class action was filed against “Hanes Brands Inc.” alleging, among other things, negligence, negligence per se, breach of implied contract, unjust enrichment, breach of implied covenant of good faith and fair dealing, unfair business practices under the California Business and Professions Code, and violations of the California Confidentiality of Medical Information Act in connection with the ransomware incident. The litigation is entitled, Roman v. Hanes Brands Inc., and is pending in the United States District Court for the Central District of California. Plaintiff Roman also subsequently filed a second putative class action with regard to the ransomware incident in the United States District Court for the Middle District of North Carolina on January 16, 2023, entitled Roman v. Hanesbrands Inc., which was voluntarily dismissed without prejudice on January 20, 2023. On October 13, 2022, another putative class action was filed against HanesBrands Inc. alleging, among other things, negligence, negligence per se, breach of implied contract, invasion of privacy, and unjust enrichment in connection with the ransomware incident. The litigation is entitled, Toussaint v. HanesBrands Inc. and is pending in the United States District Court for the Middle District of North Carolina. The pending lawsuits seek, among other things, monetary and injunctive relief. The Company is vigorously defending these matters and believes the cases are without merit. The Company does not expect any of these claims, individually or in the aggregate, to have a material adverse effect on its consolidated financial position or results of operations. However, at this early stage in the proceedings, the Company is not able to determine the probability of the outcome of these matters or a range of reasonably expected losses, if any. The Company maintains insurance, including coverage for cyber-attacks, subject to certain deductibles and policy limitations, in an amount that the Company believes appropriate.\nDuring the year ended December 31, 2022, the Company incurred costs of \\$15,427, net of expected insurance recoveries, related to the ransomware attack. The costs for the year ended December 31, 2022 included \\$14,168 related primarily to supply chain disruptions, which are reflected in the “Cost of sales” line of the Consolidated Statements of Income and \\$1,259, net of expected insurance recoveries, related primarily to legal, information technology and consulting fees, which are reflected in the “Selling, general and administrative expenses” line of the Consolidated Statements of Income. The Company continues to assess the security event and cannot determine, at this time, the full extent of the impact from such event on its business, results of operations or financial condition or whether such impact will ultimately have a material adverse effect.\nReview quarterly results\nSince this analysis mainly looks at the annual reports, a review of the quarterly reports and the most recent 12 months is needed to see if the recent quarterly trends match the yearly trends. - yahoo finance - The Compustat Company Research from Fidelity - SEC filings from Hanesbrands Inc. Investor Relations\n\n\n\n\n\n\n\n\nCondensed Consolidated Statements of Income\n04/01/23\n04/02/22\n\n\n\n\nNet sales\n\\$1,389,410\n\\$1,576,156\n\n\nCost of sales\n\\$939,717\n\\$991,978\n\n\nGross profit\n\\$449,693\n\\$584,178\n\n\nSelling, general and administrative expenses\n\\$392,374\n\\$413,666\n\n\nOperating profit\n\\$57,319\n\\$170,512\n\n\nOther expenses\n\\$14,771\n\\$987\n\n\nInterest expense, net\n\\$58,452\n\\$31,963\n\n\nIncome (loss) from continuing operations before income tax expense\n-\\$15,904\n\\$137,562\n\n\nIncome tax expense\n\\$18,500\n\\$23,385\n\n\nIncome (loss) from continuing operations\n-\\$34,404\n\\$114,177\n\n\n(amounts in thousands)\n\n\n\n\n\nAs is shown in the table above, HBI’s sales are down and they experienced a loss from continuing operations. Since I’m going to wait until later in the year to sell my shares, I can evaluate the second and third quarter results before making a final decision.\nAverage daily volume\nAverage daily volume: 5,510,768\nDividend yield\nForward dividend yield: 5.16% - no longer valid\n\\(\\Large {\\color {red} {\\text {add review of compustat report}}}\\)\nCompustat Company Research Hanesbrands Inc NYSE: HBI Mar. 14, 2023"
  },
  {
    "objectID": "OLD HBI analysis.html#load-financial-spreadsheet",
    "href": "OLD HBI analysis.html#load-financial-spreadsheet",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "2) Load financial spreadsheet ",
    "text": "2) Load financial spreadsheet \nData from consolidated financial statements and annual reports was collected and entered into a spreadsheet. All numerical data is converted from thousands or millions of dollars to dollars. The stock share price history was obtained from yahoo and is included as a tab in the spreadsheet. Other tabs in the spreadsheet are various worksheets.\n\\(\\Large {\\color {red} {\\text {need to organize folders on drive}}}\\)\n/home/jeff32/HPL_legacy_files/HPL_legacy_files/Documents/Dividend Investing/DCF data/HBI_Financials.xlsx\n/home/jeff32/Documents/Dividend Investing/DCF data/HBI_Financials.xlsx\n\nticker = 'HBI' # company ticker symbol\n#os.chdir('/home/jim/Documents/Dividend Investing/DCF data/')\nos.chdir('/home/jeff32/Documents/Dividend Investing/DCF data/')\n\nfile_name = ticker+'_Financials.xlsx'\ndf_dcf_sheet = pd.read_excel(file_name,sheet_name='DCF data')\n#df_NAIC_financials = pd.read_excel(file_name,sheet_name='NAIC data')\ndf_metrics_sheet = pd.read_excel(file_name,sheet_name='metrics')\ndf_price_history = pd.read_excel(file_name,sheet_name='Historical Prices')\n\n# change the working director back to the Jupyter folder\n#os.chdir('/home/jim/Documents/JupyterLab/Discount Cash Flow Analysis/')\nos.chdir('/home/jeff32/Documents/JupyterLab/Discount Cash Flow Analysis/')\n\n\n# convert dates from string to datetime format in stock price history\nprice_date_list = []\nfor i in range(len(df_price_history)):\n    price_date_list.append(datetime.strptime(str(df_price_history['Date'][i]), '%Y-%m-%d'))\n\ndf_price_history.insert(0, 'datetime', price_date_list)  # insert a new column with datetime data\ndf_price_history.sort_values(by=['datetime'], inplace=True) # sort data frame by datetime\n\ndf_price_history.set_index('datetime',inplace=True)\n\n#df_price_history.head()\n\n\n2.1) Format data frame \nGenerate a new data frame that holds the financial data needed for the DCF model. Data from financial statements is copied into a spreadsheet which contains the data used in the analysis. The data in the DCF_data tab is in a consistent format for ease of use by this notebook. Standard names are used for the rows and columns.\n\n#column names: fiscal years \nfy_data = df_dcf_sheet.columns[1:].values.astype('datetime64[Y]')-1970\n#line 0: Total revenue  \nrevenue_data = df_dcf_sheet.iloc[0].to_numpy()[1:].astype('float')\n#line 1: Cost of goods sold\nCost_of_goods_sold_data = df_dcf_sheet.iloc[1].to_numpy()[1:].astype('float')\n#line 2: General and administrative\nGeneral_and_administrative_data = df_dcf_sheet.iloc[2].to_numpy()[1:].astype('float')\n#line 3: Research and development\nResearch_and_development_data = df_dcf_sheet.iloc[3].to_numpy()[1:].astype('float')\n#line 4: Depreciation and amortization\nDepreciation_and_amortization_data = df_dcf_sheet.iloc[4].to_numpy()[1:].astype('float')\n#line 5: Investment\nInvestment_data = df_dcf_sheet.iloc[5].to_numpy()[1:].astype('float')\n# line 6: Income before income taxes\nIncome_before_income_taxes_data = df_dcf_sheet.iloc[6].to_numpy()[1:].astype('float')\n# line 7: Income tax\nIncome_tax_data = df_dcf_sheet.iloc[7].to_numpy()[1:].astype('float')\n# line 8: Accounts receivable\nAccounts_receivable_data = df_dcf_sheet.iloc[8].to_numpy()[1:].astype('float')\n# line 9: Inventories\nInventories_data = df_dcf_sheet.iloc[9].to_numpy()[1:].astype('float')\n# line 10: Accounts payable\nAccounts_payable_data = df_dcf_sheet.iloc[10].to_numpy()[1:].astype('float')\n# line 11: Current assets\nCurrent_assets_data = df_dcf_sheet.iloc[11].to_numpy()[1:].astype('float')\n# line 12: Current liabilities\nCurrent_liabilities_data = df_dcf_sheet.iloc[12].to_numpy()[1:].astype('float')\n# line 13: Long term debt\nLong_term_debt_data = df_dcf_sheet.iloc[13].to_numpy()[1:].astype('float')\n# line 14: Shares outstanding\nShares_outstanding_data = df_dcf_sheet.iloc[14].to_numpy()[1:].astype('float')\n\n\n# make a new data frame to store selected financial data\ndf_dcf_data = pd.DataFrame(data={\n    'FY':fy_data[::-1],\n    'revenue':revenue_data[::-1],\n    'cost_of_goods_sold':Cost_of_goods_sold_data[::-1],\n    'general_and_administrative':General_and_administrative_data[::-1],\n    'research_and_development':Research_and_development_data[::-1],\n    'depreciation':Depreciation_and_amortization_data[::-1],\n    'investment':Investment_data[::-1],\n    'income_before_income_taxes':Income_before_income_taxes_data[::-1],\n    'income_tax':Income_tax_data[::-1],\n    'accounts_receivable':Accounts_receivable_data[::-1],\n    'inventories':Inventories_data[::-1],\n    'accounts_payable':Accounts_payable_data[::-1], \n    'current_assets':Current_assets_data[::-1],\n    'current_liabilities':Current_liabilities_data[::-1],\n    'long_term_debt':Long_term_debt_data[::-1],\n    'shares_outstanding':Shares_outstanding_data[::-1]\n    })\n\n#df_dcf_data"
  },
  {
    "objectID": "OLD HBI analysis.html#discounted-cash-flow-analysis-baseline",
    "href": "OLD HBI analysis.html#discounted-cash-flow-analysis-baseline",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "3) Discounted cash flow analysis, baseline ",
    "text": "3) Discounted cash flow analysis, baseline \nDiscounted cash flow (DCF) is a valuation method used to estimate the value of an investment based on its expected future cash flows. DCF analysis attempts to figure out the value of an investment today, based on projections of how much money it will generate in the future. In finance, discounted cash flow (DCF) analysis is a method of valuing a security, project, company, or asset using the concepts of the time value of money. The DCF method used in this notebook follows [1].\nThe value of any financial investment equals the present value of the expected future cash flows, discounted for risk and timing of these cash flows. The DCF method to value stocks is a four step process.\n1. Develop a set of future free cash flows for the corporation based on revenue growth, net operating profit margin, income tax rate and fix and working capital requirements. 2. Estimate the discount rate for the cash flows based on expected timing and risk. 3. Discount the cash flows and total them to calculate the value for the corporation as a whole. 4. Subtract the debt, preferred stock value and other claims and divide by the number of shares outstanding to get the intrinsic value.\nSections - Revenue growth rate\n- Net operating profit margin\n- Tax rate\n- Depreciation Rate\n- Investment Rate\n- Working Capital Rate\n- Current Assets\n- Current Liabilities\n- Value of Debt Outstanding\n- Current stock price\n- Shares outstanding\n- 10 year treasury bond yield\n- Bond yield spread to treasury\n- Preferred stock yield\n- Equity risk premium\n- Company specific beta\n- DCF model inputs\n- Future cash flows\n\nFuture forecast based on historical data\nThe DCF model uses historical financial data to estimate future cash flows. However, future changes are largely unpredictable, so we assume that the past record can be used as a rough guide to the future. The more questionable this assumption is, the less valuable is the analysis. So the DCF model is more useful when applied to stable well established companies, since companies with stable earnings are easier to forecast.\n\n\nRevenue growth rate \nThe revenue growth rate (also sometimes called net sales) of the corporation plus any other revenues associated with the main operations of the business. It does not include dividends, interest income or non-operating income. Historic revenue data is obtained from consolidated income statements. The year over year change in revenue is calculated and converted to a percent, then an average revenue growth rate is calculated.\nAdjustments for Hanesbrands Inc.\nNo adjustments for this company.\n\n# calculate the percent change in revenue\npcr = np.zeros(len(df_dcf_data['revenue'].to_numpy())) # percent change in revenue\nfor i in range(len(df_dcf_data['revenue'].to_numpy()[0:-1])):\n    pcr[i+1] = ((df_dcf_data['revenue'].to_numpy()[i+1] - df_dcf_data['revenue'].to_numpy()[i])/\n                df_dcf_data['revenue'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Revenue, $B')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['revenue']/1e9, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcr,'+-g')\n    \nax2.set_ylabel('% Change in revenue',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((-10,15))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Revenue')\nplt.show()\n\n\n\n\nObservation:\nNet sales decreased 8% during 2022 primarily due to the following: - Softer point-of-sale trends and higher retailer inventory levels as a result of the macroeconomic pressures; - The impact of the ransomware attack to the business; - Global supply chain disruptions resulting in product delays; - Ongoing COVID-related pressures on consumer traffic in certain markets in Asia; and - The unfavorable impact from foreign currency exchange rates in our International business of approximately \\$182 million. - Partially offset by: Pricing actions taken throughout 2022.\n\n\n\nNet Sales\nfor the year ending 12/31/2022\n\n\n\n\nInnerwear\n\\$2,429,966\n\n\nActivewear\n\\$1,555,062\n\n\nInternational\n\\$1,914,268\n\n\nOther\n\\$334,354\n\n\nTotal\n\\$6,233,650\n\n\n(amounts in thousands)\n\n\n\n\nHBI products are primarily distributed through HBI wholesale customers’ stores and websites, as well as through our own stores and websites. In 2022, approximately 69% of HBI total net sales were in the United States and approximately 31% were outside the United States. HBI largest customer is Walmart Inc. (“Walmart”), accounting for 16% of total net sales in 2022.\nSales to mass merchants in the United States accounted for approximately 19% of HBI total net sales in 2022 and included all of our product categories under our Hanes, Playtex, Maidenform and JMS/Just My Size brands, as well as licensed logo apparel. Mass merchants feature high-volume, low-cost sales of basic apparel items along with a diverse variety of consumer goods products, such as grocery and drug products and other hard lines. HBI largest mass merchant customer is Walmart.\nA significant percentage of HBI total revenues (approximately 31% in 2022) is derived from markets outside the United States. HBI sells a majority of products in transactions denominated in U.S. dollars; however, HBI purchases many of their raw materials, pay a portion of wages and make other payments to participants in the supply chain in foreign currencies. As a result, when the U.S. dollar weakens against any of these currencies, HBI cost of sales could increase substantially.\nOutside the United States, HBI may pay for materials or finished products in U.S. dollars, and in some cases a strengthening of the U.S. dollar could effectively increase HBI costs where they use foreign currency to purchase the U.S. dollars they need to make such payments.\nOutlook for 2023 HBI 2023 guidance as follows: - Net sales of approximately \\$6.05 billion to \\$6.20 billion, net of approximately \\$42 million of unfavorable foreign exchange impact;\n- Operating profit of approximately \\$446 million to \\$496 million, net of approximately \\$6 million of unfavorable foreign exchange impact;\n- Restructuring and other action-related charges totaling \\$60 million including Full Potential plan-related charges of approximately \\$54 million included in operating profit and refinancing charges of \\$6 million included in other expenses;\n- Interest expense and other expenses of approximately \\$306 million combined;\n- Tax expense from continuing operations of approximately \\$90 million to \\$100 million;\n- Diluted earnings per share from continuing operations of approximately \\$0.14 to \\$0.25;\n- Cash flow from operating activities of approximately \\$500 million; and\n- Capital investments of approximately \\$150 million, including capital expenditures of \\$70 million within investing cash flow activities and cloud computing assets of \\$80 million within operating cash flow activities.\nThird-party brick-and-mortar wholesale revenue is primarily generated by sales of the Company’s products to retailers to support their brick-and-mortar operations. Also included within third-party brick-and-mortar wholesale revenue is royalty revenue from licensing agreements. The Company earns royalties through license agreements with manufacturers of other consumer products that incorporate certain of the Company’s brands. The Company accrues revenue earned under these contracts based upon reported sales from the licensees. Additionally, third-party brick-and-mortar wholesale revenue for the year ended January 2, 2021 includes \\$518,309 of revenue from contracts with governments generated from the sale of both cloth face coverings and gowns for use to help mitigate the spread of the virus during the COVID-19 pandemic.\n\nrgr_avg = pcr[-5:].mean()/100 # last five years\nprint('average revenue growth rate: {:.2f}%'.format(rgr_avg*100))\n\naverage revenue growth rate: -0.88%\n\n\n\n\nNet operating profit margin \nNet Operating Profit should reflect the future revenue generating ability and expense requirements of the operating business that comprise the ongoing operations of the company.\n\\(\\text{NOPM} = \\frac{\\text{Revenue} - \\text{Expenses}}{\\text{Revenue}}\\)\n\\(\\text{Expenses} = \\text{Cost of Goods Sold (CGS)} + \\text{General and Administrative (G\\&A)} + \\text{Research and Development (R\\&D)}\\)\nGeneral and Administrative (G&A) is also called Sales, General and Administrative (SG&A)\nAdjustments for Hanesbrands Inc.\nNo adjustments for this company.\n\n# NOP = (Revenue - Expenses)\nnop = (df_dcf_data['revenue'].to_numpy() - \\\n    (df_dcf_data['cost_of_goods_sold'].to_numpy() + \\\n    df_dcf_data['general_and_administrative'].to_numpy() + \\\n    df_dcf_data['research_and_development'].to_numpy()) )\n\n# net operating profit margin as percent of revenue\nnopm = nop/df_dcf_data['revenue'].to_numpy()\n\n# plot as four grouped bar chart with labels on right and working capital rate on left\n# calculate position of bars\nx1_bar_position = []\nx2_bar_position = []\nx3_bar_position = []\nx4_bar_position = []\nfor i in df_dcf_data['FY']:\n    x1_bar_position.append(i-relativedelta(months=3))\n    x2_bar_position.append(i-relativedelta(months=1))\n    x3_bar_position.append(i+relativedelta(months=1))\n    x4_bar_position.append(i+relativedelta(months=3))\n    \nwidth = 40  # the width of the bars\n    \n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Net operating profit, \\\\$B')\n\nax1.bar(x1_bar_position,df_dcf_data['cost_of_goods_sold'].to_numpy()/1e9, width,label='CGS')\nax1.bar(x2_bar_position,df_dcf_data['general_and_administrative'].to_numpy()/1e9, width,label='G&A')\nax1.bar(x3_bar_position,df_dcf_data['research_and_development'].to_numpy()/1e9, width,label='R&D')\nax1.bar(x4_bar_position,nop/1e9, width,label='NOP')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,4))\nax1.legend()\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:cyan'\n\nax2.plot(df_dcf_data['FY'],nopm*100,'+-c')\n    \nax2.set_ylabel('% NOPM',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,40))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Net Operating Profit')\nplt.show()\n\n\n\n\nObservation\nThe COVID-19 pandemic has impacted HBI business operations and financial results for 2020, as described in more detail under “Consolidated Results of Operations - Year Ended January 2, 2021 (“2020”) Compared with Year Ended December 28, 2019 (“2019”)” below, due to decreased customer traffic and temporary retail store closures worldwide. While most of our retail stores were temporarily closed for varying periods of time throughout 2020, most reopened by the end of the second quarter but have experienced, and are expected to continue to experience, reductions in customer traffic, and as a result, net sales. Sales of PPE, used to help mitigate the spread of the COVID-19 virus, partially offset the negative impact of the decline in net sales and earnings due to the COVID-19 pandemic on our financial results. Our e-commerce sites have remained open in all regions and online sales have grown as consumer spending continued to shift towards online shopping experiences due to the changing retail landscape as a result of the COVID-19 pandemic.\nDuring the second quarter of 2020, HBI recorded \\$11 million of bad debt charges for customer bankruptcies and \\$20 million of charges to reserve for increased excess and obsolete inventory related primarily to canceled orders of seasonal inventory. Also during the second quarter of 2020, BHI completed a quantitative impairment analysis for certain indefinite-lived intangible assets as a result of the significant impact of the COVID-19 pandemic on their performance. Based on this analysis, HBI recorded impairment charges of \\$20 million on certain indefinite-lived trademarks and other intangible assets within the European Innerwear business. In the third quarter of 2020, HBI recorded \\$49 million of supply chain re-start up charges primarily related to incremental costs incurred, such as freight and sourcing premiums, to expedite product to meet customer demand following the extended shut-down of parts of our manufacturing network as a result of the COVID-19 pandemic. Additionally, in the fourth quarter of 2020, HBI recorded a \\$25 million charge for the impairment of goodwill related to the U.S. Hosiery reporting unit primarily as a result of the significant impact that the COVID-19 pandemic has had on this business.\nOperating profit as a percentage of net sales was 8.3% in 2022, representing a decrease from 11.7% in the prior year. Operating margin decreased as a result of lower sales volume, input cost inflation, impact from the ransomware attack, costs associated with our manufacturing time-out inventory reduction actions, deleverage from a higher proportion of transportation and distribution costs, unfavorable impact from foreign currency exchange rates and increased Full Potential plan-related investments in brand marketing and technology partially offset by pricing actions and cost reduction actions. Included in operating profit in 2022 and 2021 were charges of \\$60 million and \\$132 million, respectively, related to the implementation of the Full Potential plan.\nOperating Activities Our overall liquidity has historically been driven by our cash flow provided by operating activities, which is dependent on net income and changes in our working capital. As compared to the prior year, higher net cash used by operating activities was due to changes in working capital primarily accounts payable, accruals, inventory due to inflationary increases, softer point-of-sale trends and supply chain disruptions, and increased capital investments in our cloud computing assets partially offset by improvement in accounts receivable and lower pension plan contributions in 2022. Net cash from operating activities includes a \\$40 million contribution to our U.S. pension plan made in the first quarter of 2021.\n\nAdvertising represents one of several brand building methods used by the Company. Advertising costs, which include the development and production of advertising materials and the communication of these materials through various forms of media, are expensed in the period the advertising first takes place. The Company recognized advertising expense in the “Selling, general and administrative expenses” line in the Consolidated Statements of Income of \\$208,881, \\$208,998 and \\$113,586 in 2022, 2021 and 2020, respectively.\n\nRevenue received for shipping and handling costs is included in net sales and was \\$13,578, \\$19,461 and \\$18,943 in 2022, 2021 and 2020, respectively. Shipping costs, which comprise payments to third-party shippers, and handling costs, which consist of warehousing costs in the Company’s various distribution facilities, were \\$415,989, \\$447,131 and \\$389,252 in 2022, 2021 and 2020, respectively. The Company recognizes shipping, handling and distribution costs in the “Selling, general and administrative expenses” line in the Consolidated Statements of Income.\n\nResearch and development costs are expensed as incurred and are included in the “Selling, general and administrative expenses” line in the Consolidated Statements of Income. Research and development includes expenditures for new product, technological improvements for existing products and process innovation, which primarily consist of salaries, consulting and supplies attributable to time spent on research and development activities. Additional costs include depreciation and maintenance for research and development equipment and facilities. Research and development expense was \\$38,911, \\$39,320 and \\$37,367 in 2022, 2021 and 2020, respectively.\n\n\n#Average net operating profit margin\nnopm_avg = nopm[-5:].mean()\nprint('average net operating profit margin: {:.2f}%'.format(nopm_avg*100))\n\naverage net operating profit margin: 9.13%\n\n\n\n\nTax rate \nTax payments are taken from the consolidated income statement, provision for income taxes. The effect of taxes on profits is accounted for.\n\\(\\text{Tax rate} = \\frac{\\text{Income taxes}}{\\text{Income before income taxes}}\\)\nAdjustments for Hanesbrands Inc.\nNo adjustments for this company.\n\n# plot as Grouped bar chart with labels on right and tax rate on left\n# calculate position of bars\nx1_bar_position = []\nx2_bar_position = []\nfor i in df_dcf_data['FY']:\n    x1_bar_position.append(i-relativedelta(months=1))\n    x2_bar_position.append(i+relativedelta(months=1))\n\n# calculate tax rate\ntax_rate = df_dcf_data['income_tax']/df_dcf_data['income_before_income_taxes']\n\nwidth = 50  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, \\\\$M')\n\nrects1 = ax1.bar(x1_bar_position,df_dcf_data['income_before_income_taxes']/1e6, width,\n    label='Income before income taxes')\nrects2 = ax1.bar(x2_bar_position,df_dcf_data['income_tax']/1e6, width,\n    label='Income taxes')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((-2e3,2e3))\nax1.legend()\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],tax_rate * 100,'+-g')\n    \nax2.set_ylabel('% Tax rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,100))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Tax Rate')\nplt.show()\n\n\n\n\nObservation:\nHBI has a complex multinational tax structure with multiple types of intercompany transactions, and allocation of profits and losses among HBI and subsidiaries through intercompany transfer pricing agreements is subject to review by the Internal Revenue Service and other tax authorities.\nThe company’s tax payments and tax rate from 2016 to 2022 departed from the historically contestant average of about 12% as shown in the plot above. The 10K’s for the years 2017, 2020 and 2022 describe the changes to the tax law which caused the changes in these years. Additionally, the company has deferred taxes, tax credits and unrecognized tax benefits which complicate the analysis of the tax burden. The average tax rate calculated over the last five years is 47%, which as shown below results in a base line ISV of \\$7. Adjusting the average tax rate to 20% would change the baseline ISV to \\$10, which shows that the tax rate has a significant effect on the value of the company.\nTaxes for 2017\n\nThe 2017 enacted Tax Act significantly revised U.S. corporate income tax law by, among other things, reducing the corporate income tax rate to 21%, imposing a new minimum tax on global intangible low-taxed income (“GILTI”) and implementing a modified territorial tax system that includes a one-time transition tax on deemed repatriated earnings of foreign subsidiaries. Some of the tax provisions that become effective in the fiscal year 2018 are expected to increase HBI’s effective tax rates, such as the GILTI tax. Due to the complexities involved in accounting for the enactment of the Tax Act, SEC Staff Accounting Bulletin 118 (“SAB 118”) allows companies to record provisional estimates of the impacts of the Tax Act during a measurement period which is similar to the measurement period of up to one year from the enactment which is similar to the measurement period used when accounting for business combinations. The Company will continue to assess the impact of the recently enacted tax law on its consolidated financial statements.\n\nIncome tax expense for 2017 includes a one-time provisional charge related to U.S. tax reform of \\$457 million, primarily for the transition tax on deemed repatriated earnings of foreign subsidiaries and revaluation of our deferred tax assets and liabilities, to the lower corporate income tax rate of 21%.\n\nIncome Tax Expense – income tax expense for 2017 includes a provisional charge related to the Tax Act of \\$435 million, which includes a \\$360 million transition tax charge on deemed repatriated earnings of foreign subsidiaries, a charge of \\$72 million for the revaluation of our deferred tax assets and liabilities to the lower corporate income tax rate of 21% and a \\$3 million charge related to the deductibility of employee compensation. In addition, HBI incurred incremental tax costs of approximately \\$22 million for other impacts of tax reform and other actions taken in 2017. HBI’s effective income tax rate was 6.0% and 9.5% in 2016 and 2015, respectively. The lower effective income tax rate was primarily attributable to a lower proportion of earnings attributed to domestic subsidiaries, which are taxed at rates higher than foreign subsidiaries. Income tax expense also benefited from the adoption of accounting rules related to accounting for stock compensation, which required excess tax benefits and deficiencies to be recognized in income as they occur.\n\nTaxes for 2020 (See 10K-2020 pdf pages 103 to 107)\n\nThe Company generated income (loss) before income tax expense of \\$(183,122), \\$679,727, and \\$643,581 for the years 2020, 2019, and 2018, respectively.\n\nIn 2020, the Company continued to analyze the impacts of the Tax Act and recently issued regulations that have been published to help taxpayers interpret and apply the legislation. As a result of its analysis, Management changed its estimate of the tax liability due in connection with the one-time mandatory transition tax and recognized a \\$38,315 income tax benefit in the current period.\n\nTaxes for 2022\n\nAs of December 31, 2022, the valuation allowance for deferred tax assets was \\$626,540, made up of \\$306,743 for foreign loss carry forwards, \\$21,232 for other foreign deferred tax assets, \\$63,619 for federal and state operating loss carry forwards, and \\$234,946 for other federal and state deferred tax assets. The net change in the total valuation allowance for 2022 was \\$320,319, which relates to an increase of \\$24,172 for foreign loss carry forwards, an increase of \\$9,166 for other foreign deferred tax assets, an increase of \\$52,035 for federal and state operating loss carryforwards and an increase of \\$234,946 for other federal and state deferred tax assets.\n\nDuring 2022, the Company recorded \\$696,028 of additional foreign net operating losses due to tax-deductible impairments in Switzerland and Luxembourg. These losses are subject to recapture in Switzerland and Luxembourg such that they will be taxable in a future year, therefore deferred tax liabilities were recorded. The Company believes it is reasonably possible that the deferred tax liability in Switzerland will reverse within the next twelve months due to expected actions by the Company in 2023.\n\nIncome Tax Expense – The effective income tax rate was 137.2% and 10.3% for 2022 and 2021, respectively. The higher effective tax rate for 2022 was primarily due to non-cash discrete tax charges of \\$423 million for valuation allowances established against U.S. deferred tax assets and tax impairments in Switzerland which generated deferred tax liabilities during 2022.\nCurrent and deferred tax provisions (benefits) were:\n\n\n\nYear ended December 31, 2022\nCurrent\nDeferred\nTotal\n\n\n\n\nDomestic\n\\$15,188\n\\$201,112\n\\$216,300\n\n\nForeign\n\\$83,607\n\\$95,558\n\\$179,165\n\n\nState\n-\\$2,712\n\\$91,154\n\\$88,442\n\n\nTotal\n\\$96,083\n\\$387,824\n\\$483,907\n\n\n(amounts in thousands)\n\n\n\n\n\n\nDeferred tax assets relate to temporary differences (differences between the assets and liabilities in the consolidated financial statements and the assets and liabilities in the calculation of taxable income) including net operating losses.\nHBI continues to use a portfolio approach to release the income tax effects in accumulated other comprehensive loss related to pension and post retirement benefits. Under this approach, the income tax effects are released from accumulated other comprehensive loss based on the pre-tax adjustments to pension liabilities or assets recognized within other comprehensive income. Any tax effects remaining in accumulated other comprehensive loss are released only when the entire portfolio of the pension and post retirement benefits is liquidated, sold or extinguished.\nIn December 31, 2022, the Company had domestic tax credit carry forwards totaling \\$10,859, which expire beginning after 2022.\nIn 2022, 2021, and 2020, the Company recognized reductions of unrecognized tax benefits for tax positions of prior years of \\$311, \\$12,599, and \\$18,385, respectively. In 2022, 2021, and 2020, income tax benefits recognized in connection with the expiration of statutes of limitations were \\$7,191, \\$147, and \\$16,655, respectively. The Company believes it is reasonably possible that the amount of unrecognized tax benefits may decrease by \\$3,267 within the next 12 months due to expirations in statutes of limitations. (See pdf page 100 for table)\nAt December 31, 2022, the balance of the Company’s unrecognized tax benefits, which would, if recognized, affect the Company’s annual effective tax rate was \\$28,444. The Company’s policy is to recognize interest and/or penalties related to income tax matters in income tax expense. The Company recognized \\$81, \\\\(933 and \\\\\\)(5,206) in 2022, 2021 and 2020, respectively, for interest and penalties classified as income tax expense (benefit) in the Consolidated Statements of Income. At December 31, 2022 and January 1, 2022, the Company had a total of \\$6,303 and \\$5,865, respectively, of interest and penalties accrued related to unrecognized tax benefits.\n\n# Average tax rate\ntax_rate_avg = tax_rate[-5:].mean()\nprint('average tax rate: {:.2f}%'.format(tax_rate_avg*100))\n\naverage tax rate: 46.81%\n\n\n\n\nDepreciation Rate \nThe depreciation rate is used to project the future net investment cash flows. The effect is to reduce the amount of FCFF. Depreciation amounts are from the Consolidated Statement of Cash Flows, Depreciation and Amortization.\n\\(\\text{Depreciation Rate}=\\frac{\\text{Depreciation and Amortization}}{\\text{Revenues}}\\)\nDepreciation is the write off or expensing of a percentage of the historical cost of an asset over the asset’s useful life. Property, plant and equipment (PP&E) are long term or non current assets owned or controlled by the company and used to manufacture and or sell the company’s products. The balance sheet typically shows all categories of PP&E grouped together, net of accumulated depreciation. Depreciation represents wear and tear on an asset or the fact that an asset gets used up over time. Companies record depreciation expense in the income statement every year for all depreciable assets in service or used by the company during the year. The difference between GAAP and Tax Accounting methods is handled through deferred taxes.\nAmortization is the write off or expensing of the cost of a financial instrument or an intangible asset over the shorter of its useful life or legal life. Amortization is similar to depreciation and reflects the declining useful life and value of the intangible asset over time. Companies in research and development intensive fields typically have many patents. Such industries include high technology, pharmaceuticals and chemicals.\n\n# depreciation rate\ndepreciation_rate = df_dcf_data['depreciation'] / df_dcf_data['revenue'].to_numpy()\n\n# plot depreciation on left and rate on right\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $B')\n\nax1.bar(df_dcf_data['FY'],df_dcf_data['depreciation']/1e9, width=100,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:Blue'\n\nax2.plot(df_dcf_data['FY'],depreciation_rate*100,'+-')\n    \nax2.set_ylabel('% Depreciation rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((0,30))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Depreciation')\nplt.show()\n\n\n\n\nObservation:\nAverage depreciation rate as as percentage of revenue over the last 5 years is 1.84%. The depreciation and amortization expense for 2022 is comprised of the following:\n\n\n\nDepreciation and amortization expense:\n12/31/22\n\n\n\n\nInnerwear\n\\$26,518\n\n\nActivewear\n\\$22,420\n\n\nInternational\n\\$19,670\n\n\nOther\n\\$3,341\n\n\nCorporate\n\\$32,538\n\n\nTotal depreciation and amortization expense\n\\$106,267\n\n\n(amounts in thousands)\n\n\n\n\nProperty is stated at historical cost and depreciation expense is computed using the straight-line method over the estimated useful lives of the assets. Machinery and equipment is depreciated over periods ranging from one to 15 years and buildings and building improvements over periods of up to 40 years. A change in the depreciable life is treated as a change in accounting estimate and the accelerated depreciation is accounted for in the period of change and future periods. Additions and improvements that substantially extend the useful life of a particular asset and interest costs incurred during the construction period of major properties are capitalized. Repairs and maintenance costs are expensed as incurred. Upon sale or disposition of an asset, the cost and related accumulated depreciation are removed from the accounts.\n\n# average depreciation rate\ndepreciation_rate_avg = depreciation_rate[-5:].mean()\nprint('average depreciation rate: {:.2f}%'.format(depreciation_rate_avg*100))\n\naverage depreciation rate: 1.84%\n\n\n\n\nInvestment Rate \nTaken from Consolidated Statement of Cash Flows, Cash used for investing activities. Net investment in the dollar amount needed to support the growth of the firm. Included investments in properties, plant equipment in excess of the depreciation expenses associated with past investments. Net investment decreases the amount of money available to the stockholders. Investment in property, plant and equipment is necessary to both maintain service and sales and also to grow revenues and profits. Investment amounts should include capital expenditures and research and development.\n\\(Ir=\\frac {\\text {Capital Expenditures}}{\\text{Revenues}}\\)\nFor this company, the yearly investment amounts are taken from the Consolidated Statements of Cash Flows, Net Cash Used in Investing Activities.\nAdjustments for Hanesbrands Inc.\nNo adjustments for this comapany.\n\n# investment rate\ninvestment_rate = df_dcf_data['investment'] / df_dcf_data['revenue'].to_numpy()\n\n# plot investment on left and rate on right\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, \\\\$B')\n\nax1.bar(df_dcf_data['FY'],df_dcf_data['investment']/1e9, width=100,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:Blue'\n\nax2.plot(df_dcf_data['FY'],investment_rate*100,'+-')\n    \nax2.set_ylabel('% New Investment Rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-10,40))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('New Investment')\nplt.show()\n\n\n\n\nObservation:\nAverage investment rate as as percentage of revenue over the last 5 years is 2.52%.\nThe investing activities shown in the 2022 K-10 are shown in the table below:\n\n\n\nInvesting activities\n12/31/22\n\n\n\n\nCapital expenditures\n-\\$112,122\n\n\nPurchase of trademarks\n-\\$103,000\n\n\nProceeds from sales of assets\n\\$157\n\n\nOther\n-\\$1,463\n\n\nNet cash from investing activities\n-\\$216,428\n\n\n(amounts in thousands)\n\n\n\n\nThe following investments for the years 2013, 2016 and 2018 are described below.\n\nIn October 2013, HBI expanded their portfolio of brands through the acquisition of Maidenform, a global intimate apparel company. Maidenform is a leading seller of bras, shapewear and panties under brands such as Maidenform , Flexees, Lilyette, Self Expressions and Sweet Nothings , as well as Donna Karan and DKNY intimate apparel under license. The acquisition was an all cash transaction valued at approximately \\$581 million.\n\nHBI acquired Champion Europe on June 30, 2016. The acquisition, combined with Champion brand rights previously owned, unites the Champion brand globally and gives HBI a powerful platform for growth on every continent.\n\nHBI acquired Hanes Australasia on July 14, 2016.\n\nOn July 14, 2016, the Company acquired 100% of the outstanding shares of Pacific Brands Limited (“Hanes Australasia”) for a total purchase price of AUD \\$1,049,360 (US \\$800,871).\n\nOn February 12, 2018, HBI acquired 100% of the outstanding equity of BNT Holdco Pty Limited (“Bras N Things”) for a total purchase price of AUD \\$498,236 (US \\$391,572). The purchase price was subsequently revised to AUD \\$495,224 (US \\$389,205) due to a final working capital adjustment.\n\nIn June of 2022, HBI purchased the Champion trademark for footwear in the United States, Puerto Rico and Canada from Keds, LLC (“KEDS”) for \\$103 million.\n\n\n# average investment rate\ninvestment_rate_avg = investment_rate[-5:].mean()\nprint('average investment rate: {:.2f}%'.format(investment_rate_avg*100))\n\naverage investment rate: 2.52%\n\n\n\n\nWorking Capital Rate \nWorking capital is needed to support the corporate sales effort of any company. Often a company’s incremental change in net working capital either positive or negative is approximately proportional to its change in revenue.\n\\(\\text{Working capital} = \\text{Accounts Receivable} + \\text{Inventories} - \\text{Accounts Payable}\\)\nWorking capital is a company’s net investment in its accounts receivable and its inventories (cash outflows), minus its accounts payable (a cash inflow). Working capital and taxes are cash outflows from the corporation that are not available to pay debts and stockholders.\nAdjustments for Hanesbrands Inc.\nNo adjustments for this company.\n\n# plot as four grouped bar chart with labels on right and working capital rate on left\n# calculate position of bars\nx1_bar_position = []\nx2_bar_position = []\nx3_bar_position = []\nx4_bar_position = []\nfor i in df_dcf_data['FY']:\n    x1_bar_position.append(i-relativedelta(months=3))\n    x2_bar_position.append(i-relativedelta(months=1))\n    x3_bar_position.append(i+relativedelta(months=1))\n    x4_bar_position.append(i+relativedelta(months=3))\n\n# calculate working capital rate\nworking_capital = (df_dcf_data['accounts_receivable'] + df_dcf_data['inventories']) - \\\n    df_dcf_data['accounts_payable']\nworking_capital_rate = working_capital / df_dcf_data['revenue']\n\nwidth = 40  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, \\\\$B')\n\nrects1 = ax1.bar(x1_bar_position,df_dcf_data['accounts_receivable']/1e9, width,\\\n    label='Accounts Receivable')\nrects2 = ax1.bar(x2_bar_position,df_dcf_data['inventories']/1e9, width, label='Inventory')\n\nrects2 = ax1.bar(x3_bar_position,df_dcf_data['accounts_payable']/1e9, width, label='Accounts Payable')\nrects2 = ax1.bar(x4_bar_position,working_capital/1e9, width, label='Working Capital')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((-50,200))\nax1.legend()\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:Blue'\n\nax2.plot(df_dcf_data['FY'],working_capital_rate * 100,'+-')\n    \nax2.set_ylabel('% Working Capital Rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,50))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Working Capital')\nplt.show()\n\n\n\n\nObservation:\nAverage working capital rate as as percentage of revenue over the last 5 years is 24%.\nCustomers increasingly require some of products on an exclusive basis, which could cause an increase in the number of stock keeping units, or “SKUs,” that must carried and, consequently, increase inventory levels and working capital requirements\nHBI’s overall liquidity has historically been driven by cash flow provided by operating activities, which is dependent on net income and changes in working capital. As compared to the prior year, higher net cash used by operating activities was due to changes in working capital primarily accounts payable, accruals, inventory due to inflationary increases, softer point-of-sale trends and supply chain disruptions, and increased capital investments in cloud computing assets partially offset by improvement in accounts receivable and lower pension plan contributions in 2022.\nAccounts receivable consist primarily of amounts due from customers. HBI carries accounts receivable at their net realizable value. In determining the appropriate allowance for doubtful accounts, HBI evaluates receivables on a collection (pool) basis which are aggregated based on similar risk characteristics and consider a combination of factors, such as historical losses, the aging of trade receivables, industry trends, and our customers’ financial strength, credit standing and payment and default history. Changes in the characteristics of accounts receivables and the aforementioned factors, among others, are reviewed quarterly and may lead to adjustments in allowance for doubtful accounts. The calculation of the required allowance involves judgment by our management as to the impact of these and other factors on the ultimate realization of trade receivables. Charges to the allowance for doubtful accounts are reflected in the “Selling, general and administrative expenses” line and charges to the allowance for customer chargebacks and other customer deductions are primarily reflected as a reduction in the “Net sales” line in Consolidated Statements of Income.\nAccounts receivable are stated at their net realizable value. The allowance for doubtful accounts reflects the Company’s best estimate of probable losses inherent in the accounts receivable portfolio. Trade receivables are evaluated on a collection (pool) basis and aggregated on the basis of similar risk characteristics which are determined on the basis of historical losses, the aging of trade receivables, industry trends, and its customers’ financial strength, credit standing and payment and default history.\nThe Company has entered into agreements to sell selected trade accounts receivable to financial institutions based on programs offered by certain of the Company’s largest customers as well as programs sponsored by the Company. As a result of the strong credit worthiness of these customers, the discount taken on most of these programs is less than the marginal borrowing rate on the Company’s variable rate credit facilities. In all agreements, after the sale, the Company does not retain any beneficial interests in the receivables. The applicable financial institution services and collects the accounts receivable directly from the customer for programs offered by the Company’s customers. For programs sponsored by the Company, the Company maintains continued involvement as the servicer to collect the accounts receivable from the customer and remit payment to the financial institution. Net proceeds of these accounts receivable sale programs are recognized in the Consolidated Statements of Cash Flows as part of operating cash flows. The Company recognized total funding fees of \\$8,823, \\$3,312 and \\$4,932 in 2022, 2021 and 2020, respectively, for sales of accounts receivable to financial institutions in the “Other expenses” line in the Consolidated Statements of Income.\nSee ARS Facility on pdf page 91\nHBI carries inventory on the balance sheet at the estimated lower of cost or market. Cost is determined by the first-in, first-out, or “FIFO,” method for our inventories. HBI carries obsolete, damaged and excess inventory at the net realizable value, which they determine by assessing historical recovery rates, current market conditions and our future marketing and sales plans.\nInventories are stated at the estimated lower of cost or net realizable value. Cost is determined by the first-in, first-out, or “FIFO”, method for inventories. Obsolete, damaged, and excess inventory is carried at the net realizable value, which is determined by assessing historical recovery rates, current market conditions and future marketing and sales plans. Rebates, discounts and other cash consideration received from a vendor related to inventory purchases are reflected as reductions in the cost of the related inventory item and are therefore reflected in cost of sales when the related inventory item is sold.\n\n\n\nInventories\n12/31/22\n\n\n\n\nRaw materials\n\\$69,279\n\n\nWork in process\n\\$107,904\n\n\nFinished goods\n\\$1,802,489\n\n\nTotal\n\\$1,979,672\n\n\n(amounts in thousands)\n\n\n\n\n\n# average working capital rate\nworking_capital_rate_avg = working_capital_rate[-5:].mean()\nprint('average working capital rate: {:.2f}%'.format(working_capital_rate_avg*100))\n\naverage working capital rate: 24.19%\n\n\n\n\nCurrent assets \nTotal Current Assets from the most recent balance sheet statement of the company. Current assets include inventory, cash and accounts receivables.\nAdjustments for Hanesbrands Inc.\nNone for this company.\n\n# plot Short Term Assets\nwidth = 100  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nplt.bar(df_dcf_data['FY'],df_dcf_data['current_assets']/1e9, width)\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Current assets')\nplt.ylabel('dollars, \\\\$B')\n\nplt.grid()\nplt.show()\n\n\n\n\nObservation:\nThe current assets for HBI are \\$3.13B. Current assets are used to calculate the total corporate value.\n\nsta = df_dcf_data['current_assets'].iloc[-1]\nprint('Current assets: ${:.2f}B'.format(sta/1e9))\n\nCurrent assets: $3.13B\n\n\n\n\nCurrent liabilities \nTotal Current Liabilities from the most recent balance sheet consolidated statement.\nAdjustments for Hanesbrands Inc.\nNone for this company.\n\n# plot Short Term Liabilities\n\nwidth = 100  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nplt.bar(df_dcf_data['FY'],df_dcf_data['current_liabilities']/1e9, width)\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Current liabilities')\nplt.ylabel('dollars, \\\\$B')\n\nplt.grid()\nplt.show()\n\n\n\n\n\nprint('Average of current liabilities: ${:.2f}B'.format(df_dcf_data['current_liabilities'].mean()/1e9))\n\nAverage of current liabilities: $1.54B\n\n\nObservation:\nThe current liabilities for HBI are \\$1.79B. Current liabilities are used to calculate the total value of common equity.\n\nstl = df_dcf_data['current_liabilities'].iloc[-1]\nprint('Current liabilities: ${:.2f}B'.format(stl/1e9))\n\nCurrent liabilities: $1.79B\n\n\n\n\nValue of Debt Outstanding \nAmount of debt outstanding from the most recent balance sheet of the company.\nAdjustments for Hanesbrands Inc.\nNone for this company.\n\n# calculate the percent change in debt, pcd\npcd = np.zeros(len(df_dcf_data['long_term_debt'].to_numpy())) # percent change in debt\nfor i in range(len(df_dcf_data['long_term_debt'].to_numpy()[0:-1])):\n    pcd[i+1] = ((df_dcf_data['long_term_debt'].to_numpy()[i+1] - df_dcf_data['long_term_debt'].to_numpy()[i])/\n                df_dcf_data['long_term_debt'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars, $B')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['long_term_debt']/1e9, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcd,'+-g')\n    \nax2.set_ylabel('% Change in debt',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-40,100))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('debt')\nplt.show()\n\n\n\n\n\ndgr_avg = pcd[1:].mean()/100\nprint('average debt growth rate: {:.2f}%'.format(dgr_avg*100))\n\naverage debt growth rate: 3.04%\n\n\nObservation:\nThe total long term debt and other for HBI is \\$3.61B. Currently the Debt to NOP ratio is 7. HBI Directors eliminated the quarterly cash dividend as they recently shifted their capital allocation strategy to pay down debt to bring leverage back to a range that is no greater than two to three times on a net debt-to-adjusted EBITDA basis. Future principal payments for all of the facilities described above are as follows: \\$247,000 due in 2023, \\$1,485,275 due in 2024, \\$62,500 due in 2025, and \\$2,077,500 due in 2026.\nA summary of the Company’s debt is presented below:\n\n\n\n\n\n\n\n\n\nSenior Secured Credit Facility\nInterest Rate\nPrincipal Amount\nMaturity Date\n\n\n\n\nRevolving Loan Facility\n5.83%\n\\$352,500\n11/01/26\n\n\nTerm Loan A\n5.92%\n\\$975,000\n11/01/26\n\n\n4.875% Senior Notes\n4.88%\n\\$900,000\n05/01/26\n\n\n4.625% Senior Notes\n4.63%\n\\$900,000\n05/01/24\n\n\n3.5% Senior Notes\n3.50%\n\\$535,275\n06/01/24\n\n\nAccounts Receivable Securitization Facility\n5.09%\n\\$209,500\n06/01/23\n\n\n\nas of December 31, 2022\n(amounts in thousands)\n\n\n\n\nTotal cash paid for interest related to debt in 2022, 2021 and 2020 was \\$150,452, \\$161,202 and \\$157,094, respectively.\nDuring 2022, 2021 and 2020, the Company paid \\$3,159, \\$8,346 and \\$15,010, respectively, in capitalized debt issuance costs related to the Company’s financing arrangements within continuing operations. Debt issuance costs are amortized to interest expense over the respective lives of the debt instruments, which range from one to 10 years. As of December 31, 2022, the net carrying value of unamortized debt issuance costs for the revolving loan facilities, which is included in “Other noncurrent assets” in the Consolidated Balance Sheets, was \\$6,831 and the net carrying value of unamortized debt issuance costs for the remainder of the Company’s debt, which is included in “Long-term debt” in the Consolidated Balance Sheets was \\$13,198. The Company’s debt issuance cost amortization in continuing operations was \\$7,300, \\$12,305 and \\$11,349 in 2022, 2021 and 2020, respectively.\n\nvod = df_dcf_data['long_term_debt'].iloc[-1]\nprint('Total long term debt and other: ${:.2f}B'.format(vod/1e9))\n\nTotal long term debt and other: $3.61B\n\n\n\n\nCurrent stock price \nMost recent stock price for the company. The current stock price is used to calculate the market value of the firm. Use the market value when looking at market capitalization for common stock.\n\ncsp = 4.4 # current stock price\nprint('current stock price: ${:,.2f}'.format(csp))\n\ncurrent stock price: $4.40\n\n\nThe current stock price: \\$4.40. The 52 week range is \\$3.85 to \\$12.13.\n\n\nShares outstanding \nThe number of shares outstanding is used to calculate the intrinsic stock value.\n\nso = df_dcf_data['shares_outstanding'].iloc[-1] # shares outstanding\nprint('shares outstanding, basic: {:,.0f}'.format(so))\n\nshares outstanding, basic: 349,361,517\n\n\n\n# calculate the percent change in shares outstanding, pcso\npcso = np.zeros(len(df_dcf_data['shares_outstanding'].to_numpy())) # percent change in debt\nfor i in range(len(df_dcf_data['shares_outstanding'].to_numpy()[0:-1])):\n    pcso[i+1] = ((df_dcf_data['shares_outstanding'].to_numpy()[i+1] - df_dcf_data['shares_outstanding'].to_numpy()[i])/\n                df_dcf_data['shares_outstanding'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('shares outstanding, M')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['shares_outstanding']/1e6, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcso,'+-g')\n    \nax2.set_ylabel('% Change in shares outstanding',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-5,25))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Shares outstanding')\nplt.show()\n\n\n\n\n\nprint('average shares outstanding growth rate: {:.2f}%'.format(pcso[1:].mean()))\n\naverage shares outstanding growth rate: -0.83%\n\n\nObservation:\nUnder the February 6, 2020 share repurchase program, the Company entered into transactions to repurchase 14,464 shares at a weighted average repurchase price of \\$13.83 per share for the year ended January 2, 2021. These shares were repurchased at a total cost of \\$200,269. The Company did not purchase any shares of the Company’s common stock under the February 6, 2020 share repurchase program during 2021.\nUnder the new program, the Company entered into transactions to repurchase 1,577 shares at a weighted average repurchase price of \\$15.84 per share for the year ended December 31, 2022. The shares were repurchased at a total cost of \\$25,018 including broker’s commissions of \\$31. The Company did not repurchase any shares under the previous share repurchase program during 2022 through the expiration of the program on February 2, 2022. At December 31, 2022, the remaining repurchase authorization under the current share repurchase program totaled \\$575,013.\n\n\nDilution\nDilution occurs when a company issues new shares that result in a decrease in existing stockholders’ ownership percentage of that company. Stock dilution can also occur when holders of stock options, such as company employees, or holders of other optionable securities exercise their options. When the number of shares outstanding increases, each existing stockholder owns a smaller, or diluted, percentage of the company, making each share less valuable.\nInvestigate why there is a historic growth trend in number of shares outstanding. Search annual report for dilutive actions:\n\nshare sales\n\nconvertable debt\n\nemployee options\n\nSearch results:\nThe Company established the Omnibus Incentive Plan to award stock options, stock appreciation rights, restricted stock, restricted stock units, deferred stock units, performance shares and cash to its employees, non-employee directors and employees of its subsidiaries to promote the interests of the Company, incent performance and retain employees. In April 2020, the stockholders of the Company approved the Hanesbrands Inc. 2020 Omnibus Incentive Plan (the “2020 Omnibus Plan”). The Company satisfies the requirement for common shares for share-based payments to employees pursuant to the 2020 Omnibus Plan by issuing newly authorized shares. The 2020 Omnibus Plan authorized a total of 11,000 shares of common stock of the Company for awards granted under the 2020 Omnibus Plan, plus the number of shares of common stock of the Company available for grant under the predecessor HanesbrandsInc. Omnibus Incentive Plan (the “Prior Plan”) that had not yet been made subject to awards under the Prior Plan as of the effective date of the 2020 Omnibus Plan. The 2020 Omnibus Plan authorized 74,220 shares for awards of stock options and restricted stock units, of which 14,033 shares were available for future grants as of December 31, 2022.\nIn addition, during 2020, the Company granted stock awards to two newly hired executive officers outside of the 2020 Omnibus Plan in reliance on the employment inducement exemption under the New York Stock Exchange’s Listed Company Manual Rule 303A.08.\nThere were no stock option exercises during 2022 or 2021. The total intrinsic value of options that were exercised during 2020 was \\$3,299.\nThe total fair value of shares vested during 2022, 2021 and 2020 was \\$13,199, \\$25,201 and \\$15,325, respectively.\nAt December 31, 2022, there was \\$23,329 of total unrecognized compensation cost related to non-vested stock-based compensation arrangements, of which \\$16,349, \\$6,097, and \\$883 is expected to be recognized in continuing operations in 2023, 2024, and 2025, respectively.\nCertain of the international plans, specifically those acquired in connection with the purchase of Champion Europe, are in substance nonretirement postemployment benefit plans, which are future liabilities funded through future operational results of the Company. However, for purposes of consolidation, the Company is including these plans within the defined benefit reporting. At December 31, 2022 and January 1, 2022, the total amounts accrued for these plans were \\$871 and \\$1,171, respectively and the total expense was \\$9, \\$8 and \\$16 for 2022, 2021 and 2020, respectively.\nThe potential dilution seems to be 85,224 shares (11,000 + 74,220) authorized under the Omnibus Incentive Plan. This represents 0.024% of the shares outstanding. At \\$5 per share, this has a value of \\$4.26M.\n\n\n10 year treasury bond yield \nThe 10 year treasury yield is used as a measure of the risk free rate.\nYield: 3.45%\niShares 7-10 Year Treasury Bond ETF (IEF)\nAverage Yield to Maturity: 3.5%\n\ntby = (3.45+3.5)/2/100  # 10 year treasury bond yield, average of data from sources listed above\nprint('10 year treasury bond yield: {:,.2f}%'.format(tby*100))\n\n10 year treasury bond yield: 3.48%\n\n\n\n\nBond yield spread to treasury \nThe spread to treasury implies that all corporate debt will have a higher yield than yields associated with comparable maturity US Treasury Bonds. The best way to determine default risk is to see how a particular company’s debt is trading in the market and compare it on a spread basis with comparable maturity yields.\nLook at the following or use a default rating systems that are published by the three major rating agencies, Standards and Poors Corp, Moody’s Investor Services and Fitch & Company.\nPIMCO Active Bond Exchange-Traded Fund (BOND)\nYield: 3.44%\niShares 5-10 Year Investment Grade Corporate Bond ETF (IGIB)\nAverage Yield to Maturity: 5.15%\niShares 10+ Year Investment Grade Corporate Bond ETF (IGLB)\nAverage Yield to Maturity: 5.35%\nWeb resources: - http://www.standardpoor.com/\n- http://bond.yahoo.com/rates.html\n- http://www.moodys.com/cust/default.asp\n- http://www.fitchibca.com/corporate/index.cfm\n\nbystt = ((3.44+5.15+5.35)/3-tby)/100           # bond yield spread (average) to treasury spread\nprint('Bond yield spread to treasury: {:,.2f}%'.format(bystt*100))\n\nBond yield spread to treasury: 4.61%\n\n\n\n\nPreferred stock yield \nAmount of preferred stock outstanding from the most recent balance sheet of the company.\nAdjustments for Hanesbrands Inc.\nPreferred stock (50,000,000 authorized shares; \\$.01 par value) Issued and outstanding — None\n\npsy = 0/100  # preferred stock yield\nprint('preferred stock yield: {:,.2f}%'.format(psy*100))\n\nvps = 0 # value of preferred stock\nprint('value of preferred stock: {:,.2f}'.format(vps))\n\npreferred stock yield: 0.00%\nvalue of preferred stock: 0.00\n\n\n\n\nEquity risk premium \nThe expected excess return a hypothetical average investor would require of a diversified portfolio of stock (assumed beta = 1.0) over the yield on the 10-year Treasury Bond. The equity risk premium has been going down over the years.\n- 1926 to 1990: 5.5%\n- 1962 to 1990: 3.25%\n- 1981 to 1990: 0.19%\nIn times of sustained economic growth the risk premium demanded by investors generally declines.\nI’m going to use 3% as the equity risk premium.\n\neq_rp = 3.0/100             # equity risk premium\nprint('Equity risk premium: {:,.2f}%'.format(eq_rp*100))\n\nEquity risk premium: 3.00%\n\n\n\n\nCompany specific beta \nThe Beta used is Beta of Equity. Beta is the monthly price change of a particular company relative to the monthly price change of the S&P 500. The time period for Beta is 5 years when available. This value can be obtained at yahoo finance.\nA measure of risk of an individual stock. It measures volatility of return - a higher beta means a higher risk. A financial model that uses Beta as its sole measure of risk (signal factor model) is called a Capital Asset Pricing Model (CAPM).\n\nbeta = 1.59 # company specific beta\nprint('Company specific beta: {:,.2f}'.format(beta))\n\nCompany specific beta: 1.59\n\n\n\n\nDCF model inputs \nBelow are the DCF model inputs. These values were calculated above.\n\n# various rates\nrgr = rgr_avg              # revenue growth rate\nprint('revenue growth rate: {:,.2f}%'.format(rgr*100))\nnopm = nopm_avg             # net operating profit margin\nprint('net operating profit margin: {:,.2f}%'.format(nopm*100))\ntr = tax_rate_avg               # tax rate\nprint('tax rate: {:,.2f}%'.format(tr*100))\ndr = depreciation_rate_avg              # depreciation rate (% of revenue)\nprint('depreciation rate: {:,.2f}%'.format(dr*100))\nir = investment_rate_avg              # investment rate (% of revenue)\nprint('investment rate: {:,.2f}%'.format(ir*100))\nwcr = working_capital_rate_avg            # working capital rate (% of revenue)\nprint('working capital rate: {:,.2f}%'.format(wcr*100))\n\nrevenue growth rate: -0.88%\nnet operating profit margin: 9.13%\ntax rate: 46.81%\ndepreciation rate: 1.84%\ninvestment rate: 2.52%\nworking capital rate: 24.19%\n\n\nExcess return period\nThe excess return period is based on a judgment call. The authors of [1] use the 1-5-7-10 rule. They group companies into one of four general categories and excess return periods. They use a 10 year excess return period to calculate what they would consider the maximum value. They use a more conservative 1 year, 5 year or 7 year return period to calculate a more reasonable or minimum value.\n\n1 year: Boring companies that operate in a highly competitive, low margin industry in which they have nothing particular going for them.\n\n5 year: Decent companies that have a recognizable name and decent reputation and perhaps a regulatory benefit (utility company), but can’t control pricing or growth.\n\n7 year: Good companies with good brand names, large companies of scale, good marketing channels and consumer identification (e.g. McDonald’s)\n\n10 year: Great companies with great growth potential, tremendous marketing power, band names and in-place benefits (e.g. Intel, Microsoft, Coca Cola, Disney)\n\nThe excess return period used for the base case is ten years, which should lead to a higher calculated intrinsic value.\n\n# General Inputs\nfy_start = df_dcf_data['FY'].iloc[-1].year # fiscal year to start excess return period\nerp = 10 # excess return period, years\nrev_start = df_dcf_data['revenue'].to_numpy()[-1] # starting revenues for excess return period\nprint('starting year: {:.0f}'.format(fy_start))\nprint('excess return period: {:.0f} years'.format(erp))\nprint('starting revenues: ${:,.2f}B'.format(rev_start/1e9))\nprint('shares outstanding: {:,.0f}'.format(so))\n\nstarting year: 2022\nexcess return period: 10 years\nstarting revenues: $6.23B\nshares outstanding: 349,361,517\n\n\n\nps_mv = vps               # preferred stock, market value \nprint('preferred stock, market value : ${:,.2f}B'.format(ps_mv/1e9))\ncs_mv = csp*so            # common stock, market value \nprint('common stock, market value: ${:,.2f}B'.format(cs_mv/1e9))\n\npreferred stock, market value : $0.00B\ncommon stock, market value: $1.54B\n\n\nLong Term Debt, Market Value, ltd_mv\nUse the book value for long term debt. Various online resources can be used to research this item. These include, Bondsonline and Bloomberg. The book value of debt and preferred stock is an accounting measure that relates to how much money was raised by the company when each security was issued. The market value of debt and the preferred and common stock is the price that specific obligations would trade at in today’s market.\nLong term debt for firms can take one of two forms. It can be a long-term loan from a bank or other financial institution or it can be a long-term bond issued to financial markets, in which case the creditors are the investors in the bond. Firms often have long term obligations that are not captured in the long term debt item. These include obligations to lessors on assets that firms have leased, to employees in the form of pension fund and health care benefits yet to be paid, and to the government in the form of taxes deferred. In the last two decades, accountants have increasingly moved towards quantifying these liabilities and showing them as long term liabilities.\n\nltd_mv = vod              # market value of long term debt\ntmv = ltd_mv+ps_mv+cs_mv  # total market value \nprint('total market value: ${:,.2f}B'.format(tmv/1e9))\n\ntotal market value: $5.15B\n\n\nCost of Common Equity, cce\nThe expected excess return a hypothetical average investor would require of a diversified portfolio of stock (assumed beta = 1.0) over the yield on the 10-year Treasury Bond. The annual rate of return that an investor expects to earn when investing in shares of a company is known as the cost of common equity. It includes dividends and increases in the market value.\n\ncce = tby+beta*eq_rp      # cost of common equity or the expected return for the stock\nprint('cost of common equity: {:,.2f}%'.format(cce*100))\n\ncost of common equity: 8.24%\n\n\nLong Term Debt, Average Yield, ltd_ay\nThe total cost of long term debt.\n\nltd_ay = tby+bystt        # long term debt average yield\nprint('long term debt average yield: {:,.2f}%'.format(ltd_ay*100))\n\nlong term debt average yield: 8.09%\n\n\nLong Term Debt, After Tax Yield, ltd_aty\nThe tax benefits of long term debt. Interest payments are tax deductible for the company.\n\nltd_aty = ltd_ay*(1-tr)   # long term debt after tax yield\nprint('long term debt after tax yield: {:,.2f}%'.format(ltd_aty*100))\n\nltd_pc = vod/tmv          # weight for long term debt \nltd_ate = ltd_aty*ltd_pc  # after tax effect of long term debt \nps_ay = psy               # preferred stock, average yield \nps_aty = ps_ay            # preferred stock, average yield \nprint('preferred stock, average yield: {:,.2f}%'.format(ps_aty*100))\n\nps_pc = ps_mv/tmv         # preferred stock, % capital \nps_ate = ps_aty*ps_pc     # preferred stock, after tax effect \ncs_ay = cce               # common stock, average yield \ncs_aty = cce              # common stock, after tax yield \nprint('common stock, after tax yield: {:,.2f}%'.format(cs_aty*100))\n\ncs_pc = cs_mv/tmv         # common stock, % capital \ncs_ate = cs_aty*cs_pc     # common stock, after tax effect \nprint('common stock, after tax effet: {:,.2f}%'.format(cs_ate*100))\n\ntate = ltd_ate+ps_ate+cs_ate # total after tax effect \nprint('total after tax effect: {:,.2f}%'.format(tate*100))\ntpc = ltd_pc+ps_pc+cs_pc     # total % Capital\nprint('total % Capital: {:,.2f}%'.format(tpc*100))\n\nlong term debt after tax yield: 4.30%\npreferred stock, average yield: 0.00%\ncommon stock, after tax yield: 8.24%\ncommon stock, after tax effet: 2.46%\ntotal after tax effect: 5.48%\ntotal % Capital: 100.00%\n\n\nWeighted average cost of capital\nA company’s weighted average cost of capital (WACC) is the weighted average of the company’s current cost of debt and equity calculated by using current debt, preferred stock and common stock market values. The WACC of the company, calculated after tax, is the discount rate used in the DCF valuation procedures. The WACC, which is the cost of the different components of financing used by the firm, weighted by their market value proportions. These include debt, preferred stock, and common stock.\nWACC: Weighted Average Cost of Capital, the rate used to discount cash flows, based on the following three factors. 1. Base rate of return. 2. Expected return based on debt and preferred stock. 3. Expected return on common stock and Beta.\nAll adjusted for the tax advantage of interest payments and the percentage of debt, preferred stock and common stock.\n\nwacc = tate\nprint('weighted average cost of capital: {:.1f}%'.format(wacc*100))\n\nweighted average cost of capital: 5.5%\n\n\n\n\nFuture cash flows \nThe future cash flows to the firm are projected based on revenue growth. The cash flows are then discounted using the WACC and the ISV is calculated.\n\n# make a list of the fiscal years in excess return period \nfy = np.zeros(erp+1)\nfy[0] = fy_start\nfor i in range(1,erp+1): \n    fy[i]=fy_start+i\n\nrev = np.zeros(len(fy))\nciwc = np.zeros(len(fy))\nrev[0] = rev_start  #*rgr+rev_start   # find the future revenue using constant revenue growth rate \n\nfor i in range(1,len(fy)): \n    rev[i] = rev[i-1]*rgr+rev[i-1]  # find the future revenue \n    ciwc[i] = (rev[i]-rev[i-1])*wcr  # find the change in working capital \n\nnet_op = np.zeros(len(fy)) # net operating profit\nadj_taxes = np.zeros(len(fy))\nnopat = np.zeros(len(fy))\ninvest = np.zeros(len(fy))\ndepre = np.zeros(len(fy))\nnet_invest = np.zeros(len(fy))\nfcff = np.zeros(len(fy))\ndisc_fact = np.zeros(len(fy))\ndisc_fcff = np.zeros(len(fy))                \n\n# calculate values in table \nfor i in range(1,len(fy)):\n    net_op[i] = rev[i]*nopm # net operating profit margin\n    adj_taxes[i] = net_op[i]*tr # net operating profit adjusted for taxes\n    nopat[i] = net_op[i]-adj_taxes[i] # after tax net operating profit\n    invest[i] = rev[i]*ir # future investments\n    depre[i] = rev[i]*dr # future depreciations\n    net_invest[i] = invest[i]-depre[i] # net investments\n    fcff[i] = nopat[i]-net_invest[i]-ciwc[i] # free cash flow to the firm\n    disc_fact[i] = 1/((1+wacc)**i) # discount factor\n    disc_fcff[i] = disc_fact[i]*fcff[i] # discounted free cash flow to the firm\n    \ndcrv = nopat[-1]/wacc*disc_fact[-1] # discounted corporate residual value\nderp_fcff = disc_fcff.sum() # discounted excess return period FCFF\n\ntcv = derp_fcff+dcrv+sta # total corporate value\ntvce = tcv-vod-vps-stl # total value of common equity\nisv = tvce/so # intrinsic stock value\n\n# print cash flows in a table\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format('Year','Rev','NOP','AdjTaxes',\n    'NOPAT','Invest.','Deprec.','dInvest.','dWC','FCFF','DF','DF*FCFF'))\nfor i in range(len(fy)):\n    print('{:4.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.4f}{:10,.0f}'.format(fy[i],\n        rev[i]/1e6,net_op[i]/1e6,adj_taxes[i]/1e6,nopat[i]/1e6,invest[i]/1e6,depre[i]/1e6,(invest[i]-depre[i])/1e6,ciwc[i]/1e6,\n        fcff[i]/1e6,disc_fact[i],disc_fcff[i]/1e6))\n\nYear       Rev       NOP  AdjTaxes     NOPAT   Invest.   Deprec.  dInvest.       dWC      FCFF        DF   DF*FCFF\n2022     6,234         0         0         0         0         0         0         0         0    0.0000         0\n2023     6,179       564       264       300       156       114        42       -13       271    0.9481       257\n2024     6,124       559       262       297       154       113        42       -13       269    0.8988       242\n2025     6,070       554       259       295       153       112        41       -13       267    0.8521       227\n2026     6,017       549       257       292       151       111        41       -13       264    0.8079       213\n2027     5,964       544       255       290       150       110        41       -13       262    0.7659       201\n2028     5,911       540       253       287       149       109        40       -13       260    0.7261       188\n2029     5,859       535       250       284       147       108        40       -13       257    0.6884       177\n2030     5,808       530       248       282       146       107        39       -12       255    0.6526       166\n2031     5,756       525       246       280       145       106        39       -12       253    0.6187       156\n2032     5,706       521       244       277       144       105        39       -12       251    0.5866       147\n\n\n\n# Intrinsic Value\nprint('discounted excess return period FCFF: ${:,.2f}B'.format(derp_fcff/1e9))\nprint('discounted corporate residual value: ${:,.2f}B'.format(dcrv/1e9))\nprint('total corporate value: ${:,.2f}B'.format(tcv/1e9))\nprint('total value of common equity: ${:,.2f}B'.format(tvce/1e9))\ntvce_baseline = tvce # save value as baseline case\nisv_baseline = isv # save the isv for the baseline case\nprint('intrinsic stock value, baseline case: ${:,.2f}'.format(isv_baseline))\nprint('current stock price: ${:,.2f}'.format(csp))\n\ndiscounted excess return period FCFF: $1.98B\ndiscounted corporate residual value: $2.97B\ntotal corporate value: $8.07B\ntotal value of common equity: $2.67B\nintrinsic stock value, baseline case: $7.64\ncurrent stock price: $4.40\n\n\nObservation:\nThe base line DCF analysis produces an intrinsic stock value of \\$7.64. An intrinsic value greater than the current price indicates that the stock is a potential value stock. Some adjustments will be made in the scenario 1 case.\n\n\nList of all inputs to the DCF model\nThe following print statements format the inputs to the model similar to how they are presented on the Valuepro page.\n\nprint('{:&gt;35s} {:&lt;10.0f} {:&gt;35s} {:,.3f}'.format('Excess return period, years:',erp,'Depreciation rate, %:',dr*100))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.3f}'.format('Starting revenues, $B:',\n    rev_start/1e9,'Investment rate, %:',ir*100))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:,.3f}'.format('Revenue growth rate, %:',\n    rgr*100,'Working capital rate, %:',wcr*100))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:,.3f}'.format('Net operating profit margin, %:',\n    nopm*100,'Current assets, $B:',sta/1e9))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:.3f}'.format('Tax rate, %:',\n    tr*100,'Current liabilities, $B:',stl/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.2f}'.format('Current stock price, $:',\n    csp,'Equity risk premium, %:',eq_rp*100))\nprint('{:&gt;35s} {:&lt;10,.0f} {:&gt;35s} {:,.2f}'.format('Shares outstanding, basic, M:',\n    so/1e6,'Company specific beta:',beta))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:.3f}'.format('10 year treasury bond yield, %:',\n    tby*100,'Total long term debt and other, $B:',vod/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.3f}'.format('Bond yield spread to treasury, %:',\n    bystt*100,'Value of preferred stock, $B:',vps/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f}'.format('Preferred stock yield, %:',psy*100))\n\n       Excess return period, years: 10                       Depreciation rate, %: 1.838\n             Starting revenues, $B: 6.23                       Investment rate, %: 2.517\n            Revenue growth rate, %: -0.881                Working capital rate, %: 24.192\n    Net operating profit margin, %: 9.128                      Current assets, $B: 3.132\n                       Tax rate, %: 46.805                Current liabilities, $B: 1.791\n            Current stock price, $: 4.40                   Equity risk premium, %: 3.00\n      Shares outstanding, basic, M: 349                     Company specific beta: 1.59\n    10 year treasury bond yield, %: 3.48       Total long term debt and other, $B: 3.612\n  Bond yield spread to treasury, %: 4.61             Value of preferred stock, $B: 0.000\n          Preferred stock yield, %: 0.00      \n\n\n\n# weighted average cost of capital inputs\nprint('Weighted Average Cost of Capital')\nprint('Cost of common equity')\nprint('{:s}'.format('-'*37))\nprint('{:&gt;32s} {:,.2f}'.format('10 year treasury bond yield, %:',tby*100))\nprint('{:&gt;32s} {:,.2f}'.format('Company specific beta:',beta))\nprint('{:&gt;32s} {:,.2f}'.format('Equity risk premium, %:',eq_rp*100))\nprint('{:s}'.format('-'*37))\nprint('{:&gt;32s} {:,.2f}'.format('Cost of common equity, %:',cce*100))\nprint()\n\nprint('Market Capitalization and After-Tax Weighted Average Cost of Capital')\nprint()\nprint('{:s}{:^10s}{:^10s}{:^10s}{:^15s}{:^15s}'.format(' '*20,'Current','After-Tax','Market','%','Weighted After-'))\nprint('{:s}{:^10s}{:^10s}{:^10s}{:^15s}{:^15s}'.format(' '*20,'Yield','Yield','Value','Capitalization','Tax Yield'))\n\nprint('{:s}'.format('-'*80))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Long term debt',\n    ltd_ay*100,(tby+eq_rp)*(1-tr)*100,vod/1e9,ltd_pc*100,ltd_ate*100))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Preferred stock',\n     psy*100,ps_ate*100,vps/1e9,ps_pc*100,ps_ate*100))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Common stock',\n     cs_ay*100,cs_aty*100,cs_mv/1e9,cs_pc*100,cs_aty*100))\nprint('{:s}'.format('-'*80))\nprint('{:&lt;37s}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('',tmv/1e9,tpc*100,wacc*100))\n\nWeighted Average Cost of Capital\nCost of common equity\n-------------------------------------\n 10 year treasury bond yield, %: 3.48\n          Company specific beta: 1.59\n         Equity risk premium, %: 3.00\n-------------------------------------\n       Cost of common equity, %: 8.24\n\nMarket Capitalization and After-Tax Weighted Average Cost of Capital\n\n                     Current  After-Tax   Market         %       Weighted After-\n                      Yield     Yield     Value   Capitalization    Tax Yield   \n--------------------------------------------------------------------------------\nLong term debt         8.09      3.44         4       70.15           3.02\nPreferred stock        0.00      0.00         0        0.00           0.00\nCommon stock           8.24      8.24         2       29.85           8.24\n--------------------------------------------------------------------------------\n                                              5      100.00           5.48"
  },
  {
    "objectID": "OLD HBI analysis.html#dcf-scenarios",
    "href": "OLD HBI analysis.html#dcf-scenarios",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "4) DCF Scenarios ",
    "text": "4) DCF Scenarios \nThe following adjustments were made to various model parameters.\n\nexcess return period was adjusted to a more conservative 5 years\n\nrevenue growth rate was adjusted to 0% (base case = -0.881 %)\n\nnet operating profit margin was adjusted to 9% (base case = 9.128%)\n\ntax rate was adjusted to 30% (base case = 46.805%)\n\ndepreciation rate was adjusted to 2% (base case = 1.838%)\n\ninvestment rate was adjust to 2% (base case = 2.517%)\n\nworking capital rate was set to an even 24% (base case = 24.192%)\n\nweighted average cost of capital was adjusted up by 2% to reflect higher interest rates and provide a margin of safety (base case = 5.59%)\n\n\nprint('adjusted DCF input values and rates')\nerp = 5\nprint('excess return period: {:,.0f} years'.format(erp))\nrgr = 0/100\nprint('revenue growth rate: {:,.1f}%'.format(rgr*100))\nnopm = isv_s1_nopm = 9/100  # save nopm rate for NAIC preferred method\nprint('net operating profit margin: {:.2f}%'.format(nopm*100))\ntr = isv_s1_tr = 30/100  # save tax rate for NAIC preferred method\nprint('tax rate: {:.2f}%'.format(tr*100))\ndr = 2/100\nprint('depreciation rate: {:,.2f}%'.format(dr*100))\nir = 2/100              # investment rate (% of revenue)\nprint('investment rate: {:,.2f}%'.format(ir*100))\nwcr = 24/100\nprint('working capital rate: {:,.1f}%'.format(wcr*100))\nwacc = (wacc+0.02) # weighted average cost of capital, increased by 2%\nprint('weighted average cost of capital: {:.1f}%'.format(wacc*100))\n\nadjusted DCF input values and rates\nexcess return period: 5 years\nrevenue growth rate: 0.0%\nnet operating profit margin: 9.00%\ntax rate: 30.00%\ndepreciation rate: 2.00%\ninvestment rate: 2.00%\nworking capital rate: 24.0%\nweighted average cost of capital: 7.5%\n\n\n\n# make a list of the fiscal years in excess return period \nfy = np.zeros(erp+1)\nfy[0] = fy_start\nfor i in range(1,erp+1): \n    fy[i]=fy_start+i\n\nrev = np.zeros(len(fy))\nciwc = np.zeros(len(fy))\nrev[0] = rev_start  #*rgr+rev_start   # find the future revenue using constant revenue growth rate \n\nfor i in range(1,len(fy)): \n    rev[i] = rev[i-1]*rgr+rev[i-1]  # find the future revenue \n    ciwc[i] = (rev[i]-rev[i-1])*wcr  # find the change in working capital \n\nnet_op = np.zeros(len(fy))\nadj_taxes = np.zeros(len(fy))\nnopat = np.zeros(len(fy))\ninvest = np.zeros(len(fy))\ndepre = np.zeros(len(fy))\nnet_invest = np.zeros(len(fy))\nfcff = np.zeros(len(fy))\ndisc_fact = np.zeros(len(fy))\ndisc_fcff = np.zeros(len(fy))                \n\n# calculate values in table \nfor i in range(1,len(fy)): \n    net_op[i] = rev[i]*nopm # net operating profit\n    adj_taxes[i] = net_op[i]*tr # net operating profit adjusted for taxes\n    nopat[i] = net_op[i]-adj_taxes[i] # after tax net operating profit\n    invest[i] = rev[i]*ir # future investments\n    depre[i] = rev[i]*dr # future depreciations\n    net_invest[i] = invest[i]-depre[i] # net investments\n    fcff[i] = nopat[i]-net_invest[i]-ciwc[i] # free cash flow to the firm\n    disc_fact[i] = 1/((1+wacc)**i) # discount factor\n    disc_fcff[i] = disc_fact[i]*fcff[i] # discounted free cash flow to the firm\n    \ndcrv = nopat[-1]/wacc*disc_fact[-1] # discounted corporate residual value\nderp_fcff = disc_fcff.sum() # discounted excess return period FCFF\n\ntcv = derp_fcff+dcrv+sta # total corporate value\ntvce = tcv-vod-vps-stl # total value of common equity\nisv = tvce/so # intrinsic stock value\n\n# print cash flows in a table\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format(\n    'Year','Rev','NOP','AdjTaxes',\n    'NOPAT','Invest.','Deprec.','dInvest.','dWC','FCFF','DF','DF*FCFF'))\nfor i in range(len(fy)):\n    print('{:4.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.4f}{:10,.0f}'.\n        format(fy[i],rev[i]/1e6,net_op[i]/1e6,adj_taxes[i]/1e6,nopat[i]/1e6,invest[i]/1e6,depre[i]/1e6,\n        (invest[i]-depre[i])/1e6,ciwc[i]/1e6,fcff[i]/1e6,disc_fact[i],disc_fcff[i]/1e6))\n\nYear       Rev       NOP  AdjTaxes     NOPAT   Invest.   Deprec.  dInvest.       dWC      FCFF        DF   DF*FCFF\n2022     6,234         0         0         0         0         0         0         0         0    0.0000         0\n2023     6,234       561       168       393       125       125         0         0       393    0.9304       365\n2024     6,234       561       168       393       125       125         0         0       393    0.8657       340\n2025     6,234       561       168       393       125       125         0         0       393    0.8054       316\n2026     6,234       561       168       393       125       125         0         0       393    0.7494       294\n2027     6,234       561       168       393       125       125         0         0       393    0.6972       274\n\n\n\n# Intrinsic Value\nprint('discounted excess return period FCFF: ${:,.2f}B'.format(derp_fcff/1e9))\nprint('discounted corporate residual value: ${:,.2f}B'.format(dcrv/1e9))\nprint('total corporate value: ${:,.2f}B'.format(tcv/1e9))\nprint('total value of common equity: ${:,.2f}B'.format(tvce/1e9))\ntvce_S1 = tvce # save value as scenario 1\nisv_S1 = isv # save the isv for scenario 1 case\nprint('intrinsic stock value, scenario 1 case: ${:,.2f}'.format(isv_S1))\nprint('current stock price: ${:,.2f}'.format(csp))\n\ndiscounted excess return period FCFF: $1.59B\ndiscounted corporate residual value: $3.66B\ntotal corporate value: $8.38B\ntotal value of common equity: $2.98B\nintrinsic stock value, scenario 1 case: $8.53\ncurrent stock price: $4.40\n\n\nThe DCF model calculates with adjustments an intrinsic stock value of \\$5.36, which is greater than the current stock price."
  },
  {
    "objectID": "OLD HBI analysis.html#naci-stock-selection-guide-analysis",
    "href": "OLD HBI analysis.html#naci-stock-selection-guide-analysis",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "5) NACI stock selection guide analysis ",
    "text": "5) NACI stock selection guide analysis \nThis analysis follows the NAIC stock selection guide (SSG) [2]. The SSG relates revenue growth, EPS and share price history and makes a prediction about the future share price.\nThe National Association of Investors Clubs (NAIC) is a nonprofit organization dedicated to educating individual investors and investment clubs to become successful lifelong investors. NAIC’s Stock Selection Guide (SSG) is used in the following cells to analyze the company’s growth and whether the stock is selling at a reasonable price.\nThe SSG was originally developed in the 1950s as a paper worksheet by the not-for-profit National Association of Investors Corporation (NAIC). The SSG aims to aid individual investors in the fundamental analysis and selection of common stocks by reviewing components of a company’s growth, quality, and value.\n\nLoad data from metrics sheet\n\n# column names: fiscal years \nfy_data = df_metrics_sheet.columns[1:].values.astype('datetime64[Y]')-1970\n# line 0: Net income\nnet_income_data = df_metrics_sheet.iloc[0].to_numpy()[1:].astype('float')\n# line 1: Shareholder equity\nshareholder_equity_data =  df_metrics_sheet.iloc[1].to_numpy()[1:].astype('float')\n# line 2: Total liabilities\ntotal_liabilities_data = df_metrics_sheet.iloc[2].to_numpy()[1:].astype('float')\n# line 3: Free cash flow, Net cash provided by operating activities \nfree_cash_flow_data =  df_metrics_sheet.iloc[3].to_numpy()[1:].astype('float')\n# line 4: Dividends\ndividends_data =  df_metrics_sheet.iloc[4].to_numpy()[1:].astype('float')\n# line 5: Total assets\ntotal_assets_data = df_metrics_sheet.iloc[5].to_numpy()[1:].astype('float')\n# line 6: Earnings per share\neps_data = df_metrics_sheet.iloc[6].to_numpy()[1:].astype('float')\n# line 7: Dividends per share  \ndps_data = df_metrics_sheet.iloc[7].to_numpy()[1:].astype('float')\n# line 8: Total tangible assets\ntotal_tangible_assets_data = df_metrics_sheet.iloc[8].to_numpy()[1:].astype('float')\n# line 9: Liabilities w/o deposits\nliabilities_wo_deposits_data = df_metrics_sheet.iloc[9].to_numpy()[1:].astype('float')\n# line 10: Provision for credit losses\nprovision_for_credit_losses_data = df_metrics_sheet.iloc[10].to_numpy()[1:].astype('float')\n# line 11: Short-term borrowings\nshort_term_borrowings_data = df_metrics_sheet.iloc[11].to_numpy()[1:].astype('float')\n# line 12: Preferred stock\npreferred_stock_data = df_metrics_sheet.iloc[12].to_numpy()[1:].astype('float')\n# line 13: Net cash used in investing activities \nnet_cash_used_in_investing_activities_data = df_metrics_sheet.iloc[13].to_numpy()[1:].astype('float')\n\n\n# make a new data frame to store data from metrics sheet\ndf_metrics_data = pd.DataFrame(data={\n    'FY':fy_data[::-1],\n    'net_income':net_income_data[::-1],\n    'shareholder_equity':shareholder_equity_data[::-1],\n    'total_liabilities':total_liabilities_data[::-1],\n    'free_cash_flow':free_cash_flow_data[::-1],\n    'dividends':dividends_data[::-1],\n    'total_assets':total_assets_data[::-1],\n    'eps':eps_data[::-1],    \n    'dps':dps_data[::-1],\n    'total_tangible_assets':total_tangible_assets_data[::-1],\n    'liabilities_wo_deposits':liabilities_wo_deposits_data[::-1],    \n    'provision_for_credit_losses':provision_for_credit_losses_data[::-1],\n    'short_term_borrowings':short_term_borrowings_data[::-1], \n    'preferred_stock':preferred_stock_data[::-1],\n    'net_cash_used_in_investing_activities':net_cash_used_in_investing_activities_data[::-1]\n    })\n\n#df_metrics_data\n\ncheck for matching years in both data frames\n\nif all(df_dcf_data['FY'] == df_metrics_data['FY']) != True:\n    print('error, years in data frame don\\'t match')\n    stop # this is not python code, so jupyterlab will throw an error\nelse:\n    print('OK, years in data frame match')\n\nOK, years in data frame match\n\n\n\n\nNAIC section 1: Visual analysis\nHigh and low price history for each year\nFrom the daily price history obtained from yahoo finance, the high and low closing price for each is obtained and the data saved to the financial data frame as new columns.\n\\(\\Large {\\color {red} {\\text {Avg closing price calculated, add to template}}}\\)\n\n#column names: fiscal years \nyears_list = df_metrics_sheet.columns[1:].values.astype('str')[::-1]\n\n# convert years to datetime format\nyear_ended_list = []\nfor i in years_list:\n    year_ended_list.append(datetime.strptime(i, '%Y'))\n\n# make empty lists to store open, close, average close, high and low price data for each fiscal year\nfy_open = []\nfy_close = []\nfy_avg_close = []\nfy_high = []\nfy_low = []\n\nfor i in year_ended_list:\n    start = i\n    end = i + relativedelta(years=1)\n    p1 = df_price_history.truncate(before=start, after=end)\n    if len(p1) == 0:\n        fy_open.append(np.nan)\n        fy_close.append(np.nan)\n        fy_avg_close.append(np.nan)\n        fy_high.append(np.nan)\n        fy_low.append(np.nan)\n    else:\n        fy_open.append(p1['Open'].iloc[0])\n        fy_close.append(p1['Close'].iloc[-1])\n        fy_avg_close.append(p1['Close'].mean()) # could also use median\n        fy_high.append(p1['Close'].max())\n        fy_low.append(p1['Close'].min())\n\n# convert from list to numpy array\nfy_open = np.asarray(fy_open)\nfy_close = np.asarray(fy_close)\nfy_avg_close = np.asarray(fy_avg_close)\nfy_high = np.asarray(fy_high)\nfy_low = np.asarray(fy_low)\n\n\nfy_close\n\narray([ 6.35    ,  5.465   ,  8.955   , 17.567499, 27.905001, 29.43    ,\n       21.57    , 20.91    , 12.53    , 14.85    , 14.58    , 16.719999,\n        6.36    ])\n\n\n\nfy_avg_close\n\narray([ 6.53318452,  6.7058631 ,  7.49818   , 13.47691467, 22.92302585,\n       30.99999004, 26.50714288, 22.03254979, 18.60091629, 16.0516666 ,\n       13.12505929, 18.31206347, 11.18338648])\n\n\nPlotting the data\nThe annual sales, EPS and the high and low share price is plotted on a semilog plot. A consistent percentage change in the data will plot on the semi-log chart as a straight line.\nThe stock price is plotted separately from the sales and earnings for clarity.\nfig, axs = plt.subplots(ncols=2, nrows=2, figsize=(5.5, 3.5),layout=“constrained”)\nax1 = plt.subplot(212) ax1.margins(0.05) # Default margin is 0.05, value 0 means fit ax1.plot(t1, f(t1))\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\n# figsize() function to adjust the size\nplt.subplots(1,2,figsize=(15, 5)) # }--- put fix here\n\n# using subplot function and creating\n# plot one\nplt.subplot(1, 2, 1)  # reseach this }------ generates warnings\nwidth = 3  # the width of the bars\n#plt.bar(year_ended_list,fy_high-fy_low, width,bottom=fy_low,label='price')\nj = 0\nfor i in year_ended_list:\n    color = 'green'\n    if fy_open[j] &gt; fy_close[j]: color= 'red'\n    # high/low lines\n    plt.plot([i,i],[fy_low[j],fy_high[j]],color=color, linewidth=width)\n    # open marker\n    plt.plot([i,i-relativedelta(months=1)], [fy_open[j],fy_open[j]], color=color, linewidth=width)\n    # close marker\n    plt.plot([i,i+relativedelta(months=1)], [fy_close[j],fy_close[j]], color=color, linewidth=width)\n    j += 1\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\n#plt.ylim((20,80))\nplt.title('Yearly stock high and low price range')\nplt.ylabel('stock price, $')\n#plt.legend()\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n\nplt.plot(df_metrics_data['FY'],df_dcf_data['revenue']/1e9,'+-',label='revenue, $B')\nplt.plot(df_metrics_data['FY'],df_metrics_data['eps'],'+-',label='EPS, $')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\n#plt.yscale('log')\n#plt.yticks([0.1,1,10,100,1000,10000],['0.1','1','10','100','1000','10000'])\n#plt.ylim((0.1,1000))\nplt.title('Revenue and EPS')\nplt.ylabel('Revenue and EPS')\nplt.legend()\n\nplt.grid()\n\n# space between the plots\n#plt.tight_layout(4) this line generates a warning\n'''\n/tmp/ipykernel_2327/525771534.py:10: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  plt.subplot(1, 2, 1)\n\n/tmp/ipykernel_2327/3890704171.py:10: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  plt.subplot(1, 2, 1)\n\n'''\n\n# show plot\nplt.show()\n\n\n\n\nObservation:\nThe share price dramatically fell when the dividend was discontinued and has continued to decline as the EPS has fallen. The 52 week range is between \\$3.85 and \\$11.91. The pandemic and the current recession have impacted the business. It is interesting to note that the share price has trended down since 2015, while the revenue was increasing over the period from 2013 to 2019. In 2016 HBI took on additional debt to fund acquisitions.\n\\(\\Large {\\color {red} {\\text {fix warning about axes}}}\\)\n\n\nNAIC section 3, Price earnings history\nSection 3 of the SSG is the Price-Earnings history. The following table is built from the high and low prices each year and the earnings per share. The high and low Price/Earnings ratios are calculated for each year and are listed in the columns labeled h-per and l-per.\n\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format('year','high','low','eps',\n    'h-per','l-per'))\nfor i in range(len(year_ended_list)):\n    print('{:s}{:10,.2f}{:10,.2f}{:10,.2f}{:10,.2f}{:10,.2f}'.format(year_ended_list[i].strftime(\"%Y\"),\n        fy_high[i], fy_low[i],df_metrics_data['eps'][i],\n        fy_high[i]/df_metrics_data['eps'][i],\n        fy_low[i]/df_metrics_data['eps'][i]))\n\nyear      high       low       eps     h-per     l-per\n2010      7.70      5.42      0.55     14.07      9.89\n2011      8.31      5.46      0.68     12.18      8.00\n2012      9.12      5.55      0.42     21.86     13.29\n2013     17.74      8.90      0.83     21.44     10.75\n2014     28.93     16.04      1.00     28.79     15.96\n2015     34.58     26.54      1.07     32.32     24.80\n2016     31.18     21.53      1.41     22.11     15.27\n2017     25.67     18.98      0.17    151.00    111.65\n2018     23.24     11.62      1.48     15.70      7.85\n2019     19.14     12.52      1.65     11.60      7.59\n2020     17.62      7.17     -0.21    -83.90    -34.14\n2021     22.37     14.40      0.22    101.68     65.45\n2022     17.36      5.84     -0.36    -48.22    -16.22\n\n\nAverage high and P/E for select years\nThe average price to earning ratio based on high and low stock prices is calculated.\n\n#Average high P/E for years \npe_avg_high = (fy_high/df_metrics_data['eps']).mean()\nprint('average high P/E {:.2f}'.format(pe_avg_high))\n#Average low P/E for years \npe_avg_low = (fy_low/df_metrics_data['eps']).mean()\nprint('average low P/E {:.2f}'.format(pe_avg_low))\n\naverage high P/E 23.12\naverage low P/E 18.47\n\n\n\nEstimate future EPS\nA least squares fit is used to get the slope of the EPS data points.\n\ny = df_metrics_data['eps']\nx = np.arange(len(y))\nm, c = np.polyfit(x, y, 1)\nprint('EPS slope: {:.2f}'.format(m))\nprint('EPS intercept: {:.2f}'.format(c))\nlstsq_fit = m*x + c  # data points for each year\n\nEPS slope: -0.04\nEPS intercept: 0.94\n\n\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('EPS')\n\nax1.plot(df_metrics_data['FY'],df_metrics_data['eps'], 'o',label='EPS')\nax1.plot(df_metrics_data['FY'],lstsq_fit, '-',label='least squares fit')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,4))\nax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('EPS and least squares fit')\nplt.show()\n\n\n\n\nUsing the equation for the best fit line, find the y value for the eps point at five years in the future.\n\n# estimated eps in 5 years\neps_5yr_est = m*(x[-1]+5) + c\nprint('estimated eps in 5 years: {:.1f}'.format(eps_5yr_est))\n\nestimated eps in 5 years: 0.2\n\n\nUsing the high and low price to earning ratio from above and the projected eps, calculate the range of stock price in five years.\n\nnaic_price_eps_low = eps_5yr_est*pe_avg_low\nnaic_price_eps_high = eps_5yr_est*pe_avg_high\nprint('estimated price range in 5 years: ${:.2f} to ${:.2f}'.format(naic_price_eps_low,naic_price_eps_high))\n\nestimated price range in 5 years: $4.01 to $5.02\n\n\nThis is the estimated price range of the stock based on projected EPS and is a guide for what the stock price might be if conditions remain the same. Since the slope of the EPS history is negative, the projected stock price is negative.\n\n\nNAIC section 3: 5 year estimated EPS, preferred method\nSee page 87 and figure 10-1, Need the following data:\n- estimate sales in 5 years based on sales growth\n- NOPM\n- Tax rate\n- shares outstanding\nNet Operating Profit should reflect the future revenue generating ability and expense requirements of the operating business that comprise the ongoing operations of the company.\n\\(\\text{NOPM} = \\frac{\\text{Revenue} - \\text{Expenses}}{\\text{Revenue}}\\)\nTax payments are taken from the consolidated income statement, provision for income taxes. The effect of taxes on profits is accounted for.\n\\(\\text{Tax rate} = \\frac{\\text{Income taxes}}{\\text{Income before income taxes}}\\)\nTo get future EPS\n\\(\\text{future EPS} = \\frac {\\text{future revenue} \\times \\text{NOPM} \\times \\text{(1-tax rate)}}{\\text{number of shares}}\\)\nRevenue and least square fit\n\ny = df_dcf_data['revenue']/1e6\nx = np.arange(len(y))\nm, c = np.polyfit(x, y, 1)\nprint('revenue slope: {:.2f}'.format(m))\nprint('revenue intercept: {:.2f}'.format(c))\nlstsq_fit = m*x + c  # data points for each year\n\nrevenue slope: 228.20\nrevenue intercept: 4411.07\n\n\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $M')\n\nax1.plot(df_metrics_data['FY'],df_dcf_data['revenue']/1e6, 'o',label='revenue')\nax1.plot(df_metrics_data['FY'],lstsq_fit, '-',label='least squares fit')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,4))\nax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Revenue and least squares fit')\nplt.show()\n\n\n\n\nUsing the equation for the best fit line, find the y value for the EPS point at five years in the future.\n\n# estimated revenue in 5 years\nrev_5yr_est = m*(x[-1]+5) + c\nprint('estimated rev in 5 years: ${:,.1f}M'.format(rev_5yr_est))\n\nestimated rev in 5 years: $8,290.5M\n\n\nneed to include estimate of number of shares outstanding in 5 years\n\nprint('starting revenues: ${:,.2f}'.format(rev_start/1e9))\n\nstarting revenues: $6.23\n\n\nUsing the adjusted NOPM and tax rate from scenario 1.\nadjusted DCF input values and rates\n\npm_nopm = isv_s1_nopm # use nopm from scenario 1\npm_tax_rate = isv_s1_tr # use tr from scenario 1\n\npm_eps_5yr_est = rev_5yr_est*pm_nopm*(1-pm_tax_rate)*1e6/df_dcf_data['shares_outstanding'].iloc[-1] \n#pm_eps_5yr_est = rev_5yr_est*nopm_avg*1e6/df_dcf_data['shares_outstanding'].iloc[-1] \nprint('using preferred method: estimated eps in 5 years: ${:.2f}'.format(pm_eps_5yr_est))\n\nusing preferred method: estimated eps in 5 years: $1.50\n\n\nUsing the high and low price to earning ratio from above and the projected EPS, calculate the range of stock price in five years.\n\nnaic_price_pm_low = pm_eps_5yr_est*pe_avg_low\nnaic_price_pm_high = pm_eps_5yr_est*pe_avg_high\nprint('estimated price range in 5 years from preferred method: {:.2f} to {:.2f}'.format(\n    naic_price_pm_low,naic_price_pm_high))\n\nestimated price range in 5 years from preferred method: 27.62 to 34.57\n\n\nObservation: Based on revenue growth, the projected stock price is a bit higher than the current price. However, based on price history, the stock is not expected to appreciate."
  },
  {
    "objectID": "OLD HBI analysis.html#future-stock-price",
    "href": "OLD HBI analysis.html#future-stock-price",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "6) Future stock price ",
    "text": "6) Future stock price \nThe projected future stock price is estimated from the results shown in this notebook based on DCF intrinsic stock value, the NAIC method or a combination of both. The DCF method does not consider market sentiment or popularity of the stock, whereas the NAIC method looks at the PE and EPS to develop the historical consensus that the market has put on the price of the stock. Both the NAIC and the DCF valuation should be considered. The DCF valuation is of the current ISV which is used as an indication of the future value, since it is assumed that the market price will converge eventually to the intrinsic value.\nThe estimated future stock price considers the following:\n- base case ISV\n- Senario ISV\n- NAIC EPS growth\n- NAIC preferred method\nUsing 5 year NAIC as a conservative estimate for the 10 year value and the analysis results, a judgment call is made concerning the price to put on the future value of the stock.\n\nprint('estimated price range in 5 years from EPS: ${:.2f} to ${:.2f}'.format(naic_price_eps_low,naic_price_eps_high))\nprint('estimated price range in 5 years from preferred method: ${:.2f} to ${:.2f}'.format(\n    naic_price_pm_low,naic_price_pm_high))\n\nprint('intrinsic stock value, baseline case: ${:,.2f}'.format(isv_baseline))\nprint('intrinsic stock value, scenario 1 case: ${:,.2f}'.format(isv_S1))\n\nprint('current stock price: ${:,.2f}'.format(csp))\n\nestimated price range in 5 years from EPS: $4.01 to $5.02\nestimated price range in 5 years from preferred method: $27.62 to $34.57\nintrinsic stock value, baseline case: $7.64\nintrinsic stock value, scenario 1 case: $8.53\ncurrent stock price: $4.40\n\n\nThe estimated price range in 5 years from the preferred method is \\$86.14 to \\$116.85. Taking the average and using that value on the IRR calculations.\n\nfsp = (naic_price_pm_low+isv_S1)/2 # estimated future stock price\nprint('estimated future stock price: ${:,.2f}'.format(fsp))\n\nestimated future stock price: $18.07"
  },
  {
    "objectID": "OLD HBI analysis.html#dividend-payout",
    "href": "OLD HBI analysis.html#dividend-payout",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "7) Dividend payout ",
    "text": "7) Dividend payout \nThe dividend payout examines the amount shareholders are getting from the company relative to earnings or revenue. It is an important metric to determine how the business is operating and whether it has enough growth potential.\n\nDividend history\nNo dividends paid in 2010, 2011 and 2012. HBI Board of Directors eliminated the quarterly cash dividend as they recently shifted their capital allocation strategy to pay down debt to bring leverage back to a range that is no greater than two to three times on a net debt-to-adjusted EBITDA basis.\nThe code cells below have been set to raw since there is no dividend data and the yearly future cash flows are also set to zero."
  },
  {
    "objectID": "OLD HBI analysis.html#management-performance",
    "href": "OLD HBI analysis.html#management-performance",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "8) Management performance ",
    "text": "8) Management performance \nThe following analysis somewhat follows the Warren Buffett strategy as outlined in [3]. This strategy is essentially value investing where companies are chosen that meet a set of criteria and who’s stock price is below the intrinsic value plus a margin of safety. These investments are usually held for the long term.\n\nFinancial metrics\nThe following analysis looks at financial ratios over the evaluation period. Financial ratios can be used to judge management performance. Consistent favorable trends are an indication that management is taking care of the company.\n\nTotal liabilities to total assets ratio\n\nDebt to equity and debt to NOP ratios\n\nFinancial ratios: RoE, RoA and PM\n\nNAIC section 2: Evaluating management\n\nNormalized data from consolidated statements\n\n\nMarket metrics\n\nOne dollar premise\n\nShare price vs EPS\n\nMarket capitalization\n\n\nQualitative metrics\n\nSimple and understandable business model\n\nFavorable long term prospects\n\nCommodity reliance\n\nConsistent operating history\n\nrationality:\n\nfocus on core aspects\n\nonly invest in high ROE businesses\n\nfocus on shareholder equity\n\n\n\n\nFinancial metrics \nThe following financial metrics are examined over the evaluation period. We are looking for favorable trends and evidence of consistent operations. Some red flags will also be evident in the plots.\nRed flags:\n- Shrinking gross profit margin\n- Receivables growing faster than sales\n- Rising debt-to-equity ratio\n- Several years of revenue trending down\n- Unsteady cash flow\n- Rising accounts receivable or inventory in relation to sales\n- Rising outstanding share count\n- Consistently higher liabilities than assets\n- Decreasing gross profit margin\n- Increasing revenue while cash flow remains the same\n- Unusual changes in key financial ratios\n\nTotal liabilities to total assets ratio\nThe ratio of liabilities to assets is plotted over the evaluation period. For most companies examined the liabilities are the total liabilities and the ratio is calculated using total assets and total tangible assets. Total tangible assets have goodwill and intangibles removed from the total. The ratio gives an indication of how much the company is worth versus how much the company owes. Ideally the ratio of liabilities to assets should be less than one.\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('ratio')\n\n# plot revenue as single bar\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['total_assets'], '-+',\n    label='total liabilities to total assets')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['total_tangible_assets'], '-*',\n    label='total liabilities to total tangible assets')\n\nax1.tick_params(axis='y')\nax1.legend(bbox_to_anchor=(1.8, 1))\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\n#ax2.plot(year_ended_list,pcd,'+-g')\nax2.plot(df_metrics_data['FY'],\n    (df_metrics_data['total_assets']-df_metrics_data['total_tangible_assets'])/df_metrics_data['total_assets']*100,\n    ':',color=color,label='intangible assets to total assets')\n    \nax2.set_ylabel('% intangible assets',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,100))\nax2.legend(bbox_to_anchor=(1.7, 0))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Total liabilities to total assets ratio')\nplt.show()\n\n\n\n\nHBI has significant goodwill and intangible assets. As of January 2, 2021, HBI had approximately \\$1.3 billion of goodwill and \\$1.6 billion of trademarks and other identifiable intangibles on the balance sheet, which together represent 37% of the total assets. HBI does not amortize goodwill, but they assess for impairment at least annually and more often as triggering events occur. The timing of annual goodwill impairment testing is the first day of the third fiscal quarter.\nEquity computed using tangible assets is negative. This would made debt to equity ratios shown below negative.\n\n\nDebt to equity and debt to NOP ratios\nThe debt-to-equity ratio (D/E) is another key characteristic Buffett considers carefully. Buffett prefers to see a small amount of debt so that earnings growth is being generated from shareholders’ equity as opposed to borrowed money. The D/E ratio is calculated as follows:\n\\(\\text{Debt-to-Equity Ratio} = \\frac {\\text{Total Liabilities}} {\\text{Shareholders' Equity}} \\text{  OR  } \\frac {\\text{Long term debt}} {\\text{Shareholders' Equity}}\\)\nThis ratio shows the proportion of equity and debt the company uses to finance its assets, and the higher the ratio, the more debt—rather than equity—is financing the company. A high debt level compared to equity can result in volatile earnings and large interest expenses. For a more stringent test, investors sometimes use only long-term debt instead of total liabilities in the calculation above.\nD/E is the traditional way to look at a company’s debt. Some rules of thumb say that the D/E should not be above 2 or 3. However the D/E company’s typically vary by industry. The ratio of LT debt to NOP gives the number of years it would take the company to pay back debt from NOP, the lower the number the shorter amount of time.\n\\(\\text{Debt-to-NOP Ratio} = \\frac {\\text{Total Liabilities}} {\\text{NOP}}\\)\n\ntangible_equity = df_metrics_data['total_tangible_assets'] - df_metrics_data['total_liabilities']\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('ratio')\n\nax1.plot(df_metrics_data['FY'],df_dcf_data['long_term_debt']/df_metrics_data['shareholder_equity'],\n    '-^',label='(LT debt)/Equity')\n#ax1.plot(year_ended_list,df_dcf_data['long_term_debt']/tangible_equity, '-',label='(LT debt)/(Tangible Equity)')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['shareholder_equity'],\n    '-*',label='(total liabilities)/Equity')\n#ax1.plot(year_ended_list,total_liabilities/BV, '-^',label='(total liabilities)/BV')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/nop, '-+',label='(total liabilities)/NOP')\n#ax1.plot(year_ended_list,total_liabilities/net_income, '-+',label='(total liabilities)/(net income)')\n#ax1.plot(year_ended_list,df_dcf_data['current_liabilities']/nop, '-*',label='(current liabilities)/NOP')\n#ax1.plot(year_ended_list,Liabilities_wo_deposits/nop, '-+',label='(Liabilities w/o deposits)/NOP')\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,20))\n#ax1.legend()\nax1.legend(bbox_to_anchor=(1.6, 1))\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Various debt ratios')\nplt.show()\n\n\n\n\n\\(\\Large {\\color {red} {\\text {fix plot, why the spike in 2020?}}}\\)\nThree debt ratios are plotted above. Prior to 2020 the debt ratios were somewhat consistent from year to year. In 2020 the NOP fell to almost zero and the total liabilities to NOP ratio when off the chart. From 2019 to the present the debt ratios have been rising and are now at concerning levels. It will be challenging for the HBI to reduce debt and grow revenues in the coming years.\n—- OLD (LT debt)/Equity is plotted and is below 2 for each year in the evaluation period. A threshold of 2 is traditionally the upper limit for a reasonable amount of debt that a company should carry.\n(total liabilities)/Equity is plotted and except for 2020 has been below the threshold of 2.\n(total liabilities)/NOP to is plotted for each year in the evaluation period and except for 2019 is below 10. A value of 10 has been chosen as the threshold for this ratio and indicates how many years it would take the company to pay off total liabilities from the NOP generated each year. A threshold of ten seems like a reasonable level of debt measured against NOP.\n\n\nFinancial ratios (change)\n\n\nFinancial returns\nVarious ratios can be used to judge management performance. Consistent favorable trends are an indication that management is taking care of the company.\nReturn on equity\nSometimes return on equity (RoE) is referred to as stockholder’s return on investment. It reveals the rate at which shareholders earn income on their shares. Buffett always looks at RoE to see whether a company has consistently performed well compared to other companies in the same industry. RoE is calculated as follows:\n\\(\\text{Return on Equity} = \\frac {\\text{Net Income}} {\\text{Shareholder's Equity}}\\)\nLooking at the RoE in just the last year isn’t enough. The investor should view the RoE from the past five to 10 years to analyze historical performance.\n\\(\\text{Shareholders’ Equity} = \\text{Total Assets} − \\text{Total Liabilities}\\)\nFor this company, this method of getting Shareholders’ Equity gives negative values. On the Consolidated Balance Sheets, there is a line for Total stockholders’ equity, which is used.\nReturn on Assets\nReturn on assets is a profitability ratio that provides how much profit a company is able to generate from its assets. In other words, return on assets (RoA) measures how efficient a company’s management is in generating earnings from their economic resources or assets on their balance sheet.\n\\(\\text{Return on assets} = \\frac {\\text{Net Income}} {\\text{Tangible Assets}}\\)\nCalculating the RoA of a company can be helpful in comparing a company’s profitability over multiple quarters and years as well as comparing to similar companies. However, it’s important to compare companies of similar size and industry.\nFor example, banks tend to have a large number of total assets on their books in the form of loans, cash, and investments. A large bank could easily have over \\$2 trillion in assets while putting up a net income that’s similar to companies in other industries. Although the bank’s net income or profit might be similar to an unrelated company and the bank might have high-quality assets, the bank’s RoA will be lower. The larger number of total assets must be divided into the net income, creating a lower RoA for the bank.\nSimilarly, auto manufacturing requires huge facilities and specialized equipment. A lucrative software company that sells downloadable programs online may generate the same net profits, but it could have a significantly higher RoA than its more asset-heavy counterparts. When utilizing this metric to compare productivity across businesses, it’s important to take into account what types of assets are required to function in a given industry, rather than simply comparing the figures.\nGoodwill is a historical cost that does not have to be constantly replaced. Therefore, in most cases, return on tangible capital alone (excluding goodwill) will be a more accurate reflection of a business’s return on capital going forward. The ROE and ROA calculations used by many investment analysts are therefore often distorted by ignoring the difference between reported equity and assets and tangible equity and assets.\nProfit Margin\nA company’s profitability depends not only on having a good profit margin, but also on consistently increasing it. This margin is calculated by dividing net income by net sales. For a good indication of historical profit margins, investors should look back at least five years. A high-profit margin indicates the company is executing its business well, but increasing margins mean management has been extremely efficient and successful at controlling expenses.\n\\(\\text{Profit Margin} = \\frac {\\text{Net Income}} {\\text{Revenue}}\\)\n\n\n\nNew Section\nReturn on Capital\nThe Return on Capital (RoC) is plotted to compare to RoA and RoE. RoC shows how much capital is needed to conduct the company’s business. RoC is the ratio of NOP to Working Capital, Tangible Assets and Depreciation. In addition to working capital requirements, a company must also fund the purchase of fixed assets necessary to conduct its business, such as real estate, plant, and equipment. The depreciated net cost of these fixed assets was then added to the working capital requirements to arrive at an estimate for tangible capital employed. Return on Capital (RoC) is calculated as follows:\n\\(\\text{Return on Capital} = \\frac {\\text{NOP}} {\\text{Working Capital + Tangible Assets + Depreciation}}\\)\nThe RoC calculation uses NOP instead of net income to filter distortions due to taxes and debt.\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('percent')\n\nax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_metrics_data['shareholder_equity']*100,\n    '-+',label='Return on Equity')\n\n#ax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_metrics_data['total_assets']*100,\n#    '-*',label='Return on Assets')\n\nax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_metrics_data['total_tangible_assets']*100,\n    '-*',label='Return on Tangable Assets')\n\nax1.plot(df_metrics_data['FY'],nop/(working_capital+df_metrics_data['total_tangible_assets']+df_dcf_data['depreciation'])*100,\n    '-*',label='Return on Capital')\n\n#ax1.plot(df_metrics_data['FY'],total_liabilities/shareholder_equity, '-^',label='D/E')\n\n#ax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_dcf_data['revenue']*100,\n#    '-^',label='Profit margin')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,14))\n#ax1.legend()\nax1.legend(bbox_to_anchor=(1.05, 1))\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Financial Returns')\nplt.show()"
  },
  {
    "objectID": "OLD HBI analysis.html#decision-model",
    "href": "OLD HBI analysis.html#decision-model",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "9) Decision model ",
    "text": "9) Decision model \nThe decision model establishes thresholds that are to used in the purchase decision. There are three hard decision thresholds in this model which are:\n1. Intrinic value\n2. Debt\n3. Dividend payout ratio\n4. Dividend IIR\n5. NACI managemet evaluation\n6. One dollar premise\n\\(\\Large {\\color {red} {\\text {update template}}}\\)\nThe first threshold is based on the intrinisic value of the company as calculated by the DCF model semario 1. Reconizing that absolute intrinsic value is an elusive concept, judgement, justified by facts (assets, earnings, dividends, debt and cash flow), establishes the value by adjusting various rates, based on judgement and using a five year forward projection period. This should give a intrinsic value that is based on the historical data, modified by judgement.\nI’m using a threshold of the intrinsic value calculated in senario 1 (isv_S1) that is greater than 70% of the current stock price, provided that the NAIC valuation is above the current stock price. This accounts for the inadequacy or incorrectness of the data, the uncertainties of the future, and considers the behavior of the market.\nThe second threshold is the level of debt. The ratios of (LT debt)/Equity, (total liabilities)/Equity and (total liabilities)/NOP are ploted for the evaluation period. Over the evaluation period the (LT debt)/Equity and (total liabilities)/Equity should be less than 2 and stable. A threshold of 2 has been discussed in the litureture as a level of debt that a company can reasonably take on.\nThe thereshold for (total liabilities)/NOP is set at 10. This means that the company can pay off all the liabilities with tens years worth of NOP, which seems like a reasonable timeframe for an established and stable company.\nThe third threshold is the dividend payout ratio and is a relative measure of how much the company is paying to shareholders in dividends compared to the metrics of NOP and free cash flow (Net cash provided by operating activities). The payout ratio is useful for assessing a dividend’s sustainability. Payout ratio for a REIT is established by tax law and not used as an evaluation criteria. For other industries a threshold of 50% has been set as the limit.\nThe dividend IRR threshold is the internal rate of return for investor dividend cash flow (divident_irr) should be greater than 10 year treasury bond yield (tby) plus the equity risk premium (eq_rp). Otherwise, other investment operatunities should be looked at.\nIn the decision model there are soft thresholds based on judgement. Soft thresholds are a collection of ratios and analysis that taken together tell a story of the performance of the conmpany and manatgments ability to run the company and support dividends over the long term. Use judgement and make an evalaution.\nThe third critiera is a collection of ratios and analysis that taken together tell a story of the performance of the conmpany and manatgments ability to run the company and support dividends over the long term. Use judgement and make an evalaution. These are the following:\n1. Financial metrics\n2. Market metrics\n3. Qualitative metrics\nThe soft thresholds are discused in section 10.\n\nCheck DCF and NAIC value thresholds\n\n# check DCF senario 1\ndcf_score = isv_S1/csp #ratio of isv to csp\ndcf_threshold = 0.7\nif dcf_score &lt; 0.7:\n    print('FAIL, DCF score is less than {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\nelse:\n    print('PASS, DCF score is above {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\n\n# check NAIC\nnaic_score = np.array([naic_price_eps_low,naic_price_pm_low]).min()/csp\nnaic_threshold = 1\nif naic_score &lt; 1:\n    print('FAIL, NAIC score is less than {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\nelse:\n    print('PASS, NAIC score is above {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\n\n# check both scores\nif naic_score &lt; 1 or dcf_score &lt; 0.7:\n    print('One or both DCF and NAIC scores failed')\nelse:\n    print('Both DCF and NAIC scores pass')\n\nPASS, DCF score is above 0.7 at 1.9\nFAIL, NAIC score is less than 1.0 at 0.9\nOne or both DCF and NAIC scores failed\n\n\n\n\nCheck debt thresholds\n\ndebt_lookback = 4\navg_LT_debt2EQ = df_dcf_data['long_term_debt'][-debt_lookback:].mean()/df_metrics_data['shareholder_equity'][-debt_lookback:].mean()\navg_TLiability2EQ = df_metrics_data['total_liabilities'][-debt_lookback:].mean()/df_metrics_data['shareholder_equity'][-debt_lookback:].mean()\navg_TLiability2NOP = df_metrics_data['total_liabilities'][-debt_lookback:].mean()/nop[-debt_lookback:].mean()\n\nprint('long term debt to shareholder equity ratio = {:.2f}'.format(avg_LT_debt2EQ))\nprint('total liabilities to shareholder equity ratio = {:.2f}'.format(avg_TLiability2EQ))\nprint('total liabilities to NOP ratio = {:.2f}'.format(avg_TLiability2NOP))\n\nif (avg_LT_debt2EQ &gt; 2) or (avg_TLiability2EQ &gt; 2) or (avg_TLiability2NOP &gt; 10):\n    print('FAILED one of the debt threshold limits')\n\nlong term debt to shareholder equity ratio = 4.42\ntotal liabilities to shareholder equity ratio = 8.08\ntotal liabilities to NOP ratio = 11.51\nFAILED one of the debt threshold limits\n\n\n\n\nCheck dividend payout and IIR thresholds\n\\(\\Large {\\color {red} {\\text {update comments about IIR}}}\\)\n\n# check dividend payout ratio average the last three years\nprint('Dividends are paid at {:.1f}% of cash flow'.format(\n    (df_metrics_data['dividends']/df_metrics_data['free_cash_flow'])[-3:].mean()*100))\nprint('Dividends are paid at {:.1f}% of NOP'.format((df_metrics_data['dividends']/nop)[-3:].mean()*100))\n\nif ((df_metrics_data['dividends']/nop)[-3:].mean() or (df_metrics_data['dividends']/df_metrics_data['free_cash_flow'])[-3:].mean()) &gt; 0.5:\n    print('FAIL, dividend payout ration too high')\n\nDividends are paid at 7.4% of cash flow\nDividends are paid at 1100.9% of NOP\nFAIL, dividend payout ration too high\n\n\n\n# Check dividend IRR limit\nif dividend_irr &lt; (tby+eq_rp):\n    print('FAIL, dividend IRR is less than {:.2f} at {:.2f}'.format((tby+eq_rp)*100,dividend_irr*100))\nelse:\n    print('PASS, dividend IRR is above {:.2f} at {:.2f}'.format((tby+eq_rp)*100,dividend_irr*100))\n\nFAIL, dividend IRR is less than 6.48 at 4.41\n\n\n\n# check DCF senario 1\ndcf_score = isv_S1/csp #ratio of isv to csp\ndcf_threshold = 0.7\nif dcf_score &lt; 0.7:\n    print('FAIL, DCF score is less than {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\nelse:\n    print('PASS, DCF score is above {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\n\n# check NAIC\nnaic_score = np.array([naic_price_eps_low,naic_price_pm_low]).min()/csp\nnaic_threshold = 1\nif naic_score &lt; 1:\n    print('FAIL, NAIC score is less than {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\nelse:\n    print('PASS, NAIC score is above {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\n\n# check both scores\nif naic_score &lt; 1 or dcf_score &lt; 0.7:\n    print('One or both DCF and NAIC scores failed')\nelse:\n    print('Both DCF and NAIC scores pass')\n\nPASS, DCF score is above 0.7 at 1.9\nFAIL, NAIC score is less than 1.0 at 0.9\nOne or both DCF and NAIC scores failed\n\n\ncomments:\n\nNACI managemet evaluation\n\n\n# check profit margin trends\nif pm_trend &lt; 0:\n    print('FAIL, profit margin trend over the evaluation period is negative at {:.1f}%'.format(pm_trend*100))\nelse:\n    print('PASS, profit margin trend over the evaluation period is positive at {:.1f}%'.format(pm_trend*100))\n\nif pm1_trend &lt; 0:\n    print('FAIL, profit margin trend over the last {:.0f} years is negative at {:.1f}%'.format(-left_yr,pm1_trend*100))\nelse:\n    print('PASS, profit margin trend over the last {:.0f} years is positive at {:.1f}%'.format(-left_yr,pm1_trend*100))    \n\nFAIL, profit margin trend over the evaluation period is negative at -0.5%\nFAIL, profit margin trend over the last 6 years is negative at -1.3%\n\n\nChecking trends based on EPS and NOP.\n\n# check yield trend based on EPS\nif pm_trend &lt; 0:\n    print('FAIL, EPS yield trend over the evaluation period is negative at {:.1f}%'.format(y1_trend*100))\nelse:\n    print('PASS, EPS yield trend over the evaluation period is positive at {:.1f}%'.format(y1_trend*100))\n\nif pm1_trend &lt; 0:\n    print('FAIL, EPS yield trend over the last {:.0f} years is negative at {:.1f}%'.format(-left_yr,y1a_trend*100))\nelse:\n    print('PASS, EPS yield trend over the last {:.0f} years is positive at {:.1f}%'.format(-left_yr,y1a_trend*100))    \n\nFAIL, EPS yield trend over the evaluation period is negative at -0.7%\nFAIL, EPS yield trend over the last 6 years is negative at -1.5%\n\n\n\n# check yield trend based on NOP\nif pm_trend &lt; 0:\n    print('FAIL, NOP yield trend over the evaluation period is negative at {:.1f}%'.format(y2_trend*100))\nelse:\n    print('PASS, NOP yield trend over the evaluation period is positive at {:.1f}%'.format(y2_trend*100))\n\nif pm1_trend &lt; 0:\n    print('FAIL, NOP yield trend over the last {:.0f} years is negative at {:.1f}%'.format(-left_yr,y2a_trend*100))\nelse:\n    print('PASS, NOP yield trend over the last {:.0f} years is positive at {:.1f}%'.format(-left_yr,y2a_trend*100))    \n\nFAIL, NOP yield trend over the evaluation period is negative at -0.3%\nFAIL, NOP yield trend over the last 6 years is negative at -0.2%\n\n\n\nOne dollar premise\n\nretained_earnings = df_metrics_data[‘net_income’].sum() - df_metrics_data[‘dividends’].sum()\nprint(‘retained earnings: ${:,.2f}B’.format(retained_earnings/1e9))\nretained earnings: $1.69B\nRetained earnings are $1.7B"
  },
  {
    "objectID": "OLD HBI analysis.html#conclusion",
    "href": "OLD HBI analysis.html#conclusion",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "10) Conclusion ",
    "text": "10) Conclusion \nThe following is a summary of the results described above:\n\nStock screener results: This company was selected from the Fidelity stock screener results. The company had increasing revenues and a dividend yield above 2.38%.\n\nCurrent news: A few new articles were noted, nothing of significance found.\n\nReview quarterly results: Not applicable since 10K was just released.\n\nAverage daily volume: Above 1M per day.\n\nDividend yield: Above 3%.\n\nDiscounted cash flow analysis, baseline: The baseline DCF analysis yielded a very high ISV, driven primarily by revenue growth rate and low WACC. The baseline ISV doesn’t seem realistic.\n\nDCF Scenarios: Adjustments were made and the ISV is now below the current price.\n\nNACI stock selection guide analysis: Share price has been trading in the 50 to 70 dollar range the last 7 years, indicating that the market is not impressed with the company. Average EPS (ignoring 2020) has been flat. Based on EPS growth, the estimated price range in 5 years is \\$8.99 to \\$12.19 per share. Based on NAIC’s preferred method, the estimated price range in 5 years is \\$86.14 to \\$116.85 per share.\n\nDividend payout: Dividend payout has been increasing over the years. The dividend yield for the past five years has been in the 2.5 to 3.5 percent range.\nDividend pay ratios for the last 3 years are near or below 50%. Dividend cash flow IRR is 8.01%.\n\nManagement performance:\n\nFinancial metrics: The value assigned to goodwill and intangibles is about \\$1 billion. The ratio indicates the company has taken on a lot of debt relative to assets and is something of concern. Various debt ratios seem OK. Except for 2020 performance ratios and trends are acceptable. The divergence of total liabilities from revenue is of concern.\n\nMarket metrics: The range in share price is roughly the same across the range of EPS. This means that investors are not valuing the company’s EPS. The company’s market capitalization is higher than the total value of common equity as calculated using scenario 1 DCF data. This implies that the company is overvalued.\n\nQualitative metrics: see above\n\nConcerns: increasing total liabilities, low ISV, negative retained earnings, 60% of the total assets are intangible.\nSummary: The analysis presented in this report is based on the company’s fundamentals and tries to establish the value of the company. This strategy is essentially value investing where companies are chosen that meet a set of criteria and who’s stock price is below the intrinsic value plus a margin of safety. These investments are usually held for the long term. Company revenue, earnings, debt and dividends were examined. Adjustments were made to the DCF and the ISV is below the current price. The share price has been trading in the 50 to 70 dollar range the last 7 years, indicating that the market is not impressed with the company. The dividend payout has been increasing over the years and the dividend cash flow IRR is 8.01%. The value assigned to goodwill and intangibles is about $1 billion. The ratio indicates the company has taken on a lot of debt relative to assets. The divergence of total liabilities from revenue is of concern. Negative retained earnings is a concern.\nRecommendation: Don’t buy above $50 per share."
  },
  {
    "objectID": "OLD HBI analysis.html#notes",
    "href": "OLD HBI analysis.html#notes",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "11) Notes ",
    "text": "11) Notes \nThe following notes outline the changes to the DCF model for financial and REIT companies.\nValuing a REIT\nNotes from Valuepro Book, page 237\n\nNOPM: To calculate operating income take rental revenue and subtracted total real estate expenses and G&A expenses. To arrive at the NOPM divide the adjusted income from real estate by real estate rental revenue. For the REIT, take income from real estate, which includes depreciation and amortization, and subtract GSA. Exclude other income, gains on sale of real estate and interest expenses.\n\nREIT has no traditional R&D costs\n\nREIT is not taxed at the corporate level, tax rate: should be near zero.\nDepreciation and capital expenditures are significantly higher for REITs than in other companies.\nNew property acquisitions are not directly accounted for in the DCF model for a REIT.\n\nWorking capitol: accounts payable, rents and security deposits\n\nShort term assets: cash, rents and other receivables and prepaid expenses\n\nShort term liabilities: accounts payable, advance rents security deposits\n\nWorking capital is almost zero, which is similar to other financial companies.\nThe consolidated balance sheet lists the assets as:\n- Real estate held for investment, at cost:\n- Land\n- Buildings and improvements\n- Total real estate held for investment, at cost\n- Less accumulated depreciation and amortization\n- Real estate held for investment, net\n- Real estate and lease intangibles held for sale, net\n- Cash and cash equivalents &lt;- current asset\n- Accounts receivable, net &lt;- current asset\n- Lease intangible assets, net\n- Other assets, net\nThe line items indicated above have been taken to be the current assets. Intangibles and long term items have been excluded.\nThe consolidated balance sheet lists the liabilities as:\n- Distributions payable &lt;- current liabilities\n- Accounts payable and accrued expenses &lt;- current liabilities\n- Lease intangible liabilities, net\n- Other liabilities\n- Line of credit payable and commercial paper &lt;- current liabilities\n- Term loans, net\n- Mortgages payable, net &lt;- current liabilities\n- Notes payable, net\nThe line items indicated above have been taken to be the current liabilities.\nValuing a financial company\nNotes from Valuepro Book, page 206\n\nTotal revenue comes from the total interest and dividend income line on the income statement. The calculation of operating income is more inclusive for a financial company than for an industrial or high tech company. For financial companies, operating revenue includes all normal revenue items plus interest income, dividends received and other investment income.\nCost of Goods Sold (CGS) comes from the Total interest expense line on the statement of income.\n\nGeneral and Administrative (G&A) are set to zero since they are included in the interest expense line\n\nA financial company has no traditional R&D costs\n\\(\\text{Cost of Goods Sold (CGS)} = \\text{Total interest expense} + \\text{Total non-interest expense}\\)\n\nGeneral and Administrative (G&A) are set to zero since they are included in the interest expense line\n\nA financial company has no traditional R&D costs\nDepreciation and amortization of premises and equipment from Consolidated Statements of Cash Flows.\n\nAmortization of other acquisition-related intangible assets is not included.\nNew investment and Depreciation: Property, plant and equipment expenditures and depreciation charges are significantly lower for a financial company. A typical manufacturing company, in order to grow its business, invests a significant portion of its revenues in plant, property and equipment (PPE). Financial companies invest very little in the way of PPE. However, software, risk management systems and acquisitions of other businesses, need to be included.\n\nFrom the Consolidated Statements of Cash Flows, under Cash Flows from Investing Activities\n- Purchases of premises and equipment\n- Purchases of leased equipment, net\n\nWorking capital supports manufacturing and service activities of nonfinancial companies. For financial companies, their principal liabilities and assets are financial claims that take the place of working capital. Because there is no differentiation between current and long term assets and liabilities for a financial company, we adjust working capital charges to zero. A financial company generally invests all of its funds in other financial assets, which have characteristics of current assets rather than PP&E.\n\\(\\text{Accounts Receivable} = 0\\)\n\\(\\text{Inventories} = 0\\)\n\\(\\text{Accounts Payable} = 0\\)\n\\(\\text{working capital} = 0\\)\nShort term assets: The balance sheets of most financial companies do not separate assets and liabilities into current and long term categories. When calculating the short term assets take the total assets and subtract goodwill and intangible assets also subtract other assets of questionable value. Subtract long term assets such as PP&E from total assets.\n\n\\(\\text{Short term assets} = \\text{Total assets} - \\text{good will and others of questionable value} - \\text{Premises and equipment}\\)\n\nA financial company’s principal liabilities are deposits, Federal funds purchased, trading account liabilities, insurance policy and claims reserves, contract holder funds and short term borrowing. To be consistent with the treatment of interest and an operating expense for financial companies, include long term debt in the short term liability category.\n\nShort term liabilities: Include long term debt.\n\n\\(\\text{Long term debt} = 0\\)\nExcess return period\nThe excess return period is based on a judgment call. The authors of [2] use the 1-5-7-10 rule. They group companies into one of four general categories and excess return periods. They use a 10 year excess return period to calculate what they would consider the maximum value. They use a more conservative 1 year, 5 year or 7 year return period to calculate a more reasonable or minimum value.\n- 1 year: Boring companies that operate in a highly competitive, low margin industry in which they have nothing particular going for them.\n- 5 year: Decent companies that have a recognizable name and decent reputation and perhaps a regulatory benefit (utility company), but can’t control pricing or growth.\n- 7 year: Good companies with good brand names, large companies of scale, good marketing channels and consumer identification (e.g. McDonald’s)\n- 10 year: Great companies with great growth potential, tremendous marketing power, band names and in-place benefits (e.g. Intel, Microsoft, Coca Cola, Disney)\nNotes about negative working capital\nThe company has a negative working capital rate. Negative working capital describes a situation where a company’s current liabilities exceed its current assets as stated on the firm’s balance sheet. In other words, there is more short-term debt than there are short-term assets.\nNegative working capital most often arises when a business generates cash very quickly because it can sell products to its customers before it has to pay the bills to its vendors for the original goods or raw materials. In this way, the company is effectively using the vendor’s money to grow.\nDividend Aristocrat, Achiever & Champion\nThis company was selected for analysis because it is on the Dividend Aristocrat list and passes the quick look tests. This notebook will be used as a template when analyzing other companies.\n\nAristocrat: S&P 500 Dividend Aristocrats is designed to measure the performance of S&P 500 index constituents that have followed a policy of consistently increasing dividends every year for at least 25 consecutive years.\nAchiever: The Broad Dividend Achievers Index. Eligible companies must be incorporated in the U.S. or its territories, trade on the NYSE, NASDAQ or AMEX, and have increased its annual regular dividend payments for the last 10 or more consecutive years.\nhttps://dividendvaluebuilder.com/dividend-achievers-list/\nhttps://www.marketbeat.com/dividends/achievers/\nChampion: This list includes companies that had increased their dividend for at least 25 consecutive years, and includes additional companies that had paid higher dividends without having increased the payout in every calendar year.\nhttps://dividendvaluebuilder.com/dividend-champions-list/\nhttps://www.dividendgrowthinvestor.com/p/dividend-champions-list.html"
  },
  {
    "objectID": "OLD HBI analysis.html#references",
    "href": "OLD HBI analysis.html#references",
    "title": "Hanesbrands Inc. (HBI)",
    "section": "12) References ",
    "text": "12) References \n\nGray, Gary, et al. Streetsmart Guide to Valuing a Stock: the Savvy Investors Key to Beating the Market. McGraw-Hill, 2004.\n\nO’Hara, Thomas E., and Ken Janke. Starting and Running a Profitable Investment Club: the Official Guide from the National Association of Investors Corporation. Times Business, 1998.\n\nRobert G. Hagstrom, The Warren Buffett Way, Wiley, 2013"
  },
  {
    "objectID": "OLD Symbolic Modified Nodal Analysis.html",
    "href": "OLD Symbolic Modified Nodal Analysis.html",
    "title": "Symbolic Modified Nodal Analysis",
    "section": "",
    "text": "Last update: 30 Nov 2023"
  },
  {
    "objectID": "OLD Symbolic Modified Nodal Analysis.html#introduction",
    "href": "OLD Symbolic Modified Nodal Analysis.html#introduction",
    "title": "Symbolic Modified Nodal Analysis",
    "section": "Introduction",
    "text": "Introduction\nThis nodal analysis code started as a translation from some C code to generate a nodal admittance matrix that I had written in 1988. I wrote this code for two reasons. Free versions of Spice for the PC didn’t exist at the time and I wanted to use some of the code from the Numerical Recipes in C [1]. The original C code worked well and calculated numeric solutions. I then started writing some C code to generate the matrices with symbolic values and then intended to use LISP to symbolically solve the equations. I didn’t get too far with this effort. The LISP code would generate huge symbolic strings with no simplification. The output was a big pile of trash that was not in the least bit useful or decipherable.\nIn 2014, I started to use python for my little coding projects and engineering calculations. There are some nice python libraries for numeric and symbolic calculations (such as NumPy and SymPy), so I decided to try writing a python script to generate the node equations based on the old C code I had written many years before. Part way into this project I discovered that there is a new nodal analysis technique being taught today in engineering school called the modified nodal analysis [2][3]. My motivation for reviving this coding project is my continued interest in circuit analysis and synthesis.\nThe modified nodal analysis provides an algorithmic method for generating systems of independent equations for linear circuit analysis. Some of my younger colleagues at work were taught this method, but I never heard of it until a short time ago. These days, I never really analyze a circuit by hand, unless it’s so simple that you can almost do it by inspection. Most problems that an electrical engineer encounters on the job are complex enough that they use computers to analyze the circuits. LTspice is the version of Spice that I use, since it’s free and does a good job converging when analyzing switching circuits."
  },
  {
    "objectID": "OLD Symbolic Modified Nodal Analysis.html#python-code",
    "href": "OLD Symbolic Modified Nodal Analysis.html#python-code",
    "title": "Symbolic Modified Nodal Analysis",
    "section": "Python code",
    "text": "Python code\nMy code started initially by following Erik Cheever’s Analysis of Resistive Circuits MATLAB code [4], to generate modified nodal equations. I somewhat followed his MATLAB file for resistors, capacitors, opamps and independent sources. The naming of the matrices follows his convention. The preprocessor and parser code was converted from my old C code. The use of pandas for a data frame is new and SymPy is used to do the math and the use of element stamps is from [5].\nInductors are being addressed in the D matrix, which is different than the way Erik Cheever’s code works. Erik’s code puts inductors into the G matrix as 1/s/L. My code puts the inductor contribution into the D matrix. Coupled inductors also affect the D matrix, so it makes sense to allow the inductors to be in the D matrix rather than the G matrix.\nNetwork equations: The network equations are a set of independent equations expressed in this code in matrix form. There is an equation for each node based on Kirchhoff’s current law KCL and an equation for each current unknown. The current unknowns are the currents from the voltages sources, op amps, voltage controlled voltage sources, current controlled voltage sources, current controlled current sources and inductors.\nThe network equations are in matrix form:\n\\[A\\cdot X = Z\\]\nThe A matrix describes the connectivity of the resistors, capacitors and G type (VCCS) circuit elements. The column vector X are the unknown node voltages and unknown currents terms from the voltage sources and inductors. The column vector Z is made of the known voltages and currents. The A matrix is formed by four sub matrices, G, B, C and D, which are described below.\n\\[A = \\begin{bmatrix}G B\\\\C D\\end{bmatrix}\\]\nThe matrix G is formed from the coefficients representing the KCL equations for each node. The positive diagonal of \\(G_{k,k}\\) are the conductance terms of the resistor and capacitor elements connected to node k. The off diagonal terms of \\(G_{k,j}\\) are the resistors and capacitor conductances connecting node k to node j. G type elements (VCCS) have input to the G matrix at the connection and controlling node positions.\nThe B matrix describes the connectivity of the unknown branch currents. Independent voltage sources, opamps, H, F and E type elements as well as inductors have inputs to the B matrix.\nThe C matrix describes the connectivity of the unknown branch currents and is mainly the transpose of B matrix, with the exception of the F type elements (CCCS) and includes the E type value.\nThe D matrix describes also connectivity of the unknown currents. The D matrix is composed of zeros unless there are controlled sources and inductors in the network.\nThe X vector is comprised of the V and J vectors as shown below.\n\\[X = \\begin{bmatrix}V\\\\J\\end{bmatrix}\\]\nThe V vector contains the node voltages which are the voltage unknowns to be solved for. The J vector contains the unknown currents from each voltage source.\nThe Z vector is comprised of the I and Ev vectors as shown below.\n\\[Z = \\begin{bmatrix}I\\\\Ev\\end{bmatrix}\\]\nThe I vector contains the known currents and the Ev vector contains the known voltages. The Ev designation is used for the voltage vector (and not simply ‘E’) because SymPy uses e and E sometimes for the mathematical constant 2.71, sometimes called Euler’s number. The use of E or e as a symbol in SymPy was causing errors when the code was run.\nPutting all the parts together:\n\\[\\begin{bmatrix}G B\\\\C D\\end{bmatrix} \\cdot \\begin{bmatrix}V\\\\J\\end{bmatrix} = \\begin{bmatrix}I\\\\Ev\\end{bmatrix}\\]\nStamps: Stamps are templates for modifying the B, C and D matrices and facilitate the construction of the matrices. The stamps used in this implementation of the MNA follow the stamps of [5].\n\nCode description\nThe code is divided in the following sections.\nPreprocessor: The preprocessor reads in the netlist text file and removes comments, extra spaces and blank lines. The first letter of the element type is capitalized to make subsequent parsing of the file easier. The number of lines are counted and the number of entries on each line are checked to make sure the count is consistent with the element type.\nParser: The parser code loads the preprocessed netlist into a data frame. A report is generated which consists of a count of the element types in the netlist.\nMatrix formulation: Each of the matrices and vectors are generated.\nCircuit equation generation: The circuit equations are generated in a for loop. Sympy automatically does some simplification according to its default settings. Two for loops perform the matrix multiplication on the equation.\n\\(A\\cdot X = Z\\)\n\n\nCode validation\nBasic validation of the code consisted of analyzing simple networks and examining the results. A more comprehensive evaluation of the code was performed by solving test circuits and comparing the results to LTSpice. As of October 2023 all the element types have been tested. See the circuits used for validation here. The validation circuits range from simple to large and complex. The largest validation circuit consist of 32 nodes, 59 branches and multiple instances all of the element types. For this large test circuit, there are small numerical differences between the Python modified nodal analysis (MNA) code results and the LTSpice solution, which are describe in the test report. Additionally, various interesting problem circuits have been solved using the MNA code and comparing the results to LTSpice. These problem circuits can also be found in the github repository. Code verification often looks at requirements or specifications versus what was implemented. This project didn’t have a formal set of requirements, only a general goal of implementing symbolic MNA using the Python libraries. No formal software or code verification is included."
  },
  {
    "objectID": "OLD Symbolic Modified Nodal Analysis.html#users-guide",
    "href": "OLD Symbolic Modified Nodal Analysis.html#users-guide",
    "title": "Symbolic Modified Nodal Analysis",
    "section": "User’s guide",
    "text": "User’s guide\nA netlist is text file that contains the connectivity information of the circuit. A netlist in the input to the analysis code and the output are a set of equations that can be solved for the unknown voltages and currents. The circuits are described in terms of the components and the connections to the other components. Netlist can be generated with a text editor or exported from a schematic capture program. LTspice can be used to draw the schematic and a netlist can be exported to the python code to generate the network equations.\nNetlist file format:\nA circuit description in spice, which is called a netlist, consists of a statements defining each circuit element and its connection to circuit nodes. A node is a any point on a circuit where two or more circuit elements meet. The nodes are numbered from 1 to N in any order and node 0 is the ground node or circuit common. A ground node is required. Choose a ground or reference node, which usually is taken to be at a potential of zero volt. All other node voltages constitute n unknowns. The nodes should be numbered in consecutive order. Each line in the netlist are either comments, spice directives or circuit elements.\nSpice directives are commands to spice and the first character on the line is a period. Comment lines start with a * or ;. The default file extension is ‘.net’. The python code does some preprocessing of the netlist to check the basic formatting of the netlist is correct.\nThe preprocessor performs the following steps: - remove blank lines and comments - convert first letter of element name to uppercase - removes extra spaces between entries - counts number of entries on each line to make sure the count is correct and counts each element type\nThe element types that are supported are resistors, capacitors, inductors, independent sources and controlled sources. Each line in the netlist file contains a circuit element.\nThe format for the element description is\nletterXXX n1 n2 value\nWhere:\nletter signifies the element type, i.e. R, L, C, V, I, O, E, F, G, H or K\nXXX is a string of letters or numbers that uniquely identify the element.\nn1, n2 and value are the terminal nodes and the value of the element.\nThe element types are described in the following sections.\nResistors, capacitors and inductors:\nThe resistors, capacitors and inductors are described by the following line:\nR/L/CXXXXXXX N1 N2 value \nWhere:\nXXX = the name of the component, can be any length\nN1 = the first terminal\nN2 = the second terminal\nValue = component value in ohms, farads or henrys.\nFor example, a resistor named R1 connected between nodes 1 and 2 with a value of 3000 ohms.\nR1 2 4 3000  \nSpice supports other parameters, but these are not allowed in this python implementation.\nCoupled inductors:\nTwo coupled inductors are described by the following line.\nKXX LYY LZZ VALUE  \nThe parameters are:\nLYY = the name of the first coupled inductor\nLZZ = the name of the second coupled inductor\nVALUE = the coefficient of coupling, K, where 0 &lt; K\nThe orientation of the inductors is determined by the first node, which is considered to be the dotted node. When LTspice generates a net list the phasing dot gets assocated with the negative node. This does not seem to affect the equations generated by the python code. LTspice uses the coupling coefficient, k. The symbolic equations use the mutual inductance, M.\n\\[M = k\\sqrt{L_1L_2}\\]\nIndependent sources:\nA voltage source is described by the following line.\nVXX N+ N- VALUE  \nThe parameters are:\nN+ = the name of the positive terminal\nN- = the name of the negative terminal\nVALUE = the value of the DC voltage\nA current source is described by the following line.\nIXX N+ N- VALUE  \nThe parameters are:\nN+= the name of the positive terminal, current leaves this terminal (pointy end of the arrow)\nN- = the name of the negative terminal VALUE = the value of the DC current\nControlled sources:\nThe voltage-controlled dependent sources are defined using statements of the form\nGXX or EXX nout+ nout- nc+ nc- gain  \nwhere E is a voltage-controlled voltage source, G is a voltage-controlled current source, the output voltage is connected between nodes nout+ and nout-, and the control voltage is measured at node nc+ with respect to node nc-.\nExamples:\nE1 5 1 4 3 10\ndefines a voltage source that makes node 5 a voltage 10*(v4 − v3) above the voltage at node 1.\nG1 2 1 5 8 50 \ndefines a current source connected between node 2 (the + node) and node 1 and supplying a current 50 *(v5 − v8).\nThe current-controlled dependent sources are defined by statements of the form\nFXX or HXX nout+ nout- vcontrol gain   \nwhere F is a current-controlled current source, H is a current-controlled voltage source, and the output current source is connected between nodes nout+ and nout-, with positive current flowing through the source from node nout+ to nout-. The control current flows from the positive node of the source vcontrol through the source and out the negative node.\nExamples:\nFds 11 9 Vsens 1.25\ndefines a current source connected from node 11 to node 9 that generates a current 1.25 times the current flowing through the source Vsens.\nH1 30 20 V5 100\ndefines a voltage source connected from node 30 to node 20 and supplying a voltage 100 times the current through the source V5. It is frequently necessary to add a voltage source with value 0 volts to the circuit to sense the control current for these sources.\nThe direction of positive controlling current flow is from the positive node, through the source, to the negative node of VNAM. VALUE is the current gain.\nOp Amps:\nAn opamp component is described by the following line.\nOXX N+ N- Vout\nThe output of the opamp is a voltage source. Two input terminals are at the same potential.\nThe op amp element is assumed to be an ideal op amp and use of this component is valid only when used in circuits with a DC path (a short or a resistor) from the output terminal to the negative input terminal of the op amp. No error checking is provided and if the condition is violated, the results will be likely erroneous. Need to work on implementing a better opamp model.\nProcedure:\n1. Draw the circuit to be analyzed in LTSpice or some other schematic capture program. Label the nodes. The Symbolic Modified Network Analysis code will provide warnings for netlist formatting errors and non consecutive node numbering, but will still generate nodal equations which may be erroneous. Users should verify the results. 2. Export the netlist of the circuit and convert component values to units of Ohms, Farads and Henrys. Use scientific notation, for example, replace component values such as 2k with 2e3 and 2u with 2e-6.\n3. Change Op Amp reference designators, for example U1 to O1 (capitol letter O, not zero).\n4. Voltage sources and current sources need to be set to zero in some cases.\n5. Modify the nodal analysis Jupyter notebook code to read the net list. Run all the cells in the notebook.\n6. Copy the symbol list, the A, X and Z matrices, and the element values in dictionary format to a new notebook. See end of the nodal analysis Jupyter notebook where these items are displayed.\n7. Review the test and problem circuits for examples."
  },
  {
    "objectID": "OLD Symbolic Modified Nodal Analysis.html#example",
    "href": "OLD Symbolic Modified Nodal Analysis.html#example",
    "title": "Symbolic Modified Nodal Analysis",
    "section": "Example",
    "text": "Example\nThe example circuit contains a capactior, inductor, resistors along with independent and dependent sources. This circuit is from [6], page 69, figure 4.8, with modifications. The circuit was drawn in LTSpice and the circuit nodes are labeled. For no particular reason, the reference note was chosen to be the center node of the circuit and is connected to the ground symbol. AC analysis was performed at 1 rad/sec and over a range of frequencies. The results are compared to those obtained from LTSpice.\n\nThe netlist from LTSpice is shown below with the V1 voltage source line editied.\n* SMNA_example.asc\nR2 2 5 2\nV1 1 0 1\nI1 4 0 9\nV2 0 5 0\nE1 3 0 1 4 2\nF1 2 3 V2 2\nR1 1 4 2\nC1 1 2 1\nL1 4 3 1 Rser=0\n* ;.ac list   0.159154943091895\n.ac dec 100 0.1 100\n.backanno\n.end\n\nCircuit equations\nThe following network equations were generted by the Sympy code.\n\\(- C_{1} s v_{2} + I_{V1} + v_{1} \\left(C_{1} s + \\frac{1}{R_{1}}\\right) - \\frac{v_{4}}{R_{1}} = 0\\)\n\\(- C_{1} s v_{1} + I_{F1} + v_{2} \\left(C_{1} s + \\frac{1}{R_{2}}\\right) - \\frac{v_{5}}{R_{2}} = 0\\)\n\\(I_{Ea1} - I_{F1} - I_{L1} = 0\\)\n\\(I_{L1} - \\frac{v_{1}}{R_{1}} + \\frac{v_{4}}{R_{1}} = - I_{1}\\)\n\\(- I_{V2} - \\frac{v_{2}}{R_{2}} + \\frac{v_{5}}{R_{2}} = 0\\)\n\\(v_{1} = V_{1}\\)\n\\(- v_{5} = V_{2}\\)\n\\(- ea_{1} v_{1} + ea_{1} v_{4} + v_{3} = 0\\)\n\\(I_{F1} - I_{V2} f_{1} = 0\\)\n\\(- I_{L1} L_{1} s - v_{3} + v_{4} = 0\\)\n\n\nSymbolic solution\nSympy was used to solve the network equations and the node voltage results are shown below.\n\\(v_{1} = V_{1}\\)\n\\(v_{2} = \\frac{C_{1} R_{2} V_{1} s + V_{2} f_{1} - V_{2}}{C_{1} R_{2} s - f_{1} + 1}\\)\n\\(v_{3} = \\frac{I_{1} L_{1} R_{1} ea_{1} s + R_{1} V_{1} ea_{1}}{L_{1} s + R_{1} ea_{1} + R_{1}}\\)\n\\(v_{4} = \\frac{- I_{1} L_{1} R_{1} s + L_{1} V_{1} s + R_{1} V_{1} ea_{1}}{L_{1} s + R_{1} ea_{1} + R_{1}}\\)\n\\(v_{5} = - V_{2}\\)\n\n\nThe Jupyter notebook\nThe notebook for this example is here."
  },
  {
    "objectID": "OLD Symbolic Modified Nodal Analysis.html#links",
    "href": "OLD Symbolic Modified Nodal Analysis.html#links",
    "title": "Symbolic Modified Nodal Analysis",
    "section": "Links",
    "text": "Links\nSymbolic Modified Nodal Analysis github page."
  },
  {
    "objectID": "OLD Symbolic Modified Nodal Analysis.html#references",
    "href": "OLD Symbolic Modified Nodal Analysis.html#references",
    "title": "Symbolic Modified Nodal Analysis",
    "section": "References",
    "text": "References\n\nNumerical Recipes in C: The Art of Scientific Computing, William H. Press, Brian P. Flannery, Saul A. Teukolsky, William T. Vetterling, Cambridge University Press; 1988\n\nThe modified nodal approach to network analysis, Chung-Wen Ho, A. Ruehli, P. Brennan, IEEE Transactions on Circuits and Systems ( Volume: 22, Issue: 6, Jun 1975 )\n\nModified nodal analysis\n\nAnalysis of Resistive Circuits\n\nECE 570 Session 3, Computer Aided Engineering for Integrated Circuits\n\nD. E. Johnson, J. L. Hilburn, and J. R. Johnson, Basic Electric Circuit Analysis, Prentice-Hall, Inc. 1978"
  },
  {
    "objectID": "OLD FRT analysis.html",
    "href": "OLD FRT analysis.html",
    "title": "Two amplifier RIAA phono preamp",
    "section": "",
    "text": "Last update: 7 Apr 2023\nannual report: 2022\nshare price data: 4/7/2023 \\$97.74, beta 1.22\n\n\nThis notebook was developed to use as a template to analyze dividend paying companies as potential investments. This company was selected for analysis because it is on the Dividend Aristocrat list and passes the quick look tests.\n\n\n\nFederal Realty is a recognized leader in the ownership, operation and redevelopment of high-quality retail-based properties located primarily in major coastal markets from Washington, D.C. to Boston as well as San Francisco and Los Angeles.\nThe stated primary business objective is to own, manage, acquire and redevelop a portfolio of high quality retail focused properties that will: - provide increasing cash flow for distribution to shareholders - generate higher internal growth than the shopping center industry over the long term - provide potential for capital appreciation - protect investor capital\nThe company specializes in the ownership, management, and redevelopment of high quality retail and mixed-use properties located primarily in densely populated and affluent communities in strategically selected metropolitan markets in the Northeast and Mid-Atlantic regions of the United States, as well as in California and South Florida. As of December 31, 2020, the company owned or had a majority interest in community and neighborhood shopping centers and mixed-use properties which are operated as 101 predominantly retail real estate projects comprising approximately 23.4 million square feet.\nSector(s): Real Estate\nIndustry: REIT—Retail\n\n\n\nRecommendation: buy\nFollow the link to the Conclusion.\n\n\n\nFederal Realty Investment Trust is a real estate investment trust that invests in shopping centers in the Northeastern United States, the Mid-Atlantic states, California, and South Florida.\nRevision history:\n- 1/10/2022: Copied from VZ notebook and reorganized - Feb 2022: updated quick look, reorganized flow of calculations, corrected usage of financial rates, organized end sections - 23 Mar 2022: Cleaning up financial data spreadsheet. Removed NAIC tab. Removed duplicate reveneu data. - 27 Mar 2022: MFG template copied from BMY - 24 Apr 2022: MFG template copied and modified for REIT, FRT is analyized in the REIT-template - 3 May 2022: replaced np.linalg.lstsq with np.polyfit in NAIC forecast, added Future forecast based on historical data notes, Dilution notes, decision model - 4 May 2022: saved as BANK-template analysis\n\n\n\nThe following sections of this notebook contain the financial analysis for the company.\nContents \n\nStock screener results\nLoad financial spreadsheet\nDiscounted cash flow analysis, baseline\nDCF Scenarios\nNACI stock selection guide analysis\nFuture stock price\nDividend payout\nManagement performance\nDecision model\nConclusion\nNotes\nReferences"
  },
  {
    "objectID": "OLD FRT analysis.html#abstract",
    "href": "OLD FRT analysis.html#abstract",
    "title": "Two amplifier RIAA phono preamp",
    "section": "",
    "text": "This notebook was developed to use as a template to analyze dividend paying companies as potential investments. This company was selected for analysis because it is on the Dividend Aristocrat list and passes the quick look tests."
  },
  {
    "objectID": "OLD FRT analysis.html#introduction",
    "href": "OLD FRT analysis.html#introduction",
    "title": "Two amplifier RIAA phono preamp",
    "section": "",
    "text": "Federal Realty is a recognized leader in the ownership, operation and redevelopment of high-quality retail-based properties located primarily in major coastal markets from Washington, D.C. to Boston as well as San Francisco and Los Angeles.\nThe stated primary business objective is to own, manage, acquire and redevelop a portfolio of high quality retail focused properties that will: - provide increasing cash flow for distribution to shareholders - generate higher internal growth than the shopping center industry over the long term - provide potential for capital appreciation - protect investor capital\nThe company specializes in the ownership, management, and redevelopment of high quality retail and mixed-use properties located primarily in densely populated and affluent communities in strategically selected metropolitan markets in the Northeast and Mid-Atlantic regions of the United States, as well as in California and South Florida. As of December 31, 2020, the company owned or had a majority interest in community and neighborhood shopping centers and mixed-use properties which are operated as 101 predominantly retail real estate projects comprising approximately 23.4 million square feet.\nSector(s): Real Estate\nIndustry: REIT—Retail"
  },
  {
    "objectID": "OLD FRT analysis.html#bottom-line-up-front",
    "href": "OLD FRT analysis.html#bottom-line-up-front",
    "title": "Two amplifier RIAA phono preamp",
    "section": "",
    "text": "Recommendation: buy\nFollow the link to the Conclusion."
  },
  {
    "objectID": "OLD FRT analysis.html#company-description",
    "href": "OLD FRT analysis.html#company-description",
    "title": "Two amplifier RIAA phono preamp",
    "section": "",
    "text": "Federal Realty Investment Trust is a real estate investment trust that invests in shopping centers in the Northeastern United States, the Mid-Atlantic states, California, and South Florida.\nRevision history:\n- 1/10/2022: Copied from VZ notebook and reorganized - Feb 2022: updated quick look, reorganized flow of calculations, corrected usage of financial rates, organized end sections - 23 Mar 2022: Cleaning up financial data spreadsheet. Removed NAIC tab. Removed duplicate reveneu data. - 27 Mar 2022: MFG template copied from BMY - 24 Apr 2022: MFG template copied and modified for REIT, FRT is analyized in the REIT-template - 3 May 2022: replaced np.linalg.lstsq with np.polyfit in NAIC forecast, added Future forecast based on historical data notes, Dilution notes, decision model - 4 May 2022: saved as BANK-template analysis"
  },
  {
    "objectID": "OLD FRT analysis.html#analysis",
    "href": "OLD FRT analysis.html#analysis",
    "title": "Two amplifier RIAA phono preamp",
    "section": "",
    "text": "The following sections of this notebook contain the financial analysis for the company.\nContents \n\nStock screener results\nLoad financial spreadsheet\nDiscounted cash flow analysis, baseline\nDCF Scenarios\nNACI stock selection guide analysis\nFuture stock price\nDividend payout\nManagement performance\nDecision model\nConclusion\nNotes\nReferences"
  },
  {
    "objectID": "OLD FRT analysis.html#stock-screener-results",
    "href": "OLD FRT analysis.html#stock-screener-results",
    "title": "Two amplifier RIAA phono preamp",
    "section": "1) Stock screener results ",
    "text": "1) Stock screener results \nThis company was selected to analyze because it is a Dividend Aristocrat.\n~~This company was selected from the Fidelity stock screener results. The search results are based on Dividend yield (high and very high, 2.83% and higher), Volume 90 average (high and very high. 535k and higher) and Revenue Growth 5 years (0 or higher). ~~\nCurrent news\nA review of the financial news sites from yahoo and google showed the following:\n- Federal Realty Investment Trust (NYSE: FRT) is acquiring Kingstowne Towne Center in Kingstowne, Virginia for a total purchase price of \\$200 million. The acquisition will close in two phases. Federal Realty has closed on phase one of the acquisition for \\$100 million and expects to close on phase two for \\$100 million in July, subject to customary closing conditions. Combined, the property comprises 410,000 square feet of retail space on 45 acres of land. Located in Virginia’s Fairfax County near TSA’s new headquarters, Kingstowne Towne Center is surrounded by 5,200 homes, four commercial office buildings, and a planned multifamily development, and is part of a one million-square-foot regional retail node that attracts approximately 8.3 million visits annually—amongst the most visited retail destinations in Virginia. - Through the fiscal year ended December 31, 2021, the business of the registrant was conducted by an entity known as Federal Realty Investment Trust, a Maryland real estate investment trust (the “Predecessor”). On December 2, 2021, the Predecessor’s Board of Trustees approved the reorganization of the Predecessor’s business into an umbrella partnership real estate investment trust, or “UPREIT.”\nReview quarterly results\nSince this analysis mainly looks at the annual reports, a review of the quarterly reports and the most recent 12 months is needed to see if the recent quarterly trends match the yearly trends. - yahoo finance shows TTM Total Revenue is about equal to the most current 10K revenue. - The Compustat Company Research from Fidelity (from Sep. 29, 2021) shows: not reviewed\nAverage daily volume\nAverage daily volume: 499,387\nDividend yield\nForward dividend yield: 3.51%"
  },
  {
    "objectID": "OLD FRT analysis.html#load-financial-spreadsheet",
    "href": "OLD FRT analysis.html#load-financial-spreadsheet",
    "title": "Two amplifier RIAA phono preamp",
    "section": "2) Load financial spreadsheet ",
    "text": "2) Load financial spreadsheet \nData from consolidated financial statements and annual reports was collected and entered into a spreadsheet. All numerical data is converted from thousands or millions of dollars to dollars. The stock share price history was obtained from yahoo and is included as a tab in the spreadsheet. Other tabs in the spreadsheet are various worksheets.\n\nticker = 'FRT' # company ticker symbol\nos.chdir('/home/jim/Documents/Dividend Investing/DCF data/')\n\nfile_name = ticker+'_Financials.xlsx'\ndf_dcf_sheet = pd.read_excel(file_name,sheet_name='DCF data')\n#df_NAIC_financials = pd.read_excel(file_name,sheet_name='NAIC data')\ndf_metrics_sheet = pd.read_excel(file_name,sheet_name='metrics')\ndf_price_history = pd.read_excel(file_name,sheet_name='Historical Prices')\n\n# change the working director back to the Jupyter folder\nos.chdir('/home/jim/Documents/JupyterLab/Discount Cash Flow Analysis/')\n\n\n# convert dates from string to datetime format in stock price history\nprice_date_list = []\nfor i in range(len(df_price_history)):\n    price_date_list.append(datetime.strptime(str(df_price_history['Date'][i]), '%Y-%m-%d'))\n\ndf_price_history.insert(0, 'datetime', price_date_list)  # insert a new column with datetime data\ndf_price_history.sort_values(by=['datetime'], inplace=True) # sort data frame by datetime\n\ndf_price_history.set_index('datetime',inplace=True)\n\n#df_price_history.head()\n\n\n2.1) Format data frame \nGenerate a new data frame that holds the financial data needed for the DCF model. Data from financial statements is copied into a spreadsheet which contains the data used in the analysis. The data in the DCF_data tab is in a consistent format for ease of use by this notebook. Standard names are used for the rows and columns.\n\n#column names: fiscal years \nfy_data = df_dcf_sheet.columns[1:].values.astype('datetime64[Y]')-1970\n#line 0: Total revenue  \nrevenue_data = df_dcf_sheet.iloc[0].to_numpy()[1:].astype('float')\n#line 1: Cost of goods sold\nCost_of_goods_sold_data = df_dcf_sheet.iloc[1].to_numpy()[1:].astype('float')\n#line 2: General and administrative\nGeneral_and_administrative_data = df_dcf_sheet.iloc[2].to_numpy()[1:].astype('float')\n#line 3: Research and development\nResearch_and_development_data = df_dcf_sheet.iloc[3].to_numpy()[1:].astype('float')\n#line 4: Depreciation and amortization\nDepreciation_and_amortization_data = df_dcf_sheet.iloc[4].to_numpy()[1:].astype('float')\n#line 5: Investment\nInvestment_data = df_dcf_sheet.iloc[5].to_numpy()[1:].astype('float')\n# line 6: Income before income taxes\nIncome_before_income_taxes_data = df_dcf_sheet.iloc[6].to_numpy()[1:].astype('float')\n# line 7: Income tax\nIncome_tax_data = df_dcf_sheet.iloc[7].to_numpy()[1:].astype('float')\n# line 8: Accounts receivable\nAccounts_receivable_data = df_dcf_sheet.iloc[8].to_numpy()[1:].astype('float')\n# line 9: Inventories\nInventories_data = df_dcf_sheet.iloc[9].to_numpy()[1:].astype('float')\n# line 10: Accounts payable\nAccounts_payable_data = df_dcf_sheet.iloc[10].to_numpy()[1:].astype('float')\n# line 11: Current assets\nCurrent_assets_data = df_dcf_sheet.iloc[11].to_numpy()[1:].astype('float')\n# line 12: Current liabilities\nCurrent_liabilities_data = df_dcf_sheet.iloc[12].to_numpy()[1:].astype('float')\n# line 13: Long term debt\nLong_term_debt_data = df_dcf_sheet.iloc[13].to_numpy()[1:].astype('float')\n# line 14: Shares outstanding\nShares_outstanding_data = df_dcf_sheet.iloc[14].to_numpy()[1:].astype('float')\n\n\n# make a new data frame to store selected financial data\ndf_dcf_data = pd.DataFrame(data={\n    'FY':fy_data[::-1],\n    'revenue':revenue_data[::-1],\n    'cost_of_goods_sold':Cost_of_goods_sold_data[::-1],\n    'general_and_administrative':General_and_administrative_data[::-1],\n    'research_and_development':Research_and_development_data[::-1],\n    'depreciation':Depreciation_and_amortization_data[::-1],\n    'investment':Investment_data[::-1],\n    'income_before_income_taxes':Income_before_income_taxes_data[::-1],\n    'income_tax':Income_tax_data[::-1],\n    'accounts_receivable':Accounts_receivable_data[::-1],\n    'inventories':Inventories_data[::-1],\n    'accounts_payable':Accounts_payable_data[::-1], \n    'current_assets':Current_assets_data[::-1],\n    'current_liabilities':Current_liabilities_data[::-1],\n    'long_term_debt':Long_term_debt_data[::-1],\n    'shares_outstanding':Shares_outstanding_data[::-1]\n    })\n\n#df_dcf_data"
  },
  {
    "objectID": "OLD FRT analysis.html#discounted-cash-flow-analysis-baseline",
    "href": "OLD FRT analysis.html#discounted-cash-flow-analysis-baseline",
    "title": "Two amplifier RIAA phono preamp",
    "section": "3) Discounted cash flow analysis, baseline ",
    "text": "3) Discounted cash flow analysis, baseline \nDiscounted cash flow (DCF) is a valuation method used to estimate the value of an investment based on its expected future cash flows. DCF analysis attempts to figure out the value of an investment today, based on projections of how much money it will generate in the future. In finance, discounted cash flow (DCF) analysis is a method of valuing a security, project, company, or asset using the concepts of the time value of money. The DCF method used in this notebook follows [1].\nThe value of any financial investment equals the present value of the expected future cash flows, discounted for risk and timing of these cash flows. The DCF method to value stocks is a four step process.\n1. Develop a set of future free cash flows for the corporation based on revenue growth, net operating profit margin, income tax rate and fix and working capital requirements. 2. Estimate the discount rate for the cash flows based on expected timing and risk. 3. Discount the cash flows and total them to calculate the value for the corporation as a whole. 4. Subtract the debt, preferred stock value and other claims and divide by the number of shares outstanding to get the intrinsic value.\nSections - Revenue growth rate - Net operating profit margin - Tax rate - Depreciation Rate - Investment Rate - Working Capital Rate - Current Assets - Current Liabilities - Value of Debt Outstanding - Current stock price - Shares outstanding - 10 year treasury bond yield - Bond yield spread to treasury - Preferred stock yield - Equity risk premium - Company specific beta - DCF model inputs - Future cash flows\n\nFuture forecast based on historical data\nThe DCF model uses historical financial data to estimate future cash flows. However, future changes are largely unpredictable, so we assume that the past record can be used as a rough guide to the future. The more questionable this assumption is, the less valuable is the analysis. So the DCF model is more useful when applied to stable well established companies, since companies with stable earnings are easier to forecast.\n\n\nRevenue growth rate \nThe revenue growth rate (also sometimes called net sales) of the corporation plus any other revenues associated with the main operations of the business. It does not include dividends, interest income or non-operating income. Historic revenue data is obtained from consolidated income statements. The year over year change in revenue is calculated and converted to a percent, then an average revenue growth rate is calculated.\nAdjustments for a REIT\nThe revenue is from Total revenue on the income statement and includes: - Rental income - Mortgage interest income - other - Management and other fees from affiliates\nExclude other income, gains on sale of real estate and interest expenses.\n\n# calculate the percent change in revenue\npcr = np.zeros(len(df_dcf_data['revenue'].to_numpy())) # percent change in revenue\nfor i in range(len(df_dcf_data['revenue'].to_numpy()[0:-1])):\n    pcr[i+1] = ((df_dcf_data['revenue'].to_numpy()[i+1] - df_dcf_data['revenue'].to_numpy()[i])/\n                df_dcf_data['revenue'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Revenue, $B')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['revenue']/1e9, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcr,'+-g')\n    \nax2.set_ylabel('% Change in revenue',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,50))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Revenue')\nplt.show()\n\n\n\n\nObservation:\nExcept for 2020, revenues have grown every year. If 2020 and 2021 is removed from the revenue data series:\n\n#exclude 2021 and 2020 from average calculation\nprint('average revenue growth rate 2015 to 2019: {:.2f}%'.format(pcr[-7:-2].mean()))\n\naverage revenue growth rate 2015 to 2019: 2.04%\n\n\n\nrgr_avg = pcr[-5:].mean()/100 # last five years\nprint('average revenue growth rate: {:.2f}%'.format(rgr_avg*100))\n\naverage revenue growth rate: 4.03%\n\n\n\n\nNet operating profit margin \nNet Operating Profit should reflect the future revenue generating ability and expense requirements of the operating business that comprise the ongoing operations of the company.\n\\(\\text{NOPM} = \\frac{\\text{Revenue} - \\text{Expenses}}{\\text{Revenue}}\\)\n\\(\\text{Expenses} = \\text{Cost of Goods Sold (CGS)} + \\text{General and Administrative (G&A)} + \\text{Research and Development (R&D)}\\)\nGeneral and Administrative (G&A) is also called Sales, General and Administrative (SG&A)\nAdjustments for a REIT\nG&A and R&D costs are zero. Total operating expenses include Depreciation and amortization, so this item is removed from CGS.\nTotal expenses is made up of: - Rental expenses - Real estate taxes - General and administrative - Depreciation and amortization\nThese are all lumped into CGS and Depreciation and amortization is removed. Depreciation and amortization is a non-cash charge, therefore, we add back the charge to total expenses. The idea is that Depreciation and amortization as an expense unfairly reduces our net income because realestate its value over the period.\n\n# NOP = (Revenue - Expenses)\nnop = df_dcf_data['revenue'].to_numpy() - df_dcf_data['cost_of_goods_sold'].to_numpy()\n\n# net operating profit margin as percent of revenue\nnopm = nop/df_dcf_data['revenue'].to_numpy()\n\n# plot as grouped bar chart with labels on right and working capital rate on left\n# calculate position of bars\nx1_bar_position = []\nx2_bar_position = []\nfor i in df_dcf_data['FY']:\n    x1_bar_position.append(i-relativedelta(months=1))\n    x2_bar_position.append(i+relativedelta(months=1))\n    \nwidth = 40  # the width of the bars\n    \n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Net operating profit, \\\\$B')\n\nax1.bar(x1_bar_position,df_dcf_data['cost_of_goods_sold'].to_numpy()/1e9, width,label='CGS')\nax1.bar(x2_bar_position,nop/1e9, width,label='NOP')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,0.7))\nax1.legend()\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:cyan'\n\nax2.plot(df_dcf_data['FY'],nopm*100,'+-c')\n    \nax2.set_ylabel('% NOPM',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,100))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Net Operating Profit')\nplt.show()\n\n\n\n\nObservation\nThe NOP has been between 0.4 and 0.5 billion dollars during the past few years. The average net operating profit margin for the last 5 years is calculated below.\n\n#Average net operating profit margin\nnopm_avg = nopm[-5:].mean()\nprint('average net operating profit margin: {:.2f}%'.format(nopm_avg*100))\n\naverage net operating profit margin: 62.41%\n\n\n\n\nTax rate \nTax payments are taken from the consolidated income statement, provision for income taxes. The effect of taxes on profits is accounted for.\n\\(\\text{Tax rate} = \\frac{\\text{Income taxes}}{\\text{Income before income taxes}}\\)\nAdjustments for a REIT\nREITs have very little income tax. Income average tax rate is set to zero.\n\n# Average tax rate\ntax_rate_avg = 0 #tax_rate[-5:].mean()\nprint('average tax rate: {:.2f}%'.format(tax_rate_avg*100))\n\naverage tax rate: 0.00%\n\n\n\n\nDepreciation Rate \nThe depreciation rate is used to project the future net investment cash flows. The effect is to reduce the amount of FCFF. Depreciation amounts are from the Consolidated Statement of Cash Flows, Depreciation and Amortization.\n\\(\\text{Depreciation Rate}=\\frac{\\text{Depreciation and Amortization}}{\\text{Revenues}}\\)\nDepreciation is the write off or expensing of a percentage of the historical cost of an asset over the asset’s useful life. Property, plant and equipment (PP&E) are long term or non current assets owned or controlled by the company and used to manufacture and or sell the company’s products. The balance sheet typically shows all categories of PP&E grouped together, net of accumulated depreciation. Depreciation represents wear and tear on an asset or the fact that an asset gets used up over time. Companies record depreciation expense in the income statement every year for all depreciable assets in service or used by the company during the year. The difference between GAAP and Tax Accounting methods is handled through deferred taxes.\nAmortization is the write off or expensing of the cost of a financial instrument or an intangible asset over the shorter of its useful life or legal life. Amortization is similar to depreciation and reflects the declining useful life and value of the intangible asset over time. Companies in research and development intensive fields typically have many patents. Such industries include high technology, pharmaceuticals and chemicals.\n\n# depreciation rate\ndepreciation_rate = df_dcf_data['depreciation'] / df_dcf_data['revenue'].to_numpy()\n\n# plot depreciation on left and rate on right\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $B')\n\nax1.bar(df_dcf_data['FY'],df_dcf_data['depreciation']/1e9, width=100,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:Blue'\n\nax2.plot(df_dcf_data['FY'],depreciation_rate*100,'+-')\n    \nax2.set_ylabel('% Depreciation rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,50))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Depreciation')\nplt.show()\n\n\n\n\nObservation:\nDepreciation has been running at about a consistant 30% of revenues.\n\n# average depreciation rate\ndepreciation_rate_avg = depreciation_rate[-5:].mean()\nprint('average depreciation rate: {:.2f}%'.format(depreciation_rate_avg*100))\n\naverage depreciation rate: 28.08%\n\n\n\n\nInvestment Rate \nTaken from Consolidated Statement of Cash Flows, Cash used for investing activities. Net investment in the dollar amount needed to support the growth of the firm. Included investments in properties, plant equipment in excess of the depreciation expenses associated with past investments. Net investment decreases the amount of money available to the stockholders. Investment in property, plant and equipment is necessary to both maintain service and sales and also to grow revenues and profits. Investment amounts should include capital expenditures and research and development.\n\\(Ir=\\frac {\\text {Capital Expenditures}}{\\text{Revenues}}\\)\nFor this company, the yearly investment amounts are taken from the Consolidated Statements of Cash Flows, Net Cash Used in Investing Activities.\nAdjustments for a REIT\nThe yearly investment amounts are taken from the Consolidated Statements of Cash Flows, Net Cash Used in Investing Activities\n\n# investment rate\ninvestment_rate = df_dcf_data['investment'] / df_dcf_data['revenue'].to_numpy()\n\n# plot investment on left and rate on right\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, \\\\$B')\n\nax1.bar(df_dcf_data['FY'],df_dcf_data['investment']/1e9, width=100,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:Blue'\n\nax2.plot(df_dcf_data['FY'],investment_rate*100,'+-')\n    \nax2.set_ylabel('% New Investment Rate',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,100))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('New Investment')\nplt.show()\n\n\n\n\nObservation:\n\n# average investment rate\ninvestment_rate_avg = investment_rate[-5:].mean()\nprint('average investment rate: {:.2f}%'.format(investment_rate_avg*100))\n\naverage investment rate: 48.29%\n\n\n\n\nWorking Capital Rate \nWorking capital is needed to support the corporate sales effort of any company. Often a company’s incremental change in net working capital either positive or negative is approximately proportional to its change in revenue.\n\\(\\text{Working capital} = \\text{Accounts Receivable} + \\text{Inventories} - \\text{Accounts Payable}\\)\nWorking capital is a company’s net investment in its accounts receivable and its inventories (cash outflows), minus its accounts payable (a cash inflow). Working capital and taxes are cash outflows from the corporation that are not available to pay debts and stockholders.\nAdjustments for a REIT\nFor a REIT, the working capital rate is set to zero. REITs generally have no data for inventories and accounts receivable.\n\n# average working capital rate\nworking_capital_rate_avg = 0 #working_capital_rate[-5:].mean()\nprint('average working capital rate: {:.2f}%'.format(working_capital_rate_avg*100))\n\naverage working capital rate: 0.00%\n\n\n\n\nCurrent assets \nTotal Current Assets from the most recent balance sheet statement of the company. Current assets include inventory, cash and accounts receivables.\nAdjustments for a REIT\nCurrent assets are in cluded in total assets for a REIT. Current assets are calculted as follows:\n\\(\\text{Current assets} = \\text{Cash and cash equivalents} + \\text{Accounts and notes receivable}\\)\n\n# plot Short Term Assets\nwidth = 100  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nplt.bar(df_dcf_data['FY'],df_dcf_data['current_assets']/1e9, width)\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Current assets')\nplt.ylabel('dollars, \\\\$B')\n\nplt.grid()\nplt.show()\n\n\n\n\nObservation:\nAn increase in cash and cash equivalents for 2019 on account of the pandemic.\n\nsta = df_dcf_data['current_assets'].iloc[-1]\nprint('Current assets: ${:.2f}B'.format(sta/1e9))\n\nCurrent assets: $0.28B\n\n\n\n\nCurrent liabilities \nTotal Current Liabilities from the most recent balance sheet consolidated statement.\nAdjustments for a REIT\nCurrent Liabilities are calculted as follows:\n\\(\\text{Current liabilities} = \\text{Notes payable, net} + \\text{Accounts payable and accrued expenses} + \\text{Security deposits payable}\\)\n\n# plot Short Term Liabilities\n\nwidth = 100  # the width of the bars\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nplt.bar(df_dcf_data['FY'],df_dcf_data['current_liabilities']/1e9, width)\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Current liabilities')\nplt.ylabel('dollars, \\\\$B')\n\nplt.grid()\nplt.show()\n\n\n\n\n\nprint('Average of current liabilities: ${:.2f}B'.format(df_dcf_data['current_liabilities'].mean()/1e9))\n\nAverage of current liabilities: $0.48B\n\n\nObservation:\nOther current liabilities increased in 2019 and 2020.\n\nstl = df_dcf_data['current_liabilities'].iloc[-1]\nprint('Current liabilities: ${:.2f}B'.format(stl/1e9))\n\nCurrent liabilities: $0.82B\n\n\n\n\nValue of Debt Outstanding \nAmount of debt outstanding from the most recent balance sheet of the company.\nAdjustments for a REIT\nValue of Debt Outstanding (long term debt) is calculted as follows:\n\\(\\text{Value of Debt Outstanding} = \\text{Total liabilities} - \\text{Current liabilities}\\)\n\n# calculate the percent change in debt, pcd\npcd = np.zeros(len(df_dcf_data['long_term_debt'].to_numpy())) # percent change in debt\nfor i in range(len(df_dcf_data['long_term_debt'].to_numpy()[0:-1])):\n    pcd[i+1] = ((df_dcf_data['long_term_debt'].to_numpy()[i+1] - df_dcf_data['long_term_debt'].to_numpy()[i])/\n                df_dcf_data['long_term_debt'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dollars, $B')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['long_term_debt']/1e9, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcd,'+-g')\n    \nax2.set_ylabel('% Change in debt',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-40,100))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('debt')\nplt.show()\n\n\n\n\n\ndgr_avg = pcd[1:].mean()/100\nprint('average debt growth rate: {:.2f}%'.format(dgr_avg*100))\n\naverage debt growth rate: 6.82%\n\n\nObservation:\nAs of December 31, 2021, FRT had approximately \\$4.1 billion of debt outstanding. Of that outstanding debt, approximately \\$341.6 million was secured by all or a portion of 7 of their real estate projects. As of December 31, 2021, approximately 92.6% of FRT debt was fixed rate or fixed via interest rate swap agreements, which includes all of their property secured debt and our unsecured senior notes. FRT organizational documents do not limit the level or amount of debt that they may incur. The amount of our debt outstanding from time to time could have important consequences to our shareholders.\nRising interest rates could adversely affect our cash flow and the market price of our outstanding debt and preferred shares. Of FRT’s \\$4.1 billion of debt outstanding as of December 31, 2021, approximately \\$356.5 million bears interest at a variable rate, of which, \\$300.0 million is unsecured term loan that bears interest at a variable rate of LIBOR plus 80 basis points and \\$56.5 million in mortgages payable that bear interest at a variable rate of LIBOR plus 195 basis points and are effectively fixed through two interest rate swap agreements.\nFRT also has a \\$1.0 billion revolving credit facility, on which no balance was outstanding at December 31, 2021, that bears interest at LIBOR plus 77.5 basis points.\nAs of December 31, 2021, there is no balance outstanding on FRT’s \\$1.0 billion unsecured revolving credit facility and they had cash and cash equivalents of \\$162.1 million. FRT also had outstanding forward sales agreements for net proceeds of \\$264.0 million as of December 31, 2021. FRT has no debt maturing until June 2023.\nIn addition, an increase in market interest rates may lead purchasers of FRT debt securities and preferred shares to demand a higher annual yield, which could adversely affect the market price of FRT’s outstanding debt securities and preferred shares and the cost and/or timing of refinancing or issuing additional debt securities or preferred shares.\nFor the year ended 2021, the weighted average amount of borrowings outstanding on our revolving credit facility was \\$19.6 million, and the weighted average interest rate, before amortization of debt fees, was 0.9%.\nThe interest rates on these mortgages range from 3.91% to 5.00%.\n\nvod = df_dcf_data['long_term_debt'].iloc[-1]\nprint('Total long term debt and other: ${:.2f}B'.format(vod/1e9))\n\nTotal long term debt and other: $4.20B\n\n\n\n\nCurrent stock price \nMost recent stock price for the company. The current stock price is used to calculate the market value of the firm. Use the market value when looking at market capitalization for common stock.\n\ncsp = 97.74 #95.93 # current stock price\nprint('current stock price: ${:,.2f}'.format(csp))\n\ncurrent stock price: $97.74\n\n\n\n\nShares outstanding \nThe number of shares outstanding is used to calculate the intrinsic stock value.\n\nso = df_dcf_data['shares_outstanding'].iloc[-1] # shares outstanding\nprint('shares outstanding, basic: {:,.0f}'.format(so))\n\nshares outstanding, basic: 81,353,180\n\n\n\n# calculate the percent change in shares outstanding, pcso\npcso = np.zeros(len(df_dcf_data['shares_outstanding'].to_numpy())) # percent change in debt\nfor i in range(len(df_dcf_data['shares_outstanding'].to_numpy()[0:-1])):\n    pcso[i+1] = ((df_dcf_data['shares_outstanding'].to_numpy()[i+1] - df_dcf_data['shares_outstanding'].to_numpy()[i])/\n                df_dcf_data['shares_outstanding'].to_numpy()[i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('shares outstanding, M')\n\n# plot revenue as single bar\nplt.bar(df_dcf_data['FY'],df_dcf_data['shares_outstanding']/1e6, width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(df_dcf_data['FY'],pcso,'+-g')\n    \nax2.set_ylabel('% Change in shares outstanding',color=color)\nax2.tick_params(axis='y', labelcolor=color)\n#ax2.set_ylim((-5,25))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Shares outstanding')\nplt.show()\n\n\n\n\n\nprint('average shares outstanding growth rate: {:.2f}%'.format(pcso[1:].mean()))\n\naverage shares outstanding growth rate: 2.34%\n\n\nObservation:\nFor the year ended December 31, 2021, FRT issued 847,471 common shares at a weighted average price per share of \\$104.19 for net cash proceeds of \\$87.0 million including paying \\$0.9 million in commissions and \\$0.4 million in additional offering expenses related to the sales of these common shares.\nFRT has the capacity to issue up to \\$175.0 million in common shares under the ATM program.\n\n\nDilution\nDilution occurs when a company issues new shares that result in a decrease in existing stockholders’ ownership percentage of that company. Stock dilution can also occur when holders of stock options, such as company employees, or holders of other optionable securities exercise their options. When the number of shares outstanding increases, each existing stockholder owns a smaller, or diluted, percentage of the company, making each share less valuable.\nInvestigate why there is a historic growth trend in number of shares outstanding. Search annual report for dilutive actions: - share sales - convertable debt - employee options\nSearch results:\n\n\n10 year treasury bond yield \nThe 10 year treasury yield is used as a measure of the risk free rate.\nYield: 3.45%\niShares 7-10 Year Treasury Bond ETF (IEF)\nAverage Yield to Maturity: 3.5%\n\ntby = (3.45+3.5)/2/100  # 10 year treasury bond yield, average of data from sources listed above\nprint('10 year treasury bond yield: {:,.2f}%'.format(tby*100))\n\n10 year treasury bond yield: 3.48%\n\n\n\n\nBond yield spread to treasury \nThe spread to treasury implies that all corporate debt will have a higher yield than yields associated with comparable maturity US Treasury Bonds. The best way to determine default risk is to see how a particular company’s debt is trading in the market and compare it on a spread basis with comparable maturity yields.\nLook at the following or use a default rating systems that are published by the three major rating agencies, Standards and Poors Corp, Moody’s Investor Services and Fitch & Company.\nPIMCO Active Bond Exchange-Traded Fund (BOND)\nYield: 3.44%\niShares 5-10 Year Investment Grade Corporate Bond ETF (IGIB)\nAverage Yield to Maturity: 5.15%\niShares 10+ Year Investment Grade Corporate Bond ETF (IGLB)\nAverage Yield to Maturity: 5.35%\nWeb resources: - http://www.standardpoor.com/\n- http://bond.yahoo.com/rates.html\n- http://www.moodys.com/cust/default.asp\n- http://www.fitchibca.com/corporate/index.cfm\n\nbystt = ((3.44+5.15+5.35)/3-tby)/100           # bond yield spread (average) to treasury spread\nprint('Bond yield spread to treasury: {:,.2f}%'.format(bystt*100))\n\nBond yield spread to treasury: 4.61%\n\n\n\n\nPreferred stock yield \nAmount of preferred stock outstanding from the most recent balance sheet of the company.\nFrom the balance sheet:\n- Preferred shares, authorized 15,000,000 shares, \\$.01 par: 5.0% Series C Cumulative Redeemable Preferred Shares, (stated at liquidation preference $25,000 per share), 6,000 shares issued and outstanding - 5.417% Series 1 Cumulative Convertible Preferred Shares, (stated at liquidation preference \\$25 per share), 399,896 shares issued and outstanding\nSee below for inputs to model.\n\npsy = (5+5.417)/2/100  # preferred stock yield\nprint('preferred stock yield: {:,.2f}%'.format(psy*100))\n\nvps = 6000*25000 + 399896*25 # value of preferred stock\nprint('value of preferred stock: {:,.2f}'.format(vps))\n\npreferred stock yield: 5.21%\nvalue of preferred stock: 159,997,400.00\n\n\n\n\nEquity risk premium \nThe expected excess return a hypothetical average investor would require of a diversified portfolio of stock (assumed beta = 1.0) over the yield on the 10-year Treasury Bond. The equity risk premium has been going down over the years.\n- 1926 to 1990: 5.5%\n- 1962 to 1990: 3.25%\n- 1981 to 1990: 0.19%\nIn times of sustained economic growth the risk premium demanded by investors generally declines.\nI’m going to use 3% as the equity risk premium.\n\neq_rp = 3.0/100             # equity risk premium\nprint('Equity risk premium: {:,.2f}%'.format(eq_rp*100))\n\nEquity risk premium: 3.00%\n\n\n\n\nCompany specific beta \nThe Beta used is Beta of Equity. Beta is the monthly price change of a particular company relative to the monthly price change of the S&P 500. The time period for Beta is 5 years when available. This value can be obtained at yahoo finance.\nA measure of risk of an individual stock. It measures volatility of return - a higher beta means a higher risk. A financial model that uses Beta as its sole measure of risk (signal factor model) is called a Capital Asset Pricing Model (CAPM).\n\nbeta = 1.22 # company specific beta\nprint('Company specific beta: {:,.2f}'.format(beta))\n\nCompany specific beta: 1.22\n\n\n\n\nDCF model inputs \nBelow are the DCF model inputs. These values were calculated above.\n\n# various rates\nrgr = rgr_avg              # revenue growth rate\nprint('revenue growth rate: {:,.2f}%'.format(rgr*100))\nnopm = nopm_avg             # net operating profit margin\nprint('net operating profit margin: {:,.2f}%'.format(nopm*100))\ntr = tax_rate_avg               # tax rate\nprint('tax rate: {:,.2f}%'.format(tr*100))\ndr = depreciation_rate_avg              # depreciation rate (% of revenue)\nprint('depreciation rate: {:,.2f}%'.format(dr*100))\nir = investment_rate_avg              # investment rate (% of revenue)\nprint('investment rate: {:,.2f}%'.format(ir*100))\nwcr = working_capital_rate_avg            # working capital rate (% of revenue)\nprint('working capital rate: {:,.2f}%'.format(wcr*100))\n\nrevenue growth rate: 4.03%\nnet operating profit margin: 62.41%\ntax rate: 0.00%\ndepreciation rate: 28.08%\ninvestment rate: 48.29%\nworking capital rate: 0.00%\n\n\nExcess return period\nThe excess return period is based on a judgment call. The authors of [1] use the 1-5-7-10 rule. They group companies into one of four general categories and excess return periods. They use a 10 year excess return period to calculate what they would consider the maximum value. They use a more conservative 1 year, 5 year or 7 year return period to calculate a more reasonable or minimum value.\n- 1 year: Boring companies that operate in a highly competitive, low margin industry in which they have nothing particular going for them. - 5 year: Decent companies that have a recognizable name and decent reputation and perhaps a regulatory benefit (utility company), but can’t control pricing or growth. - 7 year: Good companies with good brand names, large companies of scale, good marketing channels and consumer identification (e.g. McDonald’s) - 10 year: Great companies with great growth potential, tremendous marketing power, band names and in-place benefits (e.g. Intel, Microsoft, Coca Cola, Disney)\nThe excess return period used for the base case is ten years, which should lead to a higher calculated intrinsic value.\n\n# General Inputs\nfy_start = df_dcf_data['FY'].iloc[-1].year # fiscal year to start excess return period\nerp = 10 # excess return period, years\nrev_start = df_dcf_data['revenue'].to_numpy()[-1] # starting revenues for excess return period\nprint('starting year: {:.0f}'.format(fy_start))\nprint('excess return period: {:.0f} years'.format(erp))\nprint('starting revenues: ${:,.2f}B'.format(rev_start/1e9))\nprint('shares outstanding: {:,.0f}'.format(so))\n\nstarting year: 2022\nexcess return period: 10 years\nstarting revenues: $1.07B\nshares outstanding: 81,353,180\n\n\n\nps_mv = vps               # preferred stock, market value \nprint('preferred stock, market value : ${:,.2f}B'.format(ps_mv/1e9))\ncs_mv = csp*so            # common stock, market value \nprint('common stock, market value: ${:,.2f}B'.format(cs_mv/1e9))\n\npreferred stock, market value : $0.16B\ncommon stock, market value: $7.95B\n\n\nLong Term Debt, Market Value, ltd_mv\nUse the book value for long term debt. Various online resources can be used to research this item. These include, Bondsonline and Bloomberg. The book value of debt and preferred stock is an accounting measure that relates to how much money was raised by the company when each security was issued. The market value of debt and the preferred and common stock is the price that specific obligations would trade at in today’s market.\nLong term debt for firms can take one of two forms. It can be a long-term loan from a bank or other financial institution or it can be a long-term bond issued to financial markets, in which case the creditors are the investors in the bond. Firms often have long term obligations that are not captured in the long term debt item. These include obligations to lessors on assets that firms have leased, to employees in the form of pension fund and health care benefits yet to be paid, and to the government in the form of taxes deferred. In the last two decades, accountants have increasingly moved towards quantifying these liabilities and showing them as long term liabilities.\n\nltd_mv = vod              # market value of long term debt\ntmv = ltd_mv+ps_mv+cs_mv  # total market value \nprint('total market value: ${:,.2f}B'.format(tmv/1e9))\n\ntotal market value: $12.31B\n\n\nCost of Common Equity, cce\nThe expected excess return a hypothetical average investor would require of a diversified portfolio of stock (assumed beta = 1.0) over the yield on the 10-year Treasury Bond. The annual rate of return that an investor expects to earn when investing in shares of a company is known as the cost of common equity. It includes dividends and increases in the market value.\n\ncce = tby+beta*eq_rp      # cost of common equity or the expected return for the stock\nprint('cost of common equity: {:,.2f}%'.format(cce*100))\n\ncost of common equity: 7.13%\n\n\nLong Term Debt, Average Yield, ltd_ay\nThe total cost of long term debt.\n\nltd_ay = tby+bystt        # long term debt average yield\nprint('long term debt average yield: {:,.2f}%'.format(ltd_ay*100))\n\nlong term debt average yield: 8.09%\n\n\nLong Term Debt, After Tax Yield, ltd_aty\nThe tax benefits of long term debt. Interest payments are tax deductible for the company.\n\nltd_aty = ltd_ay*(1-tr)   # long term debt after tax yield\nprint('long term debt after tax yield: {:,.2f}%'.format(ltd_aty*100))\n\nltd_pc = vod/tmv          # weight for long term debt \nltd_ate = ltd_aty*ltd_pc  # after tax effect of long term debt \nps_ay = psy               # preferred stock, average yield \nps_aty = ps_ay            # preferred stock, average yield \nprint('preferred stock, average yield: {:,.2f}%'.format(ps_aty*100))\n\nps_pc = ps_mv/tmv         # preferred stock, % capital \nps_ate = ps_aty*ps_pc     # preferred stock, after tax effect \ncs_ay = cce               # common stock, average yield \ncs_aty = cce              # common stock, after tax yield \nprint('common stock, after tax yield: {:,.2f}%'.format(cs_aty*100))\n\ncs_pc = cs_mv/tmv         # common stock, % capital \ncs_ate = cs_aty*cs_pc     # common stock, after tax effect \nprint('common stock, after tax effet: {:,.2f}%'.format(cs_ate*100))\n\ntate = ltd_ate+ps_ate+cs_ate # total after tax effect \nprint('total after tax effect: {:,.2f}%'.format(tate*100))\ntpc = ltd_pc+ps_pc+cs_pc     # total % Capital\nprint('total % Capital: {:,.2f}%'.format(tpc*100))\n\nlong term debt after tax yield: 8.09%\npreferred stock, average yield: 5.21%\ncommon stock, after tax yield: 7.13%\ncommon stock, after tax effet: 4.61%\ntotal after tax effect: 7.43%\ntotal % Capital: 100.00%\n\n\nWeighted average cost of capital\nA company’s weighted average cost of capital (WACC) is the weighted average of the company’s current cost of debt and equity calculated by using current debt, preferred stock and common stock market values. The WACC of the company, calculated after tax, is the discount rate used in the DCF valuation procedures. The WACC, which is the cost of the different components of financing used by the firm, weighted by their market value proportions. These include debt, preferred stock, and common stock.\nWACC: Weighted Average Cost of Capital, the rate used to discount cash flows, based on the following three factors. 1. Base rate of return. 2. Expected return based on debt and preferred stock. 3. Expected return on common stock and Beta.\nAll adjusted for the tax advantage of interest payments and the percentage of debt, preferred stock and common stock.\n\nwacc = tate\nprint('weighted average cost of capital: {:.1f}%'.format(wacc*100))\n\nweighted average cost of capital: 7.4%\n\n\n\n\nFuture cash flows \nThe future cash flows to the firm are projected based on revenue growth. The cash flows are then discounted using the WACC and the ISV is calculated.\n\n# make a list of the fiscal years in excess return period \nfy = np.zeros(erp+1)\nfy[0] = fy_start\nfor i in range(1,erp+1): \n    fy[i]=fy_start+i\n\nrev = np.zeros(len(fy))\nciwc = np.zeros(len(fy))\nrev[0] = rev_start  #*rgr+rev_start   # find the future revenue using constant revenue growth rate \n\nfor i in range(1,len(fy)): \n    rev[i] = rev[i-1]*rgr+rev[i-1]  # find the future revenue \n    ciwc[i] = (rev[i]-rev[i-1])*wcr  # find the change in working capital \n\nnet_op = np.zeros(len(fy)) # net operating profit\nadj_taxes = np.zeros(len(fy))\nnopat = np.zeros(len(fy))\ninvest = np.zeros(len(fy))\ndepre = np.zeros(len(fy))\nnet_invest = np.zeros(len(fy))\nfcff = np.zeros(len(fy))\ndisc_fact = np.zeros(len(fy))\ndisc_fcff = np.zeros(len(fy))                \n\n# calculate values in table \nfor i in range(1,len(fy)):\n    net_op[i] = rev[i]*nopm # net operating profit margin\n    adj_taxes[i] = net_op[i]*tr # net operating profit adjusted for taxes\n    nopat[i] = net_op[i]-adj_taxes[i] # after tax net operating profit\n    invest[i] = rev[i]*ir # future investments\n    depre[i] = rev[i]*dr # future depreciations\n    net_invest[i] = invest[i]-depre[i] # net investments\n    fcff[i] = nopat[i]-net_invest[i]-ciwc[i] # free cash flow to the firm\n    disc_fact[i] = 1/((1+wacc)**i) # discount factor\n    disc_fcff[i] = disc_fact[i]*fcff[i] # discounted free cash flow to the firm\n    \ndcrv = nopat[-1]/wacc*disc_fact[-1] # discounted corporate residual value\nderp_fcff = disc_fcff.sum() # discounted excess return period FCFF\n\ntcv = derp_fcff+dcrv+sta # total corporate value\ntvce = tcv-vod-vps-stl # total value of common equity\nisv = tvce/so # intrinsic stock value\n\n# print cash flows in a table\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format('Year','Rev','NOP','AdjTaxes',\n    'NOPAT','Invest.','Deprec.','dInvest.','dWC','FCFF','DF','DF*FCFF'))\nfor i in range(len(fy)):\n    print('{:4.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.4f}{:10,.0f}'.format(fy[i],\n        rev[i]/1e6,net_op[i]/1e6,adj_taxes[i]/1e6,nopat[i]/1e6,invest[i]/1e6,depre[i]/1e6,(invest[i]-depre[i])/1e6,ciwc[i]/1e6,\n        fcff[i]/1e6,disc_fact[i],disc_fcff[i]/1e6))\n\nYear       Rev       NOP  AdjTaxes     NOPAT   Invest.   Deprec.  dInvest.       dWC      FCFF        DF   DF*FCFF\n2022     1,074         0         0         0         0         0         0         0         0    0.0000         0\n2023     1,118       698         0       698       540       314       226         0       472    0.9308       439\n2024     1,163       726         0       726       562       327       235         0       491    0.8664       425\n2025     1,210       755         0       755       584       340       244         0       510    0.8064       412\n2026     1,258       785         0       785       608       353       254         0       531    0.7506       399\n2027     1,309       817         0       817       632       368       265         0       552    0.6987       386\n2028     1,362       850         0       850       658       382       275         0       575    0.6503       374\n2029     1,417       884         0       884       684       398       286         0       598    0.6053       362\n2030     1,474       920         0       920       712       414       298         0       622    0.5634       350\n2031     1,533       957         0       957       740       430       310         0       647    0.5244       339\n2032     1,595       995         0       995       770       448       322         0       673    0.4881       328\n\n\n\n# Intrinsic Value\nprint('discounted excess return period FCFF: ${:,.2f}B'.format(derp_fcff/1e9))\nprint('discounted corporate residual value: ${:,.2f}B'.format(dcrv/1e9))\nprint('total corporate value: ${:,.2f}B'.format(tcv/1e9))\nprint('total value of common equity: ${:,.2f}B'.format(tvce/1e9))\ntvce_baseline = tvce # save value as baseline case\nisv_baseline = isv # save the isv for the baseline case\nprint('intrinsic stock value, baseline case: ${:,.2f}'.format(isv_baseline))\nprint('current stock price: ${:,.2f}'.format(csp))\n\ndiscounted excess return period FCFF: $3.81B\ndiscounted corporate residual value: $6.53B\ntotal corporate value: $10.63B\ntotal value of common equity: $5.45B\nintrinsic stock value, baseline case: $66.99\ncurrent stock price: $97.74\n\n\nObservation:\nThe base line DCF analysis produces an intrinsic stock value of \\$105. Some adjustments will be made in the scenario 1 case.\nThe calculations used here can be verified by using the Valuepro web site, which calculates ISV based on the same method (not working as of 2/5/2022).\n\n\nList of all inputs to the DCF model\nThe following print statements format the inputs to the model similar to how they are presented on the Valuepro page.\n\nprint('{:&gt;35s} {:&lt;10.0f} {:&gt;35s} {:,.3f}'.format('Excess return period, years:',erp,'Depreciation rate, %:',dr*100))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.3f}'.format('Starting revenues, $B:',\n    rev_start/1e9,'Investment rate, %:',ir*100))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:,.3f}'.format('Revenue growth rate, %:',\n    rgr*100,'Working capital rate, %:',wcr*100))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:,.3f}'.format('Net operating profit margin, %:',\n    nopm*100,'Current assets, $B:',sta/1e9))\nprint('{:&gt;35s} {:&lt;10,.3f} {:&gt;35s} {:.3f}'.format('Tax rate, %:',\n    tr*100,'Current liabilities, $B:',stl/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.2f}'.format('Current stock price, $:',\n    csp,'Equity risk premium, %:',eq_rp*100))\nprint('{:&gt;35s} {:&lt;10,.0f} {:&gt;35s} {:,.2f}'.format('Shares outstanding, basic, M:',\n    so/1e6,'Company specific beta:',beta))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:.3f}'.format('10 year treasury bond yield, %:',\n    tby*100,'Total long term debt and other, $B:',vod/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f} {:&gt;35s} {:,.3f}'.format('Bond yield spread to treasury, %:',\n    bystt*100,'Value of preferred stock, $B:',vps/1e9))\nprint('{:&gt;35s} {:&lt;10,.2f}'.format('Preferred stock yield, %:',psy*100))\n\n       Excess return period, years: 10                       Depreciation rate, %: 28.081\n             Starting revenues, $B: 1.07                       Investment rate, %: 48.295\n            Revenue growth rate, %: 4.029                 Working capital rate, %: 0.000\n    Net operating profit margin, %: 62.407                     Current assets, $B: 0.283\n                       Tax rate, %: 0.000                 Current liabilities, $B: 0.820\n            Current stock price, $: 97.74                  Equity risk premium, %: 3.00\n      Shares outstanding, basic, M: 81                      Company specific beta: 1.22\n    10 year treasury bond yield, %: 3.48       Total long term debt and other, $B: 4.202\n  Bond yield spread to treasury, %: 4.61             Value of preferred stock, $B: 0.160\n          Preferred stock yield, %: 5.21      \n\n\n\n# weighted average cost of capital inputs\nprint('Weighted Average Cost of Capital')\nprint('Cost of common equity')\nprint('{:s}'.format('-'*37))\nprint('{:&gt;32s} {:,.2f}'.format('10 year treasury bond yield, %:',tby*100))\nprint('{:&gt;32s} {:,.2f}'.format('Company specific beta:',beta))\nprint('{:&gt;32s} {:,.2f}'.format('Equity risk premium, %:',eq_rp*100))\nprint('{:s}'.format('-'*37))\nprint('{:&gt;32s} {:,.2f}'.format('Cost of common equity, %:',cce*100))\nprint()\n\nprint('Market Capitalization and After-Tax Weighted Average Cost of Capital')\nprint()\nprint('{:s}{:^10s}{:^10s}{:^10s}{:^15s}{:^15s}'.format(' '*20,'Current','After-Tax','Market','%','Weighted After-'))\nprint('{:s}{:^10s}{:^10s}{:^10s}{:^15s}{:^15s}'.format(' '*20,'Yield','Yield','Value','Capitalization','Tax Yield'))\n\nprint('{:s}'.format('-'*80))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Long term debt',\n    ltd_ay*100,(tby+eq_rp)*(1-tr)*100,vod/1e9,ltd_pc*100,ltd_ate*100))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Preferred stock',\n     psy*100,ps_ate*100,vps/1e9,ps_pc*100,ps_ate*100))\nprint('{:&lt;15s}{:&gt;12.2f}{:&gt;10.2f}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('Common stock',\n     cs_ay*100,cs_aty*100,cs_mv/1e9,cs_pc*100,cs_aty*100))\nprint('{:s}'.format('-'*80))\nprint('{:&lt;37s}{:&gt;10,.0f}{:&gt;12.2f}{:&gt;15.2f}'.format('',tmv/1e9,tpc*100,wacc*100))\n\nWeighted Average Cost of Capital\nCost of common equity\n-------------------------------------\n 10 year treasury bond yield, %: 3.48\n          Company specific beta: 1.22\n         Equity risk premium, %: 3.00\n-------------------------------------\n       Cost of common equity, %: 7.13\n\nMarket Capitalization and After-Tax Weighted Average Cost of Capital\n\n                     Current  After-Tax   Market         %       Weighted After-\n                      Yield     Yield     Value   Capitalization    Tax Yield   \n--------------------------------------------------------------------------------\nLong term debt         8.09      6.48         4       34.12           2.76\nPreferred stock        5.21      0.07         0        1.30           0.07\nCommon stock           7.13      7.13         8       64.58           7.13\n--------------------------------------------------------------------------------\n                                             12      100.00           7.43"
  },
  {
    "objectID": "OLD FRT analysis.html#dcf-scenarios",
    "href": "OLD FRT analysis.html#dcf-scenarios",
    "title": "Two amplifier RIAA phono preamp",
    "section": "4) DCF Scenarios ",
    "text": "4) DCF Scenarios \nThe following adjustments were made to various model parameters. - excess return period was adjusted to a more conservative 5 years - revenue growth rate was adjusted to 5% to reflect pre Covid growth. (base case = 3.037 %) - net operating profit margin was adjusted to 60% (base case = 62.820%) - tax rate was adjusted to 0% (base case = 0%) - depreciation rate was adjusted to 27% (base case = 27.492%) - investment rate was adjust to 53% (base case = 53.184%) - working capital rate was set to an even 0% (base case = 0%) - weighted average cost of capital was adjusted up by 2% to reflect higher interest rates and provide a margin of safety (base case = 3.8%)\n\nprint('adjusted DCF input values and rates')\nerp = 5\nprint('excess return period: {:,.0f} years'.format(erp))\nrgr = 5/100 # setting growth rate to 5%, since this is more in line with Covid grown \nprint('revenue growth rate: {:,.1f}%'.format(rgr*100))\nnopm = isv_s1_nopm = 60/100\nprint('net operating profit margin: {:.2f}%'.format(nopm*100))\ntr = isv_s1_tr =0/100\nprint('tax rate: {:.2f}%'.format(tr*100))\ndr = 27/100\nprint('depreciation rate: {:,.2f}%'.format(dr*100))\nir = 53/100              # investment rate (% of revenue)\nprint('investment rate: {:,.2f}%'.format(ir*100))\nwcr = 0/100\nprint('working capital rate: {:,.1f}%'.format(wcr*100))\nwacc_adj = (wacc+0.02) # weighted average cost of capital, increased by 2%\n#wacc_adj = 1/100\nprint('weighted average cost of capital: {:.1f}%'.format(wacc_adj*100))\n\nadjusted DCF input values and rates\nexcess return period: 5 years\nrevenue growth rate: 5.0%\nnet operating profit margin: 60.00%\ntax rate: 0.00%\ndepreciation rate: 27.00%\ninvestment rate: 53.00%\nworking capital rate: 0.0%\nweighted average cost of capital: 9.4%\n\n\n\n# make a list of the fiscal years in excess return period \nfy = np.zeros(erp+1)\nfy[0] = fy_start\nfor i in range(1,erp+1): \n    fy[i]=fy_start+i\n\nrev = np.zeros(len(fy))\nciwc = np.zeros(len(fy))\nrev[0] = rev_start  #*rgr+rev_start   # find the future revenue using constant revenue growth rate \n\nfor i in range(1,len(fy)): \n    rev[i] = rev[i-1]*rgr+rev[i-1]  # find the future revenue \n    ciwc[i] = (rev[i]-rev[i-1])*wcr  # find the change in working capital \n\nnet_op = np.zeros(len(fy))\nadj_taxes = np.zeros(len(fy))\nnopat = np.zeros(len(fy))\ninvest = np.zeros(len(fy))\ndepre = np.zeros(len(fy))\nnet_invest = np.zeros(len(fy))\nfcff = np.zeros(len(fy))\ndisc_fact = np.zeros(len(fy))\ndisc_fcff = np.zeros(len(fy))                \n\n# calculate values in table \nfor i in range(1,len(fy)): \n    net_op[i] = rev[i]*nopm # net operating profit\n    adj_taxes[i] = net_op[i]*tr # net operating profit adjusted for taxes\n    nopat[i] = net_op[i]-adj_taxes[i] # after tax net operating profit\n    invest[i] = rev[i]*ir # future investments\n    depre[i] = rev[i]*dr # future depreciations\n    net_invest[i] = invest[i]-depre[i] # net investments\n    fcff[i] = nopat[i]-net_invest[i]-ciwc[i] # free cash flow to the firm\n    disc_fact[i] = 1/((1+wacc_adj)**i) # discount factor\n    disc_fcff[i] = disc_fact[i]*fcff[i] # discounted free cash flow to the firm\n    \ndcrv = nopat[-1]/wacc*disc_fact[-1] # discounted corporate residual value\nderp_fcff = disc_fcff.sum() # discounted excess return period FCFF\n\ntcv = derp_fcff+dcrv+sta # total corporate value\ntvce = tcv-vod-vps-stl # total value of common equity\nisv = tvce/so # intrinsic stock value\n\n# print cash flows in a table\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format(\n    'Year','Rev','NOP','AdjTaxes',\n    'NOPAT','Invest.','Deprec.','dInvest.','dWC','FCFF','DF','DF*FCFF'))\nfor i in range(len(fy)):\n    print('{:4.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.0f}{:10,.4f}{:10,.0f}'.\n        format(fy[i],rev[i]/1e6,net_op[i]/1e6,adj_taxes[i]/1e6,nopat[i]/1e6,invest[i]/1e6,depre[i]/1e6,\n        (invest[i]-depre[i])/1e6,ciwc[i]/1e6,fcff[i]/1e6,disc_fact[i],disc_fcff[i]/1e6))\n\nYear       Rev       NOP  AdjTaxes     NOPAT   Invest.   Deprec.  dInvest.       dWC      FCFF        DF   DF*FCFF\n2022     1,074         0         0         0         0         0         0         0         0    0.0000         0\n2023     1,128       677         0       677       598       305       293         0       384    0.9138       350\n2024     1,185       711         0       711       628       320       308         0       403    0.8350       336\n2025     1,244       746         0       746       659       336       323         0       423    0.7630       323\n2026     1,306       784         0       784       692       353       340         0       444    0.6972       310\n2027     1,371       823         0       823       727       370       357         0       466    0.6371       297\n\n\n\n# Intrinsic Value\nprint('discounted excess return period FCFF: ${:,.2f}B'.format(derp_fcff/1e9))\nprint('discounted corporate residual value: ${:,.2f}B'.format(dcrv/1e9))\nprint('total corporate value: ${:,.2f}B'.format(tcv/1e9))\nprint('total value of common equity: ${:,.2f}B'.format(tvce/1e9))\ntvce_S1 = tvce # save value as scenario 1\nisv_S1 = isv # save the isv for scenario 1 case\nprint('intrinsic stock value, scenario 1 case: ${:,.2f}'.format(isv_S1))\nprint('current stock price: ${:,.2f}'.format(csp))\n\ndiscounted excess return period FCFF: $1.62B\ndiscounted corporate residual value: $7.05B\ntotal corporate value: $8.95B\ntotal value of common equity: $3.77B\nintrinsic stock value, scenario 1 case: $46.32\ncurrent stock price: $97.74\n\n\n\nprint('ISV serario 1 to current stock price ratio: {:,.2f}%'.format(isv_S1/csp*100))\n\nISV serario 1 to current stock price ratio: 47.39%\n\n\nThe DCF model calculates with adjustments an intrinsic stock value of \\$90, which is less than the current stock price. Based on ISV serario 1, current price is overvalued by 28%. Adjusting the WACC to 1% would make the ISV about equal to the current stock price."
  },
  {
    "objectID": "OLD FRT analysis.html#naci-stock-selection-guide-analysis",
    "href": "OLD FRT analysis.html#naci-stock-selection-guide-analysis",
    "title": "Two amplifier RIAA phono preamp",
    "section": "5) NACI stock selection guide analysis ",
    "text": "5) NACI stock selection guide analysis \nThis analysis follows the NAIC stock selection guide (SSG) [2]. The SSG relates revenue growth, EPS and share price history and makes a prediction about the future share price.\nThe National Association of Investors Clubs (NAIC) is a nonprofit organization dedicated to educating individual investors and investment clubs to become successful lifelong investors. NAIC’s Stock Selection Guide (SSG) is used in the following cells to analyze the company’s growth and whether the stock is selling at a reasonable price.\nThe SSG was originally developed in the 1950s as a paper worksheet by the not-for-profit National Association of Investors Corporation (NAIC). The SSG aims to aid individual investors in the fundamental analysis and selection of common stocks by reviewing components of a company’s growth, quality, and value.\n\nLoad data from metrics sheet\n\n# column names: fiscal years \nfy_data = df_metrics_sheet.columns[1:].values.astype('datetime64[Y]')-1970\n# line 0: Net income\nnet_income_data = df_metrics_sheet.iloc[0].to_numpy()[1:].astype('float')\n# line 1: Shareholder equity\nshareholder_equity_data =  df_metrics_sheet.iloc[1].to_numpy()[1:].astype('float')\n# line 2: Total liabilities\ntotal_liabilities_data = df_metrics_sheet.iloc[2].to_numpy()[1:].astype('float')\n# line 3: Free cash flow, Net cash provided by operating activities \nfree_cash_flow_data =  df_metrics_sheet.iloc[3].to_numpy()[1:].astype('float')\n# line 4: Dividends\ndividends_data =  df_metrics_sheet.iloc[4].to_numpy()[1:].astype('float')\n# line 5: Total assets\ntotal_assets_data = df_metrics_sheet.iloc[5].to_numpy()[1:].astype('float')\n# line 6: Earnings per share\neps_data = df_metrics_sheet.iloc[6].to_numpy()[1:].astype('float')\n# line 7: Dividends per share  \ndps_data = df_metrics_sheet.iloc[7].to_numpy()[1:].astype('float')\n# line 8: Total tangible assets\ntotal_tangible_assets_data = df_metrics_sheet.iloc[8].to_numpy()[1:].astype('float')\n# line 9: Liabilities w/o deposits\nliabilities_wo_deposits_data = df_metrics_sheet.iloc[9].to_numpy()[1:].astype('float')\n# line 10: Provision for credit losses\nprovision_for_credit_losses_data = df_metrics_sheet.iloc[10].to_numpy()[1:].astype('float')\n# line 11: Short-term borrowings\nshort_term_borrowings_data = df_metrics_sheet.iloc[11].to_numpy()[1:].astype('float')\n# line 12: Preferred stock\npreferred_stock_data = df_metrics_sheet.iloc[12].to_numpy()[1:].astype('float')\n# line 13: Net cash used in investing activities \nnet_cash_used_in_investing_activities_data = df_metrics_sheet.iloc[13].to_numpy()[1:].astype('float')\n\n\n# make a new data frame to store data from metrics sheet\ndf_metrics_data = pd.DataFrame(data={\n    'FY':fy_data[::-1],\n    'net_income':net_income_data[::-1],\n    'shareholder_equity':shareholder_equity_data[::-1],\n    'total_liabilities':total_liabilities_data[::-1],\n    'free_cash_flow':free_cash_flow_data[::-1],\n    'dividends':dividends_data[::-1],\n    'total_assets':total_assets_data[::-1],\n    'eps':eps_data[::-1],    \n    'dps':dps_data[::-1],\n    'total_tangible_assets':total_tangible_assets_data[::-1],\n    'liabilities_wo_deposits':liabilities_wo_deposits_data[::-1],    \n    'provision_for_credit_losses':provision_for_credit_losses_data[::-1],\n    'short_term_borrowings':short_term_borrowings_data[::-1], \n    'preferred_stock':preferred_stock_data[::-1],\n    'net_cash_used_in_investing_activities':net_cash_used_in_investing_activities_data[::-1]\n    })\n\n#df_metrics_data\n\ncheck for matching years in both data frames\n\nif all(df_dcf_data['FY'] == df_metrics_data['FY']) != True:\n    print('error, years in data frame don\\'t match')\n    stop # this is not python code, so jupyterlab will throw an error\nelse:\n    print('OK, years in data frame match')\n\nOK, years in data frame match\n\n\n\n\nNAIC section 1: Visual analysis\nHigh and low price history for each year\nFrom the daily price history obtained from yahoo finance, the high and low closing price for each is obtained and the data saved to the financial data frame as new columns.\n\n#column names: fiscal years \nyears_list = df_metrics_sheet.columns[1:].values.astype('str')[::-1]\n\n# convert years to datetime format\nyear_ended_list = []\nfor i in years_list:\n    year_ended_list.append(datetime.strptime(i, '%Y'))\n\n# make emnpy lists to store open, close, high and low price data for each fiscal year\nfy_open = []\nfy_close = []\nfy_high = []\nfy_low = []\n\nfor i in year_ended_list:\n    start = i\n    end = i + relativedelta(years=1)\n    p1 = df_price_history.truncate(before=start, after=end)\n    if len(p1) == 0:\n        fy_open.append(np.nan)\n        fy_close.append(np.nan)        \n        fy_high.append(np.nan)\n        fy_low.append(np.nan)\n    else:\n        fy_open.append(p1['Open'].iloc[0])\n        fy_close.append(p1['Close'].iloc[-1])        \n        fy_high.append(p1['Close'].max())\n        fy_low.append(p1['Close'].min())\n\n# convert from list to numpy array\nfy_open = np.asarray(fy_open)\nfy_close = np.asarray(fy_close)\nfy_high = np.asarray(fy_high)\nfy_low = np.asarray(fy_low)\n\nPlotting the data\nThe annual sales, EPS and the high and low share price is plotted on a semilog plot. A consistent percentage change in the data will plot on the semi-log chart as a straight line.\nThe stock price is plotted separately from the sales and earnings for clarity.\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\n# figsize() function to adjust the size\nplt.subplots(figsize=(15, 5))\n\n# using subplot function and creating\n# plot one\nplt.subplot(1, 2, 1)\nwidth = 3  # the width of the bars\n#plt.bar(year_ended_list,fy_high-fy_low, width,bottom=fy_low,label='price')\nj = 0\nfor i in year_ended_list:\n    color = 'green'\n    if fy_open[j] &gt; fy_close[j]: color= 'red'\n    # high/low lines\n    plt.plot([i,i],[fy_low[j],fy_high[j]],color=color, linewidth=width)\n    # open marker\n    plt.plot([i,i-relativedelta(months=1)], [fy_open[j],fy_open[j]], color=color, linewidth=width)\n    # close marker\n    plt.plot([i,i+relativedelta(months=1)], [fy_close[j],fy_close[j]], color=color, linewidth=width)\n    j += 1\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\n#plt.ylim((20,80))\nplt.title('Yearly stock high and low price range')\nplt.ylabel('stock price, $')\n#plt.legend()\nplt.grid()\n\n# using subplot function and creating plot two\nplt.subplot(1, 2, 2)\n\nplt.plot(df_metrics_data['FY'],df_dcf_data['revenue']/1e9,'+-',label='revenue, $B')\nplt.plot(df_metrics_data['FY'],df_metrics_data['eps'],'+-',label='EPS, $')\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\n#plt.yscale('log')\n#plt.yticks([0.1,1,10,100,1000,10000],['0.1','1','10','100','1000','10000'])\n#plt.ylim((0.1,1000))\nplt.title('Revenue and EPS')\nplt.ylabel('Revenue and EPS')\nplt.legend()\n\nplt.grid()\n\n# space between the plots\nplt.tight_layout(4)\n\n# show plot\nplt.show()\n\n\n\n\nObservation:\nShare price has been usually trading in the 80 to 140 dollar range the last 5 years, indicating that the market does not see FRT as a growth company. EPS have been eratic over the last 5 years.\n\n\nNAIC section 3, Price earnings history\nSection 3 of the SSG is the Price-Earnings history. The following table is built from the high and low prices each year and the earnings per share. The high and low Price/Earnings ratios are calculated for each year and are listed in the columns labeled h-per and l-per.\n\nprint('{:4s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}{:&gt;10s}'.format('year','high','low','eps',\n    'h-per','l-per'))\nfor i in range(len(year_ended_list)):\n    print('{:s}{:10,.2f}{:10,.2f}{:10,.2f}{:10,.2f}{:10,.2f}'.format(year_ended_list[i].strftime(\"%Y\"),\n        fy_high[i], fy_low[i],df_metrics_data['eps'][i],\n        fy_high[i]/df_metrics_data['eps'][i],\n        fy_low[i]/df_metrics_data['eps'][i]))\n\nyear      high       low       eps     h-per     l-per\n2010     84.32     63.07      1.99     42.37     31.69\n2011     92.45     75.31      2.29     40.37     32.89\n2012    110.03     89.23      2.36     46.62     37.81\n2013    117.96     96.21      2.47     47.76     38.95\n2014    137.18    100.90      2.42     56.69     41.69\n2015    150.27    124.96      3.04     49.43     41.11\n2016    170.35    136.98      3.51     48.53     39.03\n2017    145.29    120.52      3.97     36.60     30.36\n2018    135.55    108.11      3.18     42.63     34.00\n2019    141.16    115.81      4.61     30.62     25.12\n2020    131.07     65.81      1.62     80.91     40.62\n2021    137.12     82.27      3.26     42.06     25.24\n2022    139.37     87.91      4.71     29.59     18.66\n\n\nAverage high and P/E for select years\nThe average price to earning ratio based on high and low stock prices is calculated.\n\n#Average high P/E for years \npe_avg_high = (fy_high/df_metrics_data['eps']).mean()\nprint('average high P/E {:.2f}'.format(pe_avg_high))\n#Average low P/E for years \npe_avg_low = (fy_low/df_metrics_data['eps']).mean()\nprint('average low P/E {:.2f}'.format(pe_avg_low))\n\naverage high P/E 45.71\naverage low P/E 33.63\n\n\n\nEstimate future EPS\nUse polyfit to get EPS slope and intercept of a least square fit.\n\ny = df_metrics_data['eps']\nx = np.arange(len(y))\nm, c = np.polyfit(x, y, 1)\nprint('EPS slope: {:.2f}'.format(m))\nprint('EPS intercept: {:.2f}'.format(c))\nlstsq_fit = m*x + c  # data points for each year\n\nEPS slope: 0.15\nEPS intercept: 2.14\n\n\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('EPS')\n\nax1.plot(df_metrics_data['FY'],df_metrics_data['eps'], 'o',label='EPS')\nax1.plot(df_metrics_data['FY'],lstsq_fit, '-',label='least squares fit')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,4))\nax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('EPS and least squares fit')\nplt.show()\n\n\n\n\nUsing the equation for the best fit line, find the y value for the eps point at five years in the future.\n\n# estimated eps in 5 years\neps_5yr_est = m*(x[-1]+5) + c\nprint('estimated eps in 5 years: {:.1f}'.format(eps_5yr_est))\n\nestimated eps in 5 years: 4.7\n\n\nUsing the high and low price to earning ratio from above and the projected eps, calculate the range of stock price in five years.\n\nnaic_price_eps_low = eps_5yr_est*pe_avg_low\nnaic_price_eps_high = eps_5yr_est*pe_avg_high\nprint('estimated price range in 5 years: ${:.2f} to ${:.2f}'.format(naic_price_eps_low,naic_price_eps_high))\n\nestimated price range in 5 years: $157.04 to $213.44\n\n\nThis is the estimated price range of the stock based on projected EPS and is a guide for what the stock price might be if conditions remain the same. Since the slope of the EPS history is negative, the projected stock price is negative.\n\n\nNAIC section 3: 5 year estimated EPS, preferred method\nSee page 87 and figure 10-1, Need the following data:\n- estimate sales in 5 years based on sales growth - NOPM - Tax rate - shares outstanding\nNet Operating Profit should reflect the future revenue generating ability and expense requirements of the operating business that comprise the ongoing operations of the company.\n\\(\\text{NOPM} = \\frac{\\text{Revenue} - \\text{Expenses}}{\\text{Revenue}}\\)\nTax payments are taken from the consolidated income statement, provision for income taxes. The effect of taxes on profits is accounted for.\n\\(\\text{Tax rate} = \\frac{\\text{Income taxes}}{\\text{Income before income taxes}}\\)\nTo get future EPS\n\\(\\text{future EPS} = \\frac {\\text{future revenue} \\times \\text{NOPM} \\times \\text{(1-tax rate)}}{\\text{number of shares}}\\)\nUse polyfit to get revenue least square fit\n\ny = df_dcf_data['revenue']/1e6\nx = np.arange(len(y))\nm, c = np.polyfit(x, y, 1)\nprint('revenue slope: {:.2f}'.format(m))\nprint('revenue intercept: {:.2f}'.format(c))\nlstsq_fit = m*x + c  # data points for each year\n\nrevenue slope: 41.61\nrevenue intercept: 530.32\n\n\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('dollars, $M')\n\nax1.plot(df_metrics_data['FY'],df_dcf_data['revenue']/1e6, 'o',label='revenue')\nax1.plot(df_metrics_data['FY'],lstsq_fit, '-',label='least squares fit')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,4))\nax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Revenue and least squares fit')\nplt.show()\n\n\n\n\nUsing the equation for the best fit line, find the y value for the EPS point at five years in the future.\n\n# estimated revenue in 5 years\nrev_5yr_est = m*(x[-1]+5) + c\nprint('estimated rev in 5 years: ${:,.1f}M'.format(rev_5yr_est))\n\nestimated rev in 5 years: $1,237.6M\n\n\nNote: might need to include estimate of number of shares outstanding in 5 years.\n\nprint('starting revenues: ${:,.2f}'.format(rev_start/1e9))\n\nstarting revenues: $1.07\n\n\nUsing the adjusted NOPM and tax rate from scenario 1.\nadjusted DCF input values and rates\n\npm_nopm = isv_s1_nopm # use nopm from scenario 1\npm_tax_rate = isv_s1_tr # use tr from scenario 1\npm_eps_5yr_est = rev_5yr_est*pm_nopm*(1-pm_tax_rate)*1e6/df_dcf_data['shares_outstanding'].iloc[-1] \n#pm_eps_5yr_est = rev_5yr_est*nopm_avg*1e6/df_dcf_data['shares_outstanding'].iloc[-1] \nprint('using preferred method: estimated eps in 5 years: ${:.2f}'.format(pm_eps_5yr_est))\n\nusing preferred method: estimated eps in 5 years: $9.13\n\n\nUsing the high and low price to earning ratio from above and the projected EPS, calculate the range of stock price in five years.\n\nnaic_price_pm_low = pm_eps_5yr_est*pe_avg_low\nnaic_price_pm_high = pm_eps_5yr_est*pe_avg_high\nprint('estimated price range in 5 years from preferred method: {:.2f} to {:.2f}'.format(\n    naic_price_pm_low,naic_price_pm_high))\n\nestimated price range in 5 years from preferred method: 306.95 to 417.19\n\n\nObservation:\nBased on revenue growth, the projected stock price is higher than the current price. However, based on price history, the stock is not expected to appreciate."
  },
  {
    "objectID": "OLD FRT analysis.html#future-stock-price",
    "href": "OLD FRT analysis.html#future-stock-price",
    "title": "Two amplifier RIAA phono preamp",
    "section": "6) Future stock price ",
    "text": "6) Future stock price \nThe projected future stock price is estimated from the results shown in this notebook based on DCF intrinsic stock value, the NAIC method or a combination of both. The DCF method does not consider market sentiment or popularity of the stock, whereas the NAIC method looks at the PE and EPS to develop the historical consensus that the market has put on the price of the stock. Both the NAIC and the DCF valuation should be considered. The DCF valuation is of the current ISV which is used as an indication of the future value, since it is assumed that the market price will converge eventually to the intrinsic value.\nThe estimated future stock price considers the following:\n- base case ISV - Senario ISV - NAIC EPS growth - NAIC preferred method\nUsing 5 year NAIC as a conservative estimate for the 10 year value and the analysis results, a judgment call is made concerning the price to put on the future value of the stock.\n\nprint('estimated price range in 5 years from EPS: ${:.2f} to ${:.2f}'.format(naic_price_eps_low,naic_price_eps_high))\nprint('estimated price range in 5 years from preferred method: ${:.2f} to ${:.2f}'.format(\n    naic_price_pm_low,naic_price_pm_high))\n\nprint('intrinsic stock value, baseline case: ${:,.2f}'.format(isv_baseline))\nprint('intrinsic stock value, scenario 1 case: ${:,.2f}'.format(isv_S1))\n\nprint('current stock price: ${:,.2f}'.format(csp))\n\nestimated price range in 5 years from EPS: $157.04 to $213.44\nestimated price range in 5 years from preferred method: $306.95 to $417.19\nintrinsic stock value, baseline case: $66.99\nintrinsic stock value, scenario 1 case: $46.32\ncurrent stock price: $97.74\n\n\nThe estimated price range in 5 years from the preferred method is \\$86.14 to \\$116.85. Taking the average and using that value on the IRR calculations.\nUsing the average of:\n- low estimated price from EPS and the low - estimated price from the preferred method - intrinsic stock value, scenario 1 case\nuse average of NAIC low price\n\n#fsp = (naic_price_eps_low + naic_price_pm_low + csp)/3 # estimated future stock price\nfsp = (naic_price_eps_low) # estimated future stock price\nprint('estimated future stock price: ${:,.2f}'.format(fsp))\n\nestimated future stock price: $157.04"
  },
  {
    "objectID": "OLD FRT analysis.html#dividend-payout",
    "href": "OLD FRT analysis.html#dividend-payout",
    "title": "Two amplifier RIAA phono preamp",
    "section": "7) Dividend payout ",
    "text": "7) Dividend payout \nThe dividend payout examines the amount shareholders are getting from the company relative to earnings or revenue. It is an important metric to determine how the business is operating and whether it has enough growth potential.\n\nDividend history\n\n# calculate the percent change in dividends\npcd = np.zeros(len(df_metrics_data['dps'])) # percent change in dividend\nfor i in range(len(df_metrics_data['dps'][0:-1])):\n    pcd[i+1] = ((df_metrics_data['dps'][i+1] - df_metrics_data['dps'][i])/\n                df_metrics_data['dps'][i+1])*100\n\nwidth = 100\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Dividend per share, $')\n\n# plot revenue as single bar\nplt.bar(df_metrics_data['FY'],df_metrics_data['dps'], width,color='k')\n\nax1.tick_params(axis='y')\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\nax2.plot(year_ended_list,pcd,'+-g')\n    \nax2.set_ylabel('% Change in dividend',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,20))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Dividend history per share')\nplt.show()\n\n\n\n\n\nadgr = pcd[-6:].mean() #last 6 years\nprint('average dividend growth rate: {:.2f}%'.format(adgr))\n\naverage dividend growth rate: 2.00%\n\n\n\n\nDividend yield\nDividend yield equals the annual dividend per share divided by the stock’s price per share. The plot below shows the history of dividend yield over the evaluation period.\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nwidth = 50  # the width of the bars\nplt.bar(df_metrics_data['FY'],(df_metrics_data['dps']/fy_high-df_metrics_data['dps']/fy_low)*100, \n        width,bottom=df_metrics_data['dps']/fy_low*100,label='yield')\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.ylim((1,10))\nplt.title('Range of dividend yield each year')\nplt.ylabel('dividend yield, %')\n#plt.legend()\nplt.grid()\n\n# show plot\nplt.show()\n\n\n\n\nThe dividend yield for the past five years has been in the 2.5 to 3.5 percent range.\n\n\nDividend payout ratio\nThe dividend payout ratio is a relative measure of how much the company is paying to shareholders in dividends compared to other metrics such as revenue, earnings or cash flow. The dividend payout ratio is plotted as a ratio of dividends to net income, free cash flow (Net cash provided by operating activities) and NOP. The payout ratio is useful for assessing a dividend’s sustainability. Companies are extremely reluctant to cut dividends since it can drive the stock price down and reflect poorly on management’s abilities.\nPayout ratio using net income\nPayout ratio using net income plots the ratio of dividend payout divided by net income:\n\\(\\frac {\\text{Dividends}}{\\text{Net income}}\\)\nDepending on how net income is listed in the financial statements, it may include large other charges.\nPayout ratio using cash flow\nPayout ratio using net cash flow plots the ratio of dividend payout divided by cash flow:\n\\(\\frac {\\text{Dividends}}{\\text{cash flow}}\\)\nCash flow from operating activities usually includes a long list of items. Some insight might be obtained from this ratio. The trend should be consistent.\nPayout ratio using NOP\nPayout ratio using NOP plots the ratio of dividend payout divided by NOP:\n\\(\\frac {\\text{Dividends}}{\\text{NOP}}\\)\nNOP is calculated above and might be different from net income listed in the financial statements. This ratio should be the lowest numerically of the three plots.\n\nAdjustments for a REIT\nDividend payout ratio does not apply to a REIT because of the 90% payout requirement.\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('Payout ratio')\n\nax1.plot(df_metrics_data['FY'],df_metrics_data['dividends']/df_metrics_data['net_income'], '-+',\n    label='Payout ratio using net income')\nax1.plot(df_metrics_data['FY'],df_metrics_data['dividends']/df_metrics_data['free_cash_flow'], '-*',\n    label='Payout ratio using cash flow')\nax1.plot(df_metrics_data['FY'],df_metrics_data['dividends']/nop, '-+',\n    label='dividends/NOP')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,5))\nax1.legend()\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Payout ratio')\nplt.show()\n\n\n\n\n\n# average the last three years\nprint('Dividends are paid at {:.1f}% of net income'.format(\n    (df_metrics_data['dividends']/df_metrics_data['net_income'])[-3:].mean()*100))\nprint('Dividends are paid at {:.1f}% of cash flow'.format(\n    (df_metrics_data['dividends']/df_metrics_data['free_cash_flow'])[-3:].mean()*100))\nprint('Dividends are paid at {:.1f}% of NOP'.format((df_metrics_data['dividends']/nop)[-3:].mean()*100))\n\nDividends are paid at 150.5% of net income\nDividends are paid at 75.4% of cash flow\nDividends are paid at 58.0% of NOP\n\n\nPayout ratio using net income: Payout ratio using cash flow:\nPayout ratio using NOP:\n\n\n\nInternal Rate of Return (IRR) calculations\nThe internal rate of return (IRR) is the discount rate that makes the net present value (NPV) of all cash flows equal to zero in a discounted cash flow analysis. Generally speaking, the higher an internal rate of return, the more desirable an investment is to undertake.\nAs explained above, the stock price has not changed by much over the years, even though the revenue and dividends have been increasing. The final stock price is set equal to the current price.\nUsing the average dividend growth rate calculated above, a series of estimated future dividend payments are generated.\n\nfdp = np.zeros(len(df_metrics_data['dps'])) # future dividend payments\nfdp[0] = df_metrics_data['dps'].iat[-1]\nfor i in range(len(df_metrics_data['dps'][0:-1])):\n    fdp[i+1] = fdp[i]+fdp[i]*adgr/100\n\n\nprint('current stock price: ${:,.2f}'.format(csp))\n\n#fsp = 100 #csp #500 #(csp + 102.05 + 138.82)/3 # final stock price, $\nprint('final stock price: ${:,.2f}'.format(fsp))\n\ncurrent stock price: $97.74\nfinal stock price: $157.04\n\n\n\nest_cf = np.copy(fdp) # make a copy of the estimated cash flow\n\n# cash flows, initial purchase, dividend payments and final sale\nest_cf[0] = est_cf[0] - csp # subtract purchase price from the first dividend payment\nest_cf[-1] = est_cf[-1] + fsp # include the sale price with the final dividend payment\n\n\ndividend_irr = np.irr(est_cf)\nprint('Dividend IRR: {:.2f}%'.format(dividend_irr*100))\n\nDividend IRR: 8.57%\n\n\nAccording to global investment bank Goldman Sachs, 10-year stock market returns have averaged 9.2% over the past 140 years. and according to 10-Year Annualized Rolling Returns, the long term average is about 10%. However there are many years where the rolling 10 year average return is below 4%.\nThe calculated IRR is 8%, which is a decent return and significantly higher than current interest rates."
  },
  {
    "objectID": "OLD FRT analysis.html#management-performance",
    "href": "OLD FRT analysis.html#management-performance",
    "title": "Two amplifier RIAA phono preamp",
    "section": "8) Management performance ",
    "text": "8) Management performance \nThe following analysis somewhat follows the Warren Buffett strategy as outlined in [3]. This strategy is essentially value investing where companies are chosen that meet a set of criteria and who’s stock price is below the intrinsic value plus a margin of safety. These investments are usually held for the long term.\n\nFinancial metrics\nThe following analysis looks at financial ratios over the evaluation period. Financial ratios can be used to judge management performance. Consistent favorable trends are an indication that management is taking care of the company.\nTotal liabilities to total assets ratio\nDebt to equity and debt to NOP ratios\nFinancial ratios: RoE, RoA and PM\nNAIC section 2: Evaluating management\nNormalized data from consolidated statements\nMarket metrics\nOne dollar premise\nShare price vs EPS\nMarket capitalization\nQualitative metrics\nSimple and understandable business model\nFavorable long term prospects\nCommodity reliance\nConsistent operating history\nrationality:\n\nfocus on core aspects\nonly invest in high ROE businesses\nfocus on shareholder equity\n\n\n\nFinancial metrics \nThe following financial metrics are examined over the evaluation period. We are looking for favorable trends and evidence of consistent operations. Some red flags will also be evident in the plots.\nRed flags:\n- Shrinking gross profit margin - Receivables growing faster than sales - Rising debt-to-equity ratio - Several years of revenue trending down - Unsteady cash flow - Rising accounts receivable or inventory in relation to sales - Rising outstanding share count - Consistently higher liabilities than assets - Decreasing gross profit margin - Increasing revenue while cash flow remains the same - Unusual changes in key financial ratios\n\nTotal liabilities to total assets ratio\nThe ratio of liabilities to assets is plotted over the evaluation period. For most companies examined the liabilities are the total liabilities and the ratio is calculated using total assets and total tangible assets. Total tangible assets have goodwill and intangibles removed from the total. The ratio gives an indication of how much the company is worth versus how much the company owes. Ideally the ratio of liabilities to assets should be less than one.\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('ratio')\n\n# plot liabilities\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['total_assets'], '-+',\n    label='total liabilities to total assets')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['total_tangible_assets'], '-*',\n    label='total liabilities to total tangible assets')\n\nax1.tick_params(axis='y')\nax1.set_ylim((0,1))\nax1.legend(bbox_to_anchor=(1.8, 1))\nplt.grid()\n\n# instantiate a second y-axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:green'\n\n#ax2.plot(year_ended_list,pcd,'+-g')\nax2.plot(df_metrics_data['FY'],\n    (df_metrics_data['total_assets']-df_metrics_data['total_tangible_assets'])/df_metrics_data['total_assets']*100,\n    ':',color=color,label='intangible assets to total assets')\n    \nax2.set_ylabel('% intangible assets',color=color)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim((0,100))\nax2.legend(bbox_to_anchor=(1.7, 0))\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Total liabilities to total assets ratio')\nplt.show()\n\n\n\n\nFRT has no intangible assets, total assets are the same as total tangible assets. Total liabilities to total assets ratio is about 60%.\n\n\nDebt to equity and debt to NOP ratios\nThe debt-to-equity ratio (D/E) is another key characteristic Buffett considers carefully. Buffett prefers to see a small amount of debt so that earnings growth is being generated from shareholders’ equity as opposed to borrowed money. The D/E ratio is calculated as follows:\n\\(\\text{Debt-to-Equity Ratio} = \\frac {\\text{Total Liabilities}} {\\text{Shareholders' Equity}} \\text{  OR  } \\frac {\\text{Long term debt}} {\\text{Shareholders' Equity}}\\)\nThis ratio shows the proportion of equity and debt the company uses to finance its assets, and the higher the ratio, the more debt—rather than equity—is financing the company. A high debt level compared to equity can result in volatile earnings and large interest expenses. For a more stringent test, investors sometimes use only long-term debt instead of total liabilities in the calculation above.\nD/E is the traditional way to look at a company’s debt. Some rules of thumb say that the D/E should not be above 2 or 3. However the D/E company’s typically vary by industry. The ratio of LT debt to NOP gives the number of years it would take the company to pay back debt from NOP, the lower the number the shorter amount of time.\n\\(\\text{Debt-to-NOP Ratio} = \\frac {\\text{Total Liabilities}} {\\text{NOP}}\\)\n\ntangible_equity = df_metrics_data['total_tangible_assets'] - df_metrics_data['total_liabilities']\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('ratio')\n\nax1.plot(df_metrics_data['FY'],df_dcf_data['long_term_debt']/df_metrics_data['shareholder_equity'],\n    '-^',label='(LT debt)/Equity')\n#ax1.plot(year_ended_list,df_dcf_data['long_term_debt']/tangible_equity, '-',label='(LT debt)/(Tangible Equity)')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/df_metrics_data['shareholder_equity'],\n    '-*',label='(total liabilities)/Equity')\n#ax1.plot(year_ended_list,total_liabilities/BV, '-^',label='(total liabilities)/BV')\nax1.plot(df_metrics_data['FY'],df_metrics_data['total_liabilities']/nop, '-+',label='(total liabilities)/NOP')\n#ax1.plot(year_ended_list,total_liabilities/net_income, '-+',label='(total liabilities)/(net income)')\n#ax1.plot(year_ended_list,df_dcf_data['current_liabilities']/nop, '-*',label='(current liabilities)/NOP')\n#ax1.plot(year_ended_list,Liabilities_wo_deposits/nop, '-+',label='(Liabilities w/o deposits)/NOP')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,10))\n#ax1.legend()\nax1.legend(bbox_to_anchor=(1.6, 1))\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Various debt ratios')\nplt.show()\n\n\n\n\n(LT debt)/Equity is plotted and is below 2 for each year in the evaluation period. A threshold of 2 is traditionally the upper limit for a reasonable amount of debt that a company should carry.\n(total liabilities)/Equity is plotted and except for 2020 has been below the threshold of 2.\n(total liabilities)/NOP to is plotted for each year in the evaluation period is below 10. A value of 10 has been chosen as the threshold for this ratio and indicates how many years it would take the company to pay off total liabilities from the NOP generated each year. A threshold of ten seems like a reasonable level of debt measured against NOP.\n\n\nFinancial ratios\nVarious ratios can be used to judge management performance. Consistent favorable trends are an indication that management is taking care of the company.\nReturn on equity\nSometimes return on equity (RoE) is referred to as stockholder’s return on investment. It reveals the rate at which shareholders earn income on their shares. Buffett always looks at RoE to see whether a company has consistently performed well compared to other companies in the same industry. RoE is calculated as follows:\n\\(\\text{Return on Equity} = \\frac {\\text{Net Income}} {\\text{Shareholder's Equity}}\\)\nLooking at the RoE in just the last year isn’t enough. The investor should view the RoE from the past five to 10 years to analyze historical performance.\n\\(\\text{Shareholders’ Equity} = \\text{Total Assets} − \\text{Total Liabilities}\\)\nFor this company, this method of getting Shareholders’ Equity gives negative values. On the Consolidated Balance Sheets, there is a line for Total stockholders’ equity, which is used.\nReturn on Assets\nReturn on assets is a profitability ratio that provides how much profit a company is able to generate from its assets. In other words, return on assets (RoA) measures how efficient a company’s management is in generating earnings from their economic resources or assets on their balance sheet.\n\\(\\text{Return on assets} = \\frac {\\text{Net Income}} {\\text{Total Assets}}\\)\nCalculating the RoA of a company can be helpful in comparing a company’s profitability over multiple quarters and years as well as comparing to similar companies. However, it’s important to compare companies of similar size and industry.\nFor example, banks tend to have a large number of total assets on their books in the form of loans, cash, and investments. A large bank could easily have over \\$2 trillion in assets while putting up a net income that’s similar to companies in other industries. Although the bank’s net income or profit might be similar to an unrelated company and the bank might have high-quality assets, the bank’s RoA will be lower. The larger number of total assets must be divided into the net income, creating a lower RoA for the bank.\nSimilarly, auto manufacturing requires huge facilities and specialized equipment. A lucrative software company that sells downloadable programs online may generate the same net profits, but it could have a significantly higher RoA than its more asset-heavy counterparts. When utilizing this metric to compare productivity across businesses, it’s important to take into account what types of assets are required to function in a given industry, rather than simply comparing the figures.\nProfit Margin\nA company’s profitability depends not only on having a good profit margin, but also on consistently increasing it. This margin is calculated by dividing net income by net sales. For a good indication of historical profit margins, investors should look back at least five years. A high-profit margin indicates the company is executing its business well, but increasing margins mean management has been extremely efficient and successful at controlling expenses.\n\\(\\text{Profit Margin} = \\frac {\\text{Net Income}} {\\text{Revenue}}\\)\n\n# Set the locator\nlocator = mdates.YearLocator()  # every year\nfmt = mdates.DateFormatter('%Y')\n\nfig, ax1 = plt.subplots()\nax1.set_ylabel('percent')\n\nax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_metrics_data['shareholder_equity']*100,\n    '-+',label='RoE')\nax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_metrics_data['total_assets']*100,\n    '-*',label='RoA')\n#ax1.plot(df_metrics_data['FY'],total_liabilities/shareholder_equity, '-^',label='D/E')\nax1.plot(df_metrics_data['FY'],df_metrics_data['net_income']/df_dcf_data['revenue']*100,\n    '-^',label='Profit margin')\n\nax1.tick_params(axis='y')\n#ax1.set_ylim((0,14))\n#ax1.legend()\nax1.legend(bbox_to_anchor=(1.05, 1))\nplt.grid()\n\nX = plt.gca().xaxis\nX.set_major_locator(locator)\n# Specify formatter\nX.set_major_formatter(fmt)\nplt.gcf().autofmt_xdate()\n\nplt.title('Various ratios')\nplt.show()\n\n\n\n\nObservation:\nThe trends for RoE, RoA and profit margin are shown above. The effect of the acquisition of Celgene has caused these ratios to go negative in 2020. From 2016 to 2019, these ratios had wide variation year to year, with 2017 showing a decline followed by a large increase the following year. Normally steady performance is better than the erratic variation shown here.\n\n\n\nNAIC section 2: Evaluating management\nSee page 86, figure 9-1.\n- % pretax profit on sales, (net before taxes)/rev - % earned on equity (another way of saying RoE, using calculated equity)\nPercent earned on equity is a measure of financial performance calculated by dividing net income by equity. Because equity is equal to a company’s assets minus its debt, percent earned on equity is considered the return on net assets. Percent earned on equity is considered a gauge of a corporation’s profitability and how efficient it is in generating profits.\nThis section is not applicable to a REIT, since income before taxes and income tax are not considered since the values are low."
  },
  {
    "objectID": "OLD FRT analysis.html#decision-model",
    "href": "OLD FRT analysis.html#decision-model",
    "title": "Two amplifier RIAA phono preamp",
    "section": "9) Decision model ",
    "text": "9) Decision model \nThe decision model establishes thresholds that are to used in the purchase decision. There are three hard decision thresholds in this model which are:\n1. Intrinic value 2. Debt 3. Dividend payout ratio 4. Dividend IIR\nThe first threshold is based on the intrinisic value of the company as calculated by the DCF model semario 1. Reconizing that absolute intrinsic value is an elusive concept, judgement, justified by facts (assets, earnings, dividends, debt and cash flow), establishes the value by adjusting various rates, based on judgement and using a five year forward projection period. This should give a intrinsic value that is based on the historical data, modified by judgement.\nI’m using a threshold of the intrinsic value calculated in senario 1 (isv_S1) that is greater than 70% of the current stock price, provided that the NAIC valuation is above the current stock price. This accounts for the inadequacy or incorrectness of the data, the uncertainties of the future, and considers the behavior of the market.\nThe second threshold is the level of debt. The ratios of (LT debt)/Equity, (total liabilities)/Equity and (total liabilities)/NOP are ploted for the evaluation period. Over the evaluation period the (LT debt)/Equity and (total liabilities)/Equity should be less than 2 and stable. A threshold of 2 has been discussed in the litureture as a level of debt that a company can reasonably take on.\nThe thereshold for (total liabilities)/NOP is set at 10. This means that the company can pay off all the liabilities with tens years worth of NOP, which seems like a reasonable timeframe for an established and stable company.\nThe third threshold is the dividend payout ratio and is a relative measure of how much the company is paying to shareholders in dividends compared to the metrics of NOP and free cash flow (Net cash provided by operating activities). The payout ratio is useful for assessing a dividend’s sustainability. Payout ratio for a REIT is established by tax law and not used as an evaluation criteria. For other industries a threshold of 50% has been set as the limit.\nThe dividend IRR threshold is the internal rate of return for investor dividend cash flow (divident_irr) should be greater than 10 year treasury bond yield (tby) plus the equity risk premium (eq_rp). Otherwise, other investment operatunities should be looked at.\nIn the decision model there are soft thresholds based on judgement. Soft thresholds are a collection of ratios and analysis that taken together tell a story of the performance of the conmpany and manatgments ability to run the company and support dividends over the long term. Use judgement and make an evalaution.\nThe third critiera is a collection of ratios and analysis that taken together tell a story of the performance of the conmpany and manatgments ability to run the company and support dividends over the long term. Use judgement and make an evalaution. These are the following:\n1. Financial metrics 2. Market metrics 3. Qualitative metrics\nThe soft thresholds are discused in section 10.\n\nCheck DCF and NAIC value thresholds\n\n# check DCF senario 1\ndcf_score = isv_S1/csp #ratio of isv to csp\ndcf_threshold = 0.7\nif dcf_score &lt; 0.7:\n    print('FAIL, DCF score is less than {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\nelse:\n    print('PASS, DCF score is above {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\n\n# check NAIC\nnaic_score = np.array([naic_price_eps_low,naic_price_pm_low]).min()/csp\nnaic_threshold = 1\nif naic_score &lt; 1:\n    print('FAIL, NAIC score is less than {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\nelse:\n    print('PASS, NAIC score is above {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\n\n# check both scores\nif naic_score &lt; 1 or dcf_score &lt; 0.7:\n    print('One or both DCF and NAIC scores failed')\nelse:\n    print('Both DCF and NAIC scores pass')\n\nFAIL, DCF score is less than 0.7 at 0.5\nPASS, NAIC score is above 1.0 at 1.6\nOne or both DCF and NAIC scores failed\n\n\n\n\nCheck debt thresholds\n\ndebt_lookback = 4\navg_LT_debt2EQ = df_dcf_data['long_term_debt'][-debt_lookback:].mean()/df_metrics_data['shareholder_equity'][-debt_lookback:].mean()\navg_TLiability2EQ = df_metrics_data['total_liabilities'][-debt_lookback:].mean()/df_metrics_data['shareholder_equity'][-debt_lookback:].mean()\navg_TLiability2NOP = df_metrics_data['total_liabilities'][-debt_lookback:].mean()/nop[-debt_lookback:].mean()\n\nprint('long term debt to shareholder equity ratio = {:.2f}'.format(avg_LT_debt2EQ))\nprint('total liabilities to shareholder equity ratio = {:.2f}'.format(avg_TLiability2EQ))\nprint('total liabilities to NOP ratio = {:.2f}'.format(avg_TLiability2NOP))\n\nif (avg_LT_debt2EQ &gt; 2) or (avg_TLiability2EQ &gt; 2) or (avg_TLiability2NOP &gt; 10):\n    print('FAILED one of the debt threshold limits')\n\nlong term debt to shareholder equity ratio = 1.56\ntotal liabilities to shareholder equity ratio = 1.78\ntotal liabilities to NOP ratio = 7.97\n\n\n\n\nCheck dividend payout and IIR thresholds\n\n# check dividend payout ratio average the last three years\nprint('Dividends are paid at {:.1f}% of cash flow'.format(\n    (df_metrics_data['dividends']/df_metrics_data['free_cash_flow'])[-3:].mean()*100))\nprint('Dividends are paid at {:.1f}% of NOP'.format((df_metrics_data['dividends']/nop)[-3:].mean()*100))\n\nif ((df_metrics_data['dividends']/nop)[-3:].mean() or (df_metrics_data['dividends']/df_metrics_data['free_cash_flow'])[-3:].mean()) &gt; 0.5:\n    print('FAIL, dividend payout ration too high')\n\nDividends are paid at 75.4% of cash flow\nDividends are paid at 58.0% of NOP\nFAIL, dividend payout ration too high\n\n\n\n# Check dividend IRR limit\nif dividend_irr &lt; (tby+eq_rp):\n    print('FAIL, dividend IRR is less than {:.2f} at {:.2f}'.format((tby+eq_rp)*100,dividend_irr*100))\nelse:\n    print('PASS, dividend IRR is above {:.2f} at {:.2f}'.format((tby+eq_rp)*100,dividend_irr*100))\n\nPASS, dividend IRR is above 6.48 at 8.57\n\n\n\n# check DCF senario 1\ndcf_score = isv_S1/csp #ratio of isv to csp\ndcf_threshold = 0.7\nif dcf_score &lt; 0.7:\n    print('FAIL, DCF score is less than {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\nelse:\n    print('PASS, DCF score is above {:.1f} at {:.1f}'.format(dcf_threshold,dcf_score))\n\n# check NAIC\nnaic_score = np.array([naic_price_eps_low,naic_price_pm_low]).min()/csp\nnaic_threshold = 1\nif naic_score &lt; 1:\n    print('FAIL, NAIC score is less than {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\nelse:\n    print('PASS, NAIC score is above {:.1f} at {:.1f}'.format(naic_threshold,naic_score))\n\n# check both scores\nif naic_score &lt; 1 and dcf_score &lt; 0.7:\n    print('One or both DCF and NAIC scores failed')\nelse:\n    print('Both DCF and NAIC scores pass')\n\nFAIL, DCF score is less than 0.7 at 0.5\nPASS, NAIC score is above 1.0 at 1.6\nBoth DCF and NAIC scores pass"
  },
  {
    "objectID": "OLD FRT analysis.html#conclusion",
    "href": "OLD FRT analysis.html#conclusion",
    "title": "Two amplifier RIAA phono preamp",
    "section": "10) Conclusion ",
    "text": "10) Conclusion \nThe following is a summary of the results described above:\n- Stock screener results:. - Current news: - Review quarterly results: - Average daily volume: - Dividend yield: - Discounted cash flow analysis, baseline: - DCF Scenarios: Adjustments were made and the ISV\n- NACI stock selection guide analysis: - Dividend payout: - Management performance:\n- Financial metrics: - Market metrics: - Qualitative metrics: see above\nConcerns:\nSummary:\nRecommendation: Don’t buy above $50 per share."
  },
  {
    "objectID": "OLD FRT analysis.html#notes",
    "href": "OLD FRT analysis.html#notes",
    "title": "Two amplifier RIAA phono preamp",
    "section": "10) Notes ",
    "text": "10) Notes \nThe following notes outline the changes to the DCF model for financial and REIT companies.\nValuing a REIT\nNotes from Valuepro Book, page 237\n\nNOPM: To calculate operating income take rental revenue and subtracted total real estate expenses and G&A expenses. To arrive at the NOPM divide the adjusted income from real estate by real estate rental revenue. For the REIT, take income from real estate, which includes depreciation and amortization, and subtract GSA. Exclude other income, gains on sale of real estate and interest expenses.\nREIT has no traditional R&D costs\n\nREIT is not taxed at the corporate level, tax rate: should be near zero.\nDepreciation and capital expenditures are significantly higher for REITs than in other companies.\nNew property acquisitions are not directly accounted for in the DCF model for a REIT.\n\nWorking capitol: accounts payable, rents and security deposits\nShort term assets: cash, rents and other receivables and prepaid expenses\nShort term liabilities: accounts payable, advance rents security deposits\n\nWorking capital is almost zero, which is similar to other financial companies.\nThe consolidated balance sheet lists the assets as: - Real estate held for investment, at cost: - Land - Buildings and improvements - Total real estate held for investment, at cost - Less accumulated depreciation and amortization - Real estate held for investment, net - Real estate and lease intangibles held for sale, net - Cash and cash equivalents &lt;- current asset - Accounts receivable, net &lt;- current asset - Lease intangible assets, net - Other assets, net\nThe line items indicated above have been taken to be the current assets. Intangibles and long term items have been excluded.\nThe consolidated balance sheet lists the liabilities as: - Distributions payable &lt;- current liabilities - Accounts payable and accrued expenses &lt;- current liabilities - Lease intangible liabilities, net - Other liabilities - Line of credit payable and commercial paper &lt;- current liabilities - Term loans, net - Mortgages payable, net &lt;- current liabilities - Notes payable, net\nThe line items indicated above have been taken to be the current liabilities.\nValuing a financial company\nNotes from Valuepro Book, page 206\n\nTotal revenue comes from the total interest and dividend income line on the income statement. The calculation of operating income is more inclusive for a financial company than for an industrial or high tech company. For financial companies, operating revenue includes all normal revenue items plus interest income, dividends received and other investment income.\nCost of Goods Sold (CGS) comes from the Total interest expense line on the statement of income.\nGeneral and Administrative (G&A) are set to zero since they are included in the interest expense line\nA financial company has no traditional R&D costs\n\\(\\text{Cost of Goods Sold (CGS)} = \\text{Total interest expense} + \\text{Total non-interest expense}\\)\n\nGeneral and Administrative (G&A) are set to zero since they are included in the interest expense line\nA financial company has no traditional R&D costs\nDepreciation and amortization of premises and equipment from Consolidated Statements of Cash Flows.\n\nAmortization of other acquisition-related intangible assets is not included.\nNew investment and Depreciation: Property, plant and equipment expenditures and depreciation charges are significantly lower for a financial company. A typical manufacturing company, in order to grow its business, invests a significant portion of its revenues in plant, property and equipment (PPE). Financial companies invest very little in the way of PPE. However, software, risk management systems and acquisitions of other businesses, need to be included.\n\nFrom the Consolidated Statements of Cash Flows, under Cash Flows from Investing Activities - Purchases of premises and equipment - Purchases of leased equipment, net\n\nWorking capital supports manufacturing and service activities of nonfinancial companies. For financial companies, their principal liabilities and assets are financial claims that take the place of working capital. Because there is no differentiation between current and long term assets and liabilities for a financial company, we adjust working capital charges to zero. A financial company generally invests all of its funds in other financial assets, which have characteristics of current assets rather than PP&E.\n\\(\\text{Accounts Receivable} = 0\\)\n\\(\\text{Inventories} = 0\\)\n\\(\\text{Accounts Payable} = 0\\)\n\\(\\text{working capital} = 0\\)\nShort term assets: The balance sheets of most financial companies do not separate assets and liabilities into current and long term categories. When calculating the short term assets take the total assets and subtract goodwill and intangible assets also subtract other assets of questionable value. Subtract long term assets such as PP&E from total assets.\n\n\\(\\text{Short term assets} = \\text{Total assets} - \\text{good will and others of questionable value} - \\text{Premises and equipment}\\)\n\nA financial company’s principal liabilities are deposits, Federal funds purchased, trading account liabilities, insurance policy and claims reserves, contract holder funds and short term borrowing. To be consistent with the treatment of interest and an operating expense for financial companies, include long term debt in the short term liability category.\n\nShort term liabilities: Include long term debt.\n\n\\(\\text{Long term debt} = 0\\)\nExcess return period\nThe excess return period is based on a judgment call. The authors of [2] use the 1-5-7-10 rule. They group companies into one of four general categories and excess return periods. They use a 10 year excess return period to calculate what they would consider the maximum value. They use a more conservative 1 year, 5 year or 7 year return period to calculate a more reasonable or minimum value.\n- 1 year: Boring companies that operate in a highly competitive, low margin industry in which they have nothing particular going for them.\n- 5 year: Decent companies that have a recognizable name and decent reputation and perhaps a regulatory benefit (utility company), but can’t control pricing or growth.\n- 7 year: Good companies with good brand names, large companies of scale, good marketing channels and consumer identification (e.g. McDonald’s)\n- 10 year: Great companies with great growth potential, tremendous marketing power, band names and in-place benefits (e.g. Intel, Microsoft, Coca Cola, Disney)\nNotes about negative working capital\nThe company has a negative working capital rate. Negative working capital describes a situation where a company’s current liabilities exceed its current assets as stated on the firm’s balance sheet. In other words, there is more short-term debt than there are short-term assets.\nNegative working capital most often arises when a business generates cash very quickly because it can sell products to its customers before it has to pay the bills to its vendors for the original goods or raw materials. In this way, the company is effectively using the vendor’s money to grow.\nDividend Aristocrat, Achiever & Champion\nThis company was selected for analysis because it is on the Dividend Aristocrat list and passes the quick look tests. This notebook will be used as a template when analyzing other companies.\n\nAristocrat: S&P 500 Dividend Aristocrats is designed to measure the performance of S&P 500 index constituents that have followed a policy of consistently increasing dividends every year for at least 25 consecutive years.\nAchiever: The Broad Dividend Achievers Index. Eligible companies must be incorporated in the U.S. or its territories, trade on the NYSE, NASDAQ or AMEX, and have increased its annual regular dividend payments for the last 10 or more consecutive years.\nhttps://dividendvaluebuilder.com/dividend-achievers-list/\nhttps://www.marketbeat.com/dividends/achievers/\nChampion: This list includes companies that had increased their dividend for at least 25 consecutive years, and includes additional companies that had paid higher dividends without having increased the payout in every calendar year.\nhttps://dividendvaluebuilder.com/dividend-champions-list/\nhttps://www.dividendgrowthinvestor.com/p/dividend-champions-list.html"
  },
  {
    "objectID": "OLD FRT analysis.html#references",
    "href": "OLD FRT analysis.html#references",
    "title": "Two amplifier RIAA phono preamp",
    "section": "12) References ",
    "text": "12) References \n\nGray, Gary, et al. Streetsmart Guide to Valuing a Stock: the Savvy Investors Key to Beating the Market. McGraw-Hill, 2004.\nO’Hara, Thomas E., and Ken Janke. Starting and Running a Profitable Investment Club: the Official Guide from the National Association of Investors Corporation. Times Business, 1998.\nRobert G. Hagstrom, The Warren Buffett Way, Wiley, 2013"
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html",
    "href": "Lake District Overshot Waterwheel Project.html",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "",
    "text": "Last update: 25 Nov 2023"
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html#abstract",
    "href": "Lake District Overshot Waterwheel Project.html#abstract",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "Abstract",
    "text": "Abstract\nThis notebook examines the design of a small hydroelectric water wheel built by Kris Harbour Natural Building. The water wheel is a 2.1 meter diameter wheel and he expected to generate between 1.5 and 1.7 kW. The project is documented in four YouTube videos, links are here: Part 1, Part 2, Part 3 and Part 4. The videos provide some technical details, but not enough to satisfy me. My JupyterLab notebook will cover the project description, the water wheel design, generator, flume, inverter and other topics he did not address such as instrumentation, safety, maintenance, reliability and economics."
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html#introduction",
    "href": "Lake District Overshot Waterwheel Project.html#introduction",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "Introduction",
    "text": "Introduction\nThis notebook will review a few technical aspects of the project. I’ll try not to be too much of an armchair quarterback, or provide minute by minute commentary, since Kris does a good job of stating his frustrations as he narrates his videos. Over all, this is a very interesting project. The craftmanship and construction of the water wheel is very good. I think most of the issues are with other aspects of the project.\nThe important technical aspects for the water wheel, gear box and generator have been summarized in the table below:\n\n\n\nParameter\nValue\nNotes\n\n\n\n\nWheel diameter\n2.1 meters\nPart 1 @ 0:35\n\n\nWheel width\n0.8 meters\nsame as flume\n\n\nBucket volume\n48 liters\nPart 1 @ 27:10\n\n\nNumber of Buckets\n16\nPart 1 @ 19:50\n\n\nFlume length\n8 meters\nproject survey document\n\n\nFlume drop\n0.3 meters\nproject survey document\n\n\nFlume width\n0.8 meters\nproject survey document\n\n\nFlow rate\n70 to 120 liters/sec\nproject survey document\n\n\nGear box\n1:20 + 1:2 belt\ntotal 1:40\n\n\nInduction generator\n2.2kw, 8 pole, 750 rpm\nfinal video\n\n\n\nIn part 4 of his video, Kris shared some details of his orginal project survey. I captured images from the video and used google docs to convert the shreenshots to text. This document, converted to a pdf, can be viewed here: project survey dated 16/02/23"
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html#review-of-videos",
    "href": "Lake District Overshot Waterwheel Project.html#review-of-videos",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "Review of videos",
    "text": "Review of videos\nEach of the videos is about 40 minutes long. While watching, I took notes whenever he gave any technical details. The videos primarily document the build, installationa and some of the testing of the water wheel. The construction of the dam and flume were performed by others. Most of the testing he did on site after installation. The exception was some open circuit testing of the first generator in his workshop, which at the time, he thought was adequate.\nPowering An Old Mill - 1.5 Kw Lake District Overshot Waterwheel Project Part 1\n\n2.1m waterwheel\n150 lps max flow rate\n50 liter buckets, three per second = 150 lps -&gt; 11.25 rpm\ngear reduction 20:1, wheel will do 12 to 16 rpm\n500 rpm 2Kw, PM alternator\nflow 150 liters/sec maximum\n\n50 liter buckets\n\n3 buckets will pass per second\n\n1.5 to 1.7 kW\n\n20:1 gear reduction\n\nbetween 12 and 16 rpm depending on flow rate\n\n500 rpm 2kW PM alternator &lt;- 25 rpm at the waterwheel shaft this seems in conflict\n\nrpm of 16, rotation of 3.7 sec, 230 V from rectifer open circuit\n\n16 buckets\n\n42 L per bucket, measured 45 L\n\nPowering An Old Mill - 1.5 Kw Lake District Overshot Waterwheel Project Part 2\n\nat the site no dam or pond, just the leat\nblocked the water, made some templates and models\nflume = four boards across, walls are set internal and look a bit higher than the individual base boards are wide @1:57\nwork on the spill way\n@ 10:16 cutting back the flume\n@25:11 connected 3 conductors to bottom of the frame.\n1st test 130 volts at low rpm and flow rate, then 340 volts w/o load\n\n@32:03 load test, maybe 1 inch of flow in the flume, buckets are lessthan, half flow making 530 Watts, time @33:08\nincreased flow, maybe 1 inch in the flume\n@34:42 2nd day of testing, rained overnight and lots of water flowing around the wheel.\n@36:37 2/3rd flow, flume water level looks at the top at the enterence, but just past the gate, the water level is a few inches, reduced to like 1/2 up the side boards, buckets are ??, power 940 Watts\n\n@37:22 3rd day, showing the conduet installed\n\ncomplaning that the controller is letting the wheel spin too fast, but generating 1 kW\n@40:16 add some plywood to control the water fill angle, took some slow motion video @41:38.\n\n@42:54 put the controller into constant voltage mode 100V an di splanning to up the voltage based on testing\nwheel spinning much slower, buckets look 1/3 full @44:21\nHe thinks the alternator want to spin much faster, it will generate 1000 Watts, but the wheel is spinning too fast, when he uses the controller to slow the wheel for proper bucket filling the power is much less and the alternater got very hot.\n\nPowering An Old Mill - 1.5 Kw Lake District Overshot Waterwheel Project Part 3\n\ngot a bigger pulley, now the total ratio is 20x2=40\ncorner on the flume was improved\n\nKris built a nozzle for the water\n@9:21 installed the new pulley and a new lower voltage alternater\n@14:11 with flume nozzle buckets look like they are filling better, look about 1/2 full, generater is making 420 Watts\n\nwater flow from the main river down a man made leet,\n@16:22 removed the boards that made a temporary dam and much more water is flowing, but the wheel is spinning too fast but making 1160 Watts, earlier 1400 Watts\n2 KW alternater, but only can make 1400 Watts before it gets too hot.\n@17:21 810 Watts when flow rate reduced and wheel spinning at a near proper speed.\n@18:33 next day, ran all night, making 750 Watts\n@19:19 600 rpm on the generator (15 rpm on the water wheel), measured with a Tach, generating Alt temp is 80C when generating 750 Watts (that 15 rpm on the wheel shaft)\n@22:46 950 Watts is the most it can do before it gets too hot\ntest grid down dump load.\n@34:41 took apart the 220 V alternator and the new one is a 96V alternator.\n@42:30 new plan is to use amn 8 pole induction motor\n\nWhy did he change to a lower voltage alternater? Old alternater was making 420 volts, he wanted 220 volts.\nPowering An Old Mill 1.5 Kw Lake District Overshot Waterwheel Project Part 4\n\n3 phase induction motor, 8 pole, 750 rpm, 2.2kW (needs a water wheel speed of 18.75 rpm)\n\nmotor face plate: Tec ECHTOP UK CA, Type 2283TECA 83\n\nswitched to a 2kW induction motor, 728rpm, 1711 Watts, 184 volts\n\ntesting of induction motor in John’s hydro electric workshop, drive motor was 3kW, 728rpm, 1711 Watts, 184 volts, 80uF caps\n\nmodifiling the frame and mounting brackets\n\nlooks like a cooling fan on the back of the motor, water side.\n\nadded emergency shut off flap\n\ndump load mounted on wall in poarch, 1.5 kW load\n\nhalf flow test made 860W\n\nfull flow entrance to flume is at the top of the walls, but within everal inches the depth is 1/2, at the end of the flume, depth looks loke 1 inch,\n\nfull flow, water is impacking into the buckets with substantial splash over, generating about 1.6kW, splasing on the generator cover, at the 4 to 5 oclock postion, water sloshing out of the buckets, buckest are 2/3s full, making 1350 Watts\n\nadded plywood to block water exit from the leat in order to increase the flow\n\nflow in the flume looks a bit larger, maybe 2 inches deep at the exit, pitch on the flume looks like a couple of inches, velocity is high, lots of spash over, making 1500 to 1550 Watts, promised the customer 1500 Watts - not a qualified electrician, only doing the mechanicans and making electricity.\nchanging the caps to smaller value\n\nwith some reduced flow, getting 1150 Watts, but less splashing,\n\n@22.33 Kris shows orginal project survey dated 16/02/23, link to OCR of screen captures.\nadded a rpm sensor to run a safety shut off\n\nat time 29:41 with minimal splasing generates about 500 Watts\n\ntesting with gride off, power dumped into load, resistors shown at 50C"
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html#units",
    "href": "Lake District Overshot Waterwheel Project.html#units",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "Units",
    "text": "Units\nIn the calculations that follow, I’m using the Python package, Pint, which allows mathematical operations with units and conversions to and from different units. Keeping track of the units will provide some error checking when performing the calculations. I’ll be using metric units since these are what Kris has been using. All calculations are done using the International System of units. A list of units and conversions used by Pint is here.\n\nVariables and constants\nThere are a couple of variables and constants that will be used throughout the calculations.\n\nQ is used for flow rate\n\ntau or \\(\\tau\\) is used for torque\nomega or \\(\\omega\\) is used for angular velocity\ng is used to denote the gravity of Earth, standard gravity by definition is equal to 9.80665 \\(m/s^2\\)\nrho or \\(\\rho\\) is used for the density of water, at 50F or 10C is 999.75 \\(kg/m^3\\)\neta or \\(\\eta\\) is used for energy conversion efficiency, accounts for the energy lost to heat\n\nI’ll be using descriptive names for the variable used in my calculations.\n\n\nCode\nimport pint\nureg = pint.UnitRegistry()\nimport numpy as np\nfrom tabulate import tabulate\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\nureg = pint.UnitRegistry(autoconvert_offset_to_baseunit = True)\n\n\n\n\nCode\ng = 9.80665*ureg.meter/(ureg.sec**2) # standard gravity\nrho = 999.75*ureg.kg/(ureg.meter**3) # density of water at 50F or 10C, kg/m^3"
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html#terminology",
    "href": "Lake District Overshot Waterwheel Project.html#terminology",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "Terminology",
    "text": "Terminology\nKris uses the term ‘trough’ for the structure that carries the water to the top of the water wheel; I’ll be using the term Flume to describe the structure.\nOther terms are:\n\nMill race\n\nMill pond\n\nLeat\n\nSluice\n\nWeir\n\nPico hydro\nGrid-tied electrical system"
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html#gravitational-power-of-water",
    "href": "Lake District Overshot Waterwheel Project.html#gravitational-power-of-water",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "Gravitational power of water",
    "text": "Gravitational power of water\nThe harvesting of the gravitational energy of water is a proven source of energy. Since a waterwheel is used to extract power from the water, the head height is approximately the diameter of the waterwheel. Water enters at the top of the wheel and the weight of the water causes the wheel to rotate until a point near the bottom of the wheel. So let’s use the diameter of the waterwheel.\nThe gravitational power of water is given by:\n\\(P = \\rho \\times g \\times Q \\times H\\)\nwhere:\nP = gravitational power of water\nH = height of fall Q = flow rate\ng = acceleration due to gravity\n\\(\\rho\\) = density of water\n\nChecking Kris’ calculations\nKris provided his customer an estimated of the power that might be generated versus flow rates. The following table is from his project survey. Kris used H = 2.6 meters and \\(\\eta\\) of 0.6.\n\nKris’s estimated power, head = 2.6 meters, \\(\\eta\\) = 60%\n\n\nFlow rate\nPower, Watts\n\n\n\n\n60 lps\n833\n\n\n70 lps\n1,030\n\n\n80 lps\n1,177\n\n\n90 lps\n1,324\n\n\n100 lps\n1,472\n\n\n110 lps\n1,619\n\n\n116 lps\n1,692\n\n\n\nMy power calculations are shown below.\nH = 2.6ureg.meter eta = 0.6 Q = np.array([60,70,80,90,100,110,116])ureg.liter/(1ureg.sec) print(‘head height: {:.1fP}, 3B7: {:.1f}’.format(H,eta)) print(‘{:&lt;15s}{:&lt;10s}’.format(‘Flow rate’, ‘power’)) for i in Q: print(‘{:&lt;3.0f~P} {:&lt;,.1f~P}’.format(i.to(‘l/s’),(rhogiH*eta).to(‘watts’)))\n\n\nCode\nH = 2.6*ureg.meter\neta = 0.6\nQ = np.array([60,70,80,90,100,110,116])*ureg.liter/(1*ureg.sec)\nprint('head height: {:.1fP}, \\u03B7: {:.1f}'.format(H,eta))\ntable_header = ['Flow rate, lps', 'power, W']\ntable_row = []\n\nfor i in Q:\n    table_row.append([i.to('l/s').magnitude,(rho*g*i*H*eta).to('watts').magnitude])\n\nprint(tabulate(table_row, headers=table_header,tablefmt=\"simple\",floatfmt=('.0f',',.1f')))\n\n\nhead height: 2.6 meter, η: 0.6\n  Flow rate, lps    power, W\n----------------  ----------\n              60       917.7\n              70     1,070.6\n              80     1,223.6\n              90     1,376.5\n             100     1,529.5\n             110     1,682.4\n             116     1,774.2\n\n\nMy numbers don’t match Kris’s numbers, which he presented in his project survey. Also, seems courious that the final number in his table is 116 lps and not 120 lps."
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html#case-studies",
    "href": "Lake District Overshot Waterwheel Project.html#case-studies",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "Case studies",
    "text": "Case studies\nThe history of water wheels dates back thousands of years and the fundementals of water wheels are well understood. What is new is the restoration of historic water mills being employed to generate electricity. Water wheels have a low rotational speed with high torque and this equires the use of gears to step up the rotational speed to a level that can be used by electrical generators. I found two intreseting examples of small water wheels being used to generate electrical power. In each of these examples, we can calculate the overall efficiency. This is a useful data point since these water wheels are of simular size to the one Kris built.\nEskdale Mill\nThe Eskdale Mill is profiled in the YouTube video (link above). Smith Engineering performed the design of the water wheel. The wheel is installed at the oldest and last working corn mill in the English Lake District, providing power for the miller’s cottage. It has been positioned adjacent to the Grade II* listed mill which has been operating for hundreds of years. It is now owned and run as a tourist attraction by Eskdale Mill & Heritage Trust, a charity run by local people, who will gain a supplementary income from the power generated to cover some of the mill’s overheads and maintenance costs.\n\nEskdale Mill, miller’s cottage water wheel summary\n\n\n\n\n\n\nLocation\nBoot, Eskdale Valley, Cumbria, England\n\n\nCommissioned\nJuly 2016\n\n\nOutput\n3.5 kW\n\n\nFlow rate\n120 l/s\n\n\nWheel diameter\n4.0 m\n\n\nWheel width\n0.8 m\n\n\nGearbox\nBrevini epicyclic\n\n\nGenerator\nBrook Crompton three-phase, 415V induction motor converted to single-phase 240V\n\n\n\nThe power and efficiency is calculated below.\n\n\nCode\nH = 4*ureg.meter # diemater of the wheel, used as the head height\nQ = 120*ureg.liter/(1*ureg.sec)\nprint('available power: {:,.1fP}'.format((rho*g*Q*H).to('watts')))\nprint('efficiency: {:,.2f}%'.format(((3500*ureg.W)/(rho*g*Q*H).to('watts')).magnitude*100))\n\n\navailable power: 4,706.0 watt\nefficiency: 74.37%\n\n\nThe Langdale Hotel and Spa, English Lake District\nThis water wheel was also build by Smith Engineering and based on research and development that Smith Engineering had undertaken with Dr Paddy Quinlan of the University of Cumbria on bucket design and efficiency. There is a high degree fo simularity in the look of this water wheel to the one that Kris built.\n\nThe Langdale Hotel and Spa water sheel summary\n\n\n\n\n\n\nLocation\nGreat Langdale, Cumbria, England\n\n\nCommissioned\nNovember 2014\n\n\nOutput\n6 kW\n\n\nFlow rate\n200 l/s\n\n\nWheel diameter\n4.1 m\n\n\nWheel width\n1 m\n\n\nGearbox\nBrevini epicyclic 177:1 ratio\n\n\nGenerator\nBrook Crompton three-phase, 415V induction motor, power rating of 11kW\n\n\n\n\n\nCode\nH = 4.1*ureg.meter\nQ = 200*ureg.liter/(1*ureg.sec)\n(rho*g*Q*H).to('watts')\nprint('available power: {:,.1fP}'.format((rho*g*Q*H).to('watts')))\nprint('efficiency: {:,.2f}%'.format(((6000*ureg.W)/(rho*g*Q*H).to('watts')).magnitude*100))\nprint('generator rated power to avaliable power: {:,.2f}'.format(((11000*ureg.W)/(rho*g*Q*H).to('watts')).magnitude))\n\n\navailable power: 8,039.4 watt\nefficiency: 74.63%\ngenerator rated power to avaliable power: 1.37\n\n\nLinks to other water wheel videos:\n\nHydro electric water wheel with generator in Egloffstein: A hydro-electric water wheel in Egloffstein Franconia with 12kW Generator.\n\nStunning Franconia Aerial #6 Hydro electric water wheel in Egloffstein; The camera is flying over a hydro-electric water wheel in Egloffstein, Franconia (Germany).\n\nTwo views of the 1926 Fitz overshot waterwheel at Hanford Mills Museum: Two views of the 1926 Fitz Overshot waterwheel in operation at Hanford Mills Museum in East Meredith, NY. The power generated is used to run the sawmill and woodworking machines. The first view is from the visitor platform, and the second is from the tail race. The 10-foot diameter waterwheel is 12 feet wide. A system of belts, pulleys and gears transfers the power to machines upstairs in the Mill.\n\nWaterwheel power 4kW to grid: We re-purposed this 140 year old wheel to generate and export power back to the grid in 2007/8. 10 years on and its made well over 100,000kWh energy, most of it exported .. generation matches flow availability, we could process more but the axles at its limit and the wheel floods .. efficiency is very high, the gearbox and generator silent, its a capacitor excited induction generator with integral flow management and grid tie cabinet. The system hinges around the ponds water level, the original wheel control sluice is reused. This is a beautifully engineered large wheel running well within its capabilities, the iron structure is much more than functional, a look in detail shows considerable artisan and design skills … we believe this is the largest wheel on a domestic property in UK Update 2019/20: still working really, really well. New owner, Francis, has addressed a number of site issues and runs at full rated power most of the time. we’re well on the way to the first 500,000 kWh energy production.\n\nWater wheel - Energy Power Generator - Construction KIT: Water wheel - Upper bucket - Building kit. We offer a complete building kit or just drawings for the CNC production of a water wheel type - Upper bucket. The material for production is stainless steel or ordinary steel. It is possible to change the dimensions as required. The water wheel is a suitable water engine for all historic buildings, such as a power plant or as decoration. At reduced speeds, it can process many times smaller flows than expected, without a decrease in efficiency.\n\nSome take aways:\n\nin most cases, the water velocity in the flume seems low and there is not a lot of splashing\nthe rated power of the generator is usially higher than the avaliable power"
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html#project-analysis",
    "href": "Lake District Overshot Waterwheel Project.html#project-analysis",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "Project analysis",
    "text": "Project analysis\nThe analysis of the project will consist of looking at the performance of the water wheel based on the information avaliable from the four YouTube videos. In the calculations that follow, the head height is the same as the water wheel diameter.\n\n\nCode\nH = 2.1*ureg.meter # head equal to the wheel diameter\nprint('head height: {:.1f#~P}'.format(H))\n\n\nhead height: 2.1 m\n\n\nKris use an overall efficieny of 60% in his calculations. I’ve decided to break out the efficiencies of the major parts and then calculate an overall efficiency.\n\nEfficiencies\nA typical overshot Water wheel has the water channeled to the wheel at the top and slightly beyond the axle. The water collects in the buckets on that side of the wheel, making it heavier than the other “empty” side. The weight turns the wheel and the water flows out into the tail-water when the wheel rotates enough to invert the buckets. The overshot design can achieve 90% efficiency. Nearly all of the energy is gained from the weight of water lowered to the tailrace although a small contribution may be made by the kinetic energy of the water entering the wheel.\nThe efficiencies I’ll be using in my calculations:\n\n\\(\\eta\\) for waterwheel = 90%\n\n\\(\\eta\\) for gear box = 95%\n\n\\(\\eta\\) for generator = 90%\n\\(\\eta\\) for grid-tie inverter = 97%\n\nThese effencienies have been adjusted to give an overall efficiency in line with what has been acheived in the case studies.\n\n\nCode\neta_waterwheel = 0.9 # for a small waterwheel\neta_gear_box = 0.95\neta_generator = 0.9\neta_grid_tie_inverter = 0.97\neta_total = eta_waterwheel*eta_gear_box*eta_generator*eta_grid_tie_inverter\nprint('total efficiency: {:.3f}'.format(eta_total))\n\n\ntotal efficiency: 0.746\n\n\n\n\nDesign parameters\nIn the YouTube video and the project survey, Kris stated that the veritcal height that was available down stream from the dam would allow for a 2.1 meter diameter water wheel with some room clearace above and below. The flume would need to be about 8 meters long. The flow rate avalailbe ranged from 60 to 120 liters per second (lps or l/s) and that the goal was to generate 1,500 Watts, presummably at 120 lps.\n\nDesign parameters\n\n\navaliable vertical height for wheel\n2.1 meters\n\n\nwater flow rate\n60 to 120 lps\n\n\nflume length\n8 meters\n\n\nobjective generated power out\n1500W at 120 lps\n\n\n\nI’ve decided to include a design marging of 15% in the power estimate calculation to account for unknowns and poor estimates.\n\n\nCode\ndesign_margin = 0.15\n\n\nThe gravitation power of water at various flow rates is plotted below.\n\n\nCode\nstep_size = 20\nQ = np.arange(60, 180+step_size, step_size)*ureg.liter/(1*ureg.sec)\n\nplt.plot(Q.to('liter per sec').magnitude,(rho*g*Q*H*eta_total*(1-design_margin)).to('watts').magnitude,'-k')\nplt.grid()\n\n#plt.ylim((0,10))\nplt.ylabel('power , Watts')\nplt.xlabel('flow rate, l/s')\n\nplt.title('Gravitational power of water, head height: {:.1fP}, \\u03B7: {:.3f}'.format(H,eta_total))\n\nplt.show()\n\n\n\n\n\nThis data is dispayed in the table below:\nprint(‘head height: {:.1fP}, 3B7: {:.3f}, design margin: {:.2f}’.format(H,eta_total,design_margin)) print(‘{:&lt;15s}{:&lt;10s}’.format(‘Flow rate’, ‘power’)) for i in Q: print(‘{:&lt;3.0f~P} {:&lt;,.1f~P}’.format(i.to(‘l/s’),(rhogiHeta_total*(1-design_margin)).to(‘watts’)))\n\n\nCode\n#H = 2.6*ureg.meter\n#eta = 0.6\n#Q = np.array([60,70,80,90,100,110,116])*ureg.liter/(1*ureg.sec)\nprint('head height: {:.1fP}, \\u03B7: {:.1f}'.format(H,eta))\ntable_header = ['Flow rate, lps', 'power, W']\ntable_row = []\n\nfor i in Q:\n    table_row.append([i.to('l/s').magnitude,(rho*g*i*H*eta_total*(1-design_margin)).to('watts').magnitude])\n\nprint(tabulate(table_row, headers=table_header,tablefmt=\"simple\",floatfmt=('.0f',',.1f')))\n\n\nhead height: 2.1 meter, η: 0.6\n  Flow rate, lps    power, W\n----------------  ----------\n              60       783.8\n              80     1,045.0\n             100     1,306.3\n             120     1,567.5\n             140     1,828.8\n             160     2,090.0\n             180     2,351.3\n\n\n\n\nCode\nprint('With in the flow rate range of {:.0f~P} to {:.0f~P}, \\nthe estimated power avaliable is {:.1f~P} to {:.1f~P}.'.format(Q[0],Q[-1],(rho*g*Q[0]*H*eta_total*(1-design_margin)).to('watts'),(rho*g*Q[-1]*H*eta_total*(1-design_margin)).to('watts')))\n\n\nWith in the flow rate range of 60 l/s to 180 l/s, \nthe estimated power avaliable is 783.8 W to 2351.3 W.\n\n\nLooks like making 1500 W at 120 lps is feasable."
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html#instrumentation",
    "href": "Lake District Overshot Waterwheel Project.html#instrumentation",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "Instrumentation",
    "text": "Instrumentation\nKris utilized only a volt meter and relied on the inverter to provide him an indication of the power being generated. For testing, he should have used a resistive load, a volt meter (which he did use) and an amp meter. He could have used the dump load across the three phase recitfier. This would have eliminated any setup issues with the inverter.\n\nElectrical load for testing\nIn the videos, Kris was using inverter as the electrical load for the generator. I think he should have been using a high power resistive load for his inital tests. Three portable electric heater or his dump load could have been used for testing. Once everything was sorted out, he could then connect to the inverter for additional tests.\nTo calculate the load resistance for testing, I’m assuming that 1500 Watts is the load he should be using; I’m also assuming the geneator makes 220 volts at the rate power at 500 RPM.\nKris didn’t say too much about the inverter, except that he was able to control the speed of the wheel by changing the mode to constant voltage and setting the value to 100 volts. This casued the genrator to get too hot under full flow conditions. About this time he started to suspect that the rating of the generator was much less than advertised. But the situations is a little confusing. I suspect that the availavle power was much greater than the power indicated on the inverter display, so the difference was being dissipated as heat in the generator, since the generator was being forced to provide a breaking load to the water wheel. This is why using a fixed power resistor as the load would have been much better for inital testing.\n\n\nCode\nprint('current delivered to test load: {:.1f~P}'.format((1500*ureg.watt/(220*ureg.volt)).to_base_units()))\n\nR_load = (1500*ureg.watt/((1500*ureg.watt/(220*ureg.volt))**2)).to('ohms')\nprint('resistance of test load: {:.1f~P}'.format(R_load))\n\n\ncurrent delivered to test load: 6.8 A\nresistance of test load: 32.3 Ω\n\n\n\n\nGenerator voltage vs RPM\nI’m going to assum that the generator’s output voltage is linear with respect to the generator shaft rotational speed (at least over the normal operating RPM range). In the videos, Kris used several generators. The first one appears to have a rated voltage of 220 Volts. Later this generator was swapped out for one with a lower voltage rating of 96 Volts. In the final video, Kris changes the generator to an induction motor.\n\ndidn’t explain why he changed from 220 to 96 volts except to say he was concerned that the wheel was spinning too fast.\nThe initial design had the 220 Volt generator, so that’s what I’m going with for the initial part of the analysis, later I’ll address the generator questions.\n\n\n\nCode\nwater_wheel_rotation_frequency = (np.arange(5,16))*ureg.rpm\ngear_box = 40\ngen_voltage = ((220/500)*water_wheel_rotation_frequency.magnitude*gear_box)*ureg.volt\n\nplt.plot(water_wheel_rotation_frequency.magnitude,gen_voltage.to('volt').magnitude, '-')\n\nplt.grid()\n\nplt.ylabel('generator out put, volts')\nplt.xlabel('water wheel RPM')\n\nplt.title('generator output voltage')\nplt.show()\n\n\n\n\n\nLater, I’ll look at power flow through the hydo system. Power from gravity on the mass of the water, to the rotational power of the wheel (RPM x torque), to the genrator and finally to the electrical test load. Accounting for losses along the way, the power delivered to the electrical load should be equal to the rotational power of the wheel minus the losses."
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html#water-wheel",
    "href": "Lake District Overshot Waterwheel Project.html#water-wheel",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "Water wheel",
    "text": "Water wheel\nThe water wheel the Kris designed and fabricated has a diameter of 2.1 meters, 16 buckets, each with a capacity of 48 liters and the wheel width is 0.8 meters. In the calculations that follow, I’m assuming the wheel will have a rotational frequency of between 5 to 15 rotations per minute (rpm). The electrical load on the generator will limit the rotational frequency of the water wheel by resisting the torque delivered to the generator shaft. The electrical load is provided by the inverter. Through out the calculations that follow, I’m assuming that the wheel rotation frequency will be limited by the electrical load placed on the generator by the inverter. If the wheel is making 1000 Watts and delivering this power to the generator shaft, it is assumed that the generator will extract the power at an RPM within the range of 5 to 15 RPM based on the action of the inverter.\nThere are 16 buckets, each with a capacity of 48 liters. I’ll assum that the buckets will fill to a level of 95% of the bucket volumn. However, Kris stated in his video, that he though the bucket capacity was more like 50 liters because of the side wall of the wheel frame. In all the water wheels that I have observed on various YouTube videos, the buckets don’t fill to 100%.\n\n\nCode\nnumber_of_buckets = 16/(1*ureg.circle)\nwheel_width = 0.8*ureg.meter\nbucket_volume = 48*ureg.liter\nwater_wheel_diameter = 2.1*ureg.meter\n\nbucket_capacity_utilization = 0.95\n\nwater_wheel_rotation_frequency = (np.arange(5,16))*ureg.rpm\n\n\n\nWater carring capacity vs rpm\nFor a give rpm and fill level, the water wheel can move a fixed amount of water. The water carring capacity is calculated by taking the rotation frequency times the number if buckets times the bucket capacity utilization. The Python pint library keeps track of the units. The number of buckets has the dimention of 16 per circle, so the product of the terms has dimentions that can be converted to liters per second.\nThe water wheel carring capacity at various flow rates is plotted below.\n\n\nCode\nplt.plot(water_wheel_rotation_frequency.magnitude,(water_wheel_rotation_frequency*number_of_buckets*bucket_volume*bucket_capacity_utilization).to('liter/sec').magnitude, '-')\n\nplt.grid()\n\nplt.ylabel('water carring capacity, l/s')\nplt.xlabel('wheel RPM')\n\nplt.title('water carring capacity, buckets are {:.0f}% full'.format(bucket_capacity_utilization*100))\nplt.show()\n\n\n\n\n\nThe wheel carring capacity versus wheel RPM is tabulated below.\nprint(‘{:&lt;15s}{:&lt;10s}’.format(‘wheel rpm’, ‘water carring capacity’)) for i in water_wheel_rotation_frequency: print(‘{:&lt;4.1f~P} {:&lt;,.1f~P}’.format(i.to(‘rpm’),(inumber_of_bucketsbucket_volume*bucket_capacity_utilization).to(‘liter/sec’)))\n\n\nCode\ntable_header = ['wheel rpm', 'water carring capacity, lps']\ntable_row = []\n\nfor i in water_wheel_rotation_frequency:\n    table_row.append([i.to('rpm').magnitude,(i*number_of_buckets*bucket_volume*bucket_capacity_utilization).to('liter/sec').magnitude])\n\nprint(tabulate(table_row, headers=table_header,tablefmt=\"simple\",floatfmt=('.0f',',.1f')))\n\n\n  wheel rpm    water carring capacity, lps\n-----------  -----------------------------\n          5                           60.8\n          6                           73.0\n          7                           85.1\n          8                           97.3\n          9                          109.4\n         10                          121.6\n         11                          133.8\n         12                          145.9\n         13                          158.1\n         14                          170.2\n         15                          182.4\n\n\nAt 10 RPM, the wheel can move at most 121 lps. At higher flow rates, the excess water will over flow the buckets and not contribute any additional weight to the bucket.\n\n\nWater wheel torque vs rpm\nThe force that turns the wheel is provided by gravity and the mass of the water. Only about half of the buchets on one side of the wheel are filled with water and the weight imbalance causes the wheel to turn under the force of gravity.\ncalculations\nmoment arm\nTorque\nunits\nLet’s assume that half of the buckets have water. Assume 95% bucket capacity utilization.\n\\(\\large{\\color {red} {\\text{clean up narrative}}}\\)\nneed to convert the above to units of pound force\nThe pound of force or pound-force (symbol: lbf,[1] sometimes lbf,[2]) is a unit of force used in some systems of measurement, including English Engineering units[a] and the foot–pound–second system.[3]\nPound (force)\nThe pound of force or pound-force (symbol: lbf,[1] sometimes lbf,[2]) is a unit of force used in some systems of measurement, including English Engineering units[a] and the foot–pound–second system.[3]\nPound-force should not be confused with pound-mass (lb), often simply called “pound”, which is a unit of mass; nor should these be confused with foot-pound (ft⋅lbf), a unit of energy, or pound-foot (lbf⋅ft), a unit of torque.\nthe meter-kilogram-second gravitational unit of work and energy equal to the work done by a kilogram force acting through a distance of one meter in the direction of the force : about 7.235 foot-pounds\nTorque Torque has the dimension of force times distance, symbolically T−2L2M. Although those fundamental dimensions are the same as that for energy or work, official SI literature suggests using the unit newton-metre (N⋅m) and never the joule.[12][13] The unit newton-metre is properly denoted N⋅m.[13]\nThe traditional imperial and U.S. customary units for torque are the pound foot (lbf-ft), or for small values the pound inch (lbf-in). In the US, torque is most commonly referred to as the foot-pound (denoted as either lb-ft or ft-lb) and the inch-pound (denoted as in-lb).[14][15] Practitioners depend on context and the hyphen in the abbreviation to know that these refer to torque and not to energy or moment of mass (as the symbolism ft-lb would properly imply).\nNewton (unit)\nThe newton (symbol: N) is the unit of force in the International System of Units (SI). It is defined as 1 kg ⋅ m/s 2 { {}^{2}}, the force which gives a mass of 1 kilogram an acceleration of 1 metre per second per second. It is named after Isaac Newton in recognition of his work on classical mechanics, specifically his second law of motion.\nA newton is defined as 1 kg ⋅ m/s 2 { {}^{2}} (it is a derived unit which is defined in terms of the SI base units).[1] One newton is, therefore, the force needed to accelerate one kilogram of mass at the rate of one metre per second squared in the direction of the applied force.[2]\nThe plot above says that at 10 rpm, 116.89 lps of water can be moved with buckets at 90% full.\nMake a new plot, don’t assum the buckets are full. For example at 200 lps and 15 rpm, what is the torque?\n\n\nCode\nx = np.linspace(0, np.pi,int(number_of_buckets.magnitude/2))\nmoment_arm = (water_wheel_diameter/2)*np.sin(x)\n\n\n\n\nCode\n# do the calculations from 5 to 20 rpm using 100 points\nwater_wheel_rotation_frequency = (np.linspace(5,20,100))*ureg.rpm\n\nfor i in range(len(Q)):  # use the same flow rate from above\n    #water_capacity_per_rotation = number_of_buckets*bucket_volume\n    water_wheel_shaft_torque = np.zeros(len(water_wheel_rotation_frequency))*ureg.kilogram*ureg.g0*ureg.meter\n\n    for j in range(len(water_wheel_rotation_frequency)):\n        water_transport_rate = (number_of_buckets*bucket_volume*water_wheel_rotation_frequency[j]).to('liter/sec')\n\n        bucket_capacity_utilization = Q[i]/water_transport_rate\n\n        # limit transport rate to flow rate\n        if bucket_capacity_utilization &gt; 1:\n            bucket_capacity_utilization = 1\n            water_transport_rate = Q[i]\n\n        weight_of_water_per_bucket = (rho*bucket_volume)*bucket_capacity_utilization\n\n        torque_per_bucket = (weight_of_water_per_bucket*ureg.g0*moment_arm).to_base_units()\n        water_wheel_shaft_torque[j] = torque_per_bucket.sum()    \n\n    plt.plot(water_wheel_rotation_frequency.magnitude,water_wheel_shaft_torque.to('N m').magnitude,'-',label='{:.1f~P}'.format(Q[i]))\n\nplt.grid()\n\nplt.ylabel('water wheel shaft torque, N m')\nplt.xlabel('wheel rpm')\n\nplt.title('new plot')\nplt.legend(loc=0)\n\nplt.show()\n\n\n\n\n\nwhat does this say??\n\\(\\large{\\color {red} {\\text{clean up narrative}}}\\)\ngen_shaft_rotation_freq = np.array([125,250,500,1000])ureg.rpm/50 gen_pwr = np.array([0.5,1,2,4])1000*ureg.watts gen_pwr\ngen_shaft_rotation_freq\ngen_pwr\nwater_wheel_rotation_frequency\n\n\nCode\nfor i in range(len(Q)):\n    #water_capacity_per_rotation = number_of_buckets*bucket_volume\n    water_wheel_shaft_torque = np.zeros(len(water_wheel_rotation_frequency))*ureg.kilogram*ureg.g0*ureg.meter\n\n    for j in range(len(water_wheel_rotation_frequency)):\n        water_transport_rate = (number_of_buckets*bucket_volume*water_wheel_rotation_frequency[j]).to('liter/sec')\n\n        bucket_capacity_utilization = Q[i]/water_transport_rate\n\n        # limit transport rate to flow rate\n        if bucket_capacity_utilization &gt; 1:\n            bucket_capacity_utilization = 1\n            water_transport_rate = Q[i]\n\n        weight_of_water_per_bucket = (rho*bucket_volume)*bucket_capacity_utilization\n\n        torque_per_bucket = (weight_of_water_per_bucket*ureg.g0*moment_arm).to_base_units()\n        water_wheel_shaft_torque[j] = torque_per_bucket.sum()    \n\n    plt.plot(water_wheel_rotation_frequency.magnitude,(water_wheel_shaft_torque*water_wheel_rotation_frequency*eta_waterwheel).to('watts').magnitude,'-',label='{:.1f~P}'.format(Q[i]))\n\n#plt.plot(gen_shaft_rotation_freq.magnitude,gen_pwr.magnitude,'.k',label='{:.1f~P}'.format(Q[i]))\n\nplt.grid()\n\nplt.ylabel('water wheel shaft power, Watts')\nplt.xlabel('wheel rpm')\n\nplt.title('new plot')\nplt.legend(loc=1)\n\nplt.show()\n\n\n\n\n\nwhat does this say?\nIf the controller tries to keep the wheel rpm around 10, then the generator needs to absorb between 1000 and 2200 Watts\nat flow rates of 180 lps and above the power levels off at 3kW, so the generator should be sized to higher than 3kW\nPrint data as a table\nR_load\nwater_wheel_rotation_frequency = (np.arange(5,21))*ureg.rpm\ni = 3 #for i in range(len(Q)):\nprint(‘{:&gt;10s} {:&gt;10s} {:&gt;10s} {:&gt;20s} {:&gt;15s} {:&gt;20s} {:&gt;20s}’.format(‘flow, lps’,‘rpm’,‘torque’,‘avaliable pwr, W’,‘gen voltage’,‘Amps to the load’,‘pwr to the load’))\n#water_capacity_per_rotation = number_of_bucketsbucket_volume water_wheel_shaft_torque = np.zeros(len(water_wheel_rotation_frequency))ureg.kilogramureg.g0ureg.meter\nfor j in range(len(water_wheel_rotation_frequency)): water_transport_rate = (number_of_bucketsbucket_volumewater_wheel_rotation_frequency[j]).to(‘liter/sec’)\nbucket_capacity_utilization = Q[i]/water_transport_rate\n\n# limit transport rate to flow rate\nif bucket_capacity_utilization &gt; 1:\n    bucket_capacity_utilization = 1\n    water_transport_rate = Q[i]\n\nweight_of_water_per_bucket = (rho*bucket_volume)*bucket_capacity_utilization\n\ntorque_per_bucket = (weight_of_water_per_bucket*ureg.g0*moment_arm).to_base_units()\nwater_wheel_shaft_torque[j] = torque_per_bucket.sum()    \n\navailable_pwr = (water_wheel_shaft_torque[j]*water_wheel_rotation_frequency[j]*eta_waterwheel)\nload_volts = (220/500)*water_wheel_rotation_frequency[j].magnitude*gear_box*ureg.volt\nload_current = load_volts/R_load\nload_pwr = R_load*load_current**2\n\nprint('{:&gt;10.0f} {:&gt;10.0f} {:&gt;10.0f} {:&gt;20.1f} {:&gt;15.1f} {:&gt;20.1f} {:&gt;20.1f}'.format(Q[i].magnitude,\n    water_wheel_rotation_frequency[j].magnitude,\n    water_wheel_shaft_torque[j].to('N m').magnitude,\n    available_pwr.to('watts').magnitude,\n    load_volts.magnitude,\n    load_current.magnitude,\n    load_pwr.magnitude                                                                   \n     ))\nprint(tabulate([[‘Alice’, 24], [‘Bob’, 19]], headers=[‘Name’, ‘Age’]))\n\n\nCode\nwater_wheel_rotation_frequency = (np.arange(5,21))*ureg.rpm\n\ni = 3 #for i in range(len(Q)):\n\n#print('{:&gt;10s} {:&gt;10s} {:&gt;10s} {:&gt;20s} {:&gt;15s} {:&gt;20s} {:&gt;20s}'.format('flow, lps','rpm','torque','avaliable pwr, W','gen voltage','Amps to the load','pwr to the load'))\n\ntable_header = ['flow','rpm','torque','avaliable pwr','gen voltage','load amps','load pwr']\ntable_row = []\n\n#water_capacity_per_rotation = number_of_buckets*bucket_volume\nwater_wheel_shaft_torque = np.zeros(len(water_wheel_rotation_frequency))*ureg.kilogram*ureg.g0*ureg.meter\n\nfor j in range(len(water_wheel_rotation_frequency)):\n    water_transport_rate = (number_of_buckets*bucket_volume*water_wheel_rotation_frequency[j]).to('liter/sec')\n\n    bucket_capacity_utilization = Q[i]/water_transport_rate\n\n    # limit transport rate to flow rate\n    if bucket_capacity_utilization &gt; 1:\n        bucket_capacity_utilization = 1\n        water_transport_rate = Q[i]\n\n    weight_of_water_per_bucket = (rho*bucket_volume)*bucket_capacity_utilization\n\n    torque_per_bucket = (weight_of_water_per_bucket*ureg.g0*moment_arm).to_base_units()\n    water_wheel_shaft_torque[j] = torque_per_bucket.sum()    \n\n    available_pwr = (water_wheel_shaft_torque[j]*water_wheel_rotation_frequency[j]*eta_waterwheel)\n    load_volts = (220/500)*water_wheel_rotation_frequency[j].magnitude*gear_box*ureg.volt\n    load_current = load_volts/R_load\n    load_pwr = R_load*load_current**2\n    \n    table_row.append(\n            [Q[i].magnitude,\n            water_wheel_rotation_frequency[j].magnitude,\n            water_wheel_shaft_torque[j].to('N m').magnitude,\n            available_pwr.to('watts').magnitude,\n            load_volts.magnitude,\n            load_current.magnitude,\n            load_pwr.magnitude])                                                                   \n\nprint(tabulate(table_row, headers=table_header,tablefmt=\"simple\",floatfmt=('.0f','.0f',',.1f',',.1f','.1f','.1f',',.1f')))\n\n\n  flow    rpm    torque    avaliable pwr    gen voltage    load amps    load pwr\n------  -----  --------  ---------------  -------------  -----------  ----------\n   120      5   2,164.9          1,020.2           88.0          2.7       240.0\n   120      6   2,164.9          1,224.2          105.6          3.3       345.6\n   120      7   2,164.9          1,428.3          123.2          3.8       470.4\n   120      8   2,164.9          1,632.3          140.8          4.4       614.4\n   120      9   2,164.9          1,836.4          158.4          4.9       777.6\n   120     10   2,029.6          1,912.9          176.0          5.5       960.0\n   120     11   1,845.1          1,912.9          193.6          6.0     1,161.6\n   120     12   1,691.4          1,912.9          211.2          6.5     1,382.4\n   120     13   1,561.2          1,912.9          228.8          7.1     1,622.4\n   120     14   1,449.7          1,912.9          246.4          7.6     1,881.6\n   120     15   1,353.1          1,912.9          264.0          8.2     2,160.0\n   120     16   1,268.5          1,912.9          281.6          8.7     2,457.6\n   120     17   1,193.9          1,912.9          299.2          9.3     2,774.4\n   120     18   1,127.6          1,912.9          316.8          9.8     3,110.4\n   120     19   1,068.2          1,912.9          334.4         10.4     3,465.6\n   120     20   1,014.8          1,912.9          352.0         10.9     3,840.0\n\n\n\n\nCode\n#water_wheel_rotation_frequency = (np.arange(5,21))*ureg.rpm\nwater_wheel_rotation_frequency = (np.linspace(5,20,100))*ureg.rpm\n#i = 6 #for i in range(len(Q)):\n\nwater_wheel_shaft_torque = np.zeros(len(water_wheel_rotation_frequency))*ureg.kilogram*ureg.g0*ureg.meter\navailable_pwr = np.zeros(len(water_wheel_rotation_frequency))*ureg.watt\nload_pwr = np.zeros(len(water_wheel_rotation_frequency))*ureg.watt\n\nfor j in range(len(water_wheel_rotation_frequency)):\n    water_transport_rate = (number_of_buckets*bucket_volume*water_wheel_rotation_frequency[j]).to('liter/sec')\n\n    bucket_capacity_utilization = Q[i]/water_transport_rate\n\n    # limit transport rate to flow rate\n    if bucket_capacity_utilization &gt; 1:\n        bucket_capacity_utilization = 1\n        water_transport_rate = Q[i]\n\n    weight_of_water_per_bucket = (rho*bucket_volume)*bucket_capacity_utilization\n\n    torque_per_bucket = (weight_of_water_per_bucket*ureg.g0*moment_arm).to_base_units()\n    water_wheel_shaft_torque[j] = torque_per_bucket.sum()    \n\n    available_pwr[j] = (water_wheel_shaft_torque[j]*water_wheel_rotation_frequency[j]*eta_waterwheel)\n    load_volts = (220/500)*water_wheel_rotation_frequency[j].magnitude*gear_box*ureg.volt\n    load_current = load_volts/R_load\n    load_pwr[j] = R_load*load_current**2\n    \nplt.plot(water_wheel_rotation_frequency.magnitude,available_pwr.to('watts').magnitude,'-',label='available pwr')\nplt.plot(water_wheel_rotation_frequency.magnitude,load_pwr.to('watts').magnitude,'-',label='load pwr')  \n\nidx = np.argwhere(np.diff(np.sign(load_pwr.to('watts').magnitude - available_pwr.to('watts').magnitude))).flatten()\nplt.plot(water_wheel_rotation_frequency.magnitude[idx],load_pwr.to('watts').magnitude[idx], 'ro')\nplt.plot(water_wheel_rotation_frequency.magnitude[idx],available_pwr.to('watts').magnitude[idx], 'kx')\n\nplt.grid()\n\nplt.ylabel('power, Watts')\nplt.xlabel('wheel rpm')\n\nplt.title('available power from water wheel, power to R_load = {:.1f~P} @ flow = {:.1f~P}'.format(R_load,Q[i]))\nplt.legend(loc=0)\n\nplt.show()\n\nprint('the curves intersect at {:.1f} rpm and {:.1f} W'.format(float(water_wheel_rotation_frequency.magnitude[idx]),float(load_pwr.to('watts').magnitude[idx])))\n\n\n\n\n\nthe curves intersect at 14.1 rpm and 1906.1 W\n\n\n\nAt 120 pls, the curves intersect at 14.1 rpm and 1906.1 W\nAt 180 lps, the curves intersect at 17.3 rpm and 2864.1 W\n\nAre the graphs below correct? what do they say?\n\n\nWheel tangential velocity vs RPM\nWhich to use rotation velocity or Angular velocity??\nRotational frequency, also known as rotational speed or rate of rotation (symbols ν, lowercase Greek nu, and also n), is the frequency of rotation of an object around an axis. Its SI unit is the reciprocal seconds (s−1); other common units of measurement include the hertz (Hz), cycles per second (cps), and revolutions per minute (rpm).\nIn physics, angular velocity (symbol ω or ω → {}, the lowercase Greek letter omega), also known as angular frequency vector,[1] is a pseudovector representation of how the angular position or orientation of an object changes with time, i.e. how quickly an object rotates (spins or revolves) around an axis of rotation and how fast the axis itself changes direction.\nturn = 2 * π * radian = _ = revolution = cycle = circle\n\n[frequency] = 1 / [time]\nhertz = 1 / second = Hz\nrevolutions_per_minute = revolution / minute = rpm\nrevolutions_per_second = revolution / second = rps\ncounts_per_second = count / second = cps\n\n\nThe speed of the object traveling the circle is:\n\\(v=\\frac {2\\pi r}{T}=\\omega r\\)\nv = tangential speed\nr = radius\nT = period for one rotation \\(\\omega\\) = angular rate of rotation, also known as angular velocity\nTangential speed\n\\(v=r\\omega\\)\nWhat is \\(\\omega\\)?\nIn physics, angular velocity (symbol ω or ω → {}, the lowercase Greek letter omega), also known as angular frequency vector,[1] is a pseudovector representation of how the angular position or orientation of an object changes with time, i.e. how quickly an object rotates (spins or revolves) around an axis of rotation and how fast the axis itself changes direction.\nAngular velocity has dimension of angle per unit time; this is analogous to linear velocity, with angle replacing distance, with time in common. The SI unit of angular velocity is radians per second,[2] although degrees per second (°/s) is also common. The radian is a dimensionless quantity, thus the SI units of angular velocity are dimensionally equivalent to reciprocal seconds, s−1, although rad/s is preferable to avoid confusion with rotation velocity in units of hertz (also equivalent to s−1).[3]\nTangential velocity\n\\(v=\\omega r\\)\nv = tangential speed\nr = radius\n\\(\\omega\\) = angular rate of rotation, also known as angular velocity\n\\(v=\\frac {2\\pi r}{T}\\)\nv = tangential speed\nr = radius\nT = period for one rotation\n\n\nCode\nwater_wheel_rotation_frequency = (np.arange(5,21))*ureg.rpm\n\nplt.plot(water_wheel_rotation_frequency.magnitude,(water_wheel_diameter/2*water_wheel_rotation_frequency.to('Hz')).to_base_units().magnitude,'-')\n\nplt.grid()\n\nplt.ylabel('Wheel tangential velocity, m/s')\nplt.xlabel('wheel RPM')\n\nplt.title('Wheel tangential velocity')\nplt.show()\n\n\n\n\n\nprint(‘{:&lt;15s}{:&lt;10s}’.format(‘wheel rpm’, ‘tangential velocity’)) for i in water_wheel_rotation_frequency: print(‘{:&lt;4.1f~P} {:&lt;,.2f~P}’.format(i.to(‘rpm’), (water_wheel_diameter/2*i.to(‘Hz’)).to_base_units()))\n\n\nCode\ntable_header = ['wheel rpm', 'tangential velocity, m/s']\ntable_row = []\n\nfor i in water_wheel_rotation_frequency:\n    table_row.append([i.to('rpm').magnitude,(water_wheel_diameter/2*i.to('Hz')).to_base_units().magnitude])\n\nprint(tabulate(table_row, headers=table_header,tablefmt=\"simple\",floatfmt=('.0f',',.2f')))\n\n\n  wheel rpm    tangential velocity, m/s\n-----------  --------------------------\n          5                        0.55\n          6                        0.66\n          7                        0.77\n          8                        0.88\n          9                        0.99\n         10                        1.10\n         11                        1.21\n         12                        1.32\n         13                        1.43\n         14                        1.54\n         15                        1.65\n         16                        1.76\n         17                        1.87\n         18                        1.98\n         19                        2.09\n         20                        2.20\n\n\nFlume water velocity should match the tangential velocity at the flow rate.\nChecking on units\nturn = 2 * π * radian = _ = revolution = cycle = circle\n\n[frequency] = 1 / [time]\nhertz = 1 / second = Hz\nrevolutions_per_minute = revolution / minute = rpm\nrevolutions_per_second = revolution / second = rps\ncounts_per_second = count / second = cps\n\n\nRotational frequency units of SI unit Hz, Other units rpm, cps\nTangential speed Tangential speed is the speed of an object undergoing circular motion, i.e., moving along a circular path.[1] A point on the outside edge of a merry-go-round or turntable travels a greater distance in one complete rotation than a point nearer the center. Travelling a greater distance in the same time means a greater speed, and so linear speed is greater on the outer edge of a rotating object than it is closer to the axis. This speed along a circular path is known as tangential speed because the direction of motion is tangent to the circumference of the circle. For circular motion, the terms linear speed and tangential speed are used interchangeably, and both use units of m/s, km/h, and others\n\\(v=\\frac {2\\pi r}{T}\\)\nr = radius\nT = the rotation period\nRotational frequency can be obtained dividing angular frequency, ω, by a full turn (2π radians): ν=ω/(2π rad)."
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html#generator",
    "href": "Lake District Overshot Waterwheel Project.html#generator",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "Generator",
    "text": "Generator\nThere were two generators used in the project\nThe PM generator had low voltage and high voltage versions\nTesting and building a prototype in his shop should have been done. A normal project management process that included design reviews would have required prototypes and lab or shop testing prior to the final design.\n\ngenerator #1\nKris complained that the label didn’t match his observations.\n\n\n3 phase permanent magnet generator\nFirst generator is a 2kW PM generator, exact specifications are not clear. Different voltages were used. Kris was unhappy about the perforance of the 1st generator.\nBased on my research, the generator is a Rewindagic generator obtained from from AliExpress, which was as far as I can tell is a 2kW, low speed permanent magnet generator, available in various voltages such as 48V, 220V, 400V. The cost in the US is $555.84.\n\nRewindagic generator specifications\n\n\nModel: PMG-2000L\n\n\n\n\n\nRated power(w)\n2000W\n\n\nMax power(w)\n2500W\n\n\nRated voltage(v)\n24v-220v\n\n\nRated rotated speed(rpm)\n500rpm\n\n\nStart torque(&lt;Nm)\n0.74 Nm\n\n\nRated torque (&lt;Nm)\n16.4 Nm\n\n\nWorking temperature\n-40 - 80 centigrade\n\n\n\nKris should have test this generator on a test stand, then mocked up the generator with the gear box and pulleys. I don’t know what he was expecting the water wheel rotational speed to be when he was doing the design, but 10 RPM at 120 lps seems about right to me. He needed a gear ratio of 10:500 or 1:50, but his inital design was a 1:20. Later he upped the gear ratio to 1:40.\nTesting on a gernerator test stand, rather than on site infront of the customer, is always prefered. He would have determine if the Rewindagic generator would meet his needs.\ncomplained about heat, thin wires,\ncorresponded with the supplier,\ndetermined that many generators advertized on line are probably rate incorrectly\ndertermined that the actual capabilites of the generator is likely 500 Watts sustained.\n\nThermal calculations\n\n\nCode\nH = 2.1*ureg.meter # head equal to the wheel diameter\nQ = 120*ureg.liter/(1*ureg.sec)\navailable_power_to_generator = (rho*g*Q*H*eta_waterwheel*eta_gear_box).to('watt')\nprint('available power to generator: {:,.1f~P}'.format(available_power_to_generator))\n\n\navailable power to generator: 2,112.4 W\n\n\nAssuming that the generator will convert avaliable power to electrical energy. Then losses internal to the generator will produce heat.\n\n\nCode\nprint('generator losses as heat: {:.1f~P}'.format((available_power_to_generator*(1-eta_generator)).to('watt')))\n\n\ngenerator losses as heat: 211.2 W\n\n\n\\(\\large{\\color {red} {\\text{do heat transfer calculation to check temperature rise, get units working}}}\\)\n\n\nConvected heat\nSee heat transfer text book page 816, example 15-10\nHorizontal cylinder\nSimplified relation for natural convection heat transfer coefficient for a horizontal cylinder, Table 15-1 [1].\n\\(h_{conv}=1.32 {\\frac {\\Delta T}{D}}^{0.25}\\)\nwhere:\n\\(h_{conv}\\) = heat transfer coefficient for a horizontal cylinder\n\\(\\Delta T\\) = difference in surface temperature and surrounding air \\(D\\) = diameter of cylinder\nHeat transfer\n\\(Q_{conv}=h_{conv}A \\Delta T\\)\nwhere:\n\\(Q_{conv}\\) = convective heat transfer \\(h_{conv}\\) = heat transfer coefficient\n\\(A\\) = area of motor case\n\\(\\Delta T\\) = surface temperature with surrounding enviroment\nReferene:\n1. Yunus A. Cengel, Heat Transfer: A Practical Approach, 2nd edition, McGraw-Hill, 2003\nModeling the surface area of the generator as a cylinder with hemishpere end pieces. The diameter has been increased by 25% to account for radiating fins on the case.\nThere is heat transfer from the case to the frame through the mounting bracket.\n\n\nCode\nmax_allowed_gen_case_temp = 80*ureg.degC\nmax_ambient_temp = 38*ureg.degC\ndelta_T = max_allowed_gen_case_temp - max_ambient_temp\n\n\n\n\nCode\nD = 0.190*ureg.meter # diameter of motor case\nD = D+D*0.25 # adjust the diameter by 25% to account for radiating fins on the case\nL = 0.138*ureg.meter #length of motor case\nA = np.pi*D*L + np.pi*D**2 # heat transfer surface of the generator (area of the cylinder plus end pieces)\n\n\n\n\nCode\nh_conv = 1.32*((delta_T.magnitude)/(D.magnitude))**(0.25)\nQ_conv = h_conv*A.magnitude*delta_T.magnitude\nprint('Q_conv: {:.2f}W'.format(Q_conv))\n\n\nQ_conv: 56.64W\n\n\n\n\nRadiated heat\nPut formula here with some narrative\n\\(Q_{rad}=\\epsilon A_s \\sigma (T_s^4 - T_{surr}^4)\\)\nwhere: \\(Q_{rad}\\) = radiated heat transfer between the surface at temperature \\(T_s\\) completely surrounded by a much larger surface at temperature \\(T_{surr}\\)\n\\(\\epsilon\\) = emissivity of the surface\n\\(A_s\\) = surface area\n\\(\\sigma\\) = Stefan Boltzmann constant\n\\(T_s\\) = surface temperature \\(T_{surr}\\) = surrounding temperature\n\n\nCode\nemissivity = 0.85\nalpha = 5.67e-8 # Stefan-Blotzmann constant\nQ_rad = emissivity*A.magnitude*alpha*(max_allowed_gen_case_temp.to('degK').magnitude**4-max_ambient_temp.to('degK').magnitude**4)\nprint('Q_rad: {:.2f}W'.format(Q_rad))\n\n\nQ_rad: 83.46W\n\n\n\n\nHeat conduction through base\n\\(Q_{cond}=\\frac {\\Delta T}{R}\\)\nwhere:\n\\(Q_{cond}\\) = cinducted heat transfer between mounting feet and frame \\(\\Delta T\\) = surface temperature with surrounding enviroment\n\\(R\\) = thermal resistance of generator base to frame\n\n\nCode\nR_thermal = 10 # a guess\nQ_cond = delta_T.magnitude/R_thermal\nprint('Q_cond: {:.2f}W'.format(Q_cond))\n\n\nQ_cond: 4.20W\n\n\n\n\nTotal heat transfered from the generator body\n\n\nCode\ntotal_heat_out = Q_conv+Q_rad+Q_cond\nprint('total heat transfered from the generator body to the enviroment: {:.1f} Watts'.format(total_heat_out))\n\ngenerator_losses_as_heat = (rho*g*Q*H*eta_waterwheel*eta_gear_box*(1-eta_generator)).to('watt')\nprint('generator losses as internal heat: {:.1f~P}'.format(generator_losses_as_heat))\n\nprint('net generator heat flow: {:.1f} Watts'.format(total_heat_out - generator_losses_as_heat.magnitude))\n\n\ntotal heat transfered from the generator body to the enviroment: 144.3 Watts\ngenerator losses as internal heat: 211.2 W\nnet generator heat flow: -66.9 Watts\n\n\nSince the net heat flow is negative, the generator will heat up.\n\n\nWhy did the 1st generator get hot?\n\\(\\large{\\color {red} {\\text{why did the 1st generator get hot?}}}\\)\n\ninverter could have been in constant voltage mode with shaft to load power missmatch, wheel was spining too fast\navailable power - displayed power = 1000 W as heat\ngenerator true rated power is 500 Watts, not 2000 Watts\n\nComments on PM generator\n\nneeded more testing to verify Kris’s complaints about the genertor\n\n\n\n\nInduction generator\nThe second generator tried was a 2.2kW, 8 pole, induction generator from Tec Motors. This motor need 750 rpm for make the rated power.\nwith the gear ratio of 1:40 and a water wheel rpm of 18, the generator shaft RPM is 720.\nWith the efficiencies and derating applied, I he will not get 1500 W as shown below.\nSelf excited induction generator (part 1)\nSelf excited induction generator (part 2)\ngenerated_rated_pwr = 2200ureg.watt(720/750) generator_derating = 0.9\nprint(‘generator internal losses as heat: {:.1f~P}’.format(generated_rated_pwr(1-eta_generator)generator_derating))\n\n\nCode\nprint('generator losses as heat: {:.1f~P}'.format((available_power_to_generator*(1-eta_generator)).to('watt')))\n\n\ngenerator losses as heat: 211.2 W\n\n\nsame as above\n\\(\\large{\\color {red} {\\text{do heat transfer calculation to check temperature rise}}}\\)\n\\(\\large{\\color {red} {\\text{do heat transfer calculation to check temperature rise, get units working}}}\\)\n\nConvected heat\n\n\nCode\nD = 0.252*ureg.meter # diameter of motor case\nD = D+D*0.25 # adjust the diameter by 25% to account for radiating fins on the case\nL = 0.229*ureg.meter #length of motor case\nA = np.pi*D*L + np.pi*D**2 # heat transfer surface of the generator (area of the cylinder plus end pieces)\n\n\n\n\nCode\nh_conv = 1.32*((delta_T.magnitude)/(D.magnitude))**(0.25)\nQ_conv = h_conv*A.magnitude*delta_T.magnitude\nprint('Q_conv: {:.2f}W'.format(Q_conv))\n\n\nQ_conv: 101.42W\n\n\n\n\nRadiated heat\n\n\nCode\nemissivity = 0.85\nalpha = 5.67e-8 # Stefan-Blotzmann constant\nQ_rad = emissivity*A.magnitude*alpha*(max_allowed_gen_case_temp.to('degK').magnitude**4-max_ambient_temp.to('degK').magnitude**4)\nprint('Q_rad: {:.2f}W'.format(Q_rad))\n\n\nQ_rad: 160.36W\n\n\n\n\nHeat conduction through base\n\n\nCode\nR_thermal = 10 # a guess\nQ_cond = delta_T.magnitude/R_thermal\nprint('Q_cond: {:.2f}W'.format(Q_cond))\n\n\nQ_cond: 4.20W\n\n\n\n\nTotal heat transfered from the generator body\n\n\nCode\ntotal_heat_out = Q_conv+Q_rad+Q_cond\nprint('total heat transfered from the generator body to the enviroment: {:.1f} Watts'.format(total_heat_out))\n\ngenerator_losses_as_heat = (rho*g*Q*H*eta_waterwheel*eta_gear_box*(1-eta_generator)).to('watt')\nprint('generator losses as internal heat: {:.1f~P}'.format(generator_losses_as_heat))\n\nprint('net generator heat flow: {:.1f} Watts'.format(total_heat_out - generator_losses_as_heat.magnitude))\n\n\ntotal heat transfered from the generator body to the enviroment: 266.0 Watts\ngenerator losses as internal heat: 211.2 W\nnet generator heat flow: 54.7 Watts\n\n\nThe net heat transfer is rate is positive so OK.\nIn addition to not makeing 1500 W, he needs a new gear box with a ratio of 10:750 or 1:75.\nComments about the induction genertor\n\nneeded higher power rating\n\nneed higher gear ratio\n\n\n\n\nGenerator selection critieria\n\nrpm\n\ngenerator rated power vs available power\n\ninduction generator capacitor selection\n\npros and cons of induction generator\n\n\\(\\large{\\color {red} {\\text{Replot the power curve intercestion for the induction generator, RPM is different}}}\\)\n\n\nCode\nstep_size = 20\nQ = np.arange(60, 180+step_size, step_size)*ureg.liter/(1*ureg.sec)\n\n#water_wheel_rotation_frequency = (np.arange(5,21))*ureg.rpm\nwater_wheel_rotation_frequency = (np.linspace(5,20,100))*ureg.rpm\ni = 0 #for i in range(len(Q)):\n\nwater_wheel_shaft_torque = np.zeros(len(water_wheel_rotation_frequency))*ureg.kilogram*ureg.g0*ureg.meter\navailable_pwr = np.zeros(len(water_wheel_rotation_frequency))*ureg.watt\nload_pwr = np.zeros(len(water_wheel_rotation_frequency))*ureg.watt\n\nfor j in range(len(water_wheel_rotation_frequency)):\n    water_transport_rate = (number_of_buckets*bucket_volume*water_wheel_rotation_frequency[j]).to('liter/sec')\n\n    bucket_capacity_utilization = Q[i]/water_transport_rate\n\n    # limit transport rate to flow rate\n    if bucket_capacity_utilization &gt; 1:\n        bucket_capacity_utilization = 1\n        water_transport_rate = Q[i]\n\n    weight_of_water_per_bucket = (rho*bucket_volume)*bucket_capacity_utilization\n\n    torque_per_bucket = (weight_of_water_per_bucket*ureg.g0*moment_arm).to_base_units()\n    water_wheel_shaft_torque[j] = torque_per_bucket.sum()    \n\n    available_pwr[j] = (water_wheel_shaft_torque[j]*water_wheel_rotation_frequency[j]*eta_waterwheel)\n    load_volts = (220/750)*water_wheel_rotation_frequency[j].magnitude*gear_box*ureg.volt\n    load_current = load_volts/R_load\n    load_pwr[j] = R_load*load_current**2\n    \nplt.plot(water_wheel_rotation_frequency.magnitude,available_pwr.to('watts').magnitude,'-',label='available pwr')\nplt.plot(water_wheel_rotation_frequency.magnitude,load_pwr.to('watts').magnitude,'-',label='load pwr')  \n\nidx = np.argwhere(np.diff(np.sign(load_pwr.to('watts').magnitude - available_pwr.to('watts').magnitude))).flatten()\nplt.plot(water_wheel_rotation_frequency.magnitude[idx],load_pwr.to('watts').magnitude[idx], 'ro')\nplt.plot(water_wheel_rotation_frequency.magnitude[idx],available_pwr.to('watts').magnitude[idx], 'kx')\n\nplt.grid()\n\nplt.ylabel('power, Watts')\nplt.xlabel('wheel rpm')\n\nplt.title('available power from water wheel, power to R_load = {:.1f~P} @ flow = {:.1f~P}'.format(R_load,Q[i]))\nplt.legend(loc=0)\n\nplt.show()\n\nprint('the curves intersect at {:.1f} rpm and {:.1f} W'.format(float(water_wheel_rotation_frequency.magnitude[idx]),float(load_pwr.to('watts').magnitude[idx])))\n\n\n\n\n\nthe curves intersect at 14.8 rpm and 940.7 W\n\n\nThe flow rate need to be lowered for the cureves to intersect and to keep the wheel RPM reasonable.\nThe gear box ratio need to be\n\n\nCode\n750/10\n\n\n75.0"
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html#the-flume",
    "href": "Lake District Overshot Waterwheel Project.html#the-flume",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "The flume",
    "text": "The flume\n\nThe flume carries water from the dam to the top of the waterwheel. By examining the videos, I made some estimates for the flume dimensions.\nThe width of the flume was about the same width of the waterwheel, which makes sense, since you want a nice flow of water into the buckets.\nThe side walls of the flume looked like 2 by 6 inch boards. The depth of the water in the channel looks like a maximum of 5 inches of flow. Also, the buckets need to handle what ever the flow rate is. Need to look at the RPM.\nI can use the Manning equation for open channel flow to calculate the flow rate in the flume. Mannings equation is an empirical equation that applies to uniform flow in open channels and is a function of the channel velocity, flow area and channel slope.\n\\(V = \\frac {s^{1/2} \\times R^{2/3}} {n}\\)\nwhere:\nV - water velocity\nn - Manning’s roughness coefficient; unplaned wood = 0.013, wood sides and smooth sheet metal bottom\nR - The channel’s hydraulic radius, calculated by dividing the water flow’s cross-sectional area A by its wetted perimeter P\ns - Slope of the channel’s bottom surface\nExamining Manning’s equation, we can see that the area and slope are directly proportional to the water flow rate, which means an increase in area and slope would increase the water flow rate. On the other hand, the roughness coefficient and the wetted perimeter are inversely proportional to the water flow rate, meaning that increasing their values would decrease the water flow rate.\nThe slope of the flume was not apparent in any of the videos, so I’m using a value of 0.01.\n\\(\\large{\\color {red} {\\text{get some screen shots of the flume}}}\\)\n\nFlume slope\nThis does not agree with the text, which states: &gt; Because this has to be quite long (approximately 8 meters) there needs to be a downward slope in order to keep the water flowing. I have made this slope 1 degree and it equates to a 300mm level change.\n\n\nCode\nflume_length = 8*ureg.meter\nflume_drop = 300*ureg.mm\nflume_drop.ito('meter')\n\nnp.arctan(flume_drop/flume_length).to('degree')\n\n\n2.147585428298503 degree\n\n\nThe math works out to about 2 degrees.\nSo which is it, 1 degree or 300mm drop?\n\n\nCode\nflume_slope = flume_drop.magnitude/flume_length.magnitude\nflume_slope\n\n\n0.0375\n\n\n\n\nCode\nflume_width = 800*ureg.mm\nwater_depth = 70*ureg.mm # estimate\n# convert to base units\nflume_width.ito('meter')\nwater_depth.ito('meter')\nwater_cross_section_area = water_depth*flume_width\nprint('water_cross_section_area = {:.4f~P}'.format(water_cross_section_area))\nwetted_perimeter = 2*water_depth+flume_width\nprint('wetted_perimeter = {:.4f~P}'.format(wetted_perimeter))\nhydraulic_radius = water_cross_section_area/wetted_perimeter\nprint('hydraulic_radius = {:.4f~P}'.format(hydraulic_radius))\nroughness_coefficient = 0.013 # Wood - unplaned\nslope = flume_slope #0.0025\nprint('slope = {:.4f}'.format(slope))\n# need to use magnitude of quantity when using fractional exponents\nwater_flume_velocity = (((hydraulic_radius.magnitude)**(2/3)*slope**(1/2))/roughness_coefficient)*ureg.meter/ureg.sec\nprint('water_flume_velocity = {:.4f~P}'.format(water_flume_velocity))\nvolumetric_flow_rate = water_flume_velocity*water_cross_section_area\nprint('volumetric_flow_rate = {:.6f~P}'.format(volumetric_flow_rate))\nprint('volumetric_flow_rate = {:.2f~P}'.format(volumetric_flow_rate.to('liter/sec')))\n\n\nwater_cross_section_area = 0.0560 m²\nwetted_perimeter = 0.9400 m\nhydraulic_radius = 0.0596 m\nslope = 0.0375\nwater_flume_velocity = 2.2722 m/s\nvolumetric_flow_rate = 0.127243 m³/s\nvolumetric_flow_rate = 127.24 l/s\n\n\nThink about working the problem in reverse: the water wheel is rotating at 10 rpm, which is about 115 lps of flow rate consumed by the water wheel.\nThe flume water velocity should match the wheel tangential velocity at flow rate of 115 lps.\n\\(\\large{\\color {red} {\\text{Find a spot in the videos where the depth in the flume can be estimated and where he shows the power}}}\\)\ndoes he also say bucket fill level?\n\n\nFlume re-design\nspillway baffles\nIn this study, a two-dimensional physical model was used to evaluate the effectiveness of adding baffles and sills at step edges or of shifting them from step edges of a 1V:1H sloping, short, sharp- or round-crested stepped spillway with an ogee inlet. A comparison between different configurations of baffles and sills was carried out with respect to the energy dissipation and the flow characteristics. The baffle-shifted round-crested spillway gives the smallest discharge required for the onset of a skimming flow among all other configurations. The baffle-edged chute dissipates more energy than the sill-edged spillway. Shifting baffles or sills from the sharp edges decreases the energy dissipation. Shifting baffles or sills from the round-crested spillway increases the energy dissipation in the range of the discharges studied. Empirical equations for energy dissipation are introduced for practical application of spillways of comparable similar conditions.\nFour general aspects of spillways are worth noting. First, the uncontrolled discharge of surplus water past the dam should be automatic and not dependent upon human control. Second, the spillway intake should be wide enough so that the largest floods can pass without increasing the water level in the reservoir enough to cause a nuisance to upstream property owners. Third, the rate of floodwater discharge should not increase much above that experienced before the construction of the dam. An increase in discharge can cause flood problems downstream, but a dam usually reduces the peak discharge rate because of the lag effect caused by a flood passing through the reservoir. Fourth, floodwater discharged over the height of a dam can be destructive to the dam structure itself and to the riverbed unless its energy is controlled and dissipated in harmless turbulence.\nHYDRAULIC INVESTIGATIONS 1T ~~ 4) AND LABORATORY SERVICES OFFICIAL FILE COPY A BAFFLED APRON AS A SPILLWAY ENERGY DISSIPATOR\n\n\nCode\nflume_width = 800*ureg.mm\nwater_depth = 200*ureg.mm\n# convert to base units\nflume_width.ito('meter')\nwater_depth.ito('meter')\nwater_cross_section_area = water_depth*flume_width\nprint('water_cross_section_area = {:.4f~P}'.format(water_cross_section_area))\nwetted_perimeter = 2*water_depth+flume_width\nprint('wetted_perimeter = {:.4f~P}'.format(wetted_perimeter))\nhydraulic_radius = water_cross_section_area/wetted_perimeter\nprint('hydraulic_radius = {:.4f~P}'.format(hydraulic_radius))\nroughness_coefficient = 0.013 # Wood - unplaned\nslope = 0.0025\nprint('slope = {:.4f}'.format(slope))\n# need to use magnitude of quantity when using fractional exponents\nwater_flume_velocity = (((hydraulic_radius.magnitude)**(2/3)*slope**(1/2))/roughness_coefficient)*ureg.meter/ureg.sec\nprint('water_flume_velocity = {:.4f~P}'.format(water_flume_velocity))\nvolumetric_flow_rate = water_flume_velocity*water_cross_section_area\nprint('volumetric_flow_rate = {:.6f~P}'.format(volumetric_flow_rate))\nprint('volumetric_flow_rate = {:.2f~P}'.format(volumetric_flow_rate.to('liter/sec')))\n\n\nwater_cross_section_area = 0.1600 m²\nwetted_perimeter = 1.2000 m\nhydraulic_radius = 0.1333 m\nslope = 0.0025\nwater_flume_velocity = 1.0038 m/s\nvolumetric_flow_rate = 0.160610 m³/s\nvolumetric_flow_rate = 160.61 l/s"
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html#safety",
    "href": "Lake District Overshot Waterwheel Project.html#safety",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "safety",
    "text": "safety\n\nflood and spill way\n\nriver level monitoring and alearts\nleat and it’s wears\n\ngounding\nfailures\n\nsafety instrumentation"
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html#maintenance",
    "href": "Lake District Overshot Waterwheel Project.html#maintenance",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "maintenance",
    "text": "maintenance\n\nwet environment"
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html#economic-analysis",
    "href": "Lake District Overshot Waterwheel Project.html#economic-analysis",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "Economic analysis",
    "text": "Economic analysis\nThis project was done in the UK, but now I’m switching to my local cost of electricity and currency in USD. Assume that the house or property uses 400 Watts average.\nI pay 32 cents per kWh if I stay below 11.4 kWh per day\nincludes generating cost, delivery costs and goverment fees and taxes. I could pay more if I wanted my power classified as ‘green power’.\nThe electrical rate is $0.35 per kWh. The daily and yearily electrical costs are as follows:\n\n\nCode\nhouse_pwr_usage = 400*ureg.watt\nmeter_rate = 0.32/(1000*ureg.Wh)\nprint('house daily energy usage: {:,.0f~P}'.format((24*ureg.hr*house_pwr_usage).to('Wh')))\nprint('house daily energy usage cost: ${:.2f}'.format((24*ureg.hr*house_pwr_usage*meter_rate).to_base_units().magnitude))\nhouse_yearly_energy_usage_cost = 365*24*ureg.hr*house_pwr_usage*meter_rate\nprint('house yearly energy usage cost: ${:,.2f}'.format(house_yearly_energy_usage_cost.to_base_units().magnitude))\n\n\nhouse daily energy usage: 9,600 Wh\nhouse daily energy usage cost: $3.07\nhouse yearly energy usage cost: $1,121.28\n\n\nSince the water wheel is generating more power than needed, the cost saved per year is equal to the cost of electrical energy consumed.\n\n\nCode\nsaved_cost_yr = house_yearly_energy_usage_cost\nprint('cost saved per year: ${:,.2f}'.format(saved_cost_yr.to_base_units().magnitude))\n\n\ncost saved per year: $1,121.28\n\n\nThe net energy used per day is the generated energy per day minus the daily energy usage.\n\n\nCode\ngenerated_pwr = 1500*ureg.watts\nnet_pwr_usage = generated_pwr - house_pwr_usage\nnet_energy_day = 24*ureg.hour*net_pwr_usage\nprint('net energy per day: {:,.0f~P}'.format(net_energy_day))\n\n\nnet energy per day: 26,400 W·h\n\n\nAs an example, Southern California Edison (SCE) has a Net Surplus Compensation Rate (NSCR) that compensates Net Energy Metering (NEM) customers for electricity they produce in excess of their on-site load over the course of a 12-month period, referred to as the “Relevant Period.” NEM customers who produce excess power over their Relevant Period are known as Net Surplus Generators. NSCR is calculated using a market-based mechanism derived from an hourly day-ahead electricity market price known as the Default Load Aggregation Point (DLAP) price. Southern California Edison’s DLAP price reflects the costs SCE avoids in procuring power during the time period Net Surplus Generators are likely to produce excess power with their solar or wind generating facilities.\n\nNSCR Energy Prices\n\n\nFor Relevant Period Ending\nNSCR Energy ($/kWh)\n\n\n\n\nJanuary 2024\n0.04696\n\n\nDecember 2023\n0.05881\n\n\nNovember 2023\n0.06030\n\n\nOctober 2023\n0.06268\n\n\nSeptember 2023\n0.06818\n\n\nAugust 2023\n0.07009\n\n\nJuly 2023\n0.07224\n\n\nJune 2023\n0.07512\n\n\nMay 2023\n0.07712\n\n\nApril 2023\n0.07778\n\n\nMarch 2023\n0.07494\n\n\nFebruary 2023\n0.07276\n\n\nJanuary 2023\n0.06212\n\n\n\nThe average NSCR for the trailing 12 months is calculated below\n\n\nCode\navg_net_meter_rate = np.array([0.04696,0.05881,0.06030,0.06268,0.06818,0.07009,0.07224,0.07512,0.07712,0.07778,0.07494,0.07276,0.06212]).mean()\nprint('the average net meter rate for the trailing 12 months: {:.2} per kWh'.format(avg_net_meter_rate))\n\n\nthe average net meter rate for the trailing 12 months: 0.068 per kWh\n\n\n\n\nCode\nnet_meter_rate = avg_net_meter_rate/((1000*ureg.Wh)) # amount per kWh\n#net_meter_rate = 0.15/((1000*ureg.Wh)) # amount per kWh\nprint('net meter rate: ${:.2f} per kWh'.format(net_meter_rate.magnitude*1000))\nprint('Net meter credit: ${:,.2f} per day'.format((net_energy_day*net_meter_rate).to_base_units().magnitude))\nnet_meter_credit_yr = (365*net_energy_day*net_meter_rate).to_base_units()\nprint('Net meter credit: ${:,.2f} per year'.format(net_meter_credit_yr.magnitude))\n\n\nnet meter rate: $0.07 per kWh\nNet meter credit: $1.79 per day\nNet meter credit: $651.62 per year\n\n\nThe cost of construction for the dam, spillway and water wheel is estimated to be $20,000. But this is just a guess since Kris didn’t share any financial innformation.\nThe maintence cost per year is estimated to be $200.\nThe project life is estimated to be 15 years. This is the time frame that the internal rate of return will be calculated for. The internal rate of return is a method of quantifying the merits of a project or investment opportunity. The calculation is termed internal because it depends only on the cash flows of the investment being analyzed and excludes external factors, such as returns available elsewhere, the risk-free rate, inflation, the cost of capital, or financial risk.\n\n\nCode\nconstruction_cost = 20000 # who knows?\nmaintenance_cost = 200 # per year\n#net_meter_credit_per_year = (365*generator_energy*net_meter_rate).to_base_units().magnitude\nproject_life = 15 # years\n\ncash_flow_each_year = np.ones(project_life)*(saved_cost_yr+net_meter_credit_yr-maintenance_cost).magnitude\n\n# include construction cost in 1st year\ncash_flow_each_year[0] = cash_flow_each_year[0]-construction_cost\n#cash_flow_each_year\n\n\n\n\nCode\ndef irr(values, guess=None, tol=1e-12, maxiter=100):\n    \"\"\"\n    Return the Internal Rate of Return (IRR).\n    This is the \"average\" periodically compounded rate of return\n    that gives a net present value of 0.0; for a more complete explanation,\n    see Notes below.\n    :class:`decimal.Decimal` type is not supported.\n    Parameters\n    ----------\n    values : array_like, shape(N,)\n        Input cash flows per time period.  By convention, net \"deposits\"\n        are negative and net \"withdrawals\" are positive.  Thus, for\n        example, at least the first element of `values`, which represents\n        the initial investment, will typically be negative.\n    guess : float, optional\n        Initial guess of the IRR for the iterative solver. If no guess is\n        given an heuristic is used to estimate the guess through the ratio of\n        positive to negative cash lows\n    tol : float, optional\n        Required tolerance to accept solution. Default is 1e-12.\n    maxiter : int, optional\n        Maximum iterations to perform in finding a solution. Default is 100.\n    Returns\n    -------\n    out : float\n        Internal Rate of Return for periodic input values.\n    \"\"\"\n    values = np.atleast_1d(values)\n    if values.ndim != 1:\n        raise ValueError(\"Cashflows must be a rank-1 array\")\n\n    # If all values are of the same sign no solution exists\n    # we don't perform any further calculations and exit early\n    same_sign = np.all(values &gt; 0) if values[0] &gt; 0 else np.all(values &lt; 0)\n    if same_sign:\n        return np.nan\n\n    # If no value is passed for `guess`, then make a heuristic estimate\n    if guess is None:\n        positive_cashflow = values &gt; 0\n        inflow = values.sum(where=positive_cashflow)\n        outflow = -values.sum(where=~positive_cashflow)\n        guess = inflow / outflow - 1\n\n    npv_ = np.polynomial.Polynomial(values[::-1])\n    d_npv = npv_.deriv()\n    g = 1 + guess\n\n    for _ in range(maxiter):\n        delta = npv_(g) / d_npv(g)\n        if abs(delta) &lt; tol:\n            return g - 1\n        g -= delta\n\n    return np.nan\n\n\nThe internal rate of return is calculated and shown below.\n\n\nCode\nprint('internal rate of return: {:.1f}%'.format(irr(cash_flow_each_year)*100))\n\n\ninternal rate of return: 2.5%\n\n\nThis rate of return is very small and indicates that the funds spent on the construction might be better used elsewhere. The low IRR is primarily driven by two factors, the low net meter rate and the average Watts used. The low IRR also indicates that this hydro system probably should not be grid tied and only makes sense for true off grid situations.\nIf the house uses uses all the hydro power, then the IRR is favorable."
  },
  {
    "objectID": "Lake District Overshot Waterwheel Project.html#summary",
    "href": "Lake District Overshot Waterwheel Project.html#summary",
    "title": "Lake District Overshot Waterwheel Project  Review and Analysis",
    "section": "Summary",
    "text": "Summary\n\ngenerator not thourally tested in the shop, just an OC test\nflume design too steep\ngear ratio too low for final generator"
  }
]